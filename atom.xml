<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>SilenceZheng66-BLOG</title>
  
  <subtitle>log</subtitle>
  <link href="http://silencezheng.top/atom.xml" rel="self"/>
  
  <link href="http://silencezheng.top/"/>
  <updated>2022-11-29T06:37:37.311Z</updated>
  <id>http://silencezheng.top/</id>
  
  <author>
    <name>SilenceZheng66</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>常见字符编码介绍</title>
    <link href="http://silencezheng.top/2022/11/29/article83/"/>
    <id>http://silencezheng.top/2022/11/29/article83/</id>
    <published>2022-11-29T06:36:09.000Z</published>
    <updated>2022-11-29T06:37:37.311Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>平时写代码、写博客总少不了与字符编码打交道，应该整理一下来做系统的了解。当然，我认为这种“目录”性质的博客不应写的过于深入，只要通过简短的文字介绍基本信息即可。<br><span id="more"></span></p><h2 id="字符编码（Character-encoding）"><a href="#字符编码（Character-encoding）" class="headerlink" title="字符编码（Character encoding）"></a>字符编码（Character encoding）</h2><p>字符编码是把<strong>字符集</strong>中的字符<strong>编码</strong>为指定集合中某一对象，以便文本在计算机中存储和通过通信网络的传递。</p><p>对于软件开发中的场景来说，字符编码就是构建一个字符到数字的一一对应的映射关系。然而，在实际的应用中我们还需要解决<strong>字符间分隔问题</strong>。</p><blockquote><p>在显示器上看见的文字、图片等信息在电脑里面其实并不是我们看见的样子，即使你知道所有信息都存储在硬盘里，把它拆开也看不见里面有任何东西，只有些盘片。假设，你用显微镜把盘片放大，会看见盘片表面凹凸不平，凸起的地方被磁化，凹的地方是没有被磁化；凸起的地方代表数字1，凹的地方代表数字0。硬盘只能用0和1来表示所有文字、图片等信息。</p></blockquote><p>如上面引文中提到的，计算机只能对二进制数字进行读写，当它遇到<code>00100001 00010001</code>，它该如何知道这是一个双字节编码字符，又或是两个单字节编码的字符呢？通常解决方案要么就是规定好每个字长度（例如所有文字都是2 bytes，不够的前面用0补齐），要么就是在用0和1表示的时候，不仅需要表示出数字编码，还要暗示给计算机接下来多少个连续byte构成一个字。</p><p>不同的字符编码（有时也称为字符集）的区别主要在于两点：<strong>可以表示的字符范围</strong> 和 <strong>编码方式</strong>。几种常见的中文编码之间兼容性如下图所示，兼容是指映射间的包含关系：</p><p><img src="/assets/post_img/article83/compatibility.webp" alt=""></p><h2 id="ASCII"><a href="#ASCII" class="headerlink" title="ASCII"></a>ASCII</h2><p><strong>ASCII 字符集</strong>和 <strong>ASCII 码</strong>（ ASCII 是 American Standard Code for Information Interchange 的缩写）可能是我们最先接触到的英文字符集及其编码，它同时也被国际标准化组织（ International Organization for Standardization, <strong>ISO</strong> ）批准为国际标准。</p><p><img src="/assets/post_img/article83/ascii-1-1.png" alt="ascii"></p><p>基本的 ASCII 字符集共有 128 个字符，其中有 96 个可打印字符（可打印和可显示仍有一个字符的区别），包括常用的字母、数字、标点符号等，另外还有 32 个控制字符。标准 ASCII 码使用 7 个二进位对字符进行编码，对应的 ISO 标准为 ISO646 标准。</p><p>由于标准 ASCII 字符集字符数目有限，在实际应用中往往无法满足要求。为此，国际标准化组织又制定了 ISO2022 标准，它规定了在保持与 ISO646 兼容的前提下将 ASCII 字符集扩充为 8 位代码的统一方法。 ISO 陆续制定了一批适用于不同地区的扩充 ASCII 字符集，每种扩充 ASCII 字符集分别可以扩充 128 个字符，这些扩充字符的编码均为高位为 1 的 8 位代码（即十进制数 128~255 ），称为扩展 ASCII 码。</p><p>字母和数字的 ASCII 码的记忆是非常简单的。我们只要记住了一个字母或数字的 ASCII 码（例如记住 A 为 65 ， 0 的 ASCII 码为 48 ），知道相应的大小写字母之间差 32 ，就可以推算出其余字母、数字的 ASCII 码。</p><p>ASCII编码几乎被世界上所有编码所兼容（UTF-16和UTF-32是个例外），因此如果一个文本文档里面的内容全都由ASCII里面的字母或符号构成，那么不管你如何展示该文档的内容，都不可能出现乱码的情况。</p><h2 id="ANSI"><a href="#ANSI" class="headerlink" title="ANSI"></a>ANSI</h2><p>准确说，并不存在哪种具体的编码方式叫做ANSI，它只是一个Windows操作系统上的别称而已。在中文简体Windows操作系统上，ANSI就是GBK；在泰语操作系统上，ANSI就是TIS-620（一种泰语编码）；在韩语操作系统上，ANSI就是EUC-KR（一种韩语编码）。</p><p>为了扩充ASCII编码，以用于显示本国的语言，不同的国家和地区制定了不同的标准，由此产生了 GB2312, BIG5, JIS 等各自的编码标准。这些使用 <strong>2 个字节</strong>来代表一个字符的各种汉字延伸编码方式，称为 <strong>ANSI 编码</strong>，又称为”MBCS（Muilti-Bytes Character Set，多字节字符集）”。 <strong>不同 ANSI 编码之间互不兼容</strong>，当信息在国际间交流时，无法将属于两种语言的文字，存储在同一段 ANSI 编码的文本中。一个很大的缺点是，同一个编码值，在不同的编码体系里代表着不同的字。这样就容易造成混乱。于是催生了Unicode。</p><p>其中每个语言下的ANSI编码，都有一套一对一的编码转换器，Unicode变成所有编码转换的中间介质。所有的编码都有一个转换器可以转换到Unicode，而Unicode也可以转换到其他所有的编码。</p><h2 id="GBK、GB2312"><a href="#GBK、GB2312" class="headerlink" title="GBK、GB2312"></a>GBK、GB2312</h2><p>GB即国标，为了满足国内在计算机中使用汉字的需要，中国国家标准总局发布了一系列的汉字字符集国家标准编码，统称为GB码，或国标码。其中最有影响的是于1980年发布的《信息交换用汉字编码字符集 基本集》，标准号为GB 2312-1980,因其使用非常普遍，也常被通称为国标码。GB2312编码通行于我国内地；新加坡等地也采用此编码。几乎所有的中文系统和国际化的软件都支持GB2312。</p><p>GBK和GB2312都是双字节编码。</p><p>GB2312是一个简体中文字符集，由6763个常用汉字和682个全角的非汉字字符组成。其中汉字根据使用的频率分为两级。一级汉字3755个，二级汉字3008个。</p><p>GB2312的出现，基本满足了汉字的计算机处理需要，但对于人名、古汉语等方面出现的罕用字，GB2312不能处理，这导致了后来GBK的出现。经过GBK编码后，可以表示的汉字达到了20902个，另有984个汉语标点符号、部首等。</p><p>当GBK仍然无法满足使用的需要时，就产生了<strong>GB18030</strong>，这时2bytes已经不能满足使用的需要（2bytes最多只有65536种组合，然而为了和ASCII兼容，最高位不能为0就已经直接淘汰了一半的组合，只剩下3万多种组合无法满足全部汉字要求），因此GB18030多出来的汉字使用4bytes编码。</p><h2 id="Unicode"><a href="#Unicode" class="headerlink" title="Unicode"></a>Unicode</h2><p>Unicode是为了解决传统的字符编码方案的局限而产生的，它为每种语言中的每个字符设定了统一并且唯一的二进制编码，以满足跨语言、跨平台进行文本转换、处理的要求。Universal Multiple-Octet Coded Character Set，简称为<strong>UCS</strong>。UCS-2，即2字节编码字符集，UCS-4则是4字节编码字符集。</p><p>但是，Unicode仅仅是一本很厚的字典，规定了符合对应的二进制代码，至于这个二进制代码如何存储则没有任何规定。也就是说，其中一个字符可能只对应7位二进制数，而另一个字符则对应27位二进制数。如果按照统一用4字节存储字符编码的话，无疑会造成极大的资源浪费（对磁盘、对网络都是）。为了解决这一问题，产生了许多Unicode的实现方式，如utf-8、utf-16等等。</p><h2 id="UTF-8（8-bit-Unicode-Transformation-Format）"><a href="#UTF-8（8-bit-Unicode-Transformation-Format）" class="headerlink" title="UTF-8（8-bit Unicode Transformation Format）"></a>UTF-8（8-bit Unicode Transformation Format）</h2><p>UTF-8解决字符间分隔的方式是数<strong>二进制中最高位连续1的个数</strong>来决定这个字是几字节编码。0开头的属于单字节，和ASCII码重合，做到了兼容。</p><p><img src="/assets/post_img/article83/utf-8.png" alt="uft-8"></p><p>从这种表示方式也可以很显然地看出来，UTF-8 和 GBK 没有任何关系，除了都兼容ASCII以外。这也是文件乱码的主要原因之一。</p><p>UTF-8中，中文占 3 个字节，其他数字、英文、符号占一个字节。但 emoji 符号占 4 个字节，一些较复杂的文字、繁体字也是 4 个字节。</p><p>我们可能会注意到MySQL中有两套UTF-8的实现，分别是<code>utf8</code>和<code>utf8mb4</code>，它们的区别如下：</p><ul><li><code>utf8</code>：只支持1至3个字节。</li><li><code>utf8mb4</code>：完整实现，最多支持4个字节表示字符。</li></ul><p>因此，如果需要存储emoji类型的数据或者一些比较复杂的文字、繁体字到MySQL的话，数据库的编码一定要指定为<code>utf8mb4</code>。</p><h2 id="关于更多Unicode编码方式的内容"><a href="#关于更多Unicode编码方式的内容" class="headerlink" title="关于更多Unicode编码方式的内容"></a>关于更多Unicode编码方式的内容</h2><p>其实我认为常用的字符编码除了utf-8，还有utf-16。汉字的Unicode范围在<code>[0x4E00, 0x9FA5]</code>，这里是码点的范围，也就是<code>U+4E00 到 U+9FA5</code>。这个范围是CJK Unified Ideographs。</p><p>更多内容可以参考[7]、[8]。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1]<a href="https://baike.baidu.com/item/%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81/8446880">https://baike.baidu.com/item/%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81/8446880</a><br>[2]<a href="https://www.cnblogs.com/zhanghengscnc/p/7664120.html">https://www.cnblogs.com/zhanghengscnc/p/7664120.html</a><br>[3]<a href="https://zhuanlan.zhihu.com/p/46216008">https://zhuanlan.zhihu.com/p/46216008</a><br>[4]<a href="https://baike.baidu.com/item/%E7%BB%9F%E4%B8%80%E7%A0%81/2985798">https://baike.baidu.com/item/%E7%BB%9F%E4%B8%80%E7%A0%81/2985798</a><br>[5]<a href="https://www.cnblogs.com/crazylqy/p/10184291.html">https://www.cnblogs.com/crazylqy/p/10184291.html</a><br>[6]<a href="https://javaguide.cn/database/character-set.html">https://javaguide.cn/database/character-set.html</a><br>[7]<a href="https://cloud.tencent.com/developer/article/1341908">https://cloud.tencent.com/developer/article/1341908</a><br>[8]<a href="https://www.cnblogs.com/benbenalin/p/6921553.html">https://www.cnblogs.com/benbenalin/p/6921553.html</a></p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于 <a href="https://silencezheng.top">silencezheng.top</a>，转载请注明出处。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;平时写代码、写博客总少不了与字符编码打交道，应该整理一下来做系统的了解。当然，我认为这种“目录”性质的博客不应写的过于深入，只要通过简短的文字介绍基本信息即可。&lt;br&gt;</summary>
    
    
    
    
    <category term="字符编码" scheme="http://silencezheng.top/tags/%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81/"/>
    
  </entry>
  
  <entry>
    <title>Linux常用命令--深度学习向</title>
    <link href="http://silencezheng.top/2022/11/28/article82/"/>
    <id>http://silencezheng.top/2022/11/28/article82/</id>
    <published>2022-11-28T11:44:22.000Z</published>
    <updated>2022-11-28T11:45:41.308Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>总结一下Linux服务器使用的常见操作，深度学习向。<br><span id="more"></span></p><h2 id="SSH密码登录"><a href="#SSH密码登录" class="headerlink" title="SSH密码登录"></a>SSH密码登录</h2><p><code>ssh account@ip</code><br><code>ssh -p port account@ip</code></p><h2 id="查看进程"><a href="#查看进程" class="headerlink" title="查看进程"></a>查看进程</h2><p><code>ps aux</code>，又称BSD方式，其中a表示显示所有用户的进程(show processes for all users)；u表示显示用户(display the process’s user/owner)；x表示显示无控制终端的进程(also show processes not attached to a terminal)。</p><p><code>ps -ef</code>，又称System V方式，e效果与a相同，f表示用ASCII字符显示树状结构，表达程序间的相互关系(ASCII art forest)。</p><p>查看用户abc运行的进程：<code>ps -u abc</code></p><p>显示System V格式下java进程：<code>ps -ef|grep java</code></p><p>BSD在grep java下获取title：<code>ps aux|head -1;ps aux|grep java</code></p><p>System V在grep java下获取title：<code>ps -ef|head -1;ps -ef|grep java</code></p><p>top工具：<code>top</code></p><h2 id="查看目前登入系统的用户信息"><a href="#查看目前登入系统的用户信息" class="headerlink" title="查看目前登入系统的用户信息"></a>查看目前登入系统的用户信息</h2><p><code>w</code></p><h2 id="查看网卡信息"><a href="#查看网卡信息" class="headerlink" title="查看网卡信息"></a>查看网卡信息</h2><p><code>ifconfig</code><br><code>ip addr show</code></p><h2 id="查看CPU"><a href="#查看CPU" class="headerlink" title="查看CPU"></a>查看CPU</h2><p>查看主频信息：<code>ls cpu</code></p><p>查看详细信息：<code>cat /proc/cpuinfo</code></p><h2 id="查看内存信息"><a href="#查看内存信息" class="headerlink" title="查看内存信息"></a>查看内存信息</h2><p>以MB为单位：<code>free -m</code></p><p>以GB为单位：<code>free -g</code></p><h2 id="查看系统版本"><a href="#查看系统版本" class="headerlink" title="查看系统版本"></a>查看系统版本</h2><p>查看系统版本：<code>cat /etc/issue</code></p><h2 id="查看硬盘空间大小"><a href="#查看硬盘空间大小" class="headerlink" title="查看硬盘空间大小"></a>查看硬盘空间大小</h2><p>查看文件系统磁盘使用情况统计：<code>df -h</code></p><p>查看指定目录所属磁盘情况：<code>df -h 目录名</code></p><h2 id="Nvidia显卡配置"><a href="#Nvidia显卡配置" class="headerlink" title="Nvidia显卡配置"></a>Nvidia显卡配置</h2><p>查看显卡驱动版本、CUDA版本和显卡信息：<code>nvidia-smi</code></p><p>每隔半秒刷新一次GPU信息：<code>watch -n 0.5 nvidia-smi</code></p><p>查看CUDA Runtime版本：<code>nvcc -V</code></p><p>TensorFlow查看显卡是否可用：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(tf.config.list_physical_devices(<span class="string">&#x27;GPU&#x27;</span>))</span><br></pre></td></tr></table></figure></p><p>Pytorch查看显卡是否可用：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.cuda.is_available())</span><br><span class="line"></span><br><span class="line">ngpu = <span class="number">1</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> (torch.cuda.is_available() <span class="keyword">and</span> ngpu &gt; <span class="number">0</span>) <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(device)</span><br><span class="line"><span class="built_in">print</span>(torch.cuda.get_device_name(<span class="number">0</span>))</span><br></pre></td></tr></table></figure></p><p>显卡驱动下载：<a href="https://www.nvidia.com/download/index.aspx?lang=en-us">https://www.nvidia.com/download/index.aspx?lang=en-us</a></p><h2 id="Conda"><a href="#Conda" class="headerlink" title="Conda"></a>Conda</h2><p>查看环境信息：<code>conda info -envs</code></p><p>创建环境：<code>conda create --name ENVNAME python=3.x pkg1 pkg2 ...</code></p><p>删除环境：<code>conda env remove -n ENVNAME</code></p><p>查看通道：<code>conda config --show</code></p><p>添加通道（中科大镜像）：<code>conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/free/</code></p><p>删除通道：<code>conda config --remove channels ...</code></p><h2 id="GNU-Screen"><a href="#GNU-Screen" class="headerlink" title="GNU Screen"></a>GNU Screen</h2><p>安装screen：<code>apt/yum install screen</code></p><p>版本查看：<code>screen -v</code></p><p>查看已创建的screen终端：<code>screenv -ls</code>，同名终端要用PID区分，输出格式为<code>PID.Name</code>；screen有两种状态<code>Attached</code>（活跃）和<code>Detached</code>（挂起）。</p><p>创建一个叫Hello的虚拟终端（可创建同名）：<code>screen -S Hello</code></p><p>创建（回到）一个叫Hello的虚拟终端（不可创建同名，如果存在则直接进入该终端）：<code>screen -R Hello</code></p><p>清除虚拟终端法一：<code>进入对应终端，exit</code></p><p>清除虚拟终端法二：<code>screen -R [PID/Name] -X quit</code></p><p>后台运行虚拟终端：<code>在终端中按下ctrl+a 再按下d</code></p><p>更多绑定键信息：<code>在终端中按下ctrl+a 再输入?</code></p><p>进入活跃的虚拟终端前需要先挂起：<code>screen -d [PID/Name]</code>，否则<code>-R</code>回到活跃终端反而会创建新终端。</p><p>总之，创建和回到终端前先<code>screenv -ls</code>确认一下是最好的。</p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于 <a href="https://silencezheng.top">silencezheng.top</a>，转载请注明出处。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;总结一下Linux服务器使用的常见操作，深度学习向。&lt;br&gt;</summary>
    
    
    
    
    <category term="Linux" scheme="http://silencezheng.top/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Java位运算</title>
    <link href="http://silencezheng.top/2022/11/24/article81/"/>
    <id>http://silencezheng.top/2022/11/24/article81/</id>
    <published>2022-11-24T02:41:37.000Z</published>
    <updated>2022-11-24T02:42:45.103Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>从原补反到Java位运算，学习记录。<br><span id="more"></span></p><h2 id="机器数"><a href="#机器数" class="headerlink" title="机器数"></a>机器数</h2><p>一个数在计算机的存储形式是二进制数，我们称这些二进制数为机器数，机器数是有符号，在计算机中用机器数的最高位存放符号位，0表示正数，1表示负数。</p><p>因为带有符号位，所以机器数的形式值不等于其真值，以机器数<code>1000 0111</code>为例，其真正表示的值为-7，而形式值为135。将带符号的机器数的真正表示的值称为机器数的真值。</p><p><strong>无符号数</strong>是指整个机器字长的全部二进制位均表示数值位，相当于数的绝对值。还是以<code>1000 0111</code>为例，无符号数就是指其形式值135。Java中不存在无符号整数类型。</p><h2 id="原码、反码、补码"><a href="#原码、反码、补码" class="headerlink" title="原码、反码、补码"></a>原码、反码、补码</h2><p>简单起见，假设使用8位二进制数（一个字节）存储整数，实际上在Java中为四个字节，即32位。</p><p><strong>原码</strong>的表示与机器数真值表示的一样，即用第一位表示符号，其余位表示数值。</p><p><strong>反码</strong>的表示中，正数的反码是其原码本身，负数的反码是在其原码的基础上，符号位不变，其余各位取反。</p><p><strong>补码</strong>的表示中，正数的补码是其原码本身，负数的补码是在其原码的基础上，符号位不变，其余各位取反后加1。</p><p>以十进制正数1和-1为例，其各码如下：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">1:</span></span><br><span class="line"><span class="string">原码=反码=补码=0000</span> <span class="number">0001</span></span><br><span class="line"></span><br><span class="line"><span class="number">-1</span><span class="string">:</span></span><br><span class="line"><span class="string">原码=1000</span> <span class="number">0001</span></span><br><span class="line"><span class="string">反码=1111</span> <span class="number">1110</span></span><br><span class="line"><span class="string">补码=1111</span> <span class="number">1111</span></span><br></pre></td></tr></table></figure></p><p>计算机实际只存储补码，因为<strong>只有补码的计算是准确的</strong>，这一点在此不作证明了。同时原码转换为补码的过程，也可以理解为数据存储到计算机内存中的过程，如下图：</p><p><img src="/assets/post_img/article81/yfb.png" alt="yfb"></p><p>八位原码和八位反码能够表示的区间只有<code>[-127, 127]</code>，而八位补码可以表示的范围为<code>[-128, 127]</code>，因为十进制-127和-1的相加运算用补码表示算得的结果为<code>1000 0000</code>，即用原本的“-0”来表示-128。所以计算机中一个字节的取值范围是<code>[-128,127]</code>。</p><p>需要注意的是，在计算机运算过后，对于负数结果需要转换为原码才能得到其真值。例如补码<code>0b1000 0001</code>，先转换为反码<code>0b1000 0000</code>，再按位取反（符号位不变）得到原码<code>0b1111 1111</code>，可知其真值为-127。</p><h2 id="Java中的位运算符"><a href="#Java中的位运算符" class="headerlink" title="Java中的位运算符"></a>Java中的位运算符</h2><p>Java 定义的位运算（bitwise operators）直接对整数类型的位进行操作，这些整数类型包括 long（64位），int（32位），short（16位），char（16位） 和 byte（8位）。</p><p>位运算符主要用来对操作数二进制的位进行运算。按位运算表示<strong>按每个二进制位（bit）进行计算</strong>，其操作数和运算结果都是整型值。</p><p>Java 语言中的位运算符分为<strong>位逻辑运算符</strong>和<strong>位移运算符</strong>两类，下面详细介绍每类包含的运算符。</p><h3 id="位逻辑运算符"><a href="#位逻辑运算符" class="headerlink" title="位逻辑运算符"></a>位逻辑运算符</h3><p>位逻辑运算符包含 4 个：<code>&amp;（AND）</code>、<code>|（OR）</code>、<code>~（NOT）</code>和 <code>^（XOR）</code>。除了 <code>~</code> 为单目运算符外，其余都为双目运算符。</p><p>位与运算符为<code>&amp;</code>，其运算规则是：参与运算的数字，低位对齐，高位不足的补零，如果对应的二进制位同时为 1，那么计算结果才为 1，否则为 0。因此，任何数与 0 进行按位与运算，其结果都为 0。</p><p>位或运算符为<code>|</code>，其运算规则是：参与运算的数字，低位对齐，高位不足的补零。如果对应的二进制位只要有一个为 1，那么结果就为 1；如果对应的二进制位都为 0，结果才为 0。</p><p>位异或运算符为<code>^</code>，其运算规则是：参与运算的数字，低位对齐，高位不足的补零，如果对应的二进制位相同（同时为 0 或同时为 1）时，结果为 0；如果对应的二进制位不相同，结果则为 1。即<strong>相同为0，不同为1</strong>。</p><p>位取反运算符为<code>~</code>，其运算规则是：只对一个操作数进行运算，将操作数二进制中的 1 改为 0，0 改为 1。</p><h3 id="位移运算符"><a href="#位移运算符" class="headerlink" title="位移运算符"></a>位移运算符</h3><p>位移运算符用来将操作数向某个方向（向左或者右）移动指定的二进制位数，它们都属于双目运算符。</p><p><strong>左移位运算符</strong>为<code>&lt;&lt;</code>，其运算规则是：按二进制形式把所有的数字向左移动对应的位数，高位移出（舍弃），低位的空位补零。</p><p><strong>有符号右位移运算符</strong>为<code>&gt;&gt;</code>，其运算规则是：按二进制形式把所有的数字向右移动对应的位数，低位移出（舍弃），若操作数为正数则高位补0，操作数为负数则高位补1。也称为<strong>算数右移</strong>。</p><p><strong>无符号右位移运算符</strong>为<code>&gt;&gt;&gt;</code>，其运算规则是：按二进制形式把所有的数字向右移动对应的位数，低位移出（舍弃），高位补0。也称为<strong>逻辑右移</strong>。</p><p>特别需要注意的是，在对char、byte、short类型的数进行移位操作前，<strong>编译器都会自动地将数值转化为int类型，然后才进行移位操作（这也导致位移表达式返回值为整型）</strong>。由于int型变量只占4字节，当右移位数超过32bit时，移位运算没有任何意义。所以，在Java语言中，为了保证移动位数地有效性，以使移动的位数不超过32bit，采取了取余的操作，即<code>a&gt;&gt;n</code>等价于<code>a&gt;&gt;(n%32)</code>。这一性质对于左右移位都有效。</p><p>另外，左移$n$位表示原来的值乘$2^n$，经常用来代替乘法操作，由于CPU直接支持位运算，因此位运算比乘法运算效率高。</p><h3 id="复合位赋值运算符"><a href="#复合位赋值运算符" class="headerlink" title="复合位赋值运算符"></a>复合位赋值运算符</h3><p>所有的二进制位运算符都有一种<strong>将赋值与位运算组合在一起</strong>的简写形式。复合位赋值运算符由赋值运算符与位逻辑运算符或位移运算符组合而成。</p><p>包括<code>&amp;=</code>、<code>｜=</code>、<code>^=</code>、<code>&lt;&lt;=</code>、<code>&gt;&gt;&gt;=</code>和<code>&gt;&gt;=</code>。</p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>关于位运算呢，虽然看起来容易掌握，但是实际使用中可能还是会遇到一些问题，有时是由于真值与补码混淆造成的，我写了一个小demo来解释一下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: SilenceZheng66</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">bitwiseOperation</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">byte</span> a = -<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">byte</span> b = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        System.out.println(~a); <span class="comment">// 0</span></span><br><span class="line">        System.out.println(~b); <span class="comment">// -2</span></span><br><span class="line">        System.out.println(a&amp;b); <span class="comment">// 1</span></span><br><span class="line">        System.out.println(a|b); <span class="comment">// -1</span></span><br><span class="line">        System.out.println(b^a); <span class="comment">// -2</span></span><br><span class="line"></span><br><span class="line">        b^=a;</span><br><span class="line">        System.out.println(b); <span class="comment">// -2</span></span><br><span class="line">        System.out.println(b&gt;&gt;<span class="number">1</span>); <span class="comment">// -1</span></span><br><span class="line">        System.out.println(b&lt;&lt;<span class="number">1</span>); <span class="comment">// -4</span></span><br><span class="line"></span><br><span class="line">        System.out.println(Integer.toBinaryString(a&gt;&gt;&gt;<span class="number">2</span>));<span class="comment">// 00111111111111111111111111111111</span></span><br><span class="line">        System.out.println(Integer.toBinaryString(a&gt;&gt;<span class="number">2</span>)); <span class="comment">// 11111111111111111111111111111111</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>首先明确“真值=原码“，所以可以得到$a$的原码为<code>1000 0001</code>，$b$的原码为<code>0000 0001</code>。但在内存中，$a$被存储为<code>1111 1111</code>，这是由<code>1000 0001 -&gt; 1111 1110 -&gt; 1111 1111</code>得到的，也就是原码转换为补码的过程。$b$由于是正数，仍然被存储为原码形式。</p><p>那么<code>~a</code>为何是$0$呢？可以列出计算过程：<code>1111 1111(a) -&gt; 0000 0000(~a)</code>。由于<code>0000 0000</code>的原码与补码相同，故其真值为$0$。下面再计算一个<code>b^a</code>吧，过程如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">先计算异或：0000</span> <span class="number">0001</span> <span class="string">^</span> <span class="number">1111 </span><span class="number">1111</span> <span class="string">=</span> <span class="number">1111 </span><span class="number">1110</span></span><br><span class="line"><span class="string">补码转化为真值：1111</span> <span class="number">1110</span> <span class="string">-&gt;</span> <span class="number">1111 </span><span class="number">1101</span> <span class="string">-&gt;</span> <span class="number">1000 </span><span class="number">0010</span></span><br><span class="line"><span class="string">得到结果为-2</span></span><br></pre></td></tr></table></figure><h2 id="例题一：Leetcode190-颠倒二进制位"><a href="#例题一：Leetcode190-颠倒二进制位" class="headerlink" title="例题一：Leetcode190. 颠倒二进制位"></a>例题一：Leetcode190. 颠倒二进制位</h2><p>问题描述：颠倒给定的 32 位无符号整数的二进制位。</p><p>思路一（我的愚蠢解法）：获取读入十进制数的二进制字符串，存入字符数组，反转后再转为int输出。</p><p>实现：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: SilenceZheng66</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="comment">// you need treat n as an unsigned value</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">reverseBits</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">        String input = Integer.toBinaryString(n);</span><br><span class="line">        <span class="keyword">char</span>[] input_arr = input.toCharArray();</span><br><span class="line">        <span class="keyword">char</span>[] ans = <span class="keyword">new</span> <span class="keyword">char</span>[<span class="number">32</span>];</span><br><span class="line">        <span class="keyword">int</span> j = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = input_arr.length-<span class="number">1</span>; i&gt;=<span class="number">0</span>; i--)&#123;</span><br><span class="line">            ans[j] = input_arr[i];</span><br><span class="line">            j++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// toBinaryString中高位的0不输出，需要补充。</span></span><br><span class="line">        <span class="keyword">while</span>(j&lt;<span class="number">32</span>)&#123;</span><br><span class="line">            ans[j] = <span class="string">&#x27;0&#x27;</span>;</span><br><span class="line">            j++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> Integer.parseUnsignedInt(String.valueOf(ans), <span class="number">2</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>思路二（官解一）：将 $n$ 视作一个长为 32 的二进制串，从低位往高位枚举 $n$ 的每一位，将其倒序添加到翻转结果中。代码实现中，每枚举一位就将 $n$ 右移一位，这样当前 $n$ 的最低位就是我们要枚举的比特位。当 $n$ 为 0 时即可结束循环。</p><p>实现：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">reverseBits</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> rev = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">32</span> &amp;&amp; n != <span class="number">0</span>; ++i) &#123;</span><br><span class="line">            <span class="comment">// n&amp;1 只保留低位</span></span><br><span class="line">            <span class="comment">// &lt;&lt; (31 - i) 移到反转后的位置</span></span><br><span class="line">            <span class="comment">// rev = rev ｜ (n &amp; 1) &lt;&lt; (31 - i) 存入翻转结果 rev</span></span><br><span class="line">            rev |= (n &amp; <span class="number">1</span>) &lt;&lt; (<span class="number">31</span> - i);</span><br><span class="line">            <span class="comment">// n = n &gt;&gt;&gt; 1 逻辑右移，高位补0</span></span><br><span class="line">            n &gt;&gt;&gt;= <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> rev;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>思路三（JDK用法）：Integer包装类提供了<code>reverse</code>方法，速度很快。</p><p>实现：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">reverseBits</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> Integer.reverse(n);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1]<a href="http://c.biancheng.net/view/784.html">http://c.biancheng.net/view/784.html</a><br>[2]<a href="https://blog.csdn.net/xwu_09/article/details/78285785">https://blog.csdn.net/xwu_09/article/details/78285785</a><br>[3]<a href="https://www.cnblogs.com/linjiaxin/p/14870850.html">https://www.cnblogs.com/linjiaxin/p/14870850.html</a><br>[4]<a href="https://zhuanlan.zhihu.com/p/371184302">https://zhuanlan.zhihu.com/p/371184302</a><br>[5]<a href="https://blog.csdn.net/MaybeForever/article/details/89109596">https://blog.csdn.net/MaybeForever/article/details/89109596</a></p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于 <a href="https://silencezheng.top">silencezheng.top</a>，转载请注明出处。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;从原补反到Java位运算，学习记录。&lt;br&gt;</summary>
    
    
    
    
    <category term="Java" scheme="http://silencezheng.top/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>parseInt() 和 parseUnsignedInt()</title>
    <link href="http://silencezheng.top/2022/11/23/article80/"/>
    <id>http://silencezheng.top/2022/11/23/article80/</id>
    <published>2022-11-23T15:38:54.000Z</published>
    <updated>2022-11-25T03:16:30.922Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>22.11.25：事后发现是我蠢了，Java的意思是表示一个数直接用正负号表示符号，用原码表示数值，哪有人用补码转int的…</p><p>在做题的过程中，发现了一个神奇的“bug”，Integer.valueOf()对于32位二进制数字符串转化成整型爆出了<code>java.lang.NumberFormatException.forInputString</code>错误，往下研究了一下，发现问题出在了parseInt()上。<br><span id="more"></span></p><h2 id="Integer-valueOf"><a href="#Integer-valueOf" class="headerlink" title="Integer.valueOf()"></a>Integer.valueOf()</h2><p>该函数返回的是整型包装类，其内部调用了<code>Integer.parseInt(String s, int radix)</code>。于是问题聚焦在这个函数上。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Integer <span class="title">valueOf</span><span class="params">(String s, <span class="keyword">int</span> radix)</span> <span class="keyword">throws</span> NumberFormatException </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> Integer.valueOf(parseInt(s,radix));</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h2 id="parseInt-和parseUnsignedInt"><a href="#parseInt-和parseUnsignedInt" class="headerlink" title="parseInt()和parseUnsignedInt()"></a>parseInt()和parseUnsignedInt()</h2><p>还原一下问题场景：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: SilenceZheng66</span></span><br><span class="line">String a = <span class="string">&quot;11111111111111111111111111111101&quot;</span>;</span><br><span class="line"><span class="comment">// 可行，返回-3</span></span><br><span class="line">System.out.println(Integer.parseUnsignedInt(b, <span class="number">2</span>)); </span><br><span class="line"><span class="comment">// 报错</span></span><br><span class="line">System.out.println(Integer.parseInt(b, <span class="number">2</span>));</span><br></pre></td></tr></table></figure><p>这就有点迷惑了，明明输入的二进制字符串是32位的（在Integer范围内），为什么会说我格式错误呢？另外，这个<code>parseUnsignedInt()</code>又是干什么用的呢？带着这样的问题，我们来阅读二者的源码。首先看<code>parseUnsignedInt()</code>：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Parses the string argument as an unsigned integer in the radix</span></span><br><span class="line"><span class="comment">     * specified by the second argument.  An unsigned integer maps the</span></span><br><span class="line"><span class="comment">     * values usually associated with negative numbers to positive</span></span><br><span class="line"><span class="comment">     * numbers larger than MAX_VALUE.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">parseUnsignedInt</span><span class="params">(String s, <span class="keyword">int</span> radix)</span></span></span><br><span class="line"><span class="function">                <span class="keyword">throws</span> NumberFormatException </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (s == <span class="keyword">null</span>)  &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> NumberFormatException(<span class="string">&quot;null&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> len = s.length();</span><br><span class="line">        <span class="keyword">if</span> (len &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">char</span> firstChar = s.charAt(<span class="number">0</span>);</span><br><span class="line">            <span class="keyword">if</span> (firstChar == <span class="string">&#x27;-&#x27;</span>) &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span></span><br><span class="line">                    NumberFormatException(String.format(<span class="string">&quot;Illegal leading minus sign &quot;</span> +</span><br><span class="line">                                                       <span class="string">&quot;on unsigned string %s.&quot;</span>, s));</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">if</span> (len &lt;= <span class="number">5</span> || <span class="comment">// Integer.MAX_VALUE in Character.MAX_RADIX is 6 digits</span></span><br><span class="line">                    (radix == <span class="number">10</span> &amp;&amp; len &lt;= <span class="number">9</span>) ) &#123; <span class="comment">// Integer.MAX_VALUE in base 10 is 10 digits</span></span><br><span class="line">                    <span class="keyword">return</span> parseInt(s, radix);</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="keyword">long</span> ell = Long.parseLong(s, radix);</span><br><span class="line">                    <span class="keyword">if</span> ((ell &amp; <span class="number">0xffff_ffff_0000_0000L</span>) == <span class="number">0</span>) &#123;</span><br><span class="line">                        <span class="keyword">return</span> (<span class="keyword">int</span>) ell;</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        <span class="keyword">throw</span> <span class="keyword">new</span></span><br><span class="line">                            NumberFormatException(String.format(<span class="string">&quot;String value %s exceeds &quot;</span> +</span><br><span class="line">                                                                <span class="string">&quot;range of unsigned int.&quot;</span>, s));</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">throw</span> NumberFormatException.forInputString(s);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>Java1.8中，<code>Character.MIN_RADIX = 2, Character.MAX_RADIX = 36</code>，也就是说最大支持到36进制。<code>parseUnsignedInt</code>方法首先要求<code>s</code>不能带有负号，对于上述问题场景，该方法首先将输入转化为长整型，然后再转回整型输出。由于Java中不存在无符号整型，在整型中，32位二进制数字的第一位必须为符号位，所以返回结果为带符号数-3。再阐述一下，我输入的无符号数 <code>11111111111111111111111111111101 = 4294967293</code> 超出了Java整型的大小限制（2147483647），但没有超过长度限制（32位），于是应该是可以正常表示的。</p><p>总之<code>parseUnsignedInt</code>方法是用于获取无符号数的，它尚且可以根据输入来获得整型，为什么<code>parseInt()</code>不行呢？我们再来看一下源码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">parseInt</span><span class="params">(String s, <span class="keyword">int</span> radix)</span></span></span><br><span class="line"><span class="function">                <span class="keyword">throws</span> NumberFormatException</span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * WARNING: This method may be invoked early during VM initialization</span></span><br><span class="line"><span class="comment">         * before IntegerCache is initialized. Care must be taken to not use</span></span><br><span class="line"><span class="comment">         * the valueOf method.</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (s == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> NumberFormatException(<span class="string">&quot;null&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (radix &lt; Character.MIN_RADIX) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> NumberFormatException(<span class="string">&quot;radix &quot;</span> + radix +</span><br><span class="line">                                            <span class="string">&quot; less than Character.MIN_RADIX&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (radix &gt; Character.MAX_RADIX) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> NumberFormatException(<span class="string">&quot;radix &quot;</span> + radix +</span><br><span class="line">                                            <span class="string">&quot; greater than Character.MAX_RADIX&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> result = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">boolean</span> negative = <span class="keyword">false</span>;</span><br><span class="line">        <span class="keyword">int</span> i = <span class="number">0</span>, len = s.length();</span><br><span class="line">        <span class="keyword">int</span> limit = -Integer.MAX_VALUE;</span><br><span class="line">        <span class="keyword">int</span> multmin;</span><br><span class="line">        <span class="keyword">int</span> digit;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (len &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">char</span> firstChar = s.charAt(<span class="number">0</span>);</span><br><span class="line">            <span class="keyword">if</span> (firstChar &lt; <span class="string">&#x27;0&#x27;</span>) &#123; <span class="comment">// Possible leading &quot;+&quot; or &quot;-&quot;</span></span><br><span class="line">                <span class="keyword">if</span> (firstChar == <span class="string">&#x27;-&#x27;</span>) &#123;</span><br><span class="line">                    negative = <span class="keyword">true</span>;</span><br><span class="line">                    limit = Integer.MIN_VALUE;</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (firstChar != <span class="string">&#x27;+&#x27;</span>)</span><br><span class="line">                    <span class="keyword">throw</span> NumberFormatException.forInputString(s);</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (len == <span class="number">1</span>) <span class="comment">// Cannot have lone &quot;+&quot; or &quot;-&quot;</span></span><br><span class="line">                    <span class="keyword">throw</span> NumberFormatException.forInputString(s);</span><br><span class="line">                i++;</span><br><span class="line">            &#125;</span><br><span class="line">            multmin = limit / radix;</span><br><span class="line">            <span class="keyword">while</span> (i &lt; len) &#123;</span><br><span class="line">                <span class="comment">// Accumulating negatively avoids surprises near MAX_VALUE</span></span><br><span class="line">                digit = Character.digit(s.charAt(i++),radix);</span><br><span class="line">                <span class="keyword">if</span> (digit &lt; <span class="number">0</span>) &#123;</span><br><span class="line">                    <span class="keyword">throw</span> NumberFormatException.forInputString(s);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">if</span> (result &lt; multmin) &#123;</span><br><span class="line">                    <span class="keyword">throw</span> NumberFormatException.forInputString(s);</span><br><span class="line">                &#125;</span><br><span class="line">                result *= radix;</span><br><span class="line">                <span class="keyword">if</span> (result &lt; limit + digit) &#123;</span><br><span class="line">                    <span class="keyword">throw</span> NumberFormatException.forInputString(s);</span><br><span class="line">                &#125;</span><br><span class="line">                result -= digit;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">throw</span> NumberFormatException.forInputString(s);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> negative ? result : -result;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>我们可以发现，<code>parseInt</code>方法首先判断第一个字符是不是正负号，是负号则说明值是负的，否则，值就是正的。这个逻辑在非二进制环境下没有问题，因为非二进制表示的int变量，都会前置负号来表示负数。然而，在二进制数中，并没有所谓的“正负号”概念，数值的正负由符号位表示。所以<code>parseInt()</code>在转换<code>&quot;11111111111111111111111111111101&quot;</code>时将符号位也当做实际的值计算进去了，导致了数值溢出报错。</p><p>一种解决办法是将首位数字改为正负号，可以运行成功，但丧失了原本的意义，为什么这么说呢？请看下例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: SilenceZheng66</span></span><br><span class="line">String a = <span class="string">&quot;11111111111111111111111111111101&quot;</span>;</span><br><span class="line">String b = <span class="string">&quot;-1111111111111111111111111111101&quot;</span>;</span><br><span class="line">System.out.println(Integer.parseUnsignedInt(a, <span class="number">2</span>)); <span class="comment">// -3</span></span><br><span class="line">System.out.println(Integer.valueOf(b, <span class="number">2</span>)); <span class="comment">// -2147483645</span></span><br></pre></td></tr></table></figure><p>b的输出竟然是-2147483645，并不是想要表达的真实含义（-3），这是因为<code>parseInt()</code>首先将<code>&quot;-1111111111111111111111111111101&quot;</code> 拆分为 <code>&quot;-&quot;和&quot;01111111111111111111111111111101&quot;</code>，其中后者表示的真值为2147483645，然后再把负号放进来组合成了输出，这显然与补码<code>&quot;11111111111111111111111111111101&quot;</code>的真值不一致。</p><p>综上，在处理32位补码时，需要采用<code>parseUnsignedInt()</code>。其实感觉这个<code>parseInt()</code>可以优化一下，对32位二进制数用首位数字判断符号就好了。</p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于 <a href="https://silencezheng.top">silencezheng.top</a>，转载请注明出处。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;22.11.25：事后发现是我蠢了，Java的意思是表示一个数直接用正负号表示符号，用原码表示数值，哪有人用补码转int的…&lt;/p&gt;
&lt;p&gt;在做题的过程中，发现了一个神奇的“bug”，Integer.valueOf()对于32位二进制数字符串转化成整型爆出了&lt;code&gt;java.lang.NumberFormatException.forInputString&lt;/code&gt;错误，往下研究了一下，发现问题出在了parseInt()上。&lt;br&gt;</summary>
    
    
    
    
    <category term="Java" scheme="http://silencezheng.top/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>Java中的==和equals</title>
    <link href="http://silencezheng.top/2022/11/20/article79/"/>
    <id>http://silencezheng.top/2022/11/20/article79/</id>
    <published>2022-11-20T06:10:47.000Z</published>
    <updated>2022-11-20T06:12:25.015Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>从Java数据类型出发，聊聊Java中的==和equals。<br><span id="more"></span></p><h2 id="Java数据类型"><a href="#Java数据类型" class="headerlink" title="Java数据类型"></a>Java数据类型</h2><p>Java 语言支持的数据类型分为两种：<strong>基本数据类型</strong>（Primitive Type）和<strong>引用数据类型</strong>（Reference Type）。</p><p><img src="/assets/post_img/article79/datatype.jpeg" alt="datatype"></p><p>引用数据类型建立在基本数据类型的基础上，包括数组、类和接口。引用数据类型是由用户自定义，用来限制其他数据的类型。<strong>空类型null也是一种引用类型，不能转换成基本类型，因此不要把一个 null 值赋给基本数据类型的变量。</strong></p><p>对于基本数据类型，Java 为每种基本数据类型分别设计了对应的类，称之为<strong>包装类</strong>（Wrapper Classes），除了Integer和Character以外，包装类名都和基本数据类型相同，只是首字母大写。关于数据类型的更多细节，就不在这里赘述了。</p><p><img src="/assets/post_img/article79/pd.jpeg" alt="pd"></p><h2 id="和-equals"><a href="#和-equals" class="headerlink" title="== 和 equals"></a>== 和 equals</h2><p>首先明确一点，<strong>equals方法不能作用于基本数据类型变量</strong>。关于这句话其实可以展开说明一下，首先基本类型是不能作为方法的主体的，这是一定的，但基本类型能不能作为equals中的参数呢？有时也是可以的，因为Java的“自动装箱”机制，基本类型在传入时可以被封装为包装类，事实上参与函数的是包装类。</p><p>在比较基本数据类型时，<code>==</code>比较的是值。</p><p>在比较引用类型时，<code>==</code>比较的是对象的内存地址。如果equals方法没有经过重写，则与<code>==</code>相同，比较地址；如果equals方法经过重写，对于Java提供的类来说，则是比较对象存储的内容是否相同，也就是比较“值”。</p><p>Java提供的绝大多数类（不是全部）都重写了equals方法，以上讲述的区别与联系主要是关于Java内置类和基本数据类型。</p><h2 id="equals-和-hashCode"><a href="#equals-和-hashCode" class="headerlink" title="equals() 和 hashCode()"></a>equals() 和 hashCode()</h2><p>对于我们自己写代码，新建一个类而言，要么就不重写equals方法，此时等同于<code>==</code>；要么就同时重写equals和hashCode方法，按照我们定义的规则来比较对象是否相等。</p><blockquote><p>Java中对equals()的规范</p><ol><li>对称性：如果x.equals(y)返回是”true”，那么y.equals(x)也应该返回是”true”。</li><li>自反性：x.equals(x)必须返回是”true”。</li><li>传递性：如果x.equals(y)返回是”true”，而且y.equals(z)返回是”true”，那么x.equals(z)也应该返回是”true”。</li><li>一致性：如果x.equals(y)返回是”true”，只要x和y内容一直不变，不管重复x.equals(y)多少次，返回都是”true”。</li><li>非空性，x.equals(null)，永远返回是”false”；假设z是和x不同类型的对象，则x.equals(z)永远返回是”false”。</li></ol></blockquote><p>hashCode()的作用是用来获取哈希码，用于确定对象在哈希表中的位置，与equals()一样，所有的类都有hashCode方法。hashCode()只有在创建某个类的哈希表时才有用，需要根据方法返回值确认对象在哈希表中的位置。如果一个对象一定不会在散列表中使用，那么是没有必要复写hashCode方法的。但一般情况下我们还是会复写hashCode方法，因为谁能保证这个对象不会出现在HashMap、HashSet、HashTable…中呢？</p><blockquote><p>Object.hashCode()的通用约定</p><ol><li>在应用程序中，只要对象的equals方法的比较操作所用的信息没有修改，那么对于同一个对象的多次调用hashCode()，必须始终返回同一个哈希值。</li><li>如果两个对象通过equals()比较相等，那么它们的哈希值相同。</li><li>如果两个对象通过equals()比较不等，他们的哈希值可能相同也可能不同，取决于hashCode的实现，由此哈希表的性能也会有区别。</li></ol></blockquote><p>考虑一个常见的场景，HashSet是一个不允许有重复元素的集合，该集合会维护一个已存入对象的哈希值表。当插入一个新的对象时，我们首先会想到使用equals()逐个比较来确定是否有重复元素，但这必然造成效率问题。另外一个合理的方式就是对该对象调用hashCode()得到哈希值，然后在哈希值表中进行比对，如果不存在则直接存入；如果存在，则再调用equals()进行比较，相同的话就不再存入，不同的话散列到其他地址。这样一来实际调用equals()的次数就大大降低了。</p><p>因此，如果不重写对象的hashCode()方法，就有可能造成相同对象产生不同哈希值的情况，这就破坏了HashSet的性质。当然，这只是不重写hashCode(),或者说不遵守hashCode()规范的其中一个坏处。</p><p>对于equals() 和 hashCode()的部分，可以用两个问题来加深印象：<br>1、两个对象，如果a.equals(b)==true，那么a和b是否相等？<br>答：相等，但对象地址不一定相等。</p><p>2、两个对象，如果哈希值一样，那么两个对象是否相等？<br>答：不一定相等，判断两个对象是否相等，需要使用equals()。</p><p>最后，记录一个实现高质量equals()的诀窍：</p><blockquote><ol><li>使用<code>==</code>操作符检查“参数是否为这个对象的引用”；</li><li>使用<code>instanceof</code>操作符检查“参数是否为正确的类型”；</li><li>对于类中的关键属性，检查参数传入对象的属性是否与之相匹配；</li><li>编写完equals方法后，问自己它是否满足对称性、传递性、一致性；</li><li>重写equals方法时总是要重写hashCode方法；</li><li>不要将equals方法参数中的Object对象替换为其他的类型，在重写时不要忘掉<code>@Override</code>注解。</li></ol></blockquote><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1]<a href="http://c.biancheng.net/view/5672.html">http://c.biancheng.net/view/5672.html</a><br>[2]<a href="https://blog.csdn.net/qq_44543508/article/details/95449363">https://blog.csdn.net/qq_44543508/article/details/95449363</a><br>[3]<a href="https://www.jianshu.com/p/da7491e5be53">https://www.jianshu.com/p/da7491e5be53</a><br>[4]<a href="https://blog.csdn.net/u013063153/article/details/78808923">https://blog.csdn.net/u013063153/article/details/78808923</a><br>[5]<a href="https://cloud.tencent.com/developer/article/1018529">https://cloud.tencent.com/developer/article/1018529</a></p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于 <a href="https://silencezheng.top">silencezheng.top</a>，转载请注明出处。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;从Java数据类型出发，聊聊Java中的==和equals。&lt;br&gt;</summary>
    
    
    
    
    <category term="Java" scheme="http://silencezheng.top/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>记忆化递归</title>
    <link href="http://silencezheng.top/2022/11/19/article78/"/>
    <id>http://silencezheng.top/2022/11/19/article78/</id>
    <published>2022-11-19T05:59:19.000Z</published>
    <updated>2022-11-19T05:59:52.552Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>一旦写完一个可行的算法，便不想推倒重来，不知道有多少人和我一样有这样的想法。然而写完一个trash的递归算法，计算时间太慢固然也是不行的，这时记忆化递归可能会帮到你。</p><span id="more"></span><h2 id="记忆化递归"><a href="#记忆化递归" class="headerlink" title="记忆化递归"></a>记忆化递归</h2><p><strong>记忆化递归</strong>的核心思想就是将已经算好的值给存起来，等再次需要用到的时候，就直接取而不用计算，这样就大大节省了计算时间。笔者认为，虽然看起来是用空间换时间，但是相比于每次都进行庞大的递归计算来说，用合理的空间存放之前求得的值是明智的。</p><p>思想很简单，实现起来其实也并不复杂，下面举例说明之。</p><h2 id="例一：Leetcode119-杨辉三角-II"><a href="#例一：Leetcode119-杨辉三角-II" class="headerlink" title="例一：Leetcode119. 杨辉三角 II"></a>例一：Leetcode119. 杨辉三角 II</h2><p>简单描述：给定一个非负索引 rowIndex，返回「杨辉三角」的第 rowIndex 行。</p><p>普通递归（超时）：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: SilenceZheng66</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">recusion</span><span class="params">(<span class="keyword">int</span> pos, <span class="keyword">int</span> rowIndex)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(pos&gt;rowIndex) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">if</span>(pos==rowIndex||pos==<span class="number">0</span>) <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">return</span> (recusion(pos-<span class="number">1</span>, rowIndex-<span class="number">1</span>) + recusion(pos, rowIndex-<span class="number">1</span>));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;Integer&gt; <span class="title">getRow</span><span class="params">(<span class="keyword">int</span> rowIndex)</span> </span>&#123;</span><br><span class="line">        ArrayList&lt;Integer&gt; ans = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        ans.add(<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">if</span>(rowIndex==<span class="number">0</span>) <span class="keyword">return</span> ans;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=rowIndex;i++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(i==rowIndex) ans.add(<span class="number">1</span>);</span><br><span class="line">            <span class="keyword">else</span>&#123;</span><br><span class="line">                ans.add(recusion(i, rowIndex));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>利用对称（超时）：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: SilenceZheng66</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">recusion</span><span class="params">(<span class="keyword">int</span> pos, <span class="keyword">int</span> rowIndex)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(pos&gt;rowIndex) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">if</span>(pos==rowIndex||pos==<span class="number">0</span>) <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">return</span> (recusion(pos-<span class="number">1</span>, rowIndex-<span class="number">1</span>) + recusion(pos, rowIndex-<span class="number">1</span>));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;Integer&gt; <span class="title">getRow</span><span class="params">(<span class="keyword">int</span> rowIndex)</span> </span>&#123;</span><br><span class="line">        ArrayList&lt;Integer&gt; ans = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        ans.add(<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">if</span>(rowIndex==<span class="number">0</span>) <span class="keyword">return</span> ans;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> step = rowIndex/<span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=step;i++)&#123;</span><br><span class="line">            ans.add(recusion(i, rowIndex));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(rowIndex%<span class="number">2</span>==<span class="number">0</span>)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i=step-<span class="number">1</span>;i&gt;=<span class="number">0</span>;i--)&#123;</span><br><span class="line">                ans.add(ans.get(i));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i=step;i&gt;=<span class="number">0</span>;i--)&#123;</span><br><span class="line">                ans.add(ans.get(i));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>记忆化递归+利用对称：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: SilenceZheng66</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span>[][] rem = <span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">34</span>][<span class="number">34</span>];</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">recusion</span><span class="params">(<span class="keyword">int</span> pos, <span class="keyword">int</span> rowIndex)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(pos&gt;rowIndex) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">if</span>(pos==rowIndex||pos==<span class="number">0</span>) <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">if</span>(rem[rowIndex][pos]!=<span class="number">0</span>) <span class="keyword">return</span> rem[rowIndex][pos];</span><br><span class="line"></span><br><span class="line">        rem[rowIndex][pos] = (recusion(pos-<span class="number">1</span>, rowIndex-<span class="number">1</span>) + recusion(pos, rowIndex-<span class="number">1</span>));</span><br><span class="line">        <span class="keyword">return</span> rem[rowIndex][pos];</span><br><span class="line">    &#125;</span><br><span class="line">     </span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;Integer&gt; <span class="title">getRow</span><span class="params">(<span class="keyword">int</span> rowIndex)</span> </span>&#123;</span><br><span class="line">        ArrayList&lt;Integer&gt; ans = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        ans.add(<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">if</span>(rowIndex==<span class="number">0</span>) <span class="keyword">return</span> ans;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> step = rowIndex/<span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=step;i++)&#123;</span><br><span class="line">            ans.add(recusion(i, rowIndex));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(rowIndex%<span class="number">2</span>==<span class="number">0</span>)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i=step-<span class="number">1</span>;i&gt;=<span class="number">0</span>;i--)&#123;</span><br><span class="line">                ans.add(ans.get(i));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i=step;i&gt;=<span class="number">0</span>;i--)&#123;</span><br><span class="line">                ans.add(ans.get(i));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于 <a href="https://silencezheng.top">silencezheng.top</a>，转载请注明出处。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;一旦写完一个可行的算法，便不想推倒重来，不知道有多少人和我一样有这样的想法。然而写完一个trash的递归算法，计算时间太慢固然也是不行的，这时记忆化递归可能会帮到你。&lt;/p&gt;</summary>
    
    
    
    
    <category term="数据结构与算法" scheme="http://silencezheng.top/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>MySQL中的Join查询</title>
    <link href="http://silencezheng.top/2022/11/18/article77/"/>
    <id>http://silencezheng.top/2022/11/18/article77/</id>
    <published>2022-11-18T04:50:46.000Z</published>
    <updated>2022-11-18T04:52:50.466Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>业精于勤，荒于嬉。</p><span id="more"></span><h2 id="闲谈"><a href="#闲谈" class="headerlink" title="闲谈"></a>闲谈</h2><p>在整理Join相关的内容时，我提出了几个问题，整理到一个部分记录一下，想看“干货”的读者可以跳过了。</p><p>问题一：<strong>Join和Key有啥关系？</strong><br>Key无非主、外、候选、公共之类的内容，一个row的identifier罢了，无非是对内对外，同时它也是一个field。那么我们在规划表结构、塞数据的时候就有了一个方便的方法，把想表达的一个row的数据用一个key概括，需要获取所有数据时通过多表查询即可。总之，笔者认为，Join和Key可以说没关系，Key在任何时候都发挥着identifier的作用。</p><p>问题二：<strong>MySQL中不用Key也能Join，为什么？</strong><br>其实这个问题本身有点奇怪（我突然想出来的），首先关系代数中连接（Join）也没有要求一定要用键做连接，其次上面也说了这俩没多大关系。但我一搜吧，还真有个<a href="https://blog.csdn.net/lamanchas/article/details/121366276">回答</a>，主要是说外键约束有成本，对高并发情况不合适之类的，一时不知道是我有问题还是理解不到位，知道的大神可以告诉我，感谢。</p><p>问题三：<strong>为什么不能用Where替代Join?</strong><br>关于这个问题，我简单思考了一下，首先就拿纯<code>WHERE</code>、<code>JOIN</code>和<code>LEFT JOIN</code>来说，我写了以下三个查询：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">select</span> *</span><br><span class="line"><span class="attribute">from</span> node_graph<span class="number">1</span> left join edge_graph<span class="number">1</span> <span class="literal">on</span> node_graph<span class="number">1</span>.node_list = edge_graph<span class="number">1</span>.node<span class="number">1</span>_name;</span><br><span class="line"></span><br><span class="line"><span class="attribute">select</span> *</span><br><span class="line"><span class="attribute">from</span> node_graph<span class="number">1</span>, edge_graph<span class="number">1</span> where node_graph<span class="number">1</span>.node_list = edge_graph<span class="number">1</span>.node<span class="number">1</span>_name;</span><br><span class="line"></span><br><span class="line"><span class="attribute">select</span> *</span><br><span class="line"><span class="attribute">from</span> node_graph<span class="number">1</span> join edge_graph<span class="number">1</span> <span class="literal">on</span> node_graph<span class="number">1</span>.node_list = edge_graph<span class="number">1</span>.node<span class="number">1</span>_name;</span><br></pre></td></tr></table></figure><p>其中后两个的效果是一致的，而<code>LEFT JOIN</code>会返回node_graph1的所有结果，即使没有match到，这可能是<code>WHERE</code>做不到的一个地方。更多还是要在实践中发掘。</p><h2 id="Join查询"><a href="#Join查询" class="headerlink" title="Join查询"></a>Join查询</h2><p>SQL中Join用于根据两个或多个表中的列之间的关系，从这些表中查询数据。日常使用中对多表查询有广泛的需求，Join查询自然是必不可少。</p><p>用Join联合表时需要在每个表中选择一个<strong>字段</strong>，并对这些字段的值进行比较，值相同的两条记录将合并为一条。联合表的本质就是将不同表的记录合并起来，形成一张新表。当然，这张新表只是临时的，它仅存在于本次查询期间。</p><p>那么SQL中的Join都有哪些呢？先上一张总览：</p><p><img src="/assets/post_img/article77/SQL-Join.png" alt="overview"></p><p>下面开始逐个说一下。</p><h3 id="1-内连接（Inner-Join"><a href="#1-内连接（Inner-Join" class="headerlink" title="1. 内连接（Inner Join)"></a>1. 内连接（Inner Join)</h3><p>INNER JOIN 是 SQL 中最重要、最常用的表连接形式，只有当连接的两个或者多个表中都存在满足条件的记录时，才返回行。任何一条只存在于某一张表中的数据，都不会返回。</p><h3 id="2-左外连接（Left-Outer-Join"><a href="#2-左外连接（Left-Outer-Join" class="headerlink" title="2. 左外连接（Left Outer Join)"></a>2. 左外连接（Left Outer Join)</h3><p>LEFT OUTER JOIN 以左表为主，即左表中的<strong>所有记录</strong>都会被返回，具体分为以下三种情况：</p><ul><li>如果 TableA 中的某条记录在 TableB 中刚好只有一条记录可以匹配，那么在返回的结果中会生成一个新的行。</li><li>如果 TableA 中的某条记录在 TableB 中有 N 条记录可以匹配，那么在返回结果中也会生成 N 个新的行，这些行所包含的 TableA 的字段值是重复的。</li><li>如果 TableA 中的某条记录在 TableB 中没有匹配的记录，那么在返回结果中仍然会生成一个新的行，只是该行所包含的 TableB 的字段值都是 NULL。<h3 id="3-右外连接（Right-Outer-Join"><a href="#3-右外连接（Right-Outer-Join" class="headerlink" title="3. 右外连接（Right Outer Join)"></a>3. 右外连接（Right Outer Join)</h3>RIGHT OUTER JOIN 以右表为主，即右表中的<strong>所有记录</strong>都会被返回，具体分为以下三种情况：</li><li>如果 TableB 中的某条记录在 TableA 中刚好只有一条记录可以匹配，那么在返回的结果中会生成一个新的行。</li><li>如果 TableB 中的某条记录在 TableA 中有 N 条记录可以匹配，那么在返回的结果中也会生成 N 个新的行，这些行所包含的 TableB 的字段值是重复的。</li><li>如果 TableB 中的某条记录在 TableA 中没有匹配记录，那么在返回结果中仍然会生成一个新的行，只是该行所包含的 TableA 的字段值都是 NULL。<h3 id="4-左外连接-with-exclusion（Left-Outer-Join-with-exclusion"><a href="#4-左外连接-with-exclusion（Left-Outer-Join-with-exclusion" class="headerlink" title="4. 左外连接 with exclusion（Left Outer Join with exclusion)"></a>4. 左外连接 with exclusion（Left Outer Join with exclusion)</h3>在左外连接的基础上，去除TableB可匹配到的部分，只返回B.Key为NULL的记录。<h3 id="5-右外连接-with-exclusion（Right-Outer-Join-with-exclusion"><a href="#5-右外连接-with-exclusion（Right-Outer-Join-with-exclusion" class="headerlink" title="5. 右外连接 with exclusion（Right Outer Join with exclusion)"></a>5. 右外连接 with exclusion（Right Outer Join with exclusion)</h3>在右外连接的基础上，去除TableA可匹配到的部分，只返回A.Key为NULL的记录。<h3 id="6-全外连接（Full-Outer-Join）"><a href="#6-全外连接（Full-Outer-Join）" class="headerlink" title="6. 全外连接（Full Outer Join）"></a>6. 全外连接（Full Outer Join）</h3>FULL OUTER JOIN 先执行 LEFT OUTER JOIN 遍历左表，再执行 RIGHT OUTER JOIN 遍历右表，最后将 RIGHT OUTER JOIN 的结果直接追加到 LEFT OUTER JOIN 后面。注意，FULL OUTER JOIN 会返回重复的行，它们会被保留，不会被删除。<h3 id="7-全外连接-with-exclusion（Full-Outer-Join-with-exclusion）"><a href="#7-全外连接-with-exclusion（Full-Outer-Join-with-exclusion）" class="headerlink" title="7. 全外连接 with exclusion（Full Outer Join with exclusion）"></a>7. 全外连接 with exclusion（Full Outer Join with exclusion）</h3>两表的FULL OUTER JOIN去除重合部分，也就是返回 左外连接 with exclusion 和 右外连接 with exclusion 的 FULL OUTER JOIN 记录。</li></ul><h2 id="MySQL支持的Join方式"><a href="#MySQL支持的Join方式" class="headerlink" title="MySQL支持的Join方式"></a>MySQL支持的Join方式</h2><p>在聊这个之前，先简单了解一下<strong>驱动表和被驱动表</strong>的概念。在LEFT OUTER JOIN时，左表为驱动表，右表为被驱动表；在RIGHT OUTER JOIN时，右表为驱动表，左表为被驱动表。关于驱动表和被驱动表的作用，实际上是与MySQL表关联算法和SQL优化有关的，通常来说，用小表<strong>驱动</strong>大表能够获得更高的效率，这里不详细展开了。</p><p>以MySQL8.0.11为例，MySQL提供的JOIN关键字有：<code>JOIN</code>、<code>INNER JOIN</code>、<code>LEFT JOIN</code>、<code>LEFT OUTER JOIN</code>、<code>RIGHT JOIN</code>、<code>RIGHT OUTER JOIN</code>、<code>CROSS JOIN</code>和<code>STRAIGHT_JOIN</code>。</p><p>其中，<code>JOIN</code>和<code>INNER JOIN</code>为内连接，<code>LEFT JOIN</code>与<code>LEFT OUTER JOIN</code>是等价的，都对应着左外连接（右也是一样的道理）。也就是说，上面提到的七种JOIN方式，MySQL关键字只支持前三种，对于4、5可以结合WHERE来实现，但不提供全外连接关键字。</p><p>这三种（或六个）关键字的通用Join查询结构如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">&lt;</span>row_list<span class="operator">&gt;</span> </span><br><span class="line">  <span class="keyword">FROM</span> <span class="operator">&lt;</span>left_table<span class="operator">&gt;</span> </span><br><span class="line">    <span class="operator">&lt;</span><span class="keyword">inner</span><span class="operator">|</span><span class="keyword">left</span><span class="operator">|</span><span class="keyword">right</span><span class="operator">&gt;</span> <span class="keyword">JOIN</span> <span class="operator">&lt;</span>right_table<span class="operator">&gt;</span> </span><br><span class="line">      <span class="keyword">ON</span> <span class="operator">&lt;</span>join_condition<span class="operator">&gt;</span> </span><br><span class="line">        <span class="keyword">WHERE</span> <span class="operator">&lt;</span>where_condition<span class="operator">&gt;</span></span><br></pre></td></tr></table></figure><p>可以看到这里有两种条件，分别是<strong>join_condition</strong>和<strong>where_condition</strong>，两者执行存在先后顺序。数据库通过JOIN关键字返回记录时会先生成一张临时表，通过临时表返回记录，<strong>join_condition</strong>是在生成临时表时使用的条件，而<strong>where_condition</strong>是在临时表生成后再对其进行过滤的条件。以<code>LEFT JOIN</code>为例，在生成临时表时无论<strong>join_condition</strong>是否为真都会将左表记录加入到临时表中，所以“左表中的<strong>所有记录</strong>都会被返回”。</p><p>那么<code>CROSS JOIN</code>和<code>STRAIGHT_JOIN</code>又是什么呢？</p><p><code>CROSS JOIN</code>子句从连接的表返回行的笛卡儿积，它的通用查询结构如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">&lt;</span>row_list<span class="operator">&gt;</span> </span><br><span class="line">  <span class="keyword">FROM</span> <span class="operator">&lt;</span>left_table<span class="operator">&gt;</span> </span><br><span class="line">    <span class="keyword">CROSS</span> <span class="keyword">JOIN</span> <span class="operator">&lt;</span>right_table<span class="operator">&gt;</span> </span><br><span class="line">        <span class="keyword">ON</span> <span class="operator">&lt;</span>join_condition<span class="operator">&gt;</span></span><br><span class="line">            <span class="keyword">WHERE</span> <span class="operator">&lt;</span>where_condition<span class="operator">&gt;</span></span><br></pre></td></tr></table></figure><p>注意，仅当不添加<strong>join_condition</strong>和<strong>where_condition</strong>的时候，<code>CROSS JOIN</code>才能返回笛卡尔积，如果添加了这些条件，那么工作方式将和<code>JOIN</code>相同。</p><p>至于<code>STRAIGHT_JOIN</code>，其实是提供给用户一种自主决定驱动表与被驱动表关系的方式，它的用法与<code>JOIN</code>相同，只是<code>STRAIGHT_JOIN</code>前面的表一定是驱动表，后面的表一定是被驱动表。而在MySQL中，<code>JOIN</code>会自动选择小表作为驱动表，大表作为被驱动表。用户可以使用<code>STRAIGHT_JOIN</code>来解决MySQL优化器不能解决的部分。</p><p>关于全外连接以及其他各种连接方式在MySQL中的实现，我找到了一张图，是由Steve Stedman制作的，供读者参考。</p><p><img src="/assets/post_img/article77/MySQL-Join.png" alt="MySQLJoinType"></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1]<a href="https://blog.csdn.net/lamanchas/article/details/121366276">https://blog.csdn.net/lamanchas/article/details/121366276</a><br>[2]<a href="https://blog.csdn.net/asd051377305/article/details/115320564">https://blog.csdn.net/asd051377305/article/details/115320564</a><br>[3]<a href="http://c.biancheng.net/sql/join.html">http://c.biancheng.net/sql/join.html</a><br>[4]<a href="https://cloud.tencent.com/developer/article/1167929">https://cloud.tencent.com/developer/article/1167929</a><br>[5]<a href="https://www.jianshu.com/p/76c90b03b7bd">https://www.jianshu.com/p/76c90b03b7bd</a><br>[6]<a href="https://blog.csdn.net/javaanddonet/article/details/109693672">https://blog.csdn.net/javaanddonet/article/details/109693672</a><br>[7]<a href="https://blog.csdn.net/weixin_37692493/article/details/106970429">https://blog.csdn.net/weixin_37692493/article/details/106970429</a></p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于 <a href="https://silencezheng.top">silencezheng.top</a>，转载请注明出处。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;业精于勤，荒于嬉。&lt;/p&gt;</summary>
    
    
    
    
    <category term="MySQL" scheme="http://silencezheng.top/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>二叉树非递归遍历</title>
    <link href="http://silencezheng.top/2022/11/09/article76/"/>
    <id>http://silencezheng.top/2022/11/09/article76/</id>
    <published>2022-11-09T13:41:12.000Z</published>
    <updated>2022-11-11T02:21:08.306Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>不积跬步，无以至千里。</p><span id="more"></span><p>用Java写一下二叉树的非递归遍历，用print表示操作了，主要关注算法。</p><p>树定义如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TreeNode</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> val;</span><br><span class="line">        TreeNode left;</span><br><span class="line">        TreeNode right;</span><br><span class="line">        TreeNode() &#123;&#125;</span><br><span class="line">        TreeNode(<span class="keyword">int</span> val) &#123; <span class="keyword">this</span>.val = val; &#125;</span><br><span class="line">        TreeNode(<span class="keyword">int</span> val, TreeNode left, TreeNode right) &#123;</span><br><span class="line">          <span class="keyword">this</span>.val = val;</span><br><span class="line">          <span class="keyword">this</span>.left = left;</span><br><span class="line">          <span class="keyword">this</span>.right = right;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h2 id="先序遍历（直觉版）"><a href="#先序遍历（直觉版）" class="headerlink" title="先序遍历（直觉版）"></a>先序遍历（直觉版）</h2><p>思路：根左右，从根节点开始先走到最左下节点，然后依次出栈，如果出栈的节点有右子节点则对右子节点再走到最左下，直至栈空。</p><p>步骤：</p><ol><li>对当前节点走到最左下，每次对当前节点操作+入栈。</li><li>元素出栈，若有右子节点则重复1，若无则重复2。</li><li>栈空结束。</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">preorderTraversal</span><span class="params">(TreeNode root)</span></span>&#123;</span><br><span class="line">        Stack&lt;TreeNode&gt; nodeStack = <span class="keyword">new</span> Stack&lt;&gt;();</span><br><span class="line">        <span class="keyword">if</span>(root == <span class="keyword">null</span>)&#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;Null root error!&quot;</span>);</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            TreeNode temp = root;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> (temp!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                <span class="comment">// 操作</span></span><br><span class="line">                System.out.println(temp.val);</span><br><span class="line">                nodeStack.add(temp);</span><br><span class="line">                temp = temp.left;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> (!nodeStack.isEmpty())&#123;</span><br><span class="line">                temp = nodeStack.pop();</span><br><span class="line">                <span class="keyword">if</span> (temp.right!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                    temp = temp.right;</span><br><span class="line">                    <span class="keyword">while</span> (temp!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                        <span class="comment">// 操作</span></span><br><span class="line">                        System.out.println(temp.val);</span><br><span class="line">                        nodeStack.add(temp);</span><br><span class="line">                        temp = temp.left;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>这个版本的基础上可以进行各种优化。</p><h2 id="中序遍历（直觉版）"><a href="#中序遍历（直觉版）" class="headerlink" title="中序遍历（直觉版）"></a>中序遍历（直觉版）</h2><p>思路：左根右，从根节点走到最左下节点，然后依次出栈并操作，如果出栈的节点有右节点则对右节点再走到最左下，直至栈空。</p><p>步骤：</p><ol><li>对当前节点走到最左下，每次对当前节点入栈。</li><li>元素出栈，操作当前节点，若有右子节点则重复1，若无则重复2。</li><li>栈空结束。</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">inorderTraversal</span><span class="params">(TreeNode root)</span></span>&#123;</span><br><span class="line">        Stack&lt;TreeNode&gt; nodeStack = <span class="keyword">new</span> Stack&lt;&gt;();</span><br><span class="line">        <span class="keyword">if</span>(root == <span class="keyword">null</span>)&#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;Null root error!&quot;</span>);</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            TreeNode temp = root;</span><br><span class="line">            <span class="keyword">while</span> (temp!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                nodeStack.add(temp);</span><br><span class="line">                temp = temp.left;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            temp = nodeStack.pop();</span><br><span class="line">            <span class="keyword">while</span> (temp!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                <span class="comment">// 操作</span></span><br><span class="line">                System.out.println(temp.val);</span><br><span class="line">                <span class="keyword">if</span>(temp.right!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                    temp = temp.right;</span><br><span class="line">                    <span class="keyword">while</span> (temp!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                        nodeStack.add(temp);</span><br><span class="line">                        temp = temp.left;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (nodeStack.isEmpty())&#123;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                temp = nodeStack.pop();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>这个版本的基础上可以进行各种优化。</p><h2 id="后序遍历（直觉版）"><a href="#后序遍历（直觉版）" class="headerlink" title="后序遍历（直觉版）"></a>后序遍历（直觉版）</h2><p>思路：左右根，后序不能采用先序和中序的同款算法的主要原因是判断到当前节点存在右子树时，则不能对当前节点进行操作，而需要先对右子树做后序遍历，而即便是保留当前节点，并把右子树遍历完毕后，再对当前节点进行操作，仍然需要对当前节点的右子树是否已被遍历的状态进行判断，判断的依据是上一次操作的节点是否是右子节点。</p><p>步骤：</p><ol><li>对当前节点走到最左下，每次对当前节点入栈。</li><li>元素出栈，判断当前节点，若无右子树则操作并标记当前节点，若有右子树则判断右子树是否被访问过，若未被访问则保留当前节点状态，并依次入栈右子树左支，标记右子节点；若已访问过则操作当前节点，并标记当前节点。重复2。</li><li>栈空结束。</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">postorderTraversal</span><span class="params">(TreeNode root)</span></span>&#123;</span><br><span class="line">        Stack&lt;TreeNode&gt; nodeStack = <span class="keyword">new</span> Stack&lt;&gt;();</span><br><span class="line">        <span class="keyword">if</span>(root == <span class="keyword">null</span>)&#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;Null root error!&quot;</span>);</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            TreeNode temp = root;</span><br><span class="line">            TreeNode mark = <span class="keyword">null</span>;</span><br><span class="line">            <span class="keyword">while</span> (temp!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                nodeStack.add(temp);</span><br><span class="line">                temp = temp.left;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            temp = nodeStack.pop();</span><br><span class="line">            <span class="keyword">while</span> (temp!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                <span class="keyword">if</span>(temp.right==<span class="keyword">null</span>)&#123;</span><br><span class="line">                    <span class="comment">// 操作</span></span><br><span class="line">                    System.out.println(temp.val);</span><br><span class="line">                    <span class="comment">// 标记当前节点</span></span><br><span class="line">                    mark = temp;</span><br><span class="line">                &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                    <span class="comment">// 当前节点含右子树的情况，判断前次处理节点是否是右子节点。</span></span><br><span class="line">                    <span class="keyword">if</span>(temp.right==mark)&#123;</span><br><span class="line">                        <span class="comment">// 操作</span></span><br><span class="line">                        System.out.println(temp.val);</span><br><span class="line">                        <span class="comment">// 标记当前节点</span></span><br><span class="line">                        mark = temp;</span><br><span class="line">                    &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">                        <span class="comment">// 保留状态</span></span><br><span class="line">                        nodeStack.add(temp);</span><br><span class="line">                        <span class="comment">// 入栈右子树的左支，并标记右子节点。</span></span><br><span class="line">                        temp = temp.right;</span><br><span class="line">                        mark = temp;</span><br><span class="line">                        <span class="keyword">while</span> (temp!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                            nodeStack.add(temp);</span><br><span class="line">                            temp = temp.left;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (nodeStack.isEmpty())&#123;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                temp = nodeStack.pop();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>这个版本的基础上可以进行各种优化。</p><h2 id="层序遍历"><a href="#层序遍历" class="headerlink" title="层序遍历"></a>层序遍历</h2><p>思路：按从上到下，从左到右的顺序遍历。用队列实现，先入当前节点，出队再入左、右两子节点，然后每出一个就入队该节点的左、右子节点，直到队空。</p><p>步骤：</p><ol><li>入队当前节点。</li><li>出队一个节点，入队该节点的左、右子节点，重复2。</li><li>队空结束。</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">layerSequenceTraversal</span><span class="params">(TreeNode root)</span></span>&#123;</span><br><span class="line">        LinkedBlockingQueue&lt;TreeNode&gt; que =  <span class="keyword">new</span> LinkedBlockingQueue&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(root == <span class="keyword">null</span>)&#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;Null root error!&quot;</span>);</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            que.offer(root);</span><br><span class="line">            TreeNode temp = <span class="keyword">null</span>;</span><br><span class="line">            <span class="keyword">while</span> (!que.isEmpty())&#123;</span><br><span class="line">                temp = que.poll();</span><br><span class="line">                System.out.println(temp.val);</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (temp.left!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                    que.offer(temp.left);</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (temp.right!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                    que.offer(temp.right);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>先把我个人认为符合直觉的遍历方法写一下，看起来比较复杂但是容易理解，后面再进行优化补充。</p><p>首发于 <a href="https://silencezheng.top">silencezheng.top</a>。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;不积跬步，无以至千里。&lt;/p&gt;</summary>
    
    
    
    
    <category term="数据结构与算法" scheme="http://silencezheng.top/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>深度学习常用术语解释</title>
    <link href="http://silencezheng.top/2022/11/08/article75/"/>
    <id>http://silencezheng.top/2022/11/08/article75/</id>
    <published>2022-11-08T14:00:16.000Z</published>
    <updated>2022-11-08T14:00:52.846Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>深度学习常用术语解释，持续更新～<br><span id="more"></span></p><h2 id="Backbone"><a href="#Backbone" class="headerlink" title="Backbone"></a>Backbone</h2><p>主干网络，或称骨干网络，通常是网络的一部分，大多时候指的是提取特征的网络，其作用就是提取图片中的信息，供后面的网络使用。这些网络经常使用的是ResNet、VGG等，而不是我们自己设计的网络，因为这些网络已经证明了在分类等问题上的特征提取能力是很强的。在用这些网络作为Backbone的时候，都是直接加载官方已经训练好的模型参数，后面接着我们自己的网络。让网络的这两个部分同时进行训练，因为加载的Backbone模型已经具有提取特征的能力了，在我们的训练过程中，会对它进行微调，使得其更适合于我们自己的任务。</p><h2 id="Head"><a href="#Head" class="headerlink" title="Head"></a>Head</h2><p>Head即整个网络的头部，是获取网络输出内容的网络，利用之前（Backbone）提取的特征，做出预测。</p><h2 id="Neck"><a href="#Neck" class="headerlink" title="Neck"></a>Neck</h2><p>是指放在Backbone和Head之间的层，是为了更好的利用Backbone提取的特征。</p><h2 id="Pretext-task"><a href="#Pretext-task" class="headerlink" title="Pretext task"></a>Pretext task</h2><p>用于预训练的任务，可以翻译为前置任务或代理任务。</p><h2 id="Downstream-task"><a href="#Downstream-task" class="headerlink" title="Downstream task"></a>Downstream task</h2><p>下游任务，用于微调的任务。</p><h2 id="Warm-up"><a href="#Warm-up" class="headerlink" title="Warm up"></a>Warm up</h2><p>用一个小的学习率先训练几个epoch，这是因为网络的参数是随机初始化的，一开始就采用较大的学习率容易数值不稳定。</p><h2 id="End-to-End"><a href="#End-to-End" class="headerlink" title="End to End"></a>End to End</h2><p>端到端，给一个输入，获得一个输出，中间的处理过程处于黑箱中，相当于打包成应用了。</p><h2 id="Standardization"><a href="#Standardization" class="headerlink" title="Standardization"></a>Standardization</h2><p>标准化，指将数据按比例缩放，使其落入一个小区间中，缩放后均值为$0$，方差为$1$。在某些比较和评价的指标处理中经常会用到，去除数据的单位限制，将其转化为无量纲的纯数值，便于不同单位或量级的指标能够进行比较和加权。</p><p>例如在数据预处理时为了将所有特征放在一个共同的尺度上，会通过<strong>将特征重新缩放到零均值和单位方差</strong>来标准化数据。这既能方便优化，又能避免惩罚分配给某一特征的系数超过其他特征（一视同仁）。</p><p>在训练过程中，对输入进行规范化可以加速深度网络权重参数的收敛速度。</p><h2 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h2><p>这个词真的需要好好理解一下，其实首先应该想到翻译为“规范化”，它包括归一化、标准化甚至正则化，作为一个统称。比如Batch Normalization其实做的是Standardization的事，所以翻译成批量规范化或者批量标准化。</p><p>其次这个词又可以指归一化，即把数值放缩到$0$到$1$的小区间中。归一化可以消除量纲对最终结果的影响，使不同变量具有可比性。</p><p>关于这个词的解读是要具体问题具体分析了，甚至有时Standardization也被作为统称。</p><h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><p>正则化，一般形式是在整个平均损失函数的最后增加一个正则项（比如L2范数正则化，也有其他形式的正则化，作用不同）。正则项越大表明惩罚力度越大，等于0表示不做惩罚。</p><h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p>[1]<a href="https://zhuanlan.zhihu.com/p/348800083">https://zhuanlan.zhihu.com/p/348800083</a><br>[2]<a href="https://zhuanlan.zhihu.com/p/343692147">https://zhuanlan.zhihu.com/p/343692147</a><br>[3]<a href="https://blog.csdn.net/u014381464/article/details/81101551">https://blog.csdn.net/u014381464/article/details/81101551</a></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;深度学习常用术语解释，持续更新～&lt;br&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://silencezheng.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="机器学习" scheme="http://silencezheng.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>注意力机制--《动手学深度学习》笔记0x0B</title>
    <link href="http://silencezheng.top/2022/11/07/article74/"/>
    <id>http://silencezheng.top/2022/11/07/article74/</id>
    <published>2022-11-07T07:25:27.000Z</published>
    <updated>2022-11-07T07:27:27.834Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><p>灵长类动物的视觉系统接受了大量的感官输入，这些感官输入远远超过了大脑能够完全处理的程度。然而，并非所有刺激的影响都是相等的。意识的聚集和专注使灵长类动物能够在复杂的视觉环境中将注意力引向感兴趣的物体，例如猎物和天敌。只关注一小部分信息的能力对进化更加有意义，使人类得以生存和成功。</p><p>自19世纪以来，科学家们一直致力于研究认知神经科学领域的注意力。本章的很多章节将涉及到一些研究。<br><span id="more"></span><br>本章首先回顾一个经典注意力框架，解释如何在视觉场景中展开注意力。受此框架中的<em>注意力提示</em>（attention cues）的启发，我们将设计能够利用这些注意力提示的模型。1964年的Nadaraya-Waston核回归（kernel regression）正是具有<em>注意力机制</em>（attention mechanism）的机器学习的简单演示。</p><p>然后继续介绍注意力函数，它们在深度学习的注意力模型设计中被广泛使用。具体来说将展示如何使用这些函数来设计<em>Bahdanau注意力</em>。Bahdanau注意力是深度学习中的具有突破性价值的注意力模型，它双向对齐并且可以微分。</p><p>最后将描述仅仅基于注意力机制的<em>Transformer</em>架构，该架构中使用了<em>多头注意力</em>（multi-head attention）和<em>自注意力</em>（self-attention）。自2017年横空出世，Transformer一直都普遍存在于现代的深度学习应用中，例如语言、视觉、语音和强化学习领域。</p><p>这一章目前只做了解，关于NLP的内容没有实验，也没有详细调查。</p><h3 id="0-1-小结"><a href="#0-1-小结" class="headerlink" title="0.1. 小结"></a>0.1. 小结</h3><ul><li>人类的注意力是有限的、有价值和稀缺的资源。</li><li>受试者使用非自主性和自主性提示有选择性地引导注意力。前者基于突出性，后者则依赖于主体的意识。</li><li>注意力机制与全连接层或者池化层的区别源于增加的自主提示。</li><li>由于包含了自主性提示，注意力机制与全连接的层或池化层不同。</li><li>注意力机制通过注意力池化使选择偏向于值（感官输入），其中包含查询（自主性提示）和键（非自主性提示）。键和值是成对的。</li><li>我们可以可视化查询和键之间的注意力权重。</li><li>Nadaraya-Watson核回归是具有注意力机制的机器学习范例。</li><li>Nadaraya-Watson核回归的注意力池化是对训练数据中输出的加权平均。从注意力的角度来看，分配给每个值的注意力权重取决于将值所对应的键和查询作为输入的函数。</li><li>注意力池化可以分为非参数型和带参数型</li><li>注意力池化的输出可以计算为值的加权平均，选择不同的注意力评分函数会带来不同的注意力池化操作。</li><li>当查询和键是不同长度的矢量时，可以使用<em>加性注意力评分函数</em>。当它们的长度相同时，使用<em>缩放的“点－积”注意力评分函数</em>的计算效率更高。</li><li>在预测词元时，如果不是所有输入词元都是相关的，那么具有Bahdanau注意力的循环神经网络编码器-解码器会有选择地统计输入序列的不同部分。这是通过将上下文变量视为加性注意力池化的输出来实现的。</li><li>在循环神经网络编码器-解码器中，Bahdanau注意力将上一时间步的解码器隐状态视为查询，在所有时间步的编码器隐状态同时视为键和值。</li><li>多头注意力融合了来自于多个注意力池化的不同知识，这些知识的不同来源于相同的查询、键和值的不同的子空间表示。</li><li>基于适当的张量操作，可以实现多头注意力的并行计算。</li><li>在自注意力中，查询、键和值都来自同一组输入。</li><li>卷积神经网络和自注意力都拥有并行计算的优势，而且自注意力的最大路径长度最短。但是因为其计算复杂度是关于序列长度的二次方，所以在很长的序列中计算会非常慢。</li><li>为了使用序列的顺序信息，我们可以通过在输入表示中添加位置编码，来注入绝对的或相对的位置信息。</li><li>transformer是编码器－解码器架构的一个实践，尽管在实际情况中编码器或解码器可以单独使用。</li><li>在transformer中，多头自注意力用于表示输入序列和输出序列，不过解码器必须通过掩蔽机制来保留自回归属性。</li><li>transformer中的残差连接和层规范化是训练非常深度模型的重要工具。</li><li>transformer模型中基于位置的前馈网络使用同一个多层感知机，作用是对所有序列位置的表示进行转换。</li></ul><h2 id="1-注意力提示"><a href="#1-注意力提示" class="headerlink" title="1. 注意力提示"></a>1. 注意力提示</h2><p>感谢读者对本书的关注，因为读者的注意力是一种稀缺的资源：此刻读者正在阅读本书（而忽略了其他的书），因此读者的注意力是用机会成本（与金钱类似）来支付的。为了确保读者现在投入的注意力是值得的，作者们尽全力（全部的注意力）创作一本好书。</p><p>自经济学研究稀缺资源分配以来，人们正处在“注意力经济”时代，即人类的注意力被视为可以交换的、有限的、有价值的且稀缺的商品。许多商业模式也被开发出来去利用这一点：在音乐或视频流媒体服务上，人们要么消耗注意力在广告上，要么付钱来隐藏广告；为了在网络游戏世界的成长，人们要么消耗注意力在游戏战斗中，从而帮助吸引新的玩家，要么付钱立即变得强大。总之，注意力不是免费的。</p><p>注意力是稀缺的，而环境中的干扰注意力的信息却并不少。比如人类的视觉神经系统大约每秒收到$10^8$位的信息，这远远超过了大脑能够完全处理的水平。幸运的是，人类的祖先已经从经验（也称为数据）中认识到“并非感官的所有输入都是一样的”。在整个人类历史中，这种只将注意力引向感兴趣的一小部分信息的能力，使人类的大脑能够更明智地分配资源来生存、成长和社交，例如发现天敌、找寻食物和伴侣。</p><h3 id="1-1-生物学中的注意力提示"><a href="#1-1-生物学中的注意力提示" class="headerlink" title="1.1. 生物学中的注意力提示"></a>1.1. 生物学中的注意力提示</h3><p>注意力是如何应用于视觉世界中的呢？这要从当今十分普及的<em>双组件</em>（two-component）的框架开始讲起：这个框架的出现可以追溯到19世纪90年代的威廉·詹姆斯，他被认为是“美国心理学之父” [<code>James.2007</code>]。在这个框架中，受试者基于<em>非自主性提示</em>和<em>自主性提示</em>有选择地引导注意力的焦点。</p><p>非自主性提示是基于环境中物体的突出性和易见性。想象一下，假如我们面前有五个物品：一份报纸、一篇研究论文、一杯咖啡、一本笔记本和一本书，就像下图。所有纸制品都是黑白印刷的，但咖啡杯是红色的。换句话说，这个咖啡杯在这种视觉环境中是突出和显眼的，不由自主地引起人们的注意。所以我们会把视力最敏锐的地方放到咖啡上，如下图所示。</p><p><img src="/assets/post_img/article74/eye-coffee.svg" alt="由于突出性的非自主性提示（红杯子），注意力不自主地指向了咖啡杯"></p><p>喝咖啡后，我们会变得兴奋并想读书，所以转过头，重新聚焦眼睛，然后看看书，就像下图中描述那样。与上图中由于突出性导致的选择不同，此时选择书是受到了认知和意识的控制，因此注意力在基于自主性提示去辅助选择时将更为谨慎。受试者的主观意愿推动，选择的力量也就更强大。</p><p><img src="/assets/post_img/article74/eye-book.svg" alt="依赖于任务的意志提示（想读一本书），注意力被自主引导到书上"></p><h3 id="1-2-查询、键和值"><a href="#1-2-查询、键和值" class="headerlink" title="1.2. 查询、键和值"></a>1.2. 查询、键和值</h3><p>自主性的与非自主性的注意力提示解释了人类注意力的方式，下面来看看如何通过这两种注意力提示，用神经网络来设计注意力机制的框架。</p><p>首先，对于只使用非自主性提示的情况。要想将选择偏向于感官输入，则可以简单地使用参数化的全连接层，甚至是非参数化的最大池化层或平均池化层。个人理解，这就是说，这种情况下不需要在以往的神经网络上做出修改，因为非自主性提示来自客体的差异。</p><p>因此，“是否包含自主性提示”将<strong>注意力机制</strong>与全连接层或池化层区别开来。在注意力机制的背景下，自主性提示被称为<em>查询</em>（query）。给定任何查询，注意力机制通过<em>注意力池化</em>（attention pooling）将选择引导至<em>感官输入</em>（sensory inputs，例如中间特征表示）。在注意力机制中，这些感官输入被称为<em>值</em>（value）。更通俗的解释是，每个值都与一个感官输入的非自主提示配对，这些对应的非自主性提示称为<em>键</em>（key）。如下图所示，可以通过设计注意力池化的方式，使给定的查询（自主性提示）与键（非自主性提示）进行匹配，这将引导得出最匹配的值（感官输入）。</p><p><img src="/assets/post_img/article74/qkv.svg" alt="注意力机制通过注意力池化将*查询*（自主性提示）和*键*（非自主性提示）结合在一起，实现对*值*（感官输入）的选择倾向"></p><p>鉴于上面所提框架在上图中的主导地位，因此这个框架下的模型将成为本章的中心。然而，注意力机制的设计有许多替代方案。例如可以设计一个不可微的注意力模型，该模型可以使用强化学习方法 [<code>Mnih.Heess.Graves.ea.2014</code>]进行训练。</p><h3 id="1-3-注意力的可视化"><a href="#1-3-注意力的可视化" class="headerlink" title="1.3. 注意力的可视化"></a>1.3. 注意力的可视化</h3><p>平均池化层可以被视为输入的加权平均值，其中各输入的权重是一样的。实际上，注意力池化得到的是加权平均的总和值，其中权重是在给定的查询和不同的键之间计算得出的。</p><p>为了可视化注意力权重，需要定义一个<code>show_heatmaps</code>函数。其输入<code>matrices</code>的形状是（要显示的行数，要显示的列数，查询的数目，键的数目）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_heatmaps</span>(<span class="params">matrices, xlabel, ylabel, titles=<span class="literal">None</span>, figsize=(<span class="params"><span class="number">2.5</span>, <span class="number">2.5</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="function">                  cmap=<span class="string">&#x27;Reds&#x27;</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;显示矩阵热图&quot;&quot;&quot;</span></span><br><span class="line">    d2l.use_svg_display()</span><br><span class="line">    num_rows, num_cols = matrices.shape[<span class="number">0</span>], matrices.shape[<span class="number">1</span>]</span><br><span class="line">    fig, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize,</span><br><span class="line">                                 sharex=<span class="literal">True</span>, sharey=<span class="literal">True</span>, squeeze=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">for</span> i, (row_axes, row_matrices) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(axes, matrices)):</span><br><span class="line">        <span class="keyword">for</span> j, (ax, matrix) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(row_axes, row_matrices)):</span><br><span class="line">            pcm = ax.imshow(matrix.detach().numpy(), cmap=cmap)</span><br><span class="line">            <span class="keyword">if</span> i == num_rows - <span class="number">1</span>:</span><br><span class="line">                ax.set_xlabel(xlabel)</span><br><span class="line">            <span class="keyword">if</span> j == <span class="number">0</span>:</span><br><span class="line">                ax.set_ylabel(ylabel)</span><br><span class="line">            <span class="keyword">if</span> titles:</span><br><span class="line">                ax.set_title(titles[j])</span><br><span class="line">    fig.colorbar(pcm, ax=axes, shrink=<span class="number">0.6</span>);</span><br></pre></td></tr></table></figure><p>下面使用一个简单的例子进行演示，本例中，仅当查询和键相同时（即客体特征符合主体意识时），注意力权重为1，否则为0。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">attention_weights = torch.eye(<span class="number">10</span>).reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">show_heatmaps(attention_weights, xlabel=<span class="string">&#x27;Keys&#x27;</span>, ylabel=<span class="string">&#x27;Queries&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_attention-cues_1.svg" alt="输出"></p><p>后面的章节将经常调用<code>show_heatmaps</code>函数来显示注意力权重。</p><h2 id="2-注意力池化：Nadaraya-Watson-核回归"><a href="#2-注意力池化：Nadaraya-Watson-核回归" class="headerlink" title="2. 注意力池化：Nadaraya-Watson 核回归"></a>2. 注意力池化：Nadaraya-Watson 核回归</h2><p>上节介绍了框架下的注意力机制的主要成分：查询（自主提示）和键（非自主提示）之间的交互形成了注意力池化；注意力池化有选择地聚合了值（感官输入）以生成最终的输出。本节将介绍注意力池化的更多细节，以便从宏观上了解注意力机制在实践中的运作方式。1964年提出的Nadaraya-Watson核回归模型是一个简单但完整的例子，可以用于演示具有注意力机制的机器学习。</p><h3 id="2-1-生成数据集"><a href="#2-1-生成数据集" class="headerlink" title="2.1. 生成数据集"></a>2.1. 生成数据集</h3><p>简单起见，考虑这个回归问题：给定的成对的“输入－输出”数据集${(x_1, y_1), \ldots, (x_n, y_n)}$，如何学习$f$来预测任意新输入$x$的输出$\hat{y} = f(x)$？</p><p>根据下面的非线性函数生成一个人工数据集，其中加入的噪声项为$\epsilon$：</p><script type="math/tex; mode=display">y_i = 2\sin(x_i) + x_i^{0.8} + \epsilon,</script><p>其中$\epsilon$服从均值为$0$和标准差为$0.5$的正态分布。在这里生成了$50$个训练样本和$50$个测试样本。为了更好地可视化之后的注意力模式，需要将训练样本进行排序。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">n_train = <span class="number">50</span>  <span class="comment"># 训练样本数</span></span><br><span class="line"><span class="comment"># torch.sort返回排序后的张量和原张量在排序后张量中的对应索引</span></span><br><span class="line">x_train, _ = torch.sort(torch.rand(n_train) * <span class="number">5</span>)   <span class="comment"># 排序后的训练样本</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * torch.sin(x) + x**<span class="number">0.8</span></span><br><span class="line"></span><br><span class="line">y_train = f(x_train) + torch.normal(<span class="number">0.0</span>, <span class="number">0.5</span>, (n_train,))  <span class="comment"># 训练样本的输出</span></span><br><span class="line">x_test = torch.arange(<span class="number">0</span>, <span class="number">5</span>, <span class="number">0.1</span>)  <span class="comment"># 测试样本</span></span><br><span class="line">y_truth = f(x_test)  <span class="comment"># 测试样本的真实输出</span></span><br><span class="line">n_test = <span class="built_in">len</span>(x_test)  <span class="comment"># 测试样本数</span></span><br></pre></td></tr></table></figure><p>下面的函数将绘制所有的训练样本（样本由圆圈表示），不带噪声项的真实数据生成函数$f$（标记为“Truth”），以及学习得到的预测函数（标记为“Pred”）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_kernel_reg</span>(<span class="params">y_hat</span>):</span></span><br><span class="line">    d2l.plot(x_test, [y_truth, y_hat], <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;y&#x27;</span>, legend=[<span class="string">&#x27;Truth&#x27;</span>, <span class="string">&#x27;Pred&#x27;</span>],</span><br><span class="line">             xlim=[<span class="number">0</span>, <span class="number">5</span>], ylim=[-<span class="number">1</span>, <span class="number">5</span>])</span><br><span class="line">    d2l.plt.plot(x_train, y_train, <span class="string">&#x27;o&#x27;</span>, alpha=<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure><h3 id="2-2-平均池化"><a href="#2-2-平均池化" class="headerlink" title="2.2. 平均池化"></a>2.2. 平均池化</h3><p>先使用最简单的估计器来解决回归问题。基于平均池化来计算所有训练样本输出值的平均值：</p><script type="math/tex; mode=display">f(x) = \frac{1}{n}\sum_{i=1}^n y_i,</script><p>如下图所示，这个估计器确实不够聪明。真实函数$f$（“Truth”）和预测函数（“Pred”）相差很大。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里先求了一下训练集标签的平均值，得到一个单值张量（无维度）。</span></span><br><span class="line"><span class="comment"># torch.repeat_interleave(): 将输入张量按照指定维度进行扩展，若未指定维度则会将输入拉张开为1维向量再进行扩展。</span></span><br><span class="line"><span class="comment"># 这里没有指定dim，故先将单值张量转为1维张量，然后在该维度上复制成n_test个元素。</span></span><br><span class="line">y_hat = torch.repeat_interleave(y_train.mean(), n_test)</span><br><span class="line">plot_kernel_reg(y_hat)</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_nadaraya-waston_1.svg" alt="输出"></p><h3 id="2-3-非参数注意力池化"><a href="#2-3-非参数注意力池化" class="headerlink" title="2.3. 非参数注意力池化"></a>2.3. 非参数注意力池化</h3><p>显然，平均池化忽略了输入$x_i$。于是Nadaraya[<code>Nadaraya.1964</code>]和Watson[<code>Watson.1964</code>]提出了一个更好的想法，根据输入的位置对输出$y_i$进行加权：</p><script type="math/tex; mode=display">f(x) = \sum_{i=1}^n \frac{K(x - x_i)}{\sum_{j=1}^n K(x - x_j)} y_i,</script><p>其中$K$是<em>核</em>（kernel）。上式所描述的估计器被称为<em>Nadaraya-Watson核回归</em>（Nadaraya-Watson kernel regression）。这里不会深入讨论核函数的细节，但受此启发，我们可以从<a href="#12-查询键和值">第一节图中</a>的注意力机制框架的角度重写上式，成为一个更加通用的<em>注意力池化</em>（attention pooling）公式：</p><script type="math/tex; mode=display">f(x) = \sum_{i=1}^n \alpha(x, x_i) y_i,</script><p>其中$x$是查询，$(x_i, y_i)$是键值对。比较两个公式，注意力池化是$y_i$的加权平均。将查询$x$和键$x_i$之间的关系建模为<em>注意力权重</em>（attention weight）$\alpha(x, x_i)$，如上式所示，这个权重将被分配给每一个对应值$y_i$。对于任何查询，模型在所有键值对注意力权重都是一个有效的概率分布：它们是非负的，并且总和为1。</p><p>为了更好地理解注意力池化，考虑一个<em>高斯核</em>（Gaussian kernel），其定义为：</p><script type="math/tex; mode=display">K(u) = \frac{1}{\sqrt{2\pi}} \exp(-\frac{u^2}{2}).</script><p>将高斯核代入<em>Nadaraya-Watson核回归公式</em> 和 <em>注意力池化公式</em> 可以得到：</p><script type="math/tex; mode=display">\begin{aligned} f(x) &=\sum_{i=1}^n \alpha(x, x_i) y_i\\ &= \sum_{i=1}^n \frac{\exp\left(-\frac{1}{2}(x - x_i)^2\right)}{\sum_{j=1}^n \exp\left(-\frac{1}{2}(x - x_j)^2\right)} y_i \\&= \sum_{i=1}^n \mathrm{softmax}\left(-\frac{1}{2}(x - x_i)^2\right) y_i. \end{aligned}</script><p>在上式中，如果一个键$x_i$越是接近给定的查询$x$，那么分配给这个键对应值$y_i$的注意力权重就会越大，也就“获得了更多的注意力”。</p><p>这里穿插解释一下参数模型和非参数模型。</p><p>参数模型<br>: 在统计学中，参数模型通常假设总体服从某个分布，这个分布可以由一些参数确定，如正态分布由均值和标准差确定，在此基础上构建的模型称为参数模型。</p><p>非参数模型<br>: 非参数模型对于总体的分布不做任何假设或者说是数据分布假设自由，只知道其分布是存在的，所以就无法得到其分布的相关参数，只能通过非参数统计的方法进行推断。</p><p>总之，参数模型和非参数模型中的“参数”并不是模型中的参数，而是数据分布的参数。</p><p>Nadaraya-Watson核回归是一个非参数模型。因此，上式是<em>非参数的注意力池化</em>（nonparametric attention pooling）模型。接下来将基于这个非参数的注意力池化模型来绘制预测结果。从绘制的结果会发现新的模型预测线是平滑的，并且比平均池化的预测更接近真实。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># X_repeat的形状:(n_test,n_train),</span></span><br><span class="line"><span class="comment"># 每一行都包含着相同的测试输入（例如：同样的查询）</span></span><br><span class="line">X_repeat = x_test.repeat_interleave(n_train).reshape((-<span class="number">1</span>, n_train))</span><br><span class="line"><span class="comment"># x_train包含着键。attention_weights的形状：(n_test,n_train),</span></span><br><span class="line"><span class="comment"># 每一行都包含着要在给定的每个查询的值（y_train）之间分配的注意力权重</span></span><br><span class="line">attention_weights = nn.functional.softmax(-(X_repeat - x_train)**<span class="number">2</span> / <span class="number">2</span>, dim=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># y_hat的每个元素都是值的加权平均值，其中的权重是注意力权重</span></span><br><span class="line">y_hat = torch.matmul(attention_weights, y_train)</span><br><span class="line">plot_kernel_reg(y_hat)</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_nadaraya-waston_2.svg" alt="输出"></p><p>现在来观察注意力的权重。 这里测试数据的输入相当于查询，而训练数据的输入相当于键。 因为两个输入都是经过排序的，因此由观察可知“查询-键”对越接近，注意力池化的注意力权重就越高。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(attention_weights.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>),</span><br><span class="line">                  xlabel=<span class="string">&#x27;Sorted training inputs&#x27;</span>,</span><br><span class="line">                  ylabel=<span class="string">&#x27;Sorted testing inputs&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_nadaraya-waston_3.svg" alt="输出"></p><h3 id="2-4-带参数注意力池化"><a href="#2-4-带参数注意力池化" class="headerlink" title="2.4. 带参数注意力池化"></a>2.4. 带参数注意力池化</h3><p>非参数的Nadaraya-Watson核回归具有<em>一致性</em>（consistency）的优点：如果有足够的数据，此模型会收敛到最优结果。尽管如此，我们还是可以轻松地将可学习的参数集成到注意力池化中。</p><p>例如，与上一节略有不同，在下面的查询$x$和键$x_i$之间的距离乘以可学习参数$w$：</p><script type="math/tex; mode=display">\begin{aligned}f(x) &= \sum_{i=1}^n \alpha(x, x_i) y_i \\&= \sum_{i=1}^n \frac{\exp\left(-\frac{1}{2}((x - x_i)w)^2\right)}{\sum_{j=1}^n \exp\left(-\frac{1}{2}((x - x_j)w)^2\right)} y_i \\&= \sum_{i=1}^n \mathrm{softmax}\left(-\frac{1}{2}((x - x_i)w)^2\right) y_i \end{aligned}</script><p>本节的余下部分将通过训练这个模型来学习注意力池化的参数。</p><h4 id="2-4-1-批量矩阵乘法"><a href="#2-4-1-批量矩阵乘法" class="headerlink" title="2.4.1. 批量矩阵乘法"></a>2.4.1. 批量矩阵乘法</h4><p>为了更有效地计算小批量数据的注意力，可以利用深度学习开发框架中提供的批量矩阵乘法。</p><p>假设第一个小批量数据包含$n$个矩阵$\mathbf{X}_1,\ldots, \mathbf{X}_n$，第二个小批量包含$n$个矩阵$\mathbf{Y}_1, \ldots, \mathbf{Y}_n$，形状为$a\times b$，形状为$b\times c$。它们的批量矩阵乘法得到$n$个矩阵$\mathbf{X}_1\mathbf{Y}_1, \ldots, \mathbf{X}_n\mathbf{Y}_n$，形状为$a\times c$。因此，假定两个张量的形状分别是$(n,a,b)$和$(n,b,c)$，它们的批量矩阵乘法输出的形状为$(n,a,c)$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X = torch.ones((<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>))</span><br><span class="line">Y = torch.ones((<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>))</span><br><span class="line">torch.bmm(X, Y).shape</span><br></pre></td></tr></table></figure><p>在注意力机制的背景中，我们可以使用小批量矩阵乘法来计算小批量数据中的加权平均值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">weights = torch.ones((<span class="number">2</span>, <span class="number">10</span>)) * <span class="number">0.1</span></span><br><span class="line">values = torch.arange(<span class="number">20.0</span>).reshape((<span class="number">2</span>, <span class="number">10</span>))</span><br><span class="line"><span class="comment"># 权重在第一维升维，值在第二维升维，这里unsqueeze(-1) = unsqueeze(2)</span></span><br><span class="line">torch.bmm(weights.unsqueeze(<span class="number">1</span>), values.unsqueeze(-<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[[ <span class="number">4.5000</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">14.5000</span>]]])</span><br></pre></td></tr></table></figure><h4 id="2-4-2-定义模型"><a href="#2-4-2-定义模型" class="headerlink" title="2.4.2. 定义模型"></a>2.4.2. 定义模型</h4><p>基于上述的带参数的注意力池化，使用小批量矩阵乘法，定义Nadaraya-Watson核回归的带参数版本为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NWKernelRegression</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line">        self.w = nn.Parameter(torch.rand((<span class="number">1</span>,), requires_grad=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, queries, keys, values</span>):</span></span><br><span class="line">        <span class="comment"># queries和attention_weights的形状为(查询个数，“键－值”对个数)</span></span><br><span class="line">        queries = queries.repeat_interleave(keys.shape[<span class="number">1</span>]).reshape((-<span class="number">1</span>, keys.shape[<span class="number">1</span>]))</span><br><span class="line">        self.attention_weights = nn.functional.softmax(</span><br><span class="line">            -((queries - keys) * self.w)**<span class="number">2</span> / <span class="number">2</span>, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># values的形状为(查询个数，“键－值”对个数)</span></span><br><span class="line">        <span class="keyword">return</span> torch.bmm(self.attention_weights.unsqueeze(<span class="number">1</span>),</span><br><span class="line">                         values.unsqueeze(-<span class="number">1</span>)).reshape(-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h4 id="2-4-3-训练"><a href="#2-4-3-训练" class="headerlink" title="2.4.3. 训练"></a>2.4.3. 训练</h4><p>接下来，将训练数据集变换为键和值用于训练注意力模型。 在带参数的注意力池化模型中， 任何一个训练样本的输入都会和除自己以外的所有训练样本的“键－值”对进行计算， 从而得到其对应的预测输出。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># X_tile的形状:(n_train，n_train)，每一行都包含着相同的训练输入</span></span><br><span class="line">X_tile = x_train.repeat((n_train, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># Y_tile的形状:(n_train，n_train)，每一行都包含着相同的训练输出</span></span><br><span class="line">Y_tile = y_train.repeat((n_train, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># eye函数为了生成对角线全1，其余部分全0的二维数组。然后转化为对角线False的矩阵，从X_tile中去除了对角线元素后reshape为新的二维数组。</span></span><br><span class="line"><span class="comment"># keys的形状:(&#x27;n_train&#x27;，&#x27;n_train&#x27;-1)</span></span><br><span class="line">keys = X_tile[(<span class="number">1</span> - torch.eye(n_train)).<span class="built_in">type</span>(torch.<span class="built_in">bool</span>)].reshape((n_train, -<span class="number">1</span>))</span><br><span class="line"><span class="comment"># values的形状:(&#x27;n_train&#x27;，&#x27;n_train&#x27;-1)</span></span><br><span class="line">values = Y_tile[(<span class="number">1</span> - torch.eye(n_train)).<span class="built_in">type</span>(torch.<span class="built_in">bool</span>)].reshape((n_train, -<span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>训练带参数的注意力池化模型时，使用平方损失函数和随机梯度下降。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">net = NWKernelRegression()</span><br><span class="line">loss = nn.MSELoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.5</span>)</span><br><span class="line">animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>, xlim=[<span class="number">1</span>, <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    trainer.zero_grad()</span><br><span class="line">    l = loss(net(x_train, keys, values), y_train)</span><br><span class="line">    l.<span class="built_in">sum</span>().backward()</span><br><span class="line">    trainer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;<span class="built_in">float</span>(l.<span class="built_in">sum</span>()):<span class="number">.6</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    animator.add(epoch + <span class="number">1</span>, <span class="built_in">float</span>(l.<span class="built_in">sum</span>()))</span><br></pre></td></tr></table></figure><p>如下所示，训练完带参数的注意力池化模型后，我们发现： 在尝试拟合带噪声的训练数据时， 预测结果绘制的线不如之前非参数模型的平滑。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># keys的形状:(n_test，n_train)，每一行包含着相同的训练输入（例如，相同的键）</span></span><br><span class="line">keys = x_train.repeat((n_test, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># value的形状:(n_test，n_train)</span></span><br><span class="line">values = y_train.repeat((n_test, <span class="number">1</span>))</span><br><span class="line">y_hat = net(x_test, keys, values).unsqueeze(<span class="number">1</span>).detach()</span><br><span class="line">plot_kernel_reg(y_hat)</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_nadaraya-waston_4.svg" alt="输出"></p><p>为什么新的模型更不平滑了呢？ 来看一下输出结果的绘制图： 与非参数的注意力池化模型相比，带参数的模型加入可学习的参数后， 曲线在注意力权重较大的区域变得更不平滑。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(net.attention_weights.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>),</span><br><span class="line">                  xlabel=<span class="string">&#x27;Sorted training inputs&#x27;</span>,</span><br><span class="line">                  ylabel=<span class="string">&#x27;Sorted testing inputs&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_nadaraya-waston_5.svg" alt="5"></p><h2 id="3-注意力评分函数"><a href="#3-注意力评分函数" class="headerlink" title="3. 注意力评分函数"></a>3. 注意力评分函数</h2><p>上一节中，我们使用高斯核来对查询和键之间的关系建模。可以将其中的高斯核指数部分视为<strong>注意力评分函数</strong>（attention scoring function）， 简称<em>评分函数</em>（scoring function），然后把这个函数的输出结果输入到softmax函数中进行运算。 通过上述步骤，我们将得到与键对应的值的概率分布（即注意力权重）。 最后，注意力池化的输出就是基于这些注意力权重的值的加权和。</p><p>从宏观来看，我们可以使用上述算法来实现<a href="#12-查询键和值">1.2</a>中的注意力机制框架。下图说明了如何将注意力池化的输出计算成为值的加权和，其中 $a$ 表示注意力评分函数。 由于注意力权重是概率分布， 因此加权和其本质上是加权平均值。</p><p><img src="/assets/post_img/article74/attention-output.svg" alt="attention output"></p><p>用数学语言描述，假设有一个查询 $\mathbf{q} \in \mathbb{R}^q$ 和 $m$ 个“键－值”对 $(\mathbf{k}_1, \mathbf{v}_1), \ldots, (\mathbf{k}_m, \mathbf{v}_m)$，其中$\mathbf{k}_i \in \mathbb{R}^k$，$\mathbf{v}_i \in \mathbb{R}^v$。注意力池化函数 $f$ 就被表示成值的加权和：</p><script type="math/tex; mode=display">f(\mathbf{q}, (\mathbf{k}_1, \mathbf{v}_1), \ldots, (\mathbf{k}_m, \mathbf{v}_m)) = \sum_{i=1}^m \alpha(\mathbf{q}, \mathbf{k}_i) \mathbf{v}_i \in \mathbb{R}^v</script><p>其中查询$\mathbf{q}$和键$\mathbf{k}_i$的注意力权重（标量）是通过注意力评分函数$a$将两个向量映射成标量，再经过softmax运算得到的：</p><script type="math/tex; mode=display">\alpha(\mathbf{q}, \mathbf{k}_i) = \mathrm{softmax}(a(\mathbf{q}, \mathbf{k}_i)) = \frac{\exp(a(\mathbf{q}, \mathbf{k}_i))}{\sum_{j=1}^m \exp(a(\mathbf{q}, \mathbf{k}_j))} \in \mathbb{R}</script><p>正如上图所示，选择不同的注意力评分函数 $a$ 会导致不同的注意力池化操作。本节将介绍两个流行的评分函数，稍后将用他们来实现更复杂的注意力机制。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><h3 id="3-1-掩蔽softmax操作"><a href="#3-1-掩蔽softmax操作" class="headerlink" title="3.1. 掩蔽softmax操作"></a>3.1. 掩蔽softmax操作</h3><p>正如上面提到的，softmax操作用于输出一个概率分布作为注意力权重。在某些情况下，并非所有的值都应该被纳入到注意力池化中。例如，为了在“机器翻译与数据集”一节中高效处理小批量数据集，某些文本序列被填充了没有意义的特殊词元。为了仅将有意义的词元作为值来获取注意力池化，可以指定一个有效序列长度（即词元的个数），以便在计算softmax时过滤掉超出指定范围的位置。下面的<code>masked_softmax</code>函数实现了这样的<em>掩蔽softmax操作</em>（masked softmax operation），其中任何超出有效长度的位置都被掩蔽并置为0。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">masked_softmax</span>(<span class="params">X, valid_lens</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;通过在最后一个轴上掩蔽元素来执行softmax操作&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># X:3D张量，valid_lens:1D或2D张量</span></span><br><span class="line">    <span class="keyword">if</span> valid_lens <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> nn.functional.softmax(X, dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        shape = X.shape</span><br><span class="line">        <span class="keyword">if</span> valid_lens.dim() == <span class="number">1</span>:</span><br><span class="line">            valid_lens = torch.repeat_interleave(valid_lens, shape[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            valid_lens = valid_lens.reshape(-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 最后一轴上被掩蔽的元素使用一个非常大的负值替换，从而其softmax输出为0</span></span><br><span class="line">        X = d2l.sequence_mask(X.reshape(-<span class="number">1</span>, shape[-<span class="number">1</span>]), valid_lens,</span><br><span class="line">                              value=-<span class="number">1e6</span>)</span><br><span class="line">        <span class="keyword">return</span> nn.functional.softmax(X.reshape(shape), dim=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>为了演示此函数是如何工作的，考虑由两个$2 \times 4$矩阵表示的样本，这两个样本的有效长度分别为$2$和$3$。经过掩蔽softmax操作，超出有效长度的值都被掩蔽为0。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">masked_softmax(torch.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>), torch.tensor([<span class="number">2</span>, <span class="number">3</span>]))</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[[<span class="number">0.5423</span>, <span class="number">0.4577</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>],</span><br><span class="line">         [<span class="number">0.6133</span>, <span class="number">0.3867</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">0.3324</span>, <span class="number">0.2348</span>, <span class="number">0.4329</span>, <span class="number">0.0000</span>],</span><br><span class="line">         [<span class="number">0.2444</span>, <span class="number">0.3943</span>, <span class="number">0.3613</span>, <span class="number">0.0000</span>]]])</span><br></pre></td></tr></table></figure><p>也可以使用二维张量，为矩阵样本中的每一行指定有效长度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">masked_softmax(torch.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>), torch.tensor([[<span class="number">1</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">4</span>]]))</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[[<span class="number">1.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>],</span><br><span class="line">         [<span class="number">0.4142</span>, <span class="number">0.3582</span>, <span class="number">0.2275</span>, <span class="number">0.0000</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">0.5565</span>, <span class="number">0.4435</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>],</span><br><span class="line">         [<span class="number">0.3305</span>, <span class="number">0.2070</span>, <span class="number">0.2827</span>, <span class="number">0.1798</span>]]])</span><br></pre></td></tr></table></figure><h3 id="3-2-加性注意力"><a href="#3-2-加性注意力" class="headerlink" title="3.2. 加性注意力"></a>3.2. 加性注意力</h3><p>一般来说，当查询和键是<strong>不同长度</strong>的矢量时，可以使用加性注意力作为评分函数。给定查询$\mathbf{q} \in \mathbb{R}^q$和键$\mathbf{k} \in \mathbb{R}^k$，<em>加性注意力</em>（additive attention）的评分函数为</p><script type="math/tex; mode=display">a(\mathbf q, \mathbf k) = \mathbf w_v^\top \text{tanh}(\mathbf W_q\mathbf q + \mathbf W_k \mathbf k) \in \mathbb{R}</script><p>其中可学习的参数是$\mathbf W_q\in\mathbb R^{h\times q}$、$\mathbf W_k\in\mathbb R^{h\times k}$和$\mathbf w_v\in\mathbb R^{h}$。如上式所示，将查询和键连结起来后输入到一个多层感知机（MLP）中，感知机包含一个隐藏层，其隐藏单元数是一个超参数$h$。通过使用$\tanh$作为激活函数，并且禁用偏置项。</p><p>下面来实现加性注意力。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AdditiveAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;加性注意力&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, key_size, query_size, num_hiddens, dropout, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(AdditiveAttention, self).__init__(**kwargs)</span><br><span class="line">        self.W_k = nn.Linear(key_size, num_hiddens, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_q = nn.Linear(query_size, num_hiddens, bias=<span class="literal">False</span>)</span><br><span class="line">        self.w_v = nn.Linear(num_hiddens, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, queries, keys, values, valid_lens</span>):</span></span><br><span class="line">        queries, keys = self.W_q(queries), self.W_k(keys)</span><br><span class="line">        <span class="comment"># 在维度扩展后，</span></span><br><span class="line">        <span class="comment"># queries的形状：(batch_size，查询的个数，1，num_hidden)</span></span><br><span class="line">        <span class="comment"># key的形状：(batch_size，1，“键－值”对的个数，num_hiddens)</span></span><br><span class="line">        <span class="comment"># 使用广播方式进行求和</span></span><br><span class="line">        features = queries.unsqueeze(<span class="number">2</span>) + keys.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        features = torch.tanh(features)</span><br><span class="line">        <span class="comment"># self.w_v仅有一个输出，因此从形状中移除最后那个维度。</span></span><br><span class="line">        <span class="comment"># scores的形状：(batch_size，查询的个数，“键-值”对的个数)</span></span><br><span class="line">        scores = self.w_v(features).squeeze(-<span class="number">1</span>)</span><br><span class="line">        self.attention_weights = masked_softmax(scores, valid_lens)</span><br><span class="line">        <span class="comment"># values的形状：(batch_size，“键－值”对的个数，值的维度)</span></span><br><span class="line">        <span class="keyword">return</span> torch.bmm(self.dropout(self.attention_weights), values)</span><br></pre></td></tr></table></figure><p>用一个小例子来演示上面的<code>AdditiveAttention</code>类，其中查询、键和值的形状为（批量大小，步数或词元序列长度，特征大小），实际输出为$(2,1,20)$、$(2,10,2)$和$(2,10,4)$。注意力池化输出的形状为（批量大小，查询的步数，值的维度）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">queries, keys = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">1</span>, <span class="number">20</span>)), torch.ones((<span class="number">2</span>, <span class="number">10</span>, <span class="number">2</span>))</span><br><span class="line"><span class="comment"># values的小批量，两个值矩阵是相同的</span></span><br><span class="line">values = torch.arange(<span class="number">40</span>, dtype=torch.float32).reshape(<span class="number">1</span>, <span class="number">10</span>, <span class="number">4</span>).repeat(</span><br><span class="line">    <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">valid_lens = torch.tensor([<span class="number">2</span>, <span class="number">6</span>])</span><br><span class="line"></span><br><span class="line">attention = AdditiveAttention(key_size=<span class="number">2</span>, query_size=<span class="number">20</span>, num_hiddens=<span class="number">8</span>,</span><br><span class="line">                              dropout=<span class="number">0.1</span>)</span><br><span class="line">attention.<span class="built_in">eval</span>()</span><br><span class="line">attention(queries, keys, values, valid_lens)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[[ <span class="number">2.0000</span>,  <span class="number">3.0000</span>,  <span class="number">4.0000</span>,  <span class="number">5.0000</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">10.0000</span>, <span class="number">11.0000</span>, <span class="number">12.0000</span>, <span class="number">13.0000</span>]]], grad_fn=&lt;BmmBackward0&gt;)</span><br></pre></td></tr></table></figure><p>尽管加性注意力包含了可学习的参数，但由于本例子中每个键都是相同的， 所以注意力权重是均匀的，由指定的有效长度决定。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(attention.attention_weights.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">10</span>)),</span><br><span class="line">                  xlabel=<span class="string">&#x27;Keys&#x27;</span>, ylabel=<span class="string">&#x27;Queries&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_attention-scoring-functions.svg" alt="oasf"></p><h3 id="3-3-缩放点积注意力"><a href="#3-3-缩放点积注意力" class="headerlink" title="3.3. 缩放点积注意力"></a>3.3. 缩放点积注意力</h3><p>使用点积可以得到计算效率更高的评分函数，但是点积操作要求查询和键具有相同的长度$d$。假设查询和键的所有元素都是独立的随机变量，并且都满足零均值和单位方差，那么两个向量的点积的均值为$0$，方差为$d$。为确保无论向量长度如何，点积的方差在不考虑向量长度的情况下仍然是$1$，我们再将点积除以$\sqrt{d}$，则<em>缩放点积注意力</em>（scaled dot-product attention）评分函数为：</p><script type="math/tex; mode=display">a(\mathbf q, \mathbf k) = \mathbf{q}^\top \mathbf{k}  /\sqrt{d}</script><p>在实践中，我们通常从小批量的角度来考虑提高效率，例如基于$n$个查询和$m$个键－值对计算注意力，其中查询和键的长度为$d$，值的长度为$v$。查询$\mathbf Q\in\mathbb R^{n\times d}$、键$\mathbf K\in\mathbb R^{m\times d}$和值$\mathbf V\in\mathbb R^{m\times v}$的缩放点积注意力是：</p><script type="math/tex; mode=display">\mathrm{softmax}\left(\frac{\mathbf Q \mathbf K^\top }{\sqrt{d}}\right) \mathbf V \in \mathbb{R}^{n\times v}</script><p>下面的缩放点积注意力的实现使用了暂退法进行模型正则化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DotProductAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;缩放点积注意力&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dropout, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(DotProductAttention, self).__init__(**kwargs)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># queries的形状：(batch_size，查询的个数，d)</span></span><br><span class="line">    <span class="comment"># keys的形状：(batch_size，“键－值”对的个数，d)</span></span><br><span class="line">    <span class="comment"># values的形状：(batch_size，“键－值”对的个数，值的维度)</span></span><br><span class="line">    <span class="comment"># valid_lens的形状:(batch_size，)或者(batch_size，查询的个数)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, queries, keys, values, valid_lens=<span class="literal">None</span></span>):</span></span><br><span class="line">        d = queries.shape[-<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 设置transpose_b=True为了交换keys的最后两个维度</span></span><br><span class="line">        scores = torch.bmm(queries, keys.transpose(<span class="number">1</span>,<span class="number">2</span>)) / math.sqrt(d)</span><br><span class="line">        self.attention_weights = masked_softmax(scores, valid_lens)</span><br><span class="line">        <span class="keyword">return</span> torch.bmm(self.dropout(self.attention_weights), values)</span><br></pre></td></tr></table></figure><p>为了演示上述的DotProductAttention类， 我们使用与先前加性注意力例子中相同的键、值和有效长度。 对于点积操作，我们令查询的特征维度与键的特征维度大小相同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">queries = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">attention = DotProductAttention(dropout=<span class="number">0.5</span>)</span><br><span class="line">attention.<span class="built_in">eval</span>()</span><br><span class="line">attention(queries, keys, values, valid_lens)</span><br></pre></td></tr></table></figure><p>与加性注意力演示相同，由于键包含的是相同的元素， 而这些元素无法通过任何查询进行区分，因此获得了均匀的注意力权重。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(attention.attention_weights.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">10</span>)),</span><br><span class="line">                  xlabel=<span class="string">&#x27;Keys&#x27;</span>, ylabel=<span class="string">&#x27;Queries&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_attention-scoring-functions.svg" alt="oasf"></p><h2 id="4-Bahdanau-注意力"><a href="#4-Bahdanau-注意力" class="headerlink" title="4. Bahdanau 注意力"></a>4. Bahdanau 注意力</h2><p>之前章节中探讨了机器翻译问题：通过设计一个基于两个循环神经网络的编码器-解码器架构，用于序列到序列学习（seq2seq）。具体来说，循环神经网络编码器将长度可变的序列转换为固定形状的上下文变量，然后循环神经网络解码器根据生成的词元和上下文变量按词元生成输出（目标）序列词元。然而，即使并非所有输入（源）词元都对解码某个词元都有用，在每个解码步骤中仍使用编码<em>相同</em>的上下文变量。有什么方法能改变上下文变量呢？</p><p>试着从<code>Graves.2013</code>中找到灵感：在为给定文本序列生成手写的挑战中，Graves设计了一种可微注意力模型，将文本字符与更长的笔迹对齐，其中对齐方式仅向一个方向移动。受学习对齐想法的启发，Bahdanau等人提出了一个没有严格单向对齐限制的可微注意力模型 <code>Bahdanau.Cho.Bengio.2014</code>。在预测词元时，如果不是所有输入词元都相关，模型将仅对齐（或参与）输入序列中与当前预测相关的部分。这是通过将上下文变量视为注意力集中的输出来实现的。</p><p>由于这段是在NLP方向上加注意力机制，就粗略浏览一下，没做什么笔记。 —SZ</p><h3 id="4-1-模型"><a href="#4-1-模型" class="headerlink" title="4.1. 模型"></a>4.1. 模型</h3><p>下面描述的Bahdanau注意力模型将遵循之前seq2seq中的相同符号表达。这个新的基于注意力的模型与seq2seq中的模型相同，只不过上下文变量$\mathbf{c}$在任何解码时间步$t’$都会被$\mathbf{c}_{t’}$替换。假设输入序列中有$T$个词元，解码时间步$t’$的上下文变量是注意力集中的输出：</p><script type="math/tex; mode=display">\mathbf{c}_{t'} = \sum_{t=1}^T \alpha(\mathbf{s}_{t' - 1}, \mathbf{h}_t) \mathbf{h}_t</script><p>其中，时间步$t’ - 1$时的解码器隐状态$\mathbf{s}_{t’ - 1}$是查询，编码器隐状态$\mathbf{h}_t$既是键，也是值，注意力权重$\alpha$是使用<em>加性注意力打分函数</em>计算的。</p><p>与之前描述的循环神经网络编码器-解码器架构略有不同，下图描述了Bahdanau注意力的架构。</p><p><img src="/assets/post_img/article74/seq2seq-attention-details.svg" alt="一个带有Bahdanau注意力的循环神经网络编码器-解码器模型"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><h3 id="4-2-定义注意力解码器"><a href="#4-2-定义注意力解码器" class="headerlink" title="4.2. 定义注意力解码器"></a>4.2. 定义注意力解码器</h3><p>下面我们看看如何定义Bahdanau注意力，实现循环神经网络编码器-解码器。 其实，我们只需重新定义解码器即可。 为了更方便地显示学习的注意力权重， 以下AttentionDecoder类定义了带有注意力机制解码器的基本接口。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AttentionDecoder</span>(<span class="params">d2l.Decoder</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;带有注意力机制解码器的基本接口&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(AttentionDecoder, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">attention_weights</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure><p>接下来，让我们在接下来的<code>Seq2SeqAttentionDecoder</code>类中实现带有Bahdanau注意力的循环神经网络解码器。首先，初始化解码器的状态，需要下面的输入：</p><ol><li>编码器在所有时间步的最终层隐状态，将作为注意力的键和值；</li><li>上一时间步的编码器全层隐状态，将作为初始化解码器的隐状态；</li><li>编码器有效长度（排除在注意力池中填充词元）。</li></ol><p>在每个解码时间步骤中，解码器上一个时间步的最终层隐状态将用作查询。<br>因此，注意力输出和输入嵌入都连结为循环神经网络解码器的输入。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Seq2SeqAttentionDecoder</span>(<span class="params">AttentionDecoder</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embed_size, num_hiddens, num_layers,</span></span></span><br><span class="line"><span class="params"><span class="function">                 dropout=<span class="number">0</span>, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Seq2SeqAttentionDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.attention = d2l.AdditiveAttention(</span><br><span class="line">            num_hiddens, num_hiddens, num_hiddens, dropout)</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.rnn = nn.GRU(</span><br><span class="line">            embed_size + num_hiddens, num_hiddens, num_layers,</span><br><span class="line">            dropout=dropout)</span><br><span class="line">        self.dense = nn.Linear(num_hiddens, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_state</span>(<span class="params">self, enc_outputs, enc_valid_lens, *args</span>):</span></span><br><span class="line">        <span class="comment"># outputs的形状为(batch_size，num_steps，num_hiddens).</span></span><br><span class="line">        <span class="comment"># hidden_state的形状为(num_layers，batch_size，num_hiddens)</span></span><br><span class="line">        outputs, hidden_state = enc_outputs</span><br><span class="line">        <span class="keyword">return</span> (outputs.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>), hidden_state, enc_valid_lens)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, state</span>):</span></span><br><span class="line">        <span class="comment"># enc_outputs的形状为(batch_size,num_steps,num_hiddens).</span></span><br><span class="line">        <span class="comment"># hidden_state的形状为(num_layers,batch_size,</span></span><br><span class="line">        <span class="comment"># num_hiddens)</span></span><br><span class="line">        enc_outputs, hidden_state, enc_valid_lens = state</span><br><span class="line">        <span class="comment"># 输出X的形状为(num_steps,batch_size,embed_size)</span></span><br><span class="line">        X = self.embedding(X).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        outputs, self._attention_weights = [], []</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> X:</span><br><span class="line">            <span class="comment"># query的形状为(batch_size,1,num_hiddens)</span></span><br><span class="line">            query = torch.unsqueeze(hidden_state[-<span class="number">1</span>], dim=<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># context的形状为(batch_size,1,num_hiddens)</span></span><br><span class="line">            context = self.attention(</span><br><span class="line">                query, enc_outputs, enc_outputs, enc_valid_lens)</span><br><span class="line">            <span class="comment"># 在特征维度上连结</span></span><br><span class="line">            x = torch.cat((context, torch.unsqueeze(x, dim=<span class="number">1</span>)), dim=-<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 将x变形为(1,batch_size,embed_size+num_hiddens)</span></span><br><span class="line">            out, hidden_state = self.rnn(x.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>), hidden_state)</span><br><span class="line">            outputs.append(out)</span><br><span class="line">            self._attention_weights.append(self.attention.attention_weights)</span><br><span class="line">        <span class="comment"># 全连接层变换后，outputs的形状为</span></span><br><span class="line">        <span class="comment"># (num_steps,batch_size,vocab_size)</span></span><br><span class="line">        outputs = self.dense(torch.cat(outputs, dim=<span class="number">0</span>))</span><br><span class="line">        <span class="keyword">return</span> outputs.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>), [enc_outputs, hidden_state,</span><br><span class="line">                                          enc_valid_lens]</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">attention_weights</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self._attention_weights</span><br></pre></td></tr></table></figure><p>接下来，我们使用包含7个时间步的4个序列输入的小批量测试Bahdanau注意力解码器。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">encoder = d2l.Seq2SeqEncoder(vocab_size=<span class="number">10</span>, embed_size=<span class="number">8</span>, num_hiddens=<span class="number">16</span>,</span><br><span class="line">                             num_layers=<span class="number">2</span>)</span><br><span class="line">encoder.<span class="built_in">eval</span>()</span><br><span class="line">decoder = Seq2SeqAttentionDecoder(vocab_size=<span class="number">10</span>, embed_size=<span class="number">8</span>, num_hiddens=<span class="number">16</span>,</span><br><span class="line">                                  num_layers=<span class="number">2</span>)</span><br><span class="line">decoder.<span class="built_in">eval</span>()</span><br><span class="line">X = torch.zeros((<span class="number">4</span>, <span class="number">7</span>), dtype=torch.long)  <span class="comment"># (batch_size,num_steps)</span></span><br><span class="line">state = decoder.init_state(encoder(X), <span class="literal">None</span>)</span><br><span class="line">output, state = decoder(X, state)</span><br><span class="line">output.shape, <span class="built_in">len</span>(state), state[<span class="number">0</span>].shape, <span class="built_in">len</span>(state[<span class="number">1</span>]), state[<span class="number">1</span>][<span class="number">0</span>].shape</span><br></pre></td></tr></table></figure><h3 id="4-3-训练"><a href="#4-3-训练" class="headerlink" title="4.3. 训练"></a>4.3. 训练</h3><p>在这里指定超参数，实例化一个带有Bahdanau注意力的编码器和解码器， 并对这个模型进行机器翻译训练。 由于新增的注意力机制，训练要比没有注意力机制的慢得多。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">embed_size, num_hiddens, num_layers, dropout = <span class="number">32</span>, <span class="number">32</span>, <span class="number">2</span>, <span class="number">0.1</span></span><br><span class="line">batch_size, num_steps = <span class="number">64</span>, <span class="number">10</span></span><br><span class="line">lr, num_epochs, device = <span class="number">0.005</span>, <span class="number">250</span>, d2l.try_gpu()</span><br><span class="line"></span><br><span class="line">train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)</span><br><span class="line">encoder = d2l.Seq2SeqEncoder(</span><br><span class="line">    <span class="built_in">len</span>(src_vocab), embed_size, num_hiddens, num_layers, dropout)</span><br><span class="line">decoder = Seq2SeqAttentionDecoder(</span><br><span class="line">    <span class="built_in">len</span>(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)</span><br><span class="line">net = d2l.EncoderDecoder(encoder, decoder)</span><br><span class="line">d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)</span><br></pre></td></tr></table></figure><p>模型训练后，我们用它将几个英语句子翻译成法语并计算它们的BLEU分数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">engs = [<span class="string">&#x27;go .&#x27;</span>, <span class="string">&quot;i lost .&quot;</span>, <span class="string">&#x27;he\&#x27;s calm .&#x27;</span>, <span class="string">&#x27;i\&#x27;m home .&#x27;</span>]</span><br><span class="line">fras = [<span class="string">&#x27;va !&#x27;</span>, <span class="string">&#x27;j\&#x27;ai perdu .&#x27;</span>, <span class="string">&#x27;il est calme .&#x27;</span>, <span class="string">&#x27;je suis chez moi .&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> eng, fra <span class="keyword">in</span> <span class="built_in">zip</span>(engs, fras):</span><br><span class="line">    translation, dec_attention_weight_seq = d2l.predict_seq2seq(</span><br><span class="line">        net, eng, src_vocab, tgt_vocab, num_steps, device, <span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;eng&#125;</span> =&gt; <span class="subst">&#123;translation&#125;</span>, &#x27;</span>,</span><br><span class="line">          <span class="string">f&#x27;bleu <span class="subst">&#123;d2l.bleu(translation, fra, k=<span class="number">2</span>):<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">go . =&gt; va !,  bleu <span class="number">1.000</span></span><br><span class="line">i lost . =&gt; j<span class="string">&#x27;ai perdu .,  bleu 1.000</span></span><br><span class="line"><span class="string">he&#x27;</span>s calm . =&gt; je suis ici .,  bleu <span class="number">0.000</span></span><br><span class="line">i<span class="string">&#x27;m home . =&gt; je suis chez moi .,  bleu 1.000</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">attention_weights = torch.cat([step[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>] <span class="keyword">for</span> step <span class="keyword">in</span> dec_attention_weight_seq], <span class="number">0</span>).reshape((</span><br><span class="line">    <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, num_steps))</span><br></pre></td></tr></table></figure><p>训练结束后，通过可视化注意力权重你会发现，每个查询都会在键值对上分配不同的权重，这说明 在每个解码步中，输入序列的不同部分被选择性地聚集在注意力池中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加上一个包含序列结束词元</span></span><br><span class="line">d2l.show_heatmaps(</span><br><span class="line">    attention_weights[:, :, :, :<span class="built_in">len</span>(engs[-<span class="number">1</span>].split()) + <span class="number">1</span>].cpu(),</span><br><span class="line">    xlabel=<span class="string">&#x27;Key positions&#x27;</span>, ylabel=<span class="string">&#x27;Query positions&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_bahdanau-attention_seq.svg" alt="seq"></p><h2 id="5-多头注意力"><a href="#5-多头注意力" class="headerlink" title="5. 多头注意力"></a>5. 多头注意力</h2><p>在实践中，当给定相同的查询、键和值的集合时，我们希望模型可以基于相同的注意力机制学习到不同的行为，然后将不同的行为作为知识组合起来，捕获序列内各种范围的依赖关系（例如，短距离依赖和长距离依赖关系）。因此，允许注意力机制组合使用查询、键和值的不同<em>子空间表示</em>（representation subspaces）可能是有益的。</p><p>为此，与其只使用单独一个注意力池化，我们可以用独立学习得到的$h$组不同的<em>线性投影</em>（linear projections）来变换查询、键和值。然后，这$h$组变换后的查询、键和值将并行地送到注意力池化中。最后，将这$h$个注意力池化的输出拼接在一起，并且通过另一个可以学习的线性投影进行变换，以产生最终输出。这种设计被称为<em>多头注意力</em>（multihead attention）<code>Vaswani.Shazeer.Parmar.ea.2017</code>。对于$h$个注意力池化输出，每一个注意力池化都被称作一个<em>头</em>（head）。 下图展示了使用全连接层来实现可学习的线性变换的多头注意力。</p><p><img src="/assets/post_img/article74/multi-head-attention.svg" alt="多头注意力：多个头连结然后线性变换"></p><h3 id="5-1-模型"><a href="#5-1-模型" class="headerlink" title="5.1. 模型"></a>5.1. 模型</h3><p>在实现多头注意力之前，让我们用数学语言将这个模型形式化地描述出来。给定查询$\mathbf{q} \in \mathbb{R}^{d_q}$、键$\mathbf{k} \in \mathbb{R}^{d_k}$和值$\mathbf{v} \in \mathbb{R}^{d_v}$，每个注意力头$\mathbf{h}_i$（$i = 1, \ldots, h$）的计算方法为：</p><script type="math/tex; mode=display">\mathbf{h}_i = f(\mathbf W_i^{(q)}\mathbf q, \mathbf W_i^{(k)}\mathbf k,\mathbf W_i^{(v)}\mathbf v) \in \mathbb R^{p_v}</script><p>其中，可学习的参数包括$\mathbf W_i^{(q)}\in\mathbb R^{p_q\times d_q}$、$\mathbf W_i^{(k)}\in\mathbb R^{p_k\times d_k}$和$\mathbf W_i^{(v)}\in\mathbb R^{p_v\times d_v}$，以及代表注意力池化的函数$f$。$f$可以是<a href="#3-注意力评分函数">注意力评分函数</a>中的加性注意力和缩放点积注意力。多头注意力的输出需要经过另一个线性转换，它对应着$h$个头连结后的结果，因此其可学习参数是$\mathbf W_o\in\mathbb R^{p_o\times h p_v}$：</p><script type="math/tex; mode=display">\mathbf W_o \begin{bmatrix}\mathbf h_1\\\vdots\\\mathbf h_h\end{bmatrix} \in \mathbb{R}^{p_o}</script><p>基于这种设计，每个头都可能会关注输入的不同部分，可以表示比简单加权平均值更复杂的函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><h3 id="5-2-实现"><a href="#5-2-实现" class="headerlink" title="5.2. 实现"></a>5.2. 实现</h3><p>在实现过程中通常选择缩放点积注意力作为每一个注意力头。为了避免计算代价和参数代价的大幅增长，我们设定$p_q = p_k = p_v = p_o / h$。值得注意的是，如果将查询、键和值的线性变换的输出数量设置为$p_q h = p_k h = p_v h = p_o$，则可以并行计算$h$个头。在下面的实现中，$p_o$是通过参数<code>num_hiddens</code>指定的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;多头注意力&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, key_size, query_size, value_size, num_hiddens,</span></span></span><br><span class="line"><span class="params"><span class="function">                 num_heads, dropout, bias=<span class="literal">False</span>, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttention, self).__init__(**kwargs)</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        self.attention = d2l.DotProductAttention(dropout)</span><br><span class="line">        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)</span><br><span class="line">        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)</span><br><span class="line">        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)</span><br><span class="line">        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, queries, keys, values, valid_lens</span>):</span></span><br><span class="line">        <span class="comment"># queries，keys，values的形状:</span></span><br><span class="line">        <span class="comment"># (batch_size，查询或者“键－值”对的个数，num_hiddens)</span></span><br><span class="line">        <span class="comment"># valid_lens　的形状:</span></span><br><span class="line">        <span class="comment"># (batch_size，)或(batch_size，查询的个数)</span></span><br><span class="line">        <span class="comment"># 经过变换后，输出的queries，keys，values　的形状:</span></span><br><span class="line">        <span class="comment"># (batch_size*num_heads，查询或者“键－值”对的个数，</span></span><br><span class="line">        <span class="comment"># num_hiddens/num_heads)</span></span><br><span class="line">        queries = transpose_qkv(self.W_q(queries), self.num_heads)</span><br><span class="line">        keys = transpose_qkv(self.W_k(keys), self.num_heads)</span><br><span class="line">        values = transpose_qkv(self.W_v(values), self.num_heads)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> valid_lens <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 在轴0，将第一项（标量或者矢量）复制num_heads次，</span></span><br><span class="line">            <span class="comment"># 然后如此复制第二项，然后诸如此类。</span></span><br><span class="line">            valid_lens = torch.repeat_interleave(</span><br><span class="line">                valid_lens, repeats=self.num_heads, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># output的形状:(batch_size*num_heads，查询的个数，</span></span><br><span class="line">        <span class="comment"># num_hiddens/num_heads)</span></span><br><span class="line">        output = self.attention(queries, keys, values, valid_lens)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># output_concat的形状:(batch_size，查询的个数，num_hiddens)</span></span><br><span class="line">        output_concat = transpose_output(output, self.num_heads)</span><br><span class="line">        <span class="keyword">return</span> self.W_o(output_concat)</span><br></pre></td></tr></table></figure><p>为了能够使多个头并行计算，上面的<code>MultiHeadAttention</code>类将使用下面定义的两个转置函数。具体来说，<code>transpose_output</code>函数反转了<code>transpose_qkv</code>函数的操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transpose_qkv</span>(<span class="params">X, num_heads</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;为了多注意力头的并行计算而变换形状&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 输入X的形状:(batch_size，查询或者“键－值”对的个数，num_hiddens)</span></span><br><span class="line">    <span class="comment"># 输出X的形状:(batch_size，查询或者“键－值”对的个数，num_heads，</span></span><br><span class="line">    <span class="comment"># num_hiddens/num_heads)</span></span><br><span class="line">    X = X.reshape(X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>], num_heads, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出X的形状:(batch_size，num_heads，查询或者“键－值”对的个数,</span></span><br><span class="line">    <span class="comment"># num_hiddens/num_heads)</span></span><br><span class="line">    X = X.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 最终输出的形状:(batch_size*num_heads,查询或者“键－值”对的个数,</span></span><br><span class="line">    <span class="comment"># num_hiddens/num_heads)</span></span><br><span class="line">    <span class="keyword">return</span> X.reshape(-<span class="number">1</span>, X.shape[<span class="number">2</span>], X.shape[<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transpose_output</span>(<span class="params">X, num_heads</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;逆转transpose_qkv函数的操作&quot;&quot;&quot;</span></span><br><span class="line">    X = X.reshape(-<span class="number">1</span>, num_heads, X.shape[<span class="number">1</span>], X.shape[<span class="number">2</span>])</span><br><span class="line">    X = X.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">    <span class="keyword">return</span> X.reshape(X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>], -<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>下面我们使用键和值相同的小例子来测试我们编写的MultiHeadAttention类。 多头注意力输出的形状是（batch_size，num_queries，num_hiddens）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens, num_heads = <span class="number">100</span>, <span class="number">5</span></span><br><span class="line">attention = MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens,</span><br><span class="line">                               num_hiddens, num_heads, <span class="number">0.5</span>)</span><br><span class="line">attention.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">MultiHeadAttention(</span><br><span class="line">  (attention): DotProductAttention(</span><br><span class="line">    (dropout): Dropout(p=<span class="number">0.5</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">  )</span><br><span class="line">  (W_q): Linear(in_features=<span class="number">100</span>, out_features=<span class="number">100</span>, bias=<span class="literal">False</span>)</span><br><span class="line">  (W_k): Linear(in_features=<span class="number">100</span>, out_features=<span class="number">100</span>, bias=<span class="literal">False</span>)</span><br><span class="line">  (W_v): Linear(in_features=<span class="number">100</span>, out_features=<span class="number">100</span>, bias=<span class="literal">False</span>)</span><br><span class="line">  (W_o): Linear(in_features=<span class="number">100</span>, out_features=<span class="number">100</span>, bias=<span class="literal">False</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">batch_size, num_queries = <span class="number">2</span>, <span class="number">4</span></span><br><span class="line">num_kvpairs, valid_lens =  <span class="number">6</span>, torch.tensor([<span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">X = torch.ones((batch_size, num_queries, num_hiddens))</span><br><span class="line">Y = torch.ones((batch_size, num_kvpairs, num_hiddens))</span><br><span class="line">attention(X, Y, Y, valid_lens).shape</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">4</span>, <span class="number">100</span>])</span><br></pre></td></tr></table></figure><h2 id="6-自注意力和位置编码"><a href="#6-自注意力和位置编码" class="headerlink" title="6. 自注意力和位置编码"></a>6. 自注意力和位置编码</h2><p>在深度学习中，经常使用卷积神经网络（CNN）或循环神经网络（RNN）对序列进行编码。想象一下，有了注意力机制之后，我们将词元序列输入注意力池化中，以便同一组词元同时充当查询、键和值。具体来说，每个查询都会关注所有的键－值对并生成一个注意力输出。由于查询、键和值来自同一组输入，因此被称为<em>自注意力</em>（self-attention）<code>Lin.Feng.Santos.ea.2017,Vaswani.Shazeer.Parmar.ea.2017</code>，也被称为<em>内部注意力</em>（intra-attention）<code>Cheng.Dong.Lapata.2016,Parikh.Tackstrom.Das.ea.2016,Paulus.Xiong.Socher.2017</code>。本节将使用自注意力进行序列编码，以及如何使用序列的顺序作为补充信息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><h3 id="6-1-自注意力"><a href="#6-1-自注意力" class="headerlink" title="6.1. 自注意力"></a>6.1. 自注意力</h3><p>给定一个由词元组成的输入序列$\mathbf{x}_1, \ldots, \mathbf{x}_n$，其中任意$\mathbf{x}_i \in \mathbb{R}^d$（$1 \leq i \leq n$）。该序列的自注意力输出为一个长度相同的序列<br>$\mathbf{y}_1, \ldots, \mathbf{y}_n$，其中：</p><script type="math/tex; mode=display">\mathbf{y}_i = f(\mathbf{x}_i, (\mathbf{x}_1, \mathbf{x}_1), \ldots, (\mathbf{x}_n, \mathbf{x}_n)) \in \mathbb{R}^d</script><p>根据<a href="#24-带参数注意力池化">2.4</a>中定义的注意力池化函数$f$。下面的代码片段是基于多头注意力对一个张量完成自注意力的计算，张量的形状为（批量大小，时间步的数目或词元序列的长度，$d$）。输出与输入的张量形状相同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens, num_heads = <span class="number">100</span>, <span class="number">5</span></span><br><span class="line">attention = d2l.MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens,</span><br><span class="line">                                   num_hiddens, num_heads, <span class="number">0.5</span>)</span><br><span class="line">attention.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">MultiHeadAttention(</span><br><span class="line">  (attention): DotProductAttention(</span><br><span class="line">    (dropout): Dropout(p=<span class="number">0.5</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">  )</span><br><span class="line">  (W_q): Linear(in_features=<span class="number">100</span>, out_features=<span class="number">100</span>, bias=<span class="literal">False</span>)</span><br><span class="line">  (W_k): Linear(in_features=<span class="number">100</span>, out_features=<span class="number">100</span>, bias=<span class="literal">False</span>)</span><br><span class="line">  (W_v): Linear(in_features=<span class="number">100</span>, out_features=<span class="number">100</span>, bias=<span class="literal">False</span>)</span><br><span class="line">  (W_o): Linear(in_features=<span class="number">100</span>, out_features=<span class="number">100</span>, bias=<span class="literal">False</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">batch_size, num_queries, valid_lens = <span class="number">2</span>, <span class="number">4</span>, torch.tensor([<span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">X = torch.ones((batch_size, num_queries, num_hiddens))</span><br><span class="line">attention(X, X, X, valid_lens).shape</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">4</span>, <span class="number">100</span>])</span><br></pre></td></tr></table></figure><h3 id="6-2-比较卷积神经网络、循环神经网络和自注意力"><a href="#6-2-比较卷积神经网络、循环神经网络和自注意力" class="headerlink" title="6.2. 比较卷积神经网络、循环神经网络和自注意力"></a>6.2. 比较卷积神经网络、循环神经网络和自注意力</h3><p>接下来比较下面几个架构，目标都是将由$n$个词元组成的序列映射到另一个长度相等的序列，其中的每个输入词元或输出词元都由$d$维向量表示。具体来说，将比较的是卷积神经网络、循环神经网络和自注意力这几个架构的计算复杂性、顺序操作和最大路径长度。请注意，顺序操作会妨碍并行计算，而任意的序列位置组合之间的路径越短，则能更轻松地学习序列中的远距离依赖关系 <code>Hochreiter.Bengio.Frasconi.ea.2001</code>。</p><p><img src="/assets/post_img/article74/cnn-rnn-self-attention.svg" alt="比较卷积神经网络（填充词元被忽略）、循环神经网络和自注意力三种架构"></p><p>考虑一个卷积核大小为$k$的卷积层。在后面的章节将提供关于使用卷积神经网络处理序列的更多详细信息。目前只需要知道的是，由于序列长度是$n$，输入和输出的通道数量都是$d$，所以卷积层的计算复杂度为$\mathcal{O}(knd^2)$。如上图所示，卷积神经网络是分层的，因此为有$\mathcal{O}(1)$个顺序操作，最大路径长度为$\mathcal{O}(n/k)$。例如，$\mathbf{x}_1$和$\mathbf{x}_5$处于上图中卷积核大小为3的双层卷积神经网络的感受野内。</p><p>当更新循环神经网络的隐状态时，$d \times d$权重矩阵和$d$维隐状态的乘法计算复杂度为$\mathcal{O}(d^2)$。由于序列长度为$n$，因此循环神经网络层的计算复杂度为$\mathcal{O}(nd^2)$。根据上图，有$\mathcal{O}(n)$个顺序操作无法并行化，最大路径长度也是$\mathcal{O}(n)$。</p><p>在自注意力中，查询、键和值都是$n \times d$矩阵。考虑<a href="#33-缩放点积注意力">3.3</a>中缩放的”点－积“注意力，其中$n \times d$矩阵乘以$d \times n$矩阵。之后输出的$n \times n$矩阵乘以$n \times d$矩阵。因此，自注意力具有$\mathcal{O}(n^2d)$计算复杂性。正如在上图中所讲，每个词元都通过自注意力直接连接到任何其他词元。因此，有$\mathcal{O}(1)$个顺序操作可以并行计算，最大路径长度也是$\mathcal{O}(1)$。</p><p>总而言之，卷积神经网络和自注意力都拥有并行计算的优势，而且自注意力的最大路径长度最短。但是因为其计算复杂度是关于序列长度的二次方，所以在很长的序列中计算会非常慢。</p><h3 id="6-3-位置编码"><a href="#6-3-位置编码" class="headerlink" title="6.3. 位置编码"></a>6.3. 位置编码</h3><p>在处理词元序列时，循环神经网络是逐个的重复地处理词元的，而自注意力则因为并行计算而放弃了顺序操作。为了使用序列的顺序信息，通过在输入表示中添加<em>位置编码</em>（positional encoding）来注入绝对的或相对的位置信息。位置编码可以通过学习得到也可以直接固定得到。接下来描述的是基于正弦函数和余弦函数的固定位置编码<code>Vaswani.Shazeer.Parmar.ea.2017</code>。</p><p>假设输入表示 $\mathbf{X} \in \mathbb{R}^{n \times d}$ 包含一个序列中$n$个词元的$d$维嵌入表示。位置编码使用相同形状的位置嵌入矩阵 $\mathbf{P} \in \mathbb{R}^{n \times d}$输出$\mathbf{X} + \mathbf{P}$，矩阵第$i$行、第$2j$列和$2j+1$列上的元素为：</p><script type="math/tex; mode=display">\begin{aligned} p_{i, 2j} &= \sin\left(\frac{i}{10000^{2j/d}}\right),\\p_{i, 2j+1} &= \cos\left(\frac{i}{10000^{2j/d}}\right).\end{aligned}</script><p>乍一看，这种基于三角函数的设计看起来很奇怪。在解释这个设计之前，让我们先在下面的<code>PositionalEncoding</code>类中实现它。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;位置编码&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_hiddens, dropout, max_len=<span class="number">1000</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="comment"># 创建一个足够长的P</span></span><br><span class="line">        self.P = torch.zeros((<span class="number">1</span>, max_len, num_hiddens))</span><br><span class="line">        X = torch.arange(max_len, dtype=torch.float32).reshape(</span><br><span class="line">            -<span class="number">1</span>, <span class="number">1</span>) / torch.<span class="built_in">pow</span>(<span class="number">10000</span>, torch.arange(</span><br><span class="line">            <span class="number">0</span>, num_hiddens, <span class="number">2</span>, dtype=torch.float32) / num_hiddens)</span><br><span class="line">        self.P[:, :, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(X)</span><br><span class="line">        self.P[:, :, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(X)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        X = X + self.P[:, :X.shape[<span class="number">1</span>], :].to(X.device)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(X)</span><br></pre></td></tr></table></figure><p>在位置嵌入矩阵$\mathbf{P}$中，行代表词元在序列中的位置，列代表位置编码的不同维度。从下面的例子中可以看到位置嵌入矩阵的第$6$列和第$7$列的频率高于第$8$列和第$9$列。第$6$列和第$7$列之间的偏移量（第$8$列和第$9$列相同）是由于正弦函数和余弦函数的交替。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">encoding_dim, num_steps = <span class="number">32</span>, <span class="number">60</span></span><br><span class="line">pos_encoding = PositionalEncoding(encoding_dim, <span class="number">0</span>)</span><br><span class="line">pos_encoding.<span class="built_in">eval</span>()</span><br><span class="line">X = pos_encoding(torch.zeros((<span class="number">1</span>, num_steps, encoding_dim)))</span><br><span class="line">P = pos_encoding.P[:, :X.shape[<span class="number">1</span>], :]</span><br><span class="line">d2l.plot(torch.arange(num_steps), P[<span class="number">0</span>, :, <span class="number">6</span>:<span class="number">10</span>].T, xlabel=<span class="string">&#x27;Row (position)&#x27;</span>,</span><br><span class="line">         figsize=(<span class="number">6</span>, <span class="number">2.5</span>), legend=[<span class="string">&quot;Col %d&quot;</span> % d <span class="keyword">for</span> d <span class="keyword">in</span> torch.arange(<span class="number">6</span>, <span class="number">10</span>)])</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_self-attention-and-positional-encoding.svg" alt="row（pos）"></p><h4 id="6-3-1-绝对位置信息"><a href="#6-3-1-绝对位置信息" class="headerlink" title="6.3.1. 绝对位置信息"></a>6.3.1. 绝对位置信息</h4><p>为了明白沿着编码维度单调降低的频率与绝对位置信息的关系，让我们打印出$0, 1, \ldots, 7$的[<strong>二进制表示</strong>]形式。正如所看到的，每个数字、每两个数字和每四个数字上的比特值在第一个最低位、第二个最低位和第三个最低位上分别交替。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;i&#125;</span>的二进制是：<span class="subst">&#123;i:&gt;03b&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line"><span class="number">0</span>的二进制是：<span class="number">000</span></span><br><span class="line"><span class="number">1</span>的二进制是：001</span><br><span class="line"><span class="number">2</span>的二进制是：010</span><br><span class="line"><span class="number">3</span>的二进制是：011</span><br><span class="line"><span class="number">4</span>的二进制是：<span class="number">100</span></span><br><span class="line"><span class="number">5</span>的二进制是：<span class="number">101</span></span><br><span class="line"><span class="number">6</span>的二进制是：<span class="number">110</span></span><br><span class="line"><span class="number">7</span>的二进制是：<span class="number">111</span></span><br></pre></td></tr></table></figure><p>在二进制表示中，较高比特位的交替频率低于较低比特位， 与下面的热图所示相似，只是位置编码通过使用三角函数在编码维度上降低频率。 由于输出是浮点数，因此此类连续表示比二进制表示法更节省空间。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">P = P[<span class="number">0</span>, :, :].unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>)</span><br><span class="line">d2l.show_heatmaps(P, xlabel=<span class="string">&#x27;Column (encoding dimension)&#x27;</span>,</span><br><span class="line">                  ylabel=<span class="string">&#x27;Row (position)&#x27;</span>, figsize=(<span class="number">3.5</span>, <span class="number">4</span>), cmap=<span class="string">&#x27;Blues&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_self-attention-and-positional-encoding_2.svg" alt=""></p><h4 id="6-3-2-相对位置信息"><a href="#6-3-2-相对位置信息" class="headerlink" title="6.3.2. 相对位置信息"></a>6.3.2. 相对位置信息</h4><p>除了捕获绝对位置信息之外，上述的位置编码还允许模型学习得到输入序列中相对位置信息。这是因为对于任何确定的位置偏移$\delta$，位置$i + \delta$处的位置编码可以线性投影位置$i$处的位置编码来表示。</p><p>这种投影的数学解释是，令$\omega_j = 1/10000^{2j/d}$，对于任何确定的位置偏移$\delta$，<a href="#63-位置编码">位置编码</a>中的任何一对$(p_{i, 2j}, p_{i, 2j+1})$都可以线性投影到$(p_{i+\delta, 2j}, p_{i+\delta, 2j+1})$：</p><script type="math/tex; mode=display">\begin{aligned}&\begin{bmatrix} \cos(\delta \omega_j) & \sin(\delta \omega_j) \\  -\sin(\delta \omega_j) & \cos(\delta \omega_j) \\ \end{bmatrix}\begin{bmatrix} p_{i, 2j} \\  p_{i, 2j+1} \\ \end{bmatrix}\\=&\begin{bmatrix} \cos(\delta \omega_j) \sin(i \omega_j) + \sin(\delta \omega_j) \cos(i \omega_j) \\  -\sin(\delta \omega_j) \sin(i \omega_j) + \cos(\delta \omega_j) \cos(i \omega_j) \\ \end{bmatrix}\\=&\begin{bmatrix} \sin\left((i+\delta) \omega_j\right) \\  \cos\left((i+\delta) \omega_j\right) \\ \end{bmatrix}\\=& \begin{bmatrix} p_{i+\delta, 2j} \\  p_{i+\delta, 2j+1} \\ \end{bmatrix},\end{aligned}</script><p>$2\times 2$投影矩阵不依赖于任何位置的索引$i$。</p><h2 id="7-Transformer"><a href="#7-Transformer" class="headerlink" title="7. Transformer"></a>7. Transformer</h2><p>在<a href="#62-比较卷积神经网络循环神经网络和自注意力">6.2</a>中比较了卷积神经网络（CNN）、循环神经网络（RNN）和自注意力（self-attention）。值得注意的是，自注意力同时具有并行计算和最短的最大路径长度这两个优势。因此，使用自注意力来设计深度架构是很有吸引力的。对比之前仍然依赖循环神经网络实现输入表示的自注意力模型 <code>Cheng.Dong.Lapata.2016,Lin.Feng.Santos.ea.2017,Paulus.Xiong.Socher.2017</code>，Transformer模型完全基于注意力机制，没有任何卷积层或循环神经网络层 <code>Vaswani.Shazeer.Parmar.ea.2017</code>。尽管Transformer最初是应用于在文本数据上的序列到序列学习，但现在已经推广到各种现代的深度学习中，例如语言、视觉、语音和强化学习领域。</p><h3 id="7-1-模型"><a href="#7-1-模型" class="headerlink" title="7.1. 模型"></a>7.1. 模型</h3><p>Transformer作为编码器－解码器架构的一个实例，其整体架构图如下所示。正如所见到的，Transformer是由编码器和解码器组成的。与<a href="#41-模型">带有Bahdanau注意力的循环神经网络编码器-解码器模型</a>中基于Bahdanau注意力实现的序列到序列的学习相比，Transformer的编码器和解码器是基于自注意力的模块叠加而成的，源（输入）序列和目标（输出）序列的<em>嵌入</em>（embedding）表示将加上<em>位置编码</em>（positional encoding），再分别输入到编码器和解码器中。</p><p><img src="/assets/post_img/article74/transformer.svg" alt="transformer架构"></p><p>上图概述了Transformer的架构。从宏观角度来看，Transformer的编码器是由多个相同的层叠加而成的，每个层都有两个子层（子层表示为$\mathrm{sublayer}$）。第一个子层是<em>多头自注意力</em>（multi-head self-attention）池化；第二个子层是<em>基于位置的前馈网络</em>（positionwise feed-forward network）。具体来说，在计算编码器的自注意力时，查询、键和值都来自前一个编码器层的输出。受ResNet中残差网络的启发，每个子层都采用了<em>残差连接</em>（residual connection）。在Transformer中，对于序列中任何位置的任何输入$\mathbf{x} \in \mathbb{R}^d$，都要求满足$\mathrm{sublayer}(\mathbf{x}) \in \mathbb{R}^d$，以便残差连接满足$\mathbf{x} + \mathrm{sublayer}(\mathbf{x}) \in \mathbb{R}^d$。在残差连接的加法计算之后，紧接着应用<em>层规范化</em>（layer normalization）<code>Ba.Kiros.Hinton.2016</code>。因此，输入序列对应的每个位置，Transformer编码器都将输出一个$d$维表示向量。</p><p>Transformer解码器也是由多个相同的层叠加而成的，并且层中使用了残差连接和层规范化。除了编码器中描述的两个子层之外，解码器还在这两个子层之间插入了第三个子层，称为<em>编码器－解码器注意力</em>（encoder-decoder attention）层。在编码器－解码器注意力中，查询来自前一个解码器层的输出，而键和值来自整个编码器的输出。在解码器自注意力中，查询、键和值都来自上一个解码器层的输出。但是，解码器中的每个位置只能考虑该位置之前的所有位置。这种<em>掩蔽</em>（masked）注意力保留了<em>自回归</em>（auto-regressive）属性，确保预测仅依赖于已生成的输出词元。</p><p>在此之前已经描述并实现了基于缩放点积多头注意力和位置编码。接下来将实现Transformer模型的剩余部分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><h3 id="7-2-基于位置的前馈网络"><a href="#7-2-基于位置的前馈网络" class="headerlink" title="7.2. 基于位置的前馈网络"></a>7.2. 基于位置的前馈网络</h3><p>基于位置的前馈网络对序列中的所有位置的表示进行变换时使用的是同一个多层感知机（MLP），这就是称前馈网络是<em>基于位置的</em>（positionwise）的原因。在下面的实现中，输入<code>X</code>的形状（批量大小，时间步数或序列长度，隐单元数或特征维度）将被一个两层的感知机转换成形状为（批量大小，时间步数，<code>ffn_num_outputs</code>）的输出张量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionWiseFFN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;基于位置的前馈网络&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs,</span></span></span><br><span class="line"><span class="params"><span class="function">                 **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PositionWiseFFN, self).__init__(**kwargs)</span><br><span class="line">        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.dense2(self.relu(self.dense1(X)))</span><br></pre></td></tr></table></figure><p>下面的例子显示，改变张量的最里层维度的尺寸，会改变成基于位置的前馈网络的输出尺寸。因为用同一个多层感知机对所有位置上的输入进行变换，所以当所有这些位置的输入相同时，它们的输出也是相同的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ffn = PositionWiseFFN(<span class="number">4</span>, <span class="number">4</span>, <span class="number">8</span>)</span><br><span class="line">ffn.<span class="built_in">eval</span>()</span><br><span class="line">ffn(torch.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)))[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[-<span class="number">1.2386</span>, -<span class="number">0.9917</span>,  <span class="number">0.4708</span>, -<span class="number">0.4516</span>, -<span class="number">0.3069</span>,  <span class="number">0.0447</span>,  <span class="number">0.9740</span>,  <span class="number">0.1313</span>],</span><br><span class="line">        [-<span class="number">1.2386</span>, -<span class="number">0.9917</span>,  <span class="number">0.4708</span>, -<span class="number">0.4516</span>, -<span class="number">0.3069</span>,  <span class="number">0.0447</span>,  <span class="number">0.9740</span>,  <span class="number">0.1313</span>],</span><br><span class="line">        [-<span class="number">1.2386</span>, -<span class="number">0.9917</span>,  <span class="number">0.4708</span>, -<span class="number">0.4516</span>, -<span class="number">0.3069</span>,  <span class="number">0.0447</span>,  <span class="number">0.9740</span>,  <span class="number">0.1313</span>]],</span><br><span class="line">       grad_fn=&lt;SelectBackward0&gt;)</span><br></pre></td></tr></table></figure><h3 id="7-3-残差连接和层规范化"><a href="#7-3-残差连接和层规范化" class="headerlink" title="7.3. 残差连接和层规范化"></a>7.3. 残差连接和层规范化</h3><p>现在让我们关注<em>加法和规范化</em>（add&amp;norm）组件。正如在本节开头所述，这是由残差连接和紧随其后的层规范化组成的。两者都是构建有效的深度架构的关键。</p><p>“批量规范化”章节中解释了在一个小批量的样本内基于批量规范化对数据进行重新中心化和重新缩放的调整。层规范化和批量规范化的目标相同，但层规范化是基于特征维度进行规范化。尽管批量规范化在计算机视觉中被广泛应用，但在自然语言处理任务中（输入通常是变长序列）批量规范化通常不如层规范化的效果好。</p><p>以下代码对比不同维度的层规范化和批量规范化的效果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">ln = nn.LayerNorm(<span class="number">2</span>)</span><br><span class="line">bn = nn.BatchNorm1d(<span class="number">2</span>)</span><br><span class="line">X = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">3</span>]], dtype=torch.float32)</span><br><span class="line"><span class="comment"># 在训练模式下计算X的均值和方差</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;layer norm:&#x27;</span>, ln(X), <span class="string">&#x27;\nbatch norm:&#x27;</span>, bn(X))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># layer norm: tensor([[-1.0000,  1.0000],</span></span><br><span class="line"><span class="comment">#         [-1.0000,  1.0000]], grad_fn=&lt;NativeLayerNormBackward0&gt;)</span></span><br><span class="line"><span class="comment"># batch norm: tensor([[-1.0000, -1.0000],</span></span><br><span class="line"><span class="comment">#         [ 1.0000,  1.0000]], grad_fn=&lt;NativeBatchNormBackward0&gt;)</span></span><br></pre></td></tr></table></figure><p>现在我们可以使用残差连接和层规范化来实现AddNorm类。暂退法也被作为正则化方法使用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AddNorm</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;残差连接后进行层规范化&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, normalized_shape, dropout, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(AddNorm, self).__init__(**kwargs)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.ln = nn.LayerNorm(normalized_shape)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, Y</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.ln(self.dropout(Y) + X)</span><br></pre></td></tr></table></figure><p>残差连接要求两个输入的形状相同，以便加法操作后输出张量的形状相同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">add_norm = AddNorm([<span class="number">3</span>, <span class="number">4</span>], <span class="number">0.5</span>)</span><br><span class="line">add_norm.<span class="built_in">eval</span>()</span><br><span class="line">add_norm(torch.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)), torch.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))).shape</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure><h3 id="7-4-编码器"><a href="#7-4-编码器" class="headerlink" title="7.4. 编码器"></a>7.4. 编码器</h3><p>有了组成transformer编码器的基础组件，现在可以先实现编码器中的一个层。下面的EncoderBlock类包含两个子层：多头自注意力和基于位置的前馈网络，这两个子层都使用了残差连接和紧随的层规范化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;transformer编码器块&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, key_size, query_size, value_size, num_hiddens,</span></span></span><br><span class="line"><span class="params"><span class="function">                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span></span></span><br><span class="line"><span class="params"><span class="function">                 dropout, use_bias=<span class="literal">False</span>, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(EncoderBlock, self).__init__(**kwargs)</span><br><span class="line">        self.attention = d2l.MultiHeadAttention(</span><br><span class="line">            key_size, query_size, value_size, num_hiddens, num_heads, dropout,</span><br><span class="line">            use_bias)</span><br><span class="line">        self.addnorm1 = AddNorm(norm_shape, dropout)</span><br><span class="line">        self.ffn = PositionWiseFFN(</span><br><span class="line">            ffn_num_input, ffn_num_hiddens, num_hiddens)</span><br><span class="line">        self.addnorm2 = AddNorm(norm_shape, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, valid_lens</span>):</span></span><br><span class="line">        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))</span><br><span class="line">        <span class="keyword">return</span> self.addnorm2(Y, self.ffn(Y))</span><br></pre></td></tr></table></figure><p>正如我们所看到的，transformer编码器中的任何层都不会改变其输入的形状。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">X = torch.ones((<span class="number">2</span>, <span class="number">100</span>, <span class="number">24</span>))</span><br><span class="line">valid_lens = torch.tensor([<span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">encoder_blk = EncoderBlock(<span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, [<span class="number">100</span>, <span class="number">24</span>], <span class="number">24</span>, <span class="number">48</span>, <span class="number">8</span>, <span class="number">0.5</span>)</span><br><span class="line">encoder_blk.<span class="built_in">eval</span>()</span><br><span class="line">encoder_blk(X, valid_lens).shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># torch.Size([2, 100, 24])</span></span><br></pre></td></tr></table></figure><p>下面实现的Transformer编码器的代码中，堆叠了<code>num_layers</code>个<code>EncoderBlock</code>类的实例。由于这里使用的是值范围在$-1$和$1$之间的固定位置编码，因此通过学习得到的输入的嵌入表示的值需要先乘以嵌入维度的平方根进行重新缩放，然后再与位置编码相加。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerEncoder</span>(<span class="params">d2l.Encoder</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;transformer编码器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, key_size, query_size, value_size,</span></span></span><br><span class="line"><span class="params"><span class="function">                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,</span></span></span><br><span class="line"><span class="params"><span class="function">                 num_heads, num_layers, dropout, use_bias=<span class="literal">False</span>, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(TransformerEncoder, self).__init__(**kwargs)</span><br><span class="line">        self.num_hiddens = num_hiddens</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, num_hiddens)</span><br><span class="line">        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)</span><br><span class="line">        self.blks = nn.Sequential()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            self.blks.add_module(<span class="string">&quot;block&quot;</span>+<span class="built_in">str</span>(i),</span><br><span class="line">                EncoderBlock(key_size, query_size, value_size, num_hiddens,</span><br><span class="line">                             norm_shape, ffn_num_input, ffn_num_hiddens,</span><br><span class="line">                             num_heads, dropout, use_bias))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, valid_lens, *args</span>):</span></span><br><span class="line">        <span class="comment"># 因为位置编码值在-1和1之间，</span></span><br><span class="line">        <span class="comment"># 因此嵌入值乘以嵌入维度的平方根进行缩放，</span></span><br><span class="line">        <span class="comment"># 然后再与位置编码相加。</span></span><br><span class="line">        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))</span><br><span class="line">        self.attention_weights = [<span class="literal">None</span>] * <span class="built_in">len</span>(self.blks)</span><br><span class="line">        <span class="keyword">for</span> i, blk <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.blks):</span><br><span class="line">            X = blk(X, valid_lens)</span><br><span class="line">            self.attention_weights[</span><br><span class="line">                i] = blk.attention.attention.attention_weights</span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><p>下面我们指定了超参数来创建一个两层的transformer编码器。 Transformer编码器输出的形状是（批量大小，时间步数目，num_hiddens）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">encoder = TransformerEncoder(</span><br><span class="line">    <span class="number">200</span>, <span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, [<span class="number">100</span>, <span class="number">24</span>], <span class="number">24</span>, <span class="number">48</span>, <span class="number">8</span>, <span class="number">2</span>, <span class="number">0.5</span>)</span><br><span class="line">encoder.<span class="built_in">eval</span>()</span><br><span class="line">encoder(torch.ones((<span class="number">2</span>, <span class="number">100</span>), dtype=torch.long), valid_lens).shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># torch.Size([2, 100, 24])</span></span><br></pre></td></tr></table></figure><h3 id="7-5-解码器"><a href="#7-5-解码器" class="headerlink" title="7.5. 解码器"></a>7.5. 解码器</h3><p>如模型图所示，Transformer解码器也是由多个相同的层组成。在<code>DecoderBlock</code>类中实现的每个层包含了三个子层：解码器自注意力、“编码器-解码器”注意力和基于位置的前馈网络。这些子层也都被残差连接和紧随的层规范化围绕。</p><p>正如在本节前面所述，在掩蔽多头解码器自注意力层（第一个子层）中，查询、键和值都来自上一个解码器层的输出。关于<em>序列到序列模型</em>（sequence-to-sequence model），在训练阶段，其输出序列的所有位置（时间步）的词元都是已知的；然而，在预测阶段，其输出序列的词元是逐个生成的。因此，在任何解码器时间步中，只有生成的词元才能用于解码器的自注意力计算中。为了在解码器中保留自回归的属性，其掩蔽自注意力设定了参数<code>dec_valid_lens</code>，以便任何查询都只会与解码器中所有已经生成词元的位置（即直到该查询位置为止）进行注意力计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;解码器中第i个块&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, key_size, query_size, value_size, num_hiddens,</span></span></span><br><span class="line"><span class="params"><span class="function">                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span></span></span><br><span class="line"><span class="params"><span class="function">                 dropout, i, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(DecoderBlock, self).__init__(**kwargs)</span><br><span class="line">        self.i = i</span><br><span class="line">        self.attention1 = d2l.MultiHeadAttention(</span><br><span class="line">            key_size, query_size, value_size, num_hiddens, num_heads, dropout)</span><br><span class="line">        self.addnorm1 = AddNorm(norm_shape, dropout)</span><br><span class="line">        self.attention2 = d2l.MultiHeadAttention(</span><br><span class="line">            key_size, query_size, value_size, num_hiddens, num_heads, dropout)</span><br><span class="line">        self.addnorm2 = AddNorm(norm_shape, dropout)</span><br><span class="line">        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens,</span><br><span class="line">                                   num_hiddens)</span><br><span class="line">        self.addnorm3 = AddNorm(norm_shape, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, state</span>):</span></span><br><span class="line">        enc_outputs, enc_valid_lens = state[<span class="number">0</span>], state[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 训练阶段，输出序列的所有词元都在同一时间处理，</span></span><br><span class="line">        <span class="comment"># 因此state[2][self.i]初始化为None。</span></span><br><span class="line">        <span class="comment"># 预测阶段，输出序列是通过词元一个接着一个解码的，</span></span><br><span class="line">        <span class="comment"># 因此state[2][self.i]包含着直到当前时间步第i个块解码的输出表示</span></span><br><span class="line">        <span class="keyword">if</span> state[<span class="number">2</span>][self.i] <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            key_values = X</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            key_values = torch.cat((state[<span class="number">2</span>][self.i], X), axis=<span class="number">1</span>)</span><br><span class="line">        state[<span class="number">2</span>][self.i] = key_values</span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            batch_size, num_steps, _ = X.shape</span><br><span class="line">            <span class="comment"># dec_valid_lens的开头:(batch_size,num_steps),</span></span><br><span class="line">            <span class="comment"># 其中每一行是[1,2,...,num_steps]</span></span><br><span class="line">            dec_valid_lens = torch.arange(</span><br><span class="line">                <span class="number">1</span>, num_steps + <span class="number">1</span>, device=X.device).repeat(batch_size, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            dec_valid_lens = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 自注意力</span></span><br><span class="line">        X2 = self.attention1(X, key_values, key_values, dec_valid_lens)</span><br><span class="line">        Y = self.addnorm1(X, X2)</span><br><span class="line">        <span class="comment"># 编码器－解码器注意力。</span></span><br><span class="line">        <span class="comment"># enc_outputs的开头:(batch_size,num_steps,num_hiddens)</span></span><br><span class="line">        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)</span><br><span class="line">        Z = self.addnorm2(Y, Y2)</span><br><span class="line">        <span class="keyword">return</span> self.addnorm3(Z, self.ffn(Z)), state</span><br></pre></td></tr></table></figure><p>为了便于在“编码器－解码器”注意力中进行缩放点积计算和残差连接中进行加法计算，编码器和解码器的特征维度都是num_hiddens。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">decoder_blk = DecoderBlock(<span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, [<span class="number">100</span>, <span class="number">24</span>], <span class="number">24</span>, <span class="number">48</span>, <span class="number">8</span>, <span class="number">0.5</span>, <span class="number">0</span>)</span><br><span class="line">decoder_blk.<span class="built_in">eval</span>()</span><br><span class="line">X = torch.ones((<span class="number">2</span>, <span class="number">100</span>, <span class="number">24</span>))</span><br><span class="line">state = [encoder_blk(X, valid_lens), valid_lens, [<span class="literal">None</span>]]</span><br><span class="line">decoder_blk(X, state)[<span class="number">0</span>].shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># torch.Size([2, 100, 24])</span></span><br></pre></td></tr></table></figure><p>现在我们构建了由num_layers个DecoderBlock实例组成的完整的transformer解码器。最后，通过一个全连接层计算所有vocab_size个可能的输出词元的预测值。解码器的自注意力权重和编码器解码器注意力权重都被存储下来，方便日后可视化的需要。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerDecoder</span>(<span class="params">d2l.AttentionDecoder</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, key_size, query_size, value_size,</span></span></span><br><span class="line"><span class="params"><span class="function">                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,</span></span></span><br><span class="line"><span class="params"><span class="function">                 num_heads, num_layers, dropout, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(TransformerDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.num_hiddens = num_hiddens</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, num_hiddens)</span><br><span class="line">        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)</span><br><span class="line">        self.blks = nn.Sequential()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            self.blks.add_module(<span class="string">&quot;block&quot;</span>+<span class="built_in">str</span>(i),</span><br><span class="line">                DecoderBlock(key_size, query_size, value_size, num_hiddens,</span><br><span class="line">                             norm_shape, ffn_num_input, ffn_num_hiddens,</span><br><span class="line">                             num_heads, dropout, i))</span><br><span class="line">        self.dense = nn.Linear(num_hiddens, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_state</span>(<span class="params">self, enc_outputs, enc_valid_lens, *args</span>):</span></span><br><span class="line">        <span class="keyword">return</span> [enc_outputs, enc_valid_lens, [<span class="literal">None</span>] * self.num_layers]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, state</span>):</span></span><br><span class="line">        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))</span><br><span class="line">        self._attention_weights = [[<span class="literal">None</span>] * <span class="built_in">len</span>(self.blks) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span> (<span class="number">2</span>)]</span><br><span class="line">        <span class="keyword">for</span> i, blk <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.blks):</span><br><span class="line">            X, state = blk(X, state)</span><br><span class="line">            <span class="comment"># 解码器自注意力权重</span></span><br><span class="line">            self._attention_weights[<span class="number">0</span>][</span><br><span class="line">                i] = blk.attention1.attention.attention_weights</span><br><span class="line">            <span class="comment"># “编码器－解码器”自注意力权重</span></span><br><span class="line">            self._attention_weights[<span class="number">1</span>][</span><br><span class="line">                i] = blk.attention2.attention.attention_weights</span><br><span class="line">        <span class="keyword">return</span> self.dense(X), state</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">attention_weights</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self._attention_weights</span><br></pre></td></tr></table></figure><h3 id="7-6-训练"><a href="#7-6-训练" class="headerlink" title="7.6. 训练"></a>7.6. 训练</h3><p>依照Transformer架构来实例化编码器－解码器模型。在这里，指定Transformer的编码器和解码器都是2层，都使用4头注意力。为了进行序列到序列的学习，下面在“英语－法语”机器翻译数据集上训练Transformer模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens, num_layers, dropout, batch_size, num_steps = <span class="number">32</span>, <span class="number">2</span>, <span class="number">0.1</span>, <span class="number">64</span>, <span class="number">10</span></span><br><span class="line">lr, num_epochs, device = <span class="number">0.005</span>, <span class="number">200</span>, d2l.try_gpu()</span><br><span class="line">ffn_num_input, ffn_num_hiddens, num_heads = <span class="number">32</span>, <span class="number">64</span>, <span class="number">4</span></span><br><span class="line">key_size, query_size, value_size = <span class="number">32</span>, <span class="number">32</span>, <span class="number">32</span></span><br><span class="line">norm_shape = [<span class="number">32</span>]</span><br><span class="line"></span><br><span class="line">train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)</span><br><span class="line"></span><br><span class="line">encoder = TransformerEncoder(</span><br><span class="line">    <span class="built_in">len</span>(src_vocab), key_size, query_size, value_size, num_hiddens,</span><br><span class="line">    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span><br><span class="line">    num_layers, dropout)</span><br><span class="line">decoder = TransformerDecoder(</span><br><span class="line">    <span class="built_in">len</span>(tgt_vocab), key_size, query_size, value_size, num_hiddens,</span><br><span class="line">    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span><br><span class="line">    num_layers, dropout)</span><br><span class="line">net = d2l.EncoderDecoder(encoder, decoder)</span><br><span class="line">d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)</span><br></pre></td></tr></table></figure><p>训练结束后，使用transformer模型将一些英语句子翻译成法语，并且计算它们的BLEU分数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">engs = [<span class="string">&#x27;go .&#x27;</span>, <span class="string">&quot;i lost .&quot;</span>, <span class="string">&#x27;he\&#x27;s calm .&#x27;</span>, <span class="string">&#x27;i\&#x27;m home .&#x27;</span>]</span><br><span class="line">fras = [<span class="string">&#x27;va !&#x27;</span>, <span class="string">&#x27;j\&#x27;ai perdu .&#x27;</span>, <span class="string">&#x27;il est calme .&#x27;</span>, <span class="string">&#x27;je suis chez moi .&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> eng, fra <span class="keyword">in</span> <span class="built_in">zip</span>(engs, fras):</span><br><span class="line">    translation, dec_attention_weight_seq = d2l.predict_seq2seq(</span><br><span class="line">        net, eng, src_vocab, tgt_vocab, num_steps, device, <span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;eng&#125;</span> =&gt; <span class="subst">&#123;translation&#125;</span>, &#x27;</span>,</span><br><span class="line">          <span class="string">f&#x27;bleu <span class="subst">&#123;d2l.bleu(translation, fra, k=<span class="number">2</span>):<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># go . =&gt; va !,  bleu 1.000</span></span><br><span class="line"><span class="comment"># i lost . =&gt; je suis avons été battues .,  bleu 0.000</span></span><br><span class="line"><span class="comment"># he&#x27;s calm . =&gt; il est malade .,  bleu 0.658</span></span><br><span class="line"><span class="comment"># i&#x27;m home . =&gt; je suis chez moi .,  bleu 1.000</span></span><br></pre></td></tr></table></figure><p>当进行最后一个英语到法语的句子翻译工作时，让我们可视化transformer的注意力权重。编码器自注意力权重的形状为（编码器层数，注意力头数，num_steps或查询的数目，num_steps或“键－值”对的数目）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">enc_attention_weights = torch.cat(net.encoder.attention_weights, <span class="number">0</span>).reshape((num_layers, num_heads,</span><br><span class="line">    -<span class="number">1</span>, num_steps))</span><br><span class="line">enc_attention_weights.shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># torch.Size([2, 4, 10, 10])</span></span><br></pre></td></tr></table></figure><p>在编码器的自注意力中，查询和键都来自相同的输入序列。因为填充词元是不携带信息的，因此通过指定输入序列的有效长度可以避免查询与使用填充词元的位置计算注意力。接下来，将逐行呈现两层多头注意力的权重。每个注意力头都根据查询、键和值的不同的表示子空间来表示不同的注意力。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(</span><br><span class="line">    enc_attention_weights.cpu(), xlabel=<span class="string">&#x27;Key positions&#x27;</span>,</span><br><span class="line">    ylabel=<span class="string">&#x27;Query positions&#x27;</span>, titles=[<span class="string">&#x27;Head %d&#x27;</span> % i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">5</span>)],</span><br><span class="line">    figsize=(<span class="number">7</span>, <span class="number">3.5</span>))</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_transformer_1.svg" alt="o1"></p><p>为了可视化解码器的自注意力权重和“编码器－解码器”的注意力权重，我们需要完成更多的数据操作工作。例如用零填充被掩蔽住的注意力权重。值得注意的是，解码器的自注意力权重和“编码器－解码器”的注意力权重都有相同的查询：即以<em>序列开始词元</em>（beginning-of-sequence,BOS）打头，再与后续输出的词元共同组成序列。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">dec_attention_weights_2d = [head[<span class="number">0</span>].tolist()</span><br><span class="line">                            <span class="keyword">for</span> step <span class="keyword">in</span> dec_attention_weight_seq</span><br><span class="line">                            <span class="keyword">for</span> attn <span class="keyword">in</span> step <span class="keyword">for</span> blk <span class="keyword">in</span> attn <span class="keyword">for</span> head <span class="keyword">in</span> blk]</span><br><span class="line">dec_attention_weights_filled = torch.tensor(</span><br><span class="line">    pd.DataFrame(dec_attention_weights_2d).fillna(<span class="number">0.0</span>).values)</span><br><span class="line">dec_attention_weights = dec_attention_weights_filled.reshape((-<span class="number">1</span>, <span class="number">2</span>, num_layers, num_heads, num_steps))</span><br><span class="line">dec_self_attention_weights, dec_inter_attention_weights = \</span><br><span class="line">    dec_attention_weights.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">4</span>)</span><br><span class="line">dec_self_attention_weights.shape, dec_inter_attention_weights.shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># (torch.Size([2, 4, 6, 10]), torch.Size([2, 4, 6, 10]))</span></span><br></pre></td></tr></table></figure><p>由于解码器自注意力的自回归属性，查询不会对当前位置之后的“键－值”对进行注意力计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plusonetoincludethebeginning-of-sequencetoken</span></span><br><span class="line">d2l.show_heatmaps(</span><br><span class="line">    dec_self_attention_weights[:, :, :, :<span class="built_in">len</span>(translation.split()) + <span class="number">1</span>],</span><br><span class="line">    xlabel=<span class="string">&#x27;Key positions&#x27;</span>, ylabel=<span class="string">&#x27;Query positions&#x27;</span>,</span><br><span class="line">    titles=[<span class="string">&#x27;Head %d&#x27;</span> % i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">5</span>)], figsize=(<span class="number">7</span>, <span class="number">3.5</span>))</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_transformer_2.svg" alt="o2"></p><p>与编码器的自注意力的情况类似，通过指定输入序列的有效长度，输出序列的查询不会与输入序列中填充位置的词元进行注意力计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(</span><br><span class="line">    dec_inter_attention_weights, xlabel=<span class="string">&#x27;Key positions&#x27;</span>,</span><br><span class="line">    ylabel=<span class="string">&#x27;Query positions&#x27;</span>, titles=[<span class="string">&#x27;Head %d&#x27;</span> % i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">5</span>)],</span><br><span class="line">    figsize=(<span class="number">7</span>, <span class="number">3.5</span>))</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_transformer_3.svg" alt="o3"></p><p>尽管transformer架构是为了“序列到序列”的学习而提出的，但正如我们将在本书后面提及的那样，transformer编码器或transformer解码器通常被单独用于不同的深度学习任务中。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;p&gt;灵长类动物的视觉系统接受了大量的感官输入，这些感官输入远远超过了大脑能够完全处理的程度。然而，并非所有刺激的影响都是相等的。意识的聚集和专注使灵长类动物能够在复杂的视觉环境中将注意力引向感兴趣的物体，例如猎物和天敌。只关注一小部分信息的能力对进化更加有意义，使人类得以生存和成功。&lt;/p&gt;
&lt;p&gt;自19世纪以来，科学家们一直致力于研究认知神经科学领域的注意力。本章的很多章节将涉及到一些研究。&lt;br&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://silencezheng.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>导向滤波</title>
    <link href="http://silencezheng.top/2022/11/04/article73/"/>
    <id>http://silencezheng.top/2022/11/04/article73/</id>
    <published>2022-11-04T13:27:06.000Z</published>
    <updated>2022-11-04T13:28:52.059Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>导向滤波学习，也可以说是论文笔记了。<br><span id="more"></span></p><h2 id="各向同性（isotropy）滤波与各向异性（anisotropy）滤波"><a href="#各向同性（isotropy）滤波与各向异性（anisotropy）滤波" class="headerlink" title="各向同性（isotropy）滤波与各向异性（anisotropy）滤波"></a>各向同性（isotropy）滤波与各向异性（anisotropy）滤波</h2><p>对图像来说，各向同性滤波就是说滤波器在各方向上的梯度变化是相同的，对各个方向一视同仁的进行滤波。各向异性滤波是指滤波器会对各方向进行区别对待，有选择的进行滤波。</p><p>高斯滤波属于<strong>各向同性滤波</strong>，根据二维高斯的图像就可以看出。</p><p>双边滤波属于<strong>各向异性滤波</strong>，因为它会根据图像梯度变化情况改变滤波器形状。</p><h2 id="导向滤波（Guided-Filter）"><a href="#导向滤波（Guided-Filter）" class="headerlink" title="导向滤波（Guided Filter）"></a>导向滤波（Guided Filter）</h2><p>导向滤波也属于各向异性滤波，也可以作为一种保边（Edge-preserving）滤波算法。</p><blockquote><p>Derived from a local linear model, the guided filter generates the filtering output by considering the content of a guidance image, which can be the input image itself or another different image.</p><p>We demonstrate that the guided filter is both effective and efficient in a great variety of computer vision and computer graphics applications including noise reduction, detail smoothing/enhancement, HDR compression, image matting/feathering, haze removal, and joint upsampling.</p></blockquote><p>导向滤波通过考虑<strong>引导图像</strong>的内容来生成滤波输出，引导图像可以是输入图像本身或另一个不同的图像。[1]中提到了双边滤波相比于导向滤波的两点缺陷，“have unwanted gradient reversal artifacts near edges”和难以进行维持精度的快速计算，这也是导向滤波的优势。</p><p>文中还提到了<strong>联合双边滤波器</strong>（joint bilateral filter），它也是利用了引导图来改善双边滤波<strong>权值不稳定</strong>的问题（双边滤波边缘出现<strong>梯度翻转</strong>现象的原因）。</p><p>文中先定义了一个<strong>通用的平移不变的线性滤波过程</strong>，包括一个引导图像 $\textit{I}$、一个输入图像 $\textit{p}$ 和一个输出图像 $\textit{q}$。</p><script type="math/tex; mode=display">\begin{equation}q_i=\sum_j W_{i j}(I) p_j \tag{1}\end{equation}</script><p>这里 $i$ 和 $j$ 表示的是两个像素，而不是像素的横纵坐标！像素 $i$ 的位置坐标表示为 $\mathbb{x}_i$，$q_i$ 表示输出图的像素 $i$ 处的值，滤波核 $W_{i j}(I)$ 是关于导向图和输入图的函数，当然也与 $i$ 有关。</p><p>联合双边滤波器符合上述的线性滤波过程，其滤波核 $W^{\mathrm{bf}}$ 如下：</p><script type="math/tex; mode=display">\begin{equation}W_{i j}^{\mathrm{bf}}(I)=\frac{1}{K_i} \exp \left(-\frac{\left|\mathbf{x}_i-\mathbf{x}_j\right|^2}{\sigma_{\mathrm{s}}^2}\right) \exp \left(-\frac{\left|I_i-I_j\right|^2}{\sigma_{\mathrm{r}}^2}\right) \tag{2}\end{equation}</script><p>其中，$K_i$是归一化参数，用于保证$\sum_j W_{i j}^{\mathrm{bf}} = 1$，$\sigma_{\mathrm{s}}$ 和 $\sigma_{\mathrm{r}}$ 分别代表空域和值域（原文说值可以是 intensity 或 color）的对应参数，分别用于调整空域和值域的滤波程度（原文为similarity，我理解就和高斯中的标准差类似）。原文说当导向图和输入图相同时，联合双边滤波就退化为双边滤波，我理解里的双边滤波应该是要加两个2倍在分母的，但是效果应该一样。</p><p>下面简单讨论下导向滤波的推导，主要还是学习如何计算。 文中首先假设导向滤波是一个导向图 $I$ 和 输出图 $q$ 间的<strong>局部线性模型</strong>。令 $w_k$ 是 $I$ 中的一个正方形窗口，其中心为像素 $k$，输出 $q$ 为 $I$ 在 $w_k$ 上的一个线性变换：</p><script type="math/tex; mode=display">\begin{equation}q_i=a_k I_i+b_k, \forall i \in \omega_k \tag{3}\end{equation}</script><p>这个稍微想一下就可以理解，假设 $w_k$ 是一个在 $I$ 上移动的九宫格，由于 $q$、$I$ 和 $p$都是尺寸一致的，那么每次移动产生的九个线性变化值就是 $q$ 对应 $I$ 位置上的值。当然这会引出一个问题，即同一个像素可能会被不同的窗口计算出多个值，如何确定最终输出的 $q_i$ 呢？文中提出了一种简单的处理办法，即取所有这些输出的平均。</p><p>$a_k$ 和 $b_k$ 是窗口 $k$ 中的常量线性系数，顺便一提 $w_k$ 的半径定义为 $r$。 关于为何使用局部线性模型，是因为它确保了输出和导向图的边缘一致，具体可以看原文。</p><p>为确定上述的两个线性系数，原文将输出建模为输入减去不想要的内容（噪声、纹理等）:</p><script type="math/tex; mode=display">q_i = p_i - n_i</script><p>然后通过最小化输入和输出的差异来实现，具体来说，最小化如下函数(cost function in $w_k$)：</p><script type="math/tex; mode=display">\begin{equation}E\left(a_k, b_k\right)=\sum_{i \in \omega_k}\left(\left(a_k I_i+b_k-p_i\right)^2+\epsilon a_k^2\right) \tag{4}\end{equation}</script><p>其中 $\epsilon$ 是正则化参数，用于防止 $a_k$ 变得太大。通过线性回归求解$(4)$可得如下：</p><script type="math/tex; mode=display">\begin{equation}a_k=\frac{\frac{1}{|\omega|} \sum_{i \in \omega_k} I_i p_i-\mu_k \bar{p}_k}{\sigma_k^2+\epsilon} \tag{5}\end{equation}</script><script type="math/tex; mode=display">\begin{equation}b_k=\bar{p}_k-a_k \mu_k \tag{6}\end{equation}</script><p>其中 $\mu_k$ 和 $\sigma_k^2$ 分别是导向图在窗口 $k$ 中部分的均值和方差（像素值），$|\omega|$ 是窗口 $k$ 中的像素数，$\bar{p}_k$ 是 输入图 $p$ 在窗口 $k$ 中部分的均值，表示为 $\bar{p}_k=\frac{1}{|\omega|} \sum_{i \in \omega_k} p_i$。</p><p>解释完各个符号的含义，让我们思考将上述的局部线性模型<strong>应用在整幅图像上</strong>，将图像中每一个能放置 $w_k$ 的区域进行运算（考虑将 $I$ 和 $p$ 叠放），并采用之前提到的取均值的方式，可得到滤波器输出如下：</p><script type="math/tex; mode=display">\begin{equation}q_i =\frac{1}{|\omega|} \sum_{k: i \in \omega_k}\left(a_k I_i+b_k\right) \tag{7}\end{equation}</script><script type="math/tex; mode=display">\begin{equation}=\bar{a}_i I_i+\bar{b}_i \tag{8}\end{equation}</script><p>其中$\bar{a}_i=\frac{1}{|\omega|} \sum_{k \in \omega_i} a_k$，$\bar{b}_i=\frac{1}{|\omega|} \sum_{k \in \omega_i} b_k$。</p><p>下面原文对上述输出的保边性进行了一些论证，并指出$(5), (6), (8)$中的关系是在图像滤波中真实存在的，并且这三个关系（公式）可以分别重写为输入的加权和，如下：</p><script type="math/tex; mode=display">a_k=\sum_j A_{k j}(I) p_j \\b_k=\sum_j B_{k j}(I) p_j \\q_i=\sum_j W_{i j}(I) p_j</script><p>其中 $A_{i j}, B_{i j}, W_{i j}$ 为三个仅依赖于导向图 $I$ 的权重，可以看到最终推出了一个输出和导向图与输入的<strong>平移不变线性滤波</strong>：$q_i=\sum_j W_{i j}(I) p_j$。那么我们只需要关心权重如何计算即可：</p><script type="math/tex; mode=display">\begin{equation}W_{i j}(I)=\frac{1}{|\omega|^2} \sum_{k:(i, j) \in \omega_k}\left(1+\frac{\left(I_i-\mu_k\right)\left(I_j-\mu_k\right)}{\sigma_k^2+\epsilon}\right) \tag{9}\end{equation}</script><p>解读一下这个权重，对于给定的输出像素 $i$，我们先寻找重叠图像（$I$ 和 $p$）中包含该像素的所有窗口 $k$，对于每个窗口 $k$，在 $I$ 上计算 $\mu_k$ 和 $\sigma_k^2$ 后代入计算权重，然后求得加权和。由于可以证明 $\sum_j W_{i j}(I)=1$，所以不需要对权重再进行归一化处理。</p><p>总之，导向滤波符合一个平移不变的线性滤波模型，其权重通过导向图计算，最终结合输入图产生输出。</p><p><img src="/assets/post_img/article73/idea.jpeg" alt="idea"></p><h2 id="计算方式"><a href="#计算方式" class="headerlink" title="计算方式"></a>计算方式</h2><p>$(5), (6), (8)$ 的三个方程是导向滤波器的一种定义，令 $f_{mean}$ 表示半径为 $r$ 的均值滤波器，correlation（corr，<strong>相关系数</strong>），variance（var，<strong>方差</strong>）和covariance（cov，<strong>协方差</strong>）分别表示其本身含义。则可给出算法的伪代码如下：</p><p><img src="/assets/post_img/article73/pseudocode-normal.png" alt="normal"></p><p>算法中出现的<code>./ .*</code>等分别代表逐像素除、逐像素乘等操作。</p><h2 id="个人的一点理解"><a href="#个人的一点理解" class="headerlink" title="个人的一点理解"></a>个人的一点理解</h2><p>文中总共提出了两种对导向滤波的定义方式，先是得到全图范围上的线性模型 $q_i=\bar{a}_i I_i+\bar{b}_i$，后又重新整理成线性滤波的形式 $q_i=\sum_j W_{i j}(I) p_j$，并求出了仅依赖导向图的滤波核权重。然后从线性模型和滤波核的角度分别分析了导向滤波Edge-Preserving的性质，考虑 $I \equiv p$ 的情况。</p><h2 id="快速导向滤波（Fast-Guided-Filter）"><a href="#快速导向滤波（Fast-Guided-Filter）" class="headerlink" title="快速导向滤波（Fast Guided Filter）"></a>快速导向滤波（Fast Guided Filter）</h2><p>还没看[3]，先贴一个伪代码，日后需要再回来整理。</p><p><img src="/assets/post_img/article73/pseudocode-fast.png" alt="fast"></p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]Kaiming He, Jian Sun, Xiaoou Tang, Guided Image Filtering. IEEE Transactions on Pattern Analysis and Machine Intelligence, Volume 35, Issue 6, pp. 1397-1409, June 2013<br>[2]<a href="https://zhuanlan.zhihu.com/p/161666126">https://zhuanlan.zhihu.com/p/161666126</a><br>[3]<a href="https://arxiv.org/abs/1505.00996v1">https://arxiv.org/abs/1505.00996v1</a><br>[4]<a href="http://kaiminghe.com/eccv10/">http://kaiminghe.com/eccv10/</a><br>[5]Guided Image Filtering, by Kaiming He, Jian Sun, and Xiaoou Tang, in ECCV 2010 (Oral).</p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于 <a href="https://silencezheng.top">silencezheng.top</a>。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;导向滤波学习，也可以说是论文笔记了。&lt;br&gt;</summary>
    
    
    
    
    <category term="计算机视觉" scheme="http://silencezheng.top/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>目标检测常用评价指标</title>
    <link href="http://silencezheng.top/2022/10/24/article72/"/>
    <id>http://silencezheng.top/2022/10/24/article72/</id>
    <published>2022-10-23T16:50:28.000Z</published>
    <updated>2022-10-23T16:52:21.268Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>关于目标检测领域的常用评价指标进行一些个人解读，欢迎批评指正。</p><p>评价指标系列第二篇～上一次说了语义分割的内容，这次来搞目标检测，看了十余篇内容后终于搞明白些了，记录一下。本文说的内容比较浅，一方面是笔者水平实在菜鸡，另一方面写文的主要目的是让不了解目标检测的人读完能上手去做，而不用查三十个资料后再开始，就像博客的Slogan一样，“帮你在学习技术的路上节约半小时”。<br><span id="more"></span></p><h2 id="前置理解"><a href="#前置理解" class="headerlink" title="前置理解"></a>前置理解</h2><p>上一篇中，介绍了混淆矩阵和一些语义分割常用评价指标，例如P、R、IoU，这些在目标检测中都会用到，但是相对于语义分割来说，更加抽象一些。</p><p>目标检测对我个人来说是一个相对陌生的领域，所以本文准备先从目标检测的一般过程说起。首先，目标检测中的基本单元是框，我把Ground Truth称为<strong>真实框</strong>（标签），把Prediction称为<strong>预测框</strong>。框中的内容可能是<strong>目标</strong>（需要），也可能是<strong>背景</strong>（不需要）。</p><p>对于目标检测来说，模型需要完成的任务有两个，一是产生目标的预测框，二是对框内目标的类别进行预测，这又称为<strong>回归分支</strong>（连续）和<strong>分类分支</strong>（离散）。模型的预测输出通常如下所示（假设三分类，<strong>实际上不同模型的输出格式也不尽相同的</strong>。）：</p><script type="math/tex; mode=display">\mathrm{y}=\left[\begin{array}{l}\mathrm{p}_{\mathrm{c}} \\\mathrm{b}_{\mathrm{x}} \\\mathrm{b}_{\mathrm{y}} \\\mathrm{b}_{\mathrm{w}} \\\mathrm{b}_{\mathrm{h}} \\\mathrm{C}_1 \\\mathrm{C}_2 \\\mathrm{C}_3\end{array}\right], \mathrm{y}_{\text {true }}=\left[\begin{array}{c}1 \\40 \\45 \\80 \\60 \\0 \\1 \\0\end{array}\right], \mathrm{y}_{\text {pred }}=\left[\begin{array}{c}0.88 \\41 \\46 \\82 \\59 \\0.01 \\0.95 \\0.04\end{array}\right]</script><p>其中, $\mathrm{p}_{\mathrm{c}}$ 为预测结果的置信度（Confidence），表达预测框内包含目标的概率。$\mathrm{b}_{\mathrm{x}}, \mathrm{b}_{\mathrm{y}}, \mathrm{b}_{\mathrm{w}}, \mathrm{b}_{\mathrm{h}}$ 分别为预测框左上点$x$坐标和$y$坐标以及预测框的宽度、长度，也可以是预测框的左上、右下两点的$x$坐标和$y$坐标，表达的意思是相同的。 $\mathrm{C}_1, \mathrm{C}_2, \mathrm{C}_3$ 为目标属于某个类别的概率。</p><p>真实框和预测框的数量可能是不对等的，这是目标检测与语义分割的一大区别。现在我准备把这些内容再简化，归类为三个信息，<strong>目标置信度</strong>、<strong>定位信息</strong>和<strong>分类置信度</strong>，这里面有些事情需要讲清楚。</p><p>第一，<strong>分类置信度</strong>可能有互斥和不互斥两种，取决于是否使用了softmax计算，本文中，默认分类置信度是互斥的，也就是说当模型输出了一个预测框，框内“目标”的类别就固定了（不论是否真的存在目标）。</p><p>第二，预测框与真实框的贴合程度可以由<strong>定位信息</strong>计算IoU表示，上次提到了。</p><p>以上，前置理解完毕，可以开始进行模型评估了。</p><h2 id="重返混淆矩阵"><a href="#重返混淆矩阵" class="headerlink" title="重返混淆矩阵"></a>重返混淆矩阵</h2><p>从我们现有的信息来看，我们只有两个数值：<strong>目标置信度</strong>和<strong>定位信息</strong>。定位信息可以求IoU，但它依然是数值。基于mAP是目标检测中热门的评估指标的现状，我们希望能够继续用混淆矩阵来对预测框进行分类。众所周知，对数值分类的最简单方式就是在集合中画一条界限，将数值集分为两块，这条界限我们称为<strong>阈值</strong>。</p><p>我们有两个数值，自然产生了两个阈值，即<strong>置信度阈值</strong>和<strong>IoU阈值</strong>。显然，前者能把预测分成里面大概率是目标的框和里面大概率是背景的框，后者则把预测分成了很像某个目标的框和与某个目标不沾边的框。好了，现在我们来说混淆矩阵的事。</p><p>由于预测框类别已经固定，目标置信度和定位信息就是用来确定这框里面到底有没有目标，两个阈值用来判断一件事，这不就是二分类吗？没错，目标检测中我们还是用二分类，和之前多分类按照每个类别来看依然是二分类问题一样。</p><p>那现在事情就变得简单了，只需要搞懂如何区别正负就可以了，这是由目标置信度和定位信息共同决定的。下面给出方法，首先我们假设测试集有$N$张图片，我们从中取出一张图片$Img_n$，取该图片中属于类别$C_1$的目标的真实框和预测为类别$C_1$的预测框作为待分类集合。</p><ul><li>TP：与某一真实框的IoU值大于IoU阈值，且目标置信度大于置信度阈值的预测框数量。每个真实框仅能匹配一个预测框，这意味这TP的最大值为真实框的数量。</li><li>FN：漏检测的真实框数量。</li><li>FP：目标置信度大于置信度阈值但不满足TP条件的预测框数量。</li><li>TN：不考虑，因为Negative的样本想画可以画无数个，没有价值。</li></ul><p>初次看到这个判断方法多少还是有点懵，笔者解释一下，首先我们希望预测结果对每个目标至多只有一个预测框，这导致了即便有多个符合双阈值要求的预测框，对同一目标也只能<strong>选择一个置信度最高的预测框作为TP</strong>，其余重复预测框（虽然满足置信度要求）都进入FP。这样以后，能进入TP的目标也被确定了，剩下的真实框自然就成为了FN。</p><h2 id="准确率-召回率曲线（Precision-Recall-Curve）"><a href="#准确率-召回率曲线（Precision-Recall-Curve）" class="headerlink" title="准确率-召回率曲线（Precision-Recall Curve）"></a>准确率-召回率曲线（Precision-Recall Curve）</h2><p>先回顾一下Precision和Recall的计算方式，这里就只考虑Positive了：</p><script type="math/tex; mode=display">Precision = \frac{TP}{TP+FP} \\Recall = \frac{TP}{TP+FN}</script><p>由上面的分析可以看出，双阈值会影响P和R，通过调整阈值，就可以获得多个$(R,P)$对，通常我们通过指定IoU阈值（不小于0.5），调整置信度阈值的方式获得P-R曲线。事实上，精确率和召回率是一对由置信度阈值控制的冲突的变量，如果想要精准率提高，召回率则会下降，如果要召回率提高，精准率则会下降。当置信度阈值下降时，Recall单调上升，Precision总体呈下降趋势，这也很好理解，查的更全但是查的不准。</p><p><img src="/assets/post_img/article72/p-r.webp" alt="pr"></p><p>需要注意的是，在计算P-R曲线的过程中，置信度阈值的调整方式通常是对所有预测框按置信度降序排序，依次将置信度阈值设为某一预测框的置信度。并且<strong>P-R曲线的计算是以全测试集为域，按类别划分的</strong>，也就是说，计算类别$C_1$的P-R曲线时，预测框的基数为$C_1$在所有图片中的预测框数量之和，$100$个预测框可以计算$100$组$(R,P)$。另外，由于FP也需要满足置信度大于阈值的条件，<strong>参与整个P-R计算的预测框数量应该是持续增加的，最终达到$C_1$在所有图片中的预测框数量之和</strong>。（这里主要的意思就是粗体的地方，前面的原因可以有很多，主要就是需要按照前面定义的规则计算。）</p><p>更重要的是，<strong>TP、FP和FN却需要在单张图片的范围内按类别计算，再以图片为单位求和构成总TP、FP和FN值，进而求得类别在数据集上的一个$(R,P)$</strong>。</p><h2 id="AP（Average-Precision）"><a href="#AP（Average-Precision）" class="headerlink" title="AP（Average Precision）"></a>AP（Average Precision）</h2><p>经历了前面曲折艰难的分析，终于能够接近AP了。首先从字面上看，平均精确度，很容易联想到一种计算方式（笔者就这么干过）：对数据集中的每张图片单独计算Precision，然后求平均值，这不就是AP吗？实际上这是不对的，错误出在了对于Precision的理解上，Precision是一个<strong>Rate</strong>，我们现在的目的是评估模型<strong>对类别</strong>的检测效果，一个类别的查准率是建立在全数据集的样本之上的，显然，上述计算无法准确的表达这一内涵。</p><p>如果有读者想不明白这一点，我可以再举一个例子来验证，例如有三堆球摆在我们面前，其中两堆相同，都由$3$个白球和$1$个红球构成，另外一堆由$1$个红球和$1$个白球构成，我希望得到这些球中的红球率。如果按照上述理解，我们可以分别计算三堆中的红球率，得到$\frac{1}{4},\frac{1}{4},\frac{1}{2}$，再求它们的算数平均值得到$\frac{1}{3}$。而实际上呢，红球率是$\frac{3}{10}$，因为红球率是指以球为基本单位的红球比率（也不知道我解释清楚没）。</p><p>总之，Precision就是这样一个指标，要计算类别的AP，就要先获得P-R曲线。得到P-R曲线后，AP的计算方式就有很多了，根据不同的流行数据集，有几种常见的方式，罗列如下：</p><ul><li><p>按照VOC2007的方法，是先平滑曲线，对于每个点取其右边最大的P值，连成直线，然后等间距取11个点的最大P值，AP就是这11个Precision的平均值。</p></li><li><p>VOC2012，还是先按07的方法平滑曲线，然后计算PR曲线下面积作为AP值，因为本身P和R就是Rate，构成的正方形区域面积就是1，求曲线积分就完事了。</p></li><li><p>COCO数据集，设定多个IoU阈值（0.5至0.95，0.05为步长），在每一个IoU阈值下都有某一类别的AP值，然后求所有IOU阈值下的平均AP，以该平均AP值作为最终的某类别的AP值。</p></li></ul><p>总的来说，就是一个精度越来越高的过程。</p><h2 id="mAP（mean-Average-Precision）"><a href="#mAP（mean-Average-Precision）" class="headerlink" title="mAP（mean Average Precision）"></a>mAP（mean Average Precision）</h2><p>mAP和速度是最常用的目标检测模型评价指标，mAP顾名思义就是对所有分类的AP再求平均值。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]<a href="https://blog.csdn.net/yegeli/article/details/109861867">https://blog.csdn.net/yegeli/article/details/109861867</a><br>[2]<a href="https://zhuanlan.zhihu.com/p/70306015">https://zhuanlan.zhihu.com/p/70306015</a><br>[3]<a href="https://www.jianshu.com/p/86b8208f634f">https://www.jianshu.com/p/86b8208f634f</a><br>[4]<a href="https://zhuanlan.zhihu.com/p/94597205">https://zhuanlan.zhihu.com/p/94597205</a><br>[5]<a href="https://zhuanlan.zhihu.com/p/101566089">https://zhuanlan.zhihu.com/p/101566089</a><br>[6]<a href="https://www.jianshu.com/p/fd9b1e89f983">https://www.jianshu.com/p/fd9b1e89f983</a></p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于<a href="https://silencezheng.top/">silencezheng.top</a>。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;关于目标检测领域的常用评价指标进行一些个人解读，欢迎批评指正。&lt;/p&gt;
&lt;p&gt;评价指标系列第二篇～上一次说了语义分割的内容，这次来搞目标检测，看了十余篇内容后终于搞明白些了，记录一下。本文说的内容比较浅，一方面是笔者水平实在菜鸡，另一方面写文的主要目的是让不了解目标检测的人读完能上手去做，而不用查三十个资料后再开始，就像博客的Slogan一样，“帮你在学习技术的路上节约半小时”。&lt;br&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://silencezheng.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="计算机视觉" scheme="http://silencezheng.top/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>语义分割常用评价指标</title>
    <link href="http://silencezheng.top/2022/10/23/article71/"/>
    <id>http://silencezheng.top/2022/10/23/article71/</id>
    <published>2022-10-22T16:26:05.000Z</published>
    <updated>2022-10-22T16:33:20.039Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>关于语义分割领域的常用评价指标进行一些个人解读，欢迎批评指正。</p><p>本来是想连目标检测的一起说了的，但是关于AP的一些东西始终有疑问，留着后面搞懂了再写一篇吧。<br><span id="more"></span></p><h2 id="前置理解"><a href="#前置理解" class="headerlink" title="前置理解"></a>前置理解</h2><p>在解读这些评价指标之前，需要对深度学习方法有一个基础认识。</p><p>首先，评价指标，是指测试阶段将预测结果与真实值进行比对得到的量化结论。<strong>评价指标（Metric）和损失函数（Loss）有联系也有区别</strong>，目前在我个人理解来说，评价指标是以实用的角度评价模型，只关心模型的结果，而损失函数是从数学的角度收敛模型，但损失往往应该与一个你最关心的评价指标相对应，通过收敛损失能够达到向指标增大的方向靠拢。并且，损失应该是容易优化的，很多时候它们对模型参数可微，甚至是凸的。下面引[10]中的例子做说明。</p><blockquote><p>假设某同学备战高考，他给自己定下了一个奋斗的方向，即每周要把自己的各科总成绩提高5分；经过多年的准备，终于在高考中取得了好成绩（710分，总分750），被北大录取。<br>分析该例子，该同学“每周要把自己的各科总成绩提高5分”这个指导原则相当于目标函数，在这个指导原则的指引下，想必该同学的总分会越来越高，即模型被训练的越来越好。<br>最终，该同学高考成绩优异，相当于模型的测试效果良好，至于用从哪个角度评价这名同学，可以用其高考总分与750分的差距来衡量，也可以用其被录取的大学的水平来衡量，这就如同模型的评估指标是多种多样的，比如分类问题中的准确率、召回率等。<br>当然，模型的评估指标多样，模型的损失函数也是多样的；该例中，该同学可以将“每周要把自己的各科总成绩提高5分”作为指导原则，也可将“每周比之前多学2个知识点”作为指导原则。<br>另外，如果该同学将“每周模拟高考总分与750分的差距”同时作为指导原则与评价角度，则类似于线性回归模型将“MSE均方误差”同时作为损失函数与评估指标。<br>该例中，备考的“指导原则”相当于“损失函数”，“评价角度”相当于“评估指标”，该同学相当于一个机器学习模型。</p></blockquote><p>其次，在多分类任务中，通常包含$n$个类别，而对于某一样本的最终预测只能是$n$个类别中的一个。但是，算法对一个样本的类别预测通常以置信度的形式表示，最终选择置信度最高的类别作为预测输出。</p><p>最后，多分类任务对于每个类来看，可以看作是一个二分类问题，以样本对于该类别预测是否正确作为区分。</p><h2 id="混淆矩阵（Confusion-Matrix）"><a href="#混淆矩阵（Confusion-Matrix）" class="headerlink" title="混淆矩阵（Confusion Matrix）"></a>混淆矩阵（Confusion Matrix）</h2><p>混淆矩阵用于直观的显示模型预测结果的情形。 混淆矩阵中的横纵轴都是类别，对于$p_{ij}$（横坐标为$i$,纵坐标为$j$处的值），其含义为属于类别$i$并被预测为类别$j$的<strong>样本数量</strong>（在语义分割中通常样本等同于像素）。也就是说，每个位置的<strong>横坐标表示模型的预测，纵坐标表示真实标签</strong>。</p><p>对于二分类问题，混淆矩阵可以表示如下：</p><p><img src="/assets/post_img/article71/confusion-matrix-2classes.jpeg" alt="cm2"></p><p>若令其中$1$表示正类，$0$表示负类，则可以定义如下四个量：</p><ul><li>TP(True Positive)：将正样本预测为正类的数量，即图中的$a$。</li><li>FN(False Negative)：将正样本预测为负类的数量，即图中的$b$。</li><li>FP(False Positive)：将负样本预测为正类的数量，即图中的$c$。</li><li>TN(True Negative)：将负样本预测为负类的数量，即图中的$d$。</li></ul><p>对于多分类问题，只是把该矩阵由$2 \times 2$变化为$n \times n$，其中$n$表示类别数量。</p><p>从混淆矩阵中我们可以获得一些基础信息，如：</p><ul><li>$i$行的和$\sum^n_{j=1}p_{ij}$表示数据集中属于类别$i$的样本个数</li><li>$j$列的和$\sum^n_{i=1}p_{ij}$表示模型预测中属于类别$j$的样本个数</li><li>矩阵中所有元素的和$\sum^n_{i=1}\sum^n_{j=1}p_{ij}$表示图像中的总样本个数</li><li>…</li></ul><h2 id="精确率（Precision）和召回率（Recall）"><a href="#精确率（Precision）和召回率（Recall）" class="headerlink" title="精确率（Precision）和召回率（Recall）"></a>精确率（Precision）和召回率（Recall）</h2><p>这两个指标都是<strong>针对某一类别</strong>而言的，是分类任务的常用评价指标。</p><p>精确率又称查准率，含义是对于模型预测中属于类别$j$的样本，预测结果正确的比例。例如对于二分类问题，正类的精确率$Precision_{positive} = \frac{TP}{TP+FP} = \frac{a}{a+c}$。</p><p>召回率又称查全率，如果说精准率是站在预测的角度看问题，那么召回率就是站在现实的角度看问题，其含义是对于数据集中属于类别$i$的样本，被正确预测的比例。例如对于二分类问题，负类的召回率$Recall_{negative} = \frac{TN}{FP+TN} = \frac{d}{c+d}$。</p><h2 id="准确率（Accuracy）"><a href="#准确率（Accuracy）" class="headerlink" title="准确率（Accuracy）"></a>准确率（Accuracy）</h2><p>准确率需要和精确率区别开，准确率是站在预测的整体角度看问题，其含义是预测正确的样本占所有样本的比例。例如对于二分类问题，预测的准确率$Accuracy_{predict} = \frac{TP+TN}{TP+FN+FP+TN} = \frac{a+d}{a+b+c+d}$。可以看出，准确率其实就是混淆矩阵对角线元素和与所有元素和的比值。</p><h2 id="F1指标（F1-Score）和F-Beta指标（F-Beta-Score）"><a href="#F1指标（F1-Score）和F-Beta指标（F-Beta-Score）" class="headerlink" title="F1指标（F1 Score）和F-Beta指标（F-Beta Score）"></a>F1指标（F1 Score）和F-Beta指标（F-Beta Score）</h2><p>单独用精确率或召回率有时不能很好的评估模型，例如在二分类问题中，模型选择对所有样本预测为正类，此时所有正类样本都被“准确”的预测了，正类召回率为$1$，但模型实际上很差。</p><p>F1指标就是用来平衡精确率和召回率的重要程度的度量指标，它被定义为两者的<strong>调和平均值</strong>，表示二者重要程度一致。F1指标的计算公式如下：</p><script type="math/tex; mode=display">F_1 = 2 \times \frac{precision \cdot recall}{precision + recall}</script><p>调和平均值的一个重要特性就是如果两者极度不平衡，调和平均值会很小，只有当两者都较高时，调和平均才会比较高。</p><p>而F-Beta指标则是更一般的形式，他的计算方式如下：</p><script type="math/tex; mode=display">F_{\beta} = (1+\beta^2) \cdot \frac{precision \cdot recall}{(\beta^2 \cdot precision) + recall}</script><p>其中参数$\beta$决定了精确率和召回率的重要程度比值，当$\beta&gt;1$时召回率比重更大，当$\beta&lt;1$时精确率比重更大。</p><h2 id="特异性（Specificity）和敏感性（Sensitivity）"><a href="#特异性（Specificity）和敏感性（Sensitivity）" class="headerlink" title="特异性（Specificity）和敏感性（Sensitivity）"></a>特异性（Specificity）和敏感性（Sensitivity）</h2><p>关于这两个指标，似乎是仅针对于二分类问题而言的，这里只谈一些个人理解。</p><p>还是参照二分类的混淆矩阵，特异性实际上指的就是负类的召回率$Recall_{negative}$，而敏感性则指的是正类的召回率$Recall_{positive}$，看了网上许多解释，都是聚焦在医疗领域，把患病作为正类，健康作为负类，说什么敏感性越高，漏诊概率越低；特异性越高，确诊概率越高。</p><p>个人理解，实际上就是召回率在特定情况下的应用吧。</p><h2 id="交并比（Intersection-over-Union，IoU）和平均交并比（mIoU）"><a href="#交并比（Intersection-over-Union，IoU）和平均交并比（mIoU）" class="headerlink" title="交并比（Intersection over Union，IoU）和平均交并比（mIoU）"></a>交并比（Intersection over Union，IoU）和平均交并比（mIoU）</h2><p>给定两个区域$A$和$B$，IoU就是两区域的交集与两区域并集的比值：</p><script type="math/tex; mode=display">IoU = \frac{A \cap B}{A \cup B}</script><p><img src="/assets/post_img/article71/IoU.png" alt="iou"></p><p>在分类任务中，可以对某一类别的预测结果和真实标签求IoU，例如对于二分类求正类的IoU如下：</p><script type="math/tex; mode=display">IoU_{positive} = \frac{TP}{TP+FP+FN} = \frac{a}{a+b+c}</script><p>也就是说，混淆矩阵中$i$行和$i$列的交集比上它们的并集。</p><p><strong>平均交并比</strong>（mean IoU）就是对每一个类别求IoU，再求和求平均得到的值。</p><p>对于目标检测，IoU还有一个重要的应用，就是判断预测框与真实框的贴合程度，两部分重合面积越大，则IoU值越大。IoU是一个比较严格的评价指标，当两区域稍微有偏差时，IoU值也可能变得相当小，于是通常认为IoU大于$0.5$时就获得了一个比较不错的预测框。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]<a href="https://zhuanlan.zhihu.com/p/111234566">https://zhuanlan.zhihu.com/p/111234566</a><br>[2]<a href="https://zhuanlan.zhihu.com/p/101566089">https://zhuanlan.zhihu.com/p/101566089</a><br>[3]<a href="https://blog.csdn.net/h1yupyp/article/details/80842172">https://blog.csdn.net/h1yupyp/article/details/80842172</a><br>[4]<a href="https://blog.csdn.net/lhxez6868/article/details/108150777">https://blog.csdn.net/lhxez6868/article/details/108150777</a><br>[5]<a href="https://zhuanlan.zhihu.com/p/371819054">https://zhuanlan.zhihu.com/p/371819054</a><br>[6]<a href="https://www.jianshu.com/p/22d947ffb71e">https://www.jianshu.com/p/22d947ffb71e</a><br>[7]<a href="https://zhuanlan.zhihu.com/p/372402161">https://zhuanlan.zhihu.com/p/372402161</a><br>[8]<a href="https://zhuanlan.zhihu.com/p/373658488">https://zhuanlan.zhihu.com/p/373658488</a><br>[9]<a href="https://zhuanlan.zhihu.com/p/373032887">https://zhuanlan.zhihu.com/p/373032887</a><br>[10]<a href="https://www.cnblogs.com/pythonfl/p/13705143.html">https://www.cnblogs.com/pythonfl/p/13705143.html</a></p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于<a href="https://silencezheng.top/">silencezheng.top</a>。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;关于语义分割领域的常用评价指标进行一些个人解读，欢迎批评指正。&lt;/p&gt;
&lt;p&gt;本来是想连目标检测的一起说了的，但是关于AP的一些东西始终有疑问，留着后面搞懂了再写一篇吧。&lt;br&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://silencezheng.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="计算机视觉" scheme="http://silencezheng.top/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>M1 Mac weasyprint安装使用</title>
    <link href="http://silencezheng.top/2022/10/19/article70/"/>
    <id>http://silencezheng.top/2022/10/19/article70/</id>
    <published>2022-10-19T15:54:48.000Z</published>
    <updated>2022-10-19T15:57:18.582Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>为了把HTML转成PDF（带很多css的），折腾了一晚上。试了一下pdfkit是基本没戏（<a href="https://pypi.org/project/pdfkit/">pdfkit css bug</a>），转头想用weasyprint，没想到适配更差，记录一下。<br><span id="more"></span></p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p><code>brew install cairo pango gdk-pixbuf libffi</code></p><p><code>pip install weasyprint</code></p><h2 id="在conda环境下使用weasyprint"><a href="#在conda环境下使用weasyprint" class="headerlink" title="在conda环境下使用weasyprint"></a>在conda环境下使用weasyprint</h2><p>本来如果在brew的python3下使用应该没什么问题，但是如果要用conda环境的解释器就会报错：<code>OSError: cannot load library &#39;gobject-2.0-0&#39;</code></p><p>解决方案如下：<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo ln -s <span class="regexp">/opt/</span>homebrew<span class="regexp">/opt/g</span>lib<span class="regexp">/lib/</span>libgobject-<span class="number">2.0</span>.<span class="number">0</span>.dylib <span class="regexp">/usr/</span>local<span class="regexp">/lib/g</span>object-<span class="number">2.0</span></span><br><span class="line">sudo ln -s <span class="regexp">/opt/</span>homebrew<span class="regexp">/opt/</span>pango<span class="regexp">/lib/</span>libpango-<span class="number">1.0</span>.dylib <span class="regexp">/usr/</span>local<span class="regexp">/lib/</span>pango-<span class="number">1.0</span></span><br><span class="line">sudo ln -s <span class="regexp">/opt/</span>homebrew<span class="regexp">/opt/</span>harfbuzz<span class="regexp">/lib/</span>libharfbuzz.dylib <span class="regexp">/usr/</span>local<span class="regexp">/lib/</span>harfbuzz</span><br><span class="line">sudo ln -s <span class="regexp">/opt/</span>homebrew<span class="regexp">/opt/</span>fontconfig<span class="regexp">/lib/</span>libfontconfig.<span class="number">1</span>.dylib <span class="regexp">/usr/</span>local<span class="regexp">/lib/</span>fontconfig-<span class="number">1</span></span><br><span class="line">sudo ln -s <span class="regexp">/opt/</span>homebrew<span class="regexp">/opt/</span>pango<span class="regexp">/lib/</span>libpangoft2-<span class="number">1.0</span>.dylib <span class="regexp">/usr/</span>local<span class="regexp">/lib/</span>pangoft2-<span class="number">1.0</span></span><br></pre></td></tr></table></figure></p><p>创建对应位置的软链接，<a href="https://github.com/Kozea/WeasyPrint/issues/1448">issue在这</a>。</p><p>这样以后在终端用是没什么问题了，<code>weasyprint url xx.pdf</code>，中文支持不佳，css部分支持不好。</p><h2 id="仍然报错"><a href="#仍然报错" class="headerlink" title="仍然报错"></a>仍然报错</h2><p>在正常Python调用中仍然会报错，<code>RuntimeError: cannot use unpack() on &lt;cdata &#39;char *&#39; NULL&gt;</code>，定位到<code>cffi/api.py</code>，一个空指针，暂时不知道怎么解决。</p><p>CFFI(C Foreign Function Interface) 是Python的C语言外部函数接口。通过CFFI，Python可以与几乎任何C语言代码进行交互。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;为了把HTML转成PDF（带很多css的），折腾了一晚上。试了一下pdfkit是基本没戏（&lt;a href=&quot;https://pypi.org/project/pdfkit/&quot;&gt;pdfkit css bug&lt;/a&gt;），转头想用weasyprint，没想到适配更差，记录一下。&lt;br&gt;</summary>
    
    
    
    
    <category term="MacOS" scheme="http://silencezheng.top/tags/MacOS/"/>
    
  </entry>
  
  <entry>
    <title>动态规划</title>
    <link href="http://silencezheng.top/2022/10/11/article69/"/>
    <id>http://silencezheng.top/2022/10/11/article69/</id>
    <published>2022-10-11T10:15:29.000Z</published>
    <updated>2022-12-10T14:06:39.564Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Dynamic Programming学习，由于动态规划问题很多，且没有固定套路，本文会随时进行增补、修改。</p><p>另外，笔者对于DP的理解很可能是错误的，希望大家多多指教。<br><span id="more"></span></p><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>动态规划是研究多步决策过程<strong>最优化</strong>问题的一种数学方法。在动态规划中，为了寻找一个问题的最优解（即最优决策过程），将整个问题划分成若干个相应的阶段，并在每个阶段都根据先前所作出的决策作出当前阶段最优决策，进而得出整个问题的最优解。即<em>记住已知问题的答案，在已知的答案的基础上解决未知的问题。</em></p><h2 id="能解决的问题"><a href="#能解决的问题" class="headerlink" title="能解决的问题"></a>能解决的问题</h2><p>1、计数问题<br>例如“有多少种方式使得…”，或者“有多少种方法选出…”。</p><p>2、最值问题<br>例如经典的“最少用多少枚硬币能组合出目标面值”，或者“求最长上升子序列”。</p><p>3、存在性问题<br>例如“能不能选出k个数使得…”，或者“先手方是否必胜”。</p><p>注意，通常求所有解法的问题不能用DP方法来做，因为与最优解无关的解在动态规划中不会被全部计算。</p><h2 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h2><p>在上述问题中，可能会有许多可行解。每一个解都对应于一个值，我们希望找到具有最优值的解。动态规划算法的基本思想是将待求解问题分解成若干个子问题，先求解子问题，然后从这些子问题的解得到原问题的解。与分治法不同的是，适合于用动态规划求解的问题，经分解得到<strong>子问题往往不是互相独立的</strong>。如果能够保存已解决的子问题的答案，而在需要时再找出已求得的答案，这样就可以避免大量的重复计算，节省时间。</p><p>这种记住子问题的做法很容易联想到记忆化，但 <strong>动态规划</strong> 和 <strong>记忆化搜索</strong> 的一个重要区别是动态规划通常不使用递归实现。 另外在计算顺序方面，多数动态规划问题是自底向上的（这与状态转移有关），而记忆化搜索是自顶向下的。</p><p>通常可以用一个表来记录所有已解的子问题的答案。不管该子问题以后是否被用到，只要它被计算过，就将其最优解填入表中，方便在求解之后的子问题时可以方便调用，进而求出整个问题的最优解。这就是动态规划法的基本思路。具体的动态规划算法多种多样，但它们具有相同的填表格式。</p><p>也可以说，动态规划最核心的思想，就在于<strong>拆分子问题，记住过往，减少重复计算</strong>。</p><h2 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h2><p>通常通过动态规划求解问题由四个主要部分组成：<br>1、拆分子问题<br>2、确定状态转移方程<br>3、确定初状态和边界条件<br>4、确定计算顺序</p><p>这四个部分看起来简单易懂，但是在实践过程中却会遇到重重困难。下面分别详细的解释一下。</p><p>首先需要明确动态规划解决的是一个多步决策问题，该问题由初状态、若干中间决策和末状态组成。</p><h3 id="1、拆分子问题"><a href="#1、拆分子问题" class="headerlink" title="1、拆分子问题"></a>1、拆分子问题</h3><p>拆分子问题，即将原问题拆解成为<strong>范围更小但性质不变的子问题</strong>。</p><p>一般首先要做的是找出“最后一步”，也就是若干中间决策的最后一步，经历该决策后，问题转变到末状态。</p><p>找到“最后一步”后，我们研究“最后一步”前的状态，它应该能够构成一个子问题，i.e.，如果说“最后一步”使问题达到末状态，且这一系列决策构成问题的最优解，那么去掉“最后一步”的决策链应该是最优解的一个子集，同时“最后一步”前的状态构成一个子问题，该子集为该子问题的最优解。</p><p>这样一来，我们就从原问题中拆出了一个子问题。<strong>拆分子问题的目的是找出状态</strong>，状态是对问题各阶段的客观表述，同时需要满足<em>无后效性</em>，即当前状态之后的决策对该状态无影响。</p><h3 id="2、确定状态转移方程"><a href="#2、确定状态转移方程" class="headerlink" title="2、确定状态转移方程"></a>2、确定状态转移方程</h3><p>状态转移方程是动态规划的重中之重，一旦确定了状态转移方程，问题通常也就迎刃而解了。</p><p><strong>状态转移就是根据上一阶段的状态和之后的一次决策来导出本阶段的状态</strong>。所以如果确定了决策，状态转移方程也就可写出。 但事实上常常是反过来做，根据相邻两个阶段的状态之间的关系来确定决策方法和状态转移方程。 状态转移方程通常是一个递推公式。</p><p>同时，状态转移方程也影响着最终算法的计算顺序，通常靠后的状态会需求前方状态的信息，原问题为求到最后状态的最优解，因此动态规划问题通常都是自底向上计算的。</p><p>我们用一个数组或哈希表来记录状态的最优解，本文暂时称其为<strong>状态表</strong>。</p><h3 id="3、确定初状态和边界情况"><a href="#3、确定初状态和边界情况" class="headerlink" title="3、确定初状态和边界情况"></a>3、确定初状态和边界情况</h3><p>初状态就是问题的初始状态。 确定初状态主要是确定状态表如何初始化。</p><p>边界情况也称边界条件，即状态转移方程的边界，有时在递推公式的某一部分会出现不合理的或不属于问题范围内的项，需要通过边界条件来限制它。</p><h3 id="4、确定计算顺序"><a href="#4、确定计算顺序" class="headerlink" title="4、确定计算顺序"></a>4、确定计算顺序</h3><p>如上面所说的，根据状态转移公式来确定。</p><h2 id="案例一：摩天大楼"><a href="#案例一：摩天大楼" class="headerlink" title="案例一：摩天大楼"></a>案例一：摩天大楼</h2><p>题目如下：</p><p><img src="/assets/post_img/article69/question.jpeg" alt="skyscraper"></p><p>拿到题目我们首先考虑一下暴力法，从暴力法的实现中可以看出有没有优化的余地，下面是我手写的一个过程，以输入$[2, 5, 1, 4, 8]$为例。</p><p><img src="/assets/post_img/article69/brutal-force.png" alt="bf"></p><p>可以发现有很多重复计算，可以通过记忆化进行优化。 暴力法代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># author：SilenceZheng66</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">brutal_force</span>(<span class="params">L: <span class="built_in">list</span>[<span class="built_in">int</span>]</span>):</span></span><br><span class="line">    n = <span class="built_in">len</span>(L)</span><br><span class="line"></span><br><span class="line">    results = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> n &lt;= <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">to_top</span>(<span class="params">pos, times</span>):</span></span><br><span class="line">        <span class="keyword">if</span> pos == n - <span class="number">1</span>:</span><br><span class="line">            results.append(times)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(pos + <span class="number">1</span>, pos + L[pos] + <span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> j &lt; n:</span><br><span class="line">                    to_top(j, times + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> L[<span class="number">0</span>] &gt;= n - <span class="number">1</span>:</span><br><span class="line">        results.append(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, L[<span class="number">0</span>] + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> i &lt; n:</span><br><span class="line">                to_top(i, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">min</span>(results)</span><br></pre></td></tr></table></figure><p>下面我们开始用动态规划的解题步骤尝试去优化我们的算法。</p><h3 id="1、拆分子问题-1"><a href="#1、拆分子问题-1" class="headerlink" title="1、拆分子问题"></a>1、拆分子问题</h3><p>原问题是：找出从底层到楼顶的最少乘电梯数。我们以数组的下标$pos$来进行说明，底层的$pos = 0$，楼顶的$pos = n-1$。</p><p>假设“最后一步”前的状态为 $pos = m$ ，那么“最后一步”就是 $n-1$ 位于 $m + 1$ 和 $m + L[m]$ 之内，也可以表示为 $m + L[m] \geq n-1$。 此时原问题就被拆分成了子问题：找出从底层到$m+1$层的最少乘电梯数。 原问题的答案为该子问题答案$+1$。 </p><p>那么此时我们就可以描述状态了，$f(x) = 到达x层的最少乘电梯数$。</p><h3 id="2、确定状态转移方程-1"><a href="#2、确定状态转移方程-1" class="headerlink" title="2、确定状态转移方程"></a>2、确定状态转移方程</h3><p>第一步中我们拆分了子问题，确定了状态$f(x)$，则原问题可以描述为状态表中的最后一个状态，即$f(n-1)$。 根据“最后一步”中的推断，最理想的状态是能直接找到$f(n-1)$的前一个最优状态$f(m)$，然后再找到$f(m)$的前一最优个状态…直到正好找到$f(0)$。但是这是不可能实现的。</p><p>回到现实，还是考虑输入$[2, 5, 1, 4, 8]$，很自然的能够得出$f(4) = min(f(3), f(1)) + 1$，那么问题来了，为什么不是$min(f(3),f(2),f(1),f(0))$？很明显，我们需要考虑电梯的上升能力，即$5$层前有哪些层是能直接到达$5$层的？这些能直接到达的层里，哪些层的$f(x)$最小？找出他们，再加上$1$，就得到了$f(4)$。</p><p>基于以上分析，我们可以得出状态转移方程：</p><script type="math/tex; mode=display">f(x) = min(g(f(x-1), ... ,f(0))) + 1</script><p>其中$g(f(x-1), … ,f(0))$表示从$pos=0$到$pos=x-1$中选出那些满足$L[pos] + pos \geq x $的$f(pos)$。</p><p>得出了状态转移方程，这道题也就拿下来一多半了。</p><h3 id="3、确定初状态和边界情况-1"><a href="#3、确定初状态和边界情况-1" class="headerlink" title="3、确定初状态和边界情况"></a>3、确定初状态和边界情况</h3><p>初状态是好确定的，因为要做求最小运算，所以状态表初始化时所有元素应为系统能取到的最大正整数值。而$f(0)$应该被置为$0$，因为到达ground floor的最少乘电梯次数是$0$次。</p><p>关于边界情况，可以看到状态转移方程中有涉及到$L[pos] + pos$的运算，这可能会超出数组上界，故需要考虑进去。 同时如果大楼的层数$\leq 1$，则可以直接得出$f(n-1) = f(0) = 0$，也可以考虑进去。</p><h3 id="4、确定计算顺序-1"><a href="#4、确定计算顺序-1" class="headerlink" title="4、确定计算顺序"></a>4、确定计算顺序</h3><p>从状态转移公式可以看出，靠后的状态依赖前方状态信息，故应为自底向上。</p><p>最后就是写代码了，这里提供一个我写的示范。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># author：SilenceZheng66</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dp</span>(<span class="params">L: <span class="built_in">list</span>[<span class="built_in">int</span>]</span>):</span></span><br><span class="line">    n = <span class="built_in">len</span>(L)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 状态数组</span></span><br><span class="line">    conditions = [sys.maxsize <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始状态</span></span><br><span class="line">    conditions[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 边界情况</span></span><br><span class="line">    <span class="keyword">if</span> n &lt;= <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 自底向上</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n - <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 边界情况判断</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i + <span class="number">1</span>, i + L[i] + <span class="number">1</span>) <span class="keyword">if</span> i + L[i] + <span class="number">1</span> &lt;= n <span class="keyword">else</span> <span class="built_in">range</span>(i + <span class="number">1</span>, n):</span><br><span class="line">            conditions[j] = <span class="built_in">min</span>(conditions[i] + <span class="number">1</span>, conditions[j])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> conditions[-<span class="number">1</span>]</span><br></pre></td></tr></table></figure></p><h2 id="案例二：比特位计数"><a href="#案例二：比特位计数" class="headerlink" title="案例二：比特位计数"></a>案例二：比特位计数</h2><p>Leetcode的一道简单题，但有三种DP官解，很适合练习思路。</p><p>题目：给你一个整数 n ，对于 0 &lt;= i &lt;= n 中的每个 i ，计算其二进制表示中 1 的个数 ，返回一个长度为 n + 1 的数组 ans 作为答案。</p><p>先考虑暴力法，外层循环肯定是必要的，对1计数的话，我的想法是从最低位一直与一做与操作，然后逻辑右移，这样内层就是固定32次，外层n次：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: SilenceZheng66</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span>[] countBits(<span class="keyword">int</span> n) &#123;</span><br><span class="line">        <span class="keyword">int</span>[] ans = <span class="keyword">new</span> <span class="keyword">int</span>[n+<span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;=n;i++)&#123;</span><br><span class="line">            <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">int</span> tmp = i;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;<span class="number">32</span>;j++)&#123;</span><br><span class="line">                <span class="keyword">if</span>((tmp&amp;<span class="number">1</span>)==<span class="number">1</span>) count++;</span><br><span class="line">                tmp&gt;&gt;&gt;=<span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            ans[i] = count;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>那么这种方式有没有重复计算的地方呢？不妨用数字10举例，对于数字10（<code>1010</code>），在计算一次最低位后，剩余部分是之前已经计算过的数字5（<code>0101</code>）。那么可以采用记忆化方法，这样一来能提高不少效率：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: SilenceZheng66</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span>[] countBits(<span class="keyword">int</span> n) &#123;</span><br><span class="line">        <span class="keyword">int</span>[] ans = <span class="keyword">new</span> <span class="keyword">int</span>[n+<span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;=n;i++)&#123;</span><br><span class="line">            <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">int</span> tmp = i;</span><br><span class="line">            ans[i] = -<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;<span class="number">32</span>;j++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(ans[tmp]!=-<span class="number">1</span>)&#123;</span><br><span class="line">                    count+=ans[tmp];</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">if</span>((tmp&amp;<span class="number">1</span>)==<span class="number">1</span>) count++;</span><br><span class="line">                tmp&gt;&gt;&gt;=<span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            ans[i] = count;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>好了，以上是我对这道题的思路，但是对于以上解法，我没有想出如何使用DP来优化，下面是官解思路。</p><p>官解首先提出了另一个计算1bit数量的算法，即对于任意整数 x，令 <code>x = x&amp;(x-1)</code>，该运算将 x 的二进制表示的最后一个 1 变成 0。因此，对 x 重复该操作，直到 x 变成 0，则操作次数即为 x 的「一比特数」。 该方法称为Brian Kernighan 算法。</p><p>利用这个计数法写出暴力法不是什么难事：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: LeetCode-Solution</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span>[] countBits(<span class="keyword">int</span> n) &#123;</span><br><span class="line">        <span class="keyword">int</span>[] bits = <span class="keyword">new</span> <span class="keyword">int</span>[n + <span class="number">1</span>];</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt;= n; i++) &#123;</span><br><span class="line">            bits[i] = countOnes(i);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> bits;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">countOnes</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> ones = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (x &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            x &amp;= (x - <span class="number">1</span>);</span><br><span class="line">            ones++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ones;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>下面开始用动态规划的解题步骤尝试去优化此算法。</p><h3 id="1、拆分子问题-2"><a href="#1、拆分子问题-2" class="headerlink" title="1、拆分子问题"></a>1、拆分子问题</h3><p>首先需要明确该案例与案例一的区别，案例一为“最值问题”，显然可使用动态规划求解。而该案例并不明显的属于前文所提到的三类问题中的某一类，为什么可以由采用DP求解呢？我个人的理解是，题目要求返回的序列中后方的信息可以依赖前方的信息求出，这暗中符合了状态转移的性质，如若独立对每一项进行计算，则会出现计算冗余，此时采用DP是合适的。</p><p>在我的解法中，已经体现了一部分借助前方信息的思想，但并没有整理出一个状态转移方程，仅仅是简单的记忆化。现在，基于Brian Kernighan算法，我们来寻找一种使用动态规划解决问题的方法。</p><p>对于该问题，可以很明显的找到“最后一步”，即求n的二进制表示中的1bit数，我们将其记为$bits[n]$那么去除掉最后一步后的子问题就变为了“求$0$到$n-1$的$1$比特位计数序列”。原问题的答案为该子问题的答案$bits[0…n-1]$+$bits[n]$。</p><p>那么此时我们可以描述状态为$f(x) = 在求出0到x-1序列的基础上，求x的二进制表示中的1比特数量$。</p><h3 id="2、确定状态转移方程-2"><a href="#2、确定状态转移方程-2" class="headerlink" title="2、确定状态转移方程"></a>2、确定状态转移方程</h3><p>状态转移方程是DP的关键，这道题有很多状态转移的思路，这里介绍从最高有效位入手的思路。</p><p>对于正整数$x$，若可以知道最大正整数$y$，使得$y\leq x$且$y$是$2$的整数次幂，则$y$的二进制表示中只有最高位是$1$，其余为$0$。此时将$y$称为$x$的<strong>最高有效位</strong>。则显然存在$z = x - y, 0 \leq z \le x$，有$bits[x] = bits[z] + 1$。根据上述推导，我们可以发现$y, z$都落在$bits[0…x]$中，我们只需要找到对$x$求$y$的方法即可，但这个方法一定要高效，在$O(1)$里计算。</p><p>可以利用<code>y&amp;(y-1)==0</code>来判断，因为$y$中仅含有一个最高位$1$，也就是说正整数$y$是$2$的整数次幂，当且仅当<code>y&amp;(y-1)==0</code>。由于我们自底向上进行求解，数字$x$的最高有效位逐步递增，我们可以在遍历的过程中更新$y$。</p><p>至此，我们可以写出状态转移方程：</p><script type="math/tex; mode=display">f(x) = f(x - g(0, ..., x)) + 1, 其中函数g表示在0到x中寻找x的最高有效位。</script><h3 id="3、确定初状态和边界情况-2"><a href="#3、确定初状态和边界情况-2" class="headerlink" title="3、确定初状态和边界情况"></a>3、确定初状态和边界情况</h3><p>初状态自然就是$f(0)$了，此时$bits[0]=0$。</p><p>边界的话，不存在越上界的情况，下界保证初状态不越界即可。</p><h3 id="4、确定计算顺序-2"><a href="#4、确定计算顺序-2" class="headerlink" title="4、确定计算顺序"></a>4、确定计算顺序</h3><p>计算顺序自底向上，最终代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: LeetCode-Solution</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span>[] countBits(<span class="keyword">int</span> n) &#123;</span><br><span class="line">        <span class="keyword">int</span>[] bits = <span class="keyword">new</span> <span class="keyword">int</span>[n + <span class="number">1</span>];</span><br><span class="line">        <span class="keyword">int</span> highBit = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> ((i &amp; (i - <span class="number">1</span>)) == <span class="number">0</span>) &#123;</span><br><span class="line">                highBit = i;</span><br><span class="line">            &#125;</span><br><span class="line">            bits[i] = bits[i - highBit] + <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> bits;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="5、其他思路"><a href="#5、其他思路" class="headerlink" title="5、其他思路"></a>5、其他思路</h3><p>除了从最高有效位出发以外，从我原本的思路出发也可以转化到DP解法，即最低有效位方式。</p><p>其核心思想是$bits[x]$的值等于$\textit{bits}\big[\lfloor \frac{x}{2} \rfloor\big]$的值加上$x$除以$2$的余数。用代码表示就是<code>bits[x] = bits[x&gt;&gt;1] + (x&amp;1)</code>。代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: LeetCode-Solution</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span>[] countBits(<span class="keyword">int</span> n) &#123;</span><br><span class="line">        <span class="keyword">int</span>[] bits = <span class="keyword">new</span> <span class="keyword">int</span>[n + <span class="number">1</span>];</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++) &#123;</span><br><span class="line">            bits[i] = bits[i &gt;&gt; <span class="number">1</span>] + (i &amp; <span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> bits;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于 <a href="https://silencezheng.top">silencezheng.top</a>，转载请注明出处。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;Dynamic Programming学习，由于动态规划问题很多，且没有固定套路，本文会随时进行增补、修改。&lt;/p&gt;
&lt;p&gt;另外，笔者对于DP的理解很可能是错误的，希望大家多多指教。&lt;br&gt;</summary>
    
    
    
    
    <category term="数据结构与算法" scheme="http://silencezheng.top/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>复活Google翻译</title>
    <link href="http://silencezheng.top/2022/10/06/article68/"/>
    <id>http://silencezheng.top/2022/10/06/article68/</id>
    <published>2022-10-06T08:21:31.000Z</published>
    <updated>2022-11-07T08:01:45.097Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>9月末，谷歌旗下网页翻译工具谷歌翻译停止了中国区服务，现在访问<code>translate.google.cn</code>网页会指向谷歌香港站，此做法与此前谷歌搜索、谷歌地图等功能退出中国大陆时一致。</p><p>也就是说，Chrome的内置翻译也不能正常使用了。好在目前可以通过修改host文件的方式复活谷歌翻译。<br><span id="more"></span></p><h2 id="22年11月7日更新"><a href="#22年11月7日更新" class="headerlink" title="22年11月7日更新"></a>22年11月7日更新</h2><p>自 2022 年 10 月 21 日起，Google 断开了谷歌翻译与在华运营的其它网络服务 IP 的关联，因此通过可访问的 Google 域名获取 IP 不再可行，想要通过修改 hosts 恢复谷歌翻译功能，需要寻找其它可用 IP 地址。如有条件可以科学上网，没有则可尝试使用下面这些 IP 地址：<br><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">74.125.137.90</span></span><br><span class="line"><span class="number">74.125.193.186</span></span><br><span class="line"><span class="number">74.125.196.113</span></span><br><span class="line"><span class="number">108.177.97.100</span></span><br><span class="line"><span class="number">108.177.111.90</span></span><br><span class="line"><span class="number">108.177.122.90</span></span><br><span class="line"><span class="number">108.177.125.186</span></span><br><span class="line"><span class="number">108.177.126.90</span></span><br><span class="line"><span class="number">108.177.127.90</span></span><br><span class="line"><span class="number">142.250.0.90</span></span><br><span class="line"><span class="number">142.250.1.90</span></span><br><span class="line"><span class="number">142.250.4.90</span></span><br><span class="line"><span class="number">142.250.8.90</span></span><br><span class="line"><span class="number">142.250.9.90</span></span><br><span class="line"><span class="number">142.250.10.90</span></span><br><span class="line"><span class="number">142.250.11.90</span></span><br><span class="line"><span class="number">142.250.12.90</span></span><br><span class="line"><span class="number">142.250.13.90</span></span><br><span class="line"><span class="number">142.250.27.90</span></span><br><span class="line"><span class="number">142.250.28.90</span></span><br><span class="line"><span class="number">142.250.30.90</span></span><br><span class="line"><span class="number">142.250.31.90</span></span><br><span class="line"><span class="number">142.250.96.90</span></span><br><span class="line"><span class="number">142.250.97.90</span></span><br><span class="line"><span class="number">142.250.98.90</span></span><br><span class="line"><span class="number">142.250.99.90</span></span><br><span class="line"><span class="number">142.250.100.90</span></span><br><span class="line"><span class="number">142.250.101.90</span></span><br><span class="line"><span class="number">142.250.102.90</span></span><br><span class="line"><span class="number">142.250.103.90</span></span><br><span class="line"><span class="number">142.250.105.90</span></span><br><span class="line"><span class="number">142.250.107.90</span></span><br><span class="line"><span class="number">142.250.111.90</span></span><br><span class="line"><span class="number">142.250.112.90</span></span><br><span class="line"><span class="number">142.250.113.90</span></span><br><span class="line"><span class="number">142.250.114.90</span></span><br><span class="line"><span class="number">142.250.115.90</span></span><br><span class="line"><span class="number">142.250.123.90</span></span><br><span class="line"><span class="number">142.250.125.90</span></span><br><span class="line"><span class="number">142.250.126.90</span></span><br><span class="line"><span class="number">142.250.128.90</span></span><br><span class="line"><span class="number">142.250.138.90</span></span><br><span class="line"><span class="number">142.250.141.90</span></span><br><span class="line"><span class="number">142.250.142.90</span></span><br><span class="line"><span class="number">142.250.145.90</span></span><br><span class="line"><span class="number">142.250.152.90</span></span><br><span class="line"><span class="number">142.250.153.90</span></span><br><span class="line"><span class="number">142.250.157.90</span></span><br><span class="line"><span class="number">142.250.157.183</span></span><br><span class="line"><span class="number">142.250.157.184</span></span><br><span class="line"><span class="number">142.250.157.186</span></span><br><span class="line"><span class="number">142.250.158.90</span></span><br><span class="line"><span class="number">142.250.159.90</span></span><br><span class="line"><span class="number">142.251.1.90</span></span><br><span class="line"><span class="number">142.251.2.90</span></span><br><span class="line"><span class="number">142.251.4.90</span></span><br><span class="line"><span class="number">142.251.5.90</span></span><br><span class="line"><span class="number">142.251.6.90</span></span><br><span class="line"><span class="number">142.251.8.90</span></span><br><span class="line"><span class="number">142.251.9.90</span></span><br><span class="line"><span class="number">142.251.10.90</span></span><br><span class="line"><span class="number">142.251.12.90</span></span><br><span class="line"><span class="number">142.251.15.90</span></span><br><span class="line"><span class="number">142.251.16.90</span></span><br><span class="line"><span class="number">142.251.18.90</span></span><br><span class="line"><span class="number">142.251.107.90</span></span><br><span class="line"><span class="number">142.251.111.90</span></span><br><span class="line"><span class="number">142.251.112.90</span></span><br><span class="line"><span class="number">142.251.116.90</span></span><br><span class="line"><span class="number">142.251.117.90</span></span><br><span class="line"><span class="number">142.251.120.90</span></span><br><span class="line"><span class="number">142.251.160.90</span></span><br><span class="line"><span class="number">142.251.161.90</span></span><br><span class="line"><span class="number">142.251.162.90</span></span><br><span class="line"><span class="number">142.251.163.90</span></span><br><span class="line"><span class="number">142.251.166.90</span></span><br><span class="line"><span class="number">172.217.192.90</span></span><br><span class="line"><span class="number">172.217.195.90</span></span><br><span class="line"><span class="number">172.217.203.90</span></span><br><span class="line"><span class="number">172.217.204.90</span></span><br><span class="line"><span class="number">172.217.214.90</span></span><br><span class="line"><span class="number">172.217.215.90</span></span><br><span class="line"><span class="number">172.253.58.90</span></span><br><span class="line"><span class="number">172.253.62.90</span></span><br><span class="line"><span class="number">172.253.63.90</span></span><br><span class="line"><span class="number">172.253.112.90</span></span><br><span class="line"><span class="number">172.253.113.90</span></span><br><span class="line"><span class="number">172.253.114.90</span></span><br><span class="line"><span class="number">172.253.115.90</span></span><br><span class="line"><span class="number">172.253.116.90</span></span><br><span class="line"><span class="number">172.253.117.90</span></span><br><span class="line"><span class="number">172.253.118.90</span></span><br><span class="line"><span class="number">172.253.119.90</span></span><br><span class="line"><span class="number">172.253.123.90</span></span><br><span class="line"><span class="number">172.253.124.90</span></span><br><span class="line"><span class="number">172.253.125.90</span></span><br><span class="line"><span class="number">172.253.126.90</span></span><br><span class="line"><span class="number">172.253.127.90</span></span><br><span class="line"><span class="number">216.58.227.65</span></span><br><span class="line"><span class="number">216.58.227.66</span></span><br><span class="line"><span class="number">216.58.227.67</span></span><br></pre></td></tr></table></figure></p><h2 id="找可用IP"><a href="#找可用IP" class="headerlink" title="找可用IP"></a>找可用IP</h2><p>打开：<a href="https://ping.chinaz.com/translate.google.cn">https://ping.chinaz.com/translate.google.cn</a></p><p>随便找一个IP就行，如果想再精细一点，可以找到距离自己物理位置较近的同网络运营商的IP地址。</p><h2 id="修改host文件"><a href="#修改host文件" class="headerlink" title="修改host文件"></a>修改host文件</h2><p>把如下内容增加到host文件中，其中IP就是第一步找的IP。<br><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">220.181.174.98</span> translate.googleapis.com</span><br></pre></td></tr></table></figure></p><p>对于Windows，该文件的目录通常为<code>C:\Windows\System32\drivers\etc\hosts</code>；</p><p>对于MacOS，为<code>/private/etc/hosts</code>；</p><p>对于Linux，为<code>/etc/hosts</code>；</p><p>Android和ios也有对应方式，没啥用就不写了，详细可以看<a href="https://pangniao.net/google-translate.html">这里</a>。</p><p>修改完了source一下就可以使用了，如果不行就重启浏览器。</p><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>这个方法就是通过关联可用IP和翻译API的域名使翻译服务可用，如果没有可用IP了这个办法就行不通了。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;9月末，谷歌旗下网页翻译工具谷歌翻译停止了中国区服务，现在访问&lt;code&gt;translate.google.cn&lt;/code&gt;网页会指向谷歌香港站，此做法与此前谷歌搜索、谷歌地图等功能退出中国大陆时一致。&lt;/p&gt;
&lt;p&gt;也就是说，Chrome的内置翻译也不能正常使用了。好在目前可以通过修改host文件的方式复活谷歌翻译。&lt;br&gt;</summary>
    
    
    
    
    <category term="Chrome" scheme="http://silencezheng.top/tags/Chrome/"/>
    
  </entry>
  
  <entry>
    <title>高斯滤波与双边滤波</title>
    <link href="http://silencezheng.top/2022/10/05/article67/"/>
    <id>http://silencezheng.top/2022/10/05/article67/</id>
    <published>2022-10-05T15:14:18.000Z</published>
    <updated>2022-10-06T01:47:02.633Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>学习一下高斯滤波和双边滤波，及其需要的前置知识，仅记录一下个人理解，先后逻辑比较混乱，参考请谨慎。如有错误请评论区指正，感谢。</p><p>文中图片多数来自网络，我尽量擦去了水印，是为了提升观感，参考文献在文末有注明。本文的出发点，大概是在尽量用直观、快速的方式了解这些抽象概念吧。<br><span id="more"></span></p><h2 id="频域（Frequency-domain）、时域（Time-domain）"><a href="#频域（Frequency-domain）、时域（Time-domain）" class="headerlink" title="频域（Frequency domain）、时域（Time domain）"></a>频域（Frequency domain）、时域（Time domain）</h2><p><strong>时域</strong>（Time domain）是描述数学函数或物理信号对时间的关系。例如一个信号的时域波形可以表达信号随着时间的变化。</p><p><strong>频域</strong>（Frequency domain）是描述信号在频率方面特性时用到的一种坐标系。频域最重要的性质是：它不是真实的，而是一个数学构造。时域是惟一客观存在的域，而频域是一个遵循特定规则的数学范畴，频域也被一些学者称为上帝视角。</p><p>以上是对时域和频域下的初步定义，时域是好理解的，但要真正理解频域，还需要若干补充文字。</p><h3 id="补充一：频谱（Spectrum）"><a href="#补充一：频谱（Spectrum）" class="headerlink" title="补充一：频谱（Spectrum）"></a>补充一：频谱（Spectrum）</h3><p>频域即频率域，在频域图像（<strong>频谱</strong>）中，自变量是频率（横轴），纵轴是该频率信号的幅度（<strong>振幅</strong>）。频谱图描述了信号的频率结构 及 频率与该频率信号幅度的关系。</p><p><img src="/assets/post_img/article67/FD.png" alt="fm"></p><p>所以，以时间作为变量所进行的研究就是时域，以频率作为变量所进行的研究就是频域。下图生动的表示了这一点：</p><p><img src="/assets/post_img/article67/TDandFD.png" alt="fdandtd"></p><p>时域中的红色波形为三个正弦波相叠加的结果，频域中的三条蓝色竖线分别对应三个正弦波的频率与振幅关系。</p><h3 id="补充二：傅立叶分析（Fourier-analysis）"><a href="#补充二：傅立叶分析（Fourier-analysis）" class="headerlink" title="补充二：傅立叶分析（Fourier analysis）"></a>补充二：傅立叶分析（Fourier analysis）</h3><p>傅立叶分析是分析学中逐渐形成的一个重要分支，它研究并扩展<strong>傅立叶级数</strong>和<strong>傅立叶变换</strong>的概念，又称调和分析。</p><blockquote><p>Fourier在1807年发表的传记和1822年出版的《热分析理论》一书中指出：无论函数多么复杂，只要它是<strong>周期</strong>的，并且满足某些适度的条件，都可以表示为不同频率的正弦和（或）余弦函数之和的形式，每个正弦和（或）余弦函数都乘以不同的系数。</p></blockquote><p>我先简单说明一下傅立叶分析究竟用来干什么，仅对于图像方面。假如我现在有一个波形如下：</p><p><img src="/assets/post_img/article67/sin3sin5.webp" alt="sin35"></p><p>假如告诉你该<em>复杂周期信号</em>是由两个<em>简单周期信号</em>组成的（后面会提到这两个概念），其中一个是$\sin (3x)$，要求你把该信号从图中取出，并绘制出剩下的信号波形。显然，这个任务在时域几乎不可能完成。</p><p>但是在频域中，无非是几条竖线而已，取出其中一个再容易不过了。这是频域的意义。但是不要忘了，现实世界中只有时域，于是，<strong>傅立叶分析作为时域与频域的桥梁</strong>出现了，他允许我们从时域跨越到频域完成任务（取出$\sin (3x)$）。顺便一提，这个任务在工程上称为<strong>滤波</strong>。</p><p>傅立叶分析可以分为两块，傅立叶级数（Fourier series）用于处理连续周期函数，傅立叶变换(Fourier transformation)则可以处理连续非周期函数。这里的函数是数学上的说法，在物理上，称为<strong>信号</strong>。总而言之，傅立叶提出的两个重要观点如下：</p><ol><li>周期信号都可以表示为谐波关系的正弦信号的<strong>加权和</strong>（傅里叶级数）</li><li>非周期信号都可用正弦信号的<strong>加权积分</strong>表示（傅里叶变换）</li></ol><p>傅立叶选这个正余弦函数也是有原因的，具体可以看下面对<strong>正弦波</strong>的补充。其实信号不只能分解为正余弦信号，也可以是什么幂级数展开、泰勒展开之类的。</p><p>傅立叶变换（傅立叶级数是其特例）就是通过数学的方法反向分解复杂的信号，使之成为若干简单的正余弦信号，那么问题又来了，这么多简单的信号，每个正弦信号都在时域图上画出来也没有什么意义，那么人类只需要记住每个正弦信号的相位和角频率就可以了，记住后随时随地我们都可以制造这些简单的信号，把它们在时域上相加，就会再现相同的复杂信号。</p><p>这时，我们就需要考虑如何记录信号的相位和角频率。如我们所知，<strong>频谱</strong>只能反映角频率和振幅间的关系，但并未包含$\mathrm{A} \sin (\omega x + \phi)$中的全部信息。因此，还需要<strong>相位谱</strong>的参与。</p><h3 id="补充三：正弦波（Sine-wave）"><a href="#补充三：正弦波（Sine-wave）" class="headerlink" title="补充三：正弦波（Sine wave）"></a>补充三：正弦波（Sine wave）</h3><p>首先，正弦波就是一个圆周运动在一条直线上的投影。</p><p><img src="/assets/post_img/article67/circle-wave.gif" alt="s-w"></p><p><strong>正弦波</strong>是频域中唯一存在的波形，这是频域中最重要的规则，即正弦波是对频域的描述，因为频域中的任何波形都可用正弦波合成。这是正弦波的一个非常重要的性质。然而，它并不是正弦波的独有特性，还有许多其他的波形也有这样的性质。正弦波有四个性质使它可以有效地描述其他任一波形：</p><p>（1）频域中的任何波形都可以由正弦波的组合完全且惟一地描述。</p><p>（2）任何两个频率不同的正弦波都是<strong>正交</strong>的。如果将两个正弦波相乘并在整个时间轴上求积分，则积分值为零。这说明可以将不同的频率分量相互分离开。</p><p>（3）正弦波有精确的数学定义。</p><p>（4）正弦波及其微分值处处存在，没有上下边界。</p><p>使用正弦波作为频域中的函数形式有它特别的地方。如果变换到频域并使用正弦波描述，有时会比仅仅在时域中能更快地得到答案。</p><p>这里一个重要的意识是：<strong>正弦和余弦可以通过相位的转变相互转化</strong>。暂时把正余弦信号统称为正弦波。</p><p>正弦曲线可表示为：</p><script type="math/tex; mode=display">\mathrm{y}=\mathrm{A} \sin (\omega x + \phi)+\mathrm{k}</script><p>$k$、$\omega$ 和 $\phi$ 是常数 $(k, \omega, \phi \in R$ 且 $\omega \neq 0)$</p><p>A —— <strong>振幅</strong>，当物体作轨迹符合正弦曲线的直线往复运动时，其值为行程的 $\frac{1}{2}$ 。<br>$(\omega x + \phi)$ —— <strong>相位</strong>（Phase）， 反映变量y所处的状态。<br>$\phi$ —— 初相, $x=0$ 时的相位，反映在坐标系上则为图像的左右移动。<br>$\mathrm{k}$ —— 偏距, 反映在坐标系上则为图像的上移或下移。<br>$\omega$ —— 角速度，或<strong>角频率</strong>，控制正弦周期(单位弧度内震动的次数)。</p><p>由于正余弦信号只含有一个频率成分 $\omega$ ，当横轴上的$x$表示时间时，上式不仅是其时域描述也是其频域描述，无需进行变换，可见时域与频域是合二为一的，因此适合将其做为合成其他任意信号的基本信号。</p><h3 id="补充四：傅立叶级数与其频谱、相位谱"><a href="#补充四：傅立叶级数与其频谱、相位谱" class="headerlink" title="补充四：傅立叶级数与其频谱、相位谱"></a>补充四：傅立叶级数与其频谱、相位谱</h3><p>首先，周期信号可以分为 <strong>简单周期信号</strong> 和 <strong>复杂周期信号</strong>，两者的最大区别就是频率结构上分别是<strong>单频</strong>和<strong>多频</strong>。 对下面的例子来说，正余弦信号是单频信号，而方波信号是多频信号。</p><p>将若干个正余弦信号叠加，可得到方波信号，而通过傅立叶级数即可逆向将方波信号分解为多个正余弦信号之和。</p><p><img src="/assets/post_img/article67/F-series.gif" alt="f-series"></p><p>但若要形成标准的90度角矩形波，需要无穷多个正弦波叠加。我们需要接受一个设定：<strong>不仅仅是矩形，任何波形都是可以如此方法用正弦波叠加起来获得。</strong></p><p>下面再换一个角度看正弦波叠加：</p><p><img src="/assets/post_img/article67/rec-wave.png" alt="rec-wave"></p><p>在这几幅图中，最前面黑色的线就是所有正弦波叠加而成的总和，也就是越来越接近矩形波的那个图形。</p><p>后面依不同颜色排列而成的正弦波就是组合为矩形波的各个分量，称为<strong>频率分量</strong>。这些正弦波按照频率<em>从低到高</em>从前向后排列开来，且每一个波的振幅都是不同的。频率越高，波形越密集。</p><p>每两个正弦波之间都还有一条直线，那是<strong>振幅为0的正弦波</strong>。也就是说，为了组成特殊的曲线，有些正弦波成分是不需要的。</p><p>如果我们把第一个频率最低的频率分量看作<strong>基本单元</strong>，该正弦波的角频率为$w_0$，那么频域的基本单元就是$\omega_0$。 假设$\omega_0 = 10$，回到<a href="#补充一频谱">频谱</a>中，可以看到该频谱的基本单元正是10。</p><p>如果说频域的基本单元是“1”，那么频域的“0”就是一个周期无限长的正弦波（角频率为0），也就是一条直线，0频率也被称为<strong>直流分量</strong>。</p><p>上面已经把频域的事研究的差不多了，但如前文所述，频谱中的信息只是时域中信息的一部分，我们还需要反映相位的<strong>相位谱</strong>。那么相位谱在哪呢？请看下图，鉴于正弦波是周期的，需要设定一个用来标记正弦波位置的东西，即红色点，红点是距离频率轴最近的波峰。粉色点是红点在下平面的投影，<strong>粉点用于标注波峰距离频率轴的距离</strong>。</p><p><img src="/assets/post_img/article67/phase-detail.jpeg" alt="phase-detail"></p><p>如我们所见，如果说频谱为侧面投影的话，相位谱就是波形在下方的“投影”。但是这个投影不是直接投影，而是间接的。请看下图：</p><p><img src="/assets/post_img/article67/phase-spectrum.png" alt="phase-map"></p><p>说间接投影，就是因为<strong>相位不能直接用时间差表示</strong>，我们可以看到，时间差是粉点到时域纵轴的距离，我们记为$\varDelta t$，又可以得到当前频率分量的周期$T$，则有相位$p = \frac{\varDelta t}{T} \times 2\pi$，最终相位谱上的纵轴值（相位）是依据时间差和周期算出的。</p><p>最终的图如下所示，其实，频域图像也可以叫做<strong>幅度频谱</strong>，对应的，相位谱叫做<strong>相位频谱</strong>，因为他们的定义域都是频域，只是值域不同。</p><p><img src="/assets/post_img/article67/tfp.jpeg" alt="tfp"></p><h3 id="补充五：傅立叶变换与其频谱"><a href="#补充五：傅立叶变换与其频谱" class="headerlink" title="补充五：傅立叶变换与其频谱"></a>补充五：傅立叶变换与其频谱</h3><p>前面我们说到，傅立叶级数是傅立叶变换的一个特例，也就是说，傅立叶变换推广了傅立叶级数，这个“推广“体现在傅立叶变换可以处理<strong>时域上的非周期的连续信号</strong>，将其转换为一个在<strong>频域上的非周期连续信号</strong>。如下图：</p><p><img src="/assets/post_img/article67/Fourier-transformation.jpeg" alt="f-t"></p><p>图片右上角的<code>Frequency resolution</code>是频率分辨率，即数字信号处理系统将相距最近的两个频率分量区分开的能力，它与周期成反比，当一个连续信号是非周期信号时，可以将其周期看作无限大，则该信号近似看为周期无限大的周期连续信号。这代表<strong>非周期信号可以通过傅立叶变换分解为无限多个频率无限接近的正余弦信号的和</strong>。</p><p><img src="/assets/post_img/article67/discrete-spectrum.png" alt="ds"></p><p>试想一下，上面离散的频率分量逐渐靠近…直至连在一起，无穷无尽的分量铺满了空间。</p><p><img src="/assets/post_img/article67/continuous-spectrum.png" alt="cs"></p><p>变成了波涛汹涌的大海，这就是连续的频谱下发生的事情。那么，计算时离散频谱下分量的累加，自然也就变成了连续频谱上的<strong>积分</strong>。</p><p>至此，对于研究图像处理来说，似乎已经足够，数学上的东西，需要时再深究也不迟。</p><h3 id="补充六：虚数（Imaginary-number）-和-欧拉公式（Euler’s-formula）"><a href="#补充六：虚数（Imaginary-number）-和-欧拉公式（Euler’s-formula）" class="headerlink" title="补充六：虚数（Imaginary number） 和 欧拉公式（Euler’s formula）"></a>补充六：虚数（Imaginary number） 和 欧拉公式（Euler’s formula）</h3><p>我们知道，在实数轴上，乘$-1$其实就是使线段逆时针旋转了$180$度，那么乘一个$i$呢？显然，旋转$90$度。</p><p><img src="/assets/post_img/article67/imagine-real.jpeg" alt="i-r"></p><p>同时，我们获得了一个垂直的虚数轴。实数轴与虚数轴共同构成了一个复数的平面，也称复平面。这样我们就了解到，乘虚数i的一个功能——旋转。</p><p>欧拉公式如下（关于欧拉公式为何被称为“上帝创造的公式”此处不再赘述）：</p><script type="math/tex; mode=display">e^{i x}=\cos x+i \sin x</script><p>当$x = \pi$时，就有$e^{i x} + 1 = 0$。</p><p>这个公式的关键作用之一，是将正弦波统一成了简单的指数形式。</p><p><img src="/assets/post_img/article67/Euler.png" alt="euler"></p><p>欧拉公式所描绘的，是一个随着时间变化，在复平面上做圆周运动的点，随着时间的改变，在时间轴上就成了一条螺旋线。如果只看它的实数部分，也就是螺旋线在左侧的投影，就是一个最基础的余弦函数。而右侧的投影则是一个正弦函数。</p><p>欧拉公式的另一种形式是：</p><script type="math/tex; mode=display">\sin x=\frac{e^{i x}-e^{-i x}}{2 i}, \cos x=\frac{e^{i x}+e^{-i x}}{2}</script><h3 id="补充七：数学上的傅立叶级数和傅立叶变换"><a href="#补充七：数学上的傅立叶级数和傅立叶变换" class="headerlink" title="补充七：数学上的傅立叶级数和傅立叶变换"></a>补充七：数学上的傅立叶级数和傅立叶变换</h3><p>傅立叶级数的三角函数形式如下：</p><script type="math/tex; mode=display">f(t)=a_0+\sum_{n=1}^{\infty}\left[a_n \cos \left(n \omega t\right)+b_n \sin \left(n \omega t\right)\right], \\其中 \omega = \frac{2 \pi}{T}为基波频率, a_n和b_n为傅立叶系数，分别代表余弦分量振幅和正弦分量振幅.</script><p>傅立叶变换相关的，先放放吧。可以参考<a href="https://blog.csdn.net/Thera_qing/article/details/106154313">这</a><a href="https://blog.csdn.net/weixin_40851250/article/details/84780221">三</a><a href="https://blog.csdn.net/jack__linux/article/details/99671469">篇</a>。</p><h2 id="从信号的角度看图像"><a href="#从信号的角度看图像" class="headerlink" title="从信号的角度看图像"></a>从信号的角度看图像</h2><p>图像是信号，这是直觉上极难理解的。</p><p>首先，我们需要摆脱信号和时间“绑定”的固有直觉，信号只是由自变量和因变量构成的，自变量不一定是时间。</p><p>实际上，图像是一个沿空间分布的信号，他的定义域是图像的$x$轴或$y$轴。图像既然是沿着空间分布的信号，说明它是一个<strong>二维信号</strong>。如果一维数字信号可以看作是一组数字序列，那么二维数字信号则可以看作是一组数字阵列。图像中沿着水平方向的任何一行可以看作是像素点随着$x$轴变化的信号。而沿着竖直方向的任何一列则可以看作是像素点随着$y$轴变化的信号。单独提取一行或者是一列数据出来，按照像素值的大小随轴的变化画出来，就可以得到我们熟悉的信号波形，如下。</p><p><img src="/assets/post_img/article67/pic-signal.png" alt="pic-signal"></p><p>可以明显的看出，图片右方的波形代表黑色横线，因为除了塔尖以外像素变化不大。而下方波形则对应竖线，由上到下。</p><p>图像既然是信号，那么自然有频率、幅度和相位的特征。</p><p><strong>图像的频率对应到的是图像细节的多少</strong>，比如城市的部分频率高于天空的部分。再比如在灰度图中，图像的频率是<strong>表征图像中灰度变化剧烈程度的指标</strong>，是灰度在平面空间上的梯度。</p><p><strong>图像的幅度对应到的是图像中像素的大小</strong>，比如天空中太阳照亮的区域幅度高于其他区域。</p><p>假设有图片A和图片B，分别获取A、B的幅度频谱和相位频谱，然后将图片A的幅度谱与图片B的相位谱结合做逆傅立叶变换，最终结果的视觉效果会是图片B加上一些噪声，通过对比实验可以得到一个结论：<strong>图像的相位谱中，保留了图像的边缘以及整体结构的信息。</strong> 而错误的幅度谱看起来则像是噪声覆盖在原图上，但并没有影响图像的内容本身。</p><p>同时，和我们在一维信号中看到的一样，如果想要通过修改频域中相位或振幅的方式来间接修改原图像，则需要同时保留幅度频谱和相位频谱，来通过逆傅立叶变换得到修改后的图像。</p><h3 id="空域（Spatial-domain）"><a href="#空域（Spatial-domain）" class="headerlink" title="空域（Spatial domain）"></a>空域（Spatial domain）</h3><p>空间域也叫<strong>空域</strong>，即所说的像素域，在空域的处理就是在像素级的处理，即直接对图像上的像素值进行增加或减少。对应的另一种操作是在频域上的，空域通过傅立叶变换后，得到的是图像的频谱。表示图像的能量梯度。</p><h3 id="图像变换技术"><a href="#图像变换技术" class="headerlink" title="图像变换技术"></a>图像变换技术</h3><p>为了有效和快速地对图像进行处理和分析，需要将原定义在图像空间的图像以某种形式转换到另外的空间，利用空间的特有性质方便地进行一定的加工，最后再转换回图像空间以得到所需的效果。 如：空域与频域间的相互转化。</p><h3 id="信号分析在图像处理中的应用"><a href="#信号分析在图像处理中的应用" class="headerlink" title="信号分析在图像处理中的应用"></a>信号分析在图像处理中的应用</h3><p>很多图像处理事实上就是信号处理。比如图像去噪 (Denoise)，一般就是应用数字<strong>低通滤波器</strong>对图像进行滤波。图像增强 (Detail enhancement)，一般就是应用数字<strong>高通滤波器</strong>得到图像的高频信号，并对高频信号进行增强。对比度增强 (Contrast enhancement)，一般就是参考画面的亮暗程度 (图像的幅度)，并人为修改亮暗的一种处理。相位的概念一般会在图像的缩放 (Scaling) 中使用到。</p><h2 id="图像的灰度、亮度、强度"><a href="#图像的灰度、亮度、强度" class="headerlink" title="图像的灰度、亮度、强度"></a>图像的灰度、亮度、强度</h2><p><strong>图像灰度</strong>(Image grayscale)：把白色与黑色之间按对数关系分为若干等级，称为灰度，灰度分为256阶，从0到255。用灰度表示的图像称为<strong>灰度图</strong>，实际上，<strong>灰度表征的是单色的亮暗程度</strong>。</p><p><strong>图象亮度</strong>(Image brightness)：指画面的明亮程度，单位是 堪德拉每平米(cd/m2) 或称 nits。图象亮度是从白色表面到黑色表面的感觉连续体，由反射系数决定，亮度侧重物体，重在“反射”。在灰度图像中，亮度等于灰度，图像运算处理方式相同。但是在彩色图像中，亮度和对比度相关，即通过对RGB颜色分量的增加（增加亮度）或减少（减少亮度）相同的增量来显示，亮度的调整就是给每个分量乘以一个百分比值。</p><p><strong>图像强度</strong>(Image intensity)：表示单通道图像像素的强度（值的大小）。在灰度图像中，它是图像的灰度。在RGB颜色空间中，可以理解把它为是R通道的像素灰度值，G通道的像素灰度值，或是B通道的像素灰度值，也就是RGB中含三个图像强度。在其他颜色空间类似，也就是每个通道的图像的像素灰度值。</p><p>这里多说一下，为什么RGB也有灰度？实际上，<strong>RGB各单通道的图像也都是灰度图</strong>，只是最终成像时将他们分别放置在红、绿、蓝的单色通道中再叠加罢了，也就是说，一个RGB彩色图像，是由三个表示对应位置亮与暗程度的图像，通过放置在三个单色通道中再叠加形成的（这里属于理糙话不糙了）。</p><p><strong>认识到所有图片都是灰度图或其叠加后，很多图像处理方法的应用都可以举一反三了。</strong></p><h2 id="图像滤波"><a href="#图像滤波" class="headerlink" title="图像滤波"></a>图像滤波</h2><p>图像滤波，就是对图像的频率进行过滤。本文关注两类，即<strong>高通滤波</strong>和<strong>低通滤波</strong>。这两种滤波方式不难理解，从名字就可以看出，高通滤波就是<em>减弱或阻隔低频信号，保留高频信号</em>。而低通滤波则是<em>减弱或阻隔高频信号，保留低频信号</em>。</p><p>现在关键的问题是，图像中的高频信号和低频信号分别代表什么？结合前面所提到的，高（低）频信号也叫做高（低）频分量，图像的频率是灰度值变化剧烈程度的标准，可以推出<strong>高频信号就是灰度值变化剧烈的地方</strong>，同理，<strong>低频信号是图像灰度值变化平缓的地方</strong>。</p><p>具体些说，<em>高频信号就是相邻区域之间灰度相差很大的地方</em>，例如一个影像与背景的边缘部位，通常会有明显的差别，灰度变化很快，也就是变化频率高的部位。同样的，图像的细节处也是属于灰度值急剧变化的区域。另外，噪声（即噪点）也是这样，噪点之所以是噪点，就是因为它与附近的像素点灰度不一样了。</p><p>相对应的，<em>低频信号就是那些连续渐变的区域</em>。人眼对图像中的高频信号更为敏感，举例来说，在一张白纸上有一行字，那么人眼会直接聚焦在文字上，而不会太在意白纸本身，这里文字就是高频信号，而白纸就是低频信号。 </p><h3 id="频谱中心化"><a href="#频谱中心化" class="headerlink" title="频谱中心化"></a>频谱中心化</h3><p>研究数字图像有时需要变换到频域做处理，比如滤波等。但直接对数字图像进行二维DFT（离散傅立叶变换）得到的频谱图是<em>高频在中间，低频在四角</em>。频谱图比较亮的地方就是低频，因为<strong>图像的能量一般都是集中在低频部分</strong>。为了把能量集中起来便于使用滤波器，可以利用二维DFT的平移性质对频谱进行中心化。如下：</p><p><img src="/assets/post_img/article67/spectrum-central.png" alt="s-c"></p><p>经二维傅立叶变换的平移性质推导，结论是<strong>频谱中心化只需对数字图像的每个像素点的取值直接乘以$(-1)^{x+y}$就可以了，其中$x，y$为像素坐标</strong>。</p><h3 id="频域滤波"><a href="#频域滤波" class="headerlink" title="频域滤波"></a>频域滤波</h3><p>频域滤波的基本过程如下：<br>1、对原图像做频谱中心化处理，<br>2、对第一步的结果进行DFT，<br>3、使用某滤波器乘第二步的结果，<br>4、对第三步结果做反DFT，再做一次频谱中心化，得到滤波后图像。</p><h3 id="空域滤波"><a href="#空域滤波" class="headerlink" title="空域滤波"></a>空域滤波</h3><p><strong>空域滤波</strong>是指利用像素及像素邻域组成的空间进行图像增强的方法。之所以用“滤波”这个词，是因为借助了频域里的概念。并且，频率域的卷积与空间域的乘积存在对应关系，由卷积定理可知<strong>所有频域的滤波理论上都可以转化为空域的卷积操作</strong>。</p><p>空域滤波是在图像空间通过邻域操作完成的，邻域操作常借助<strong>模板运算</strong>来实现。由此也可以看出，空域滤波的算法比较简单，处理速度快。而频域滤波算法复杂，计算慢。（这里只是相比于做DFT来说，现在好像还有FFT可以加速运算，具体如何先不深究。）</p><p>图像的空域滤波还可以按照运算的方式分为<strong>线性滤波</strong>和<strong>非线性滤波</strong>，如果运算只是加权求和之类的操作，则为线性滤波。而若是在模版内进行求最值、绝对值等操作，则属于非线性滤波。</p><h3 id="模版运算"><a href="#模版运算" class="headerlink" title="模版运算"></a>模版运算</h3><p>模板也称为核（kernel）。模板运算的基本思路是将 某个像素的值 替换为 它本身灰度值和其相邻像素灰度值的函数值。</p><p>模板一般是$n \times n$的<strong>方阵</strong>（$n$通常是3、5、7、9等很小的<strong>奇数</strong>）。当$n$为奇数时，可以定义<em>模板的半径</em>$r = \frac{(n-1)}{2}$。模板中有一个<strong>锚点</strong>（anchor point），通常是矩阵中心点，和原图像中待计算点对应。整个模板对应的区域，就是原图像中像素点的相邻区域。一个$n \times n$的模板最多可有$n \times n$个系数，该模板的功能由这些系数的取值所决定。</p><p>以模版运算中最常用的<strong>模版卷积</strong>举例，其在空域实现的主要步骤如下：<br>1、将模板在图中漫游，并将模板锚点与图中某个像素位置重合（待计算点），<br>2、将模板上的各个系数与模板下各对应像素的灰度值相乘，<br>3、将所有乘积相加（为保持灰度范围，常将结果再除以模板的系数个数），<br>4、将上述运算结果（模板的输出响应）赋给待计算点的像素。</p><h2 id="高斯滤波（Gaussian-filter）"><a href="#高斯滤波（Gaussian-filter）" class="headerlink" title="高斯滤波（Gaussian filter）"></a>高斯滤波（Gaussian filter）</h2><p>高斯滤波是一种<strong>根据高斯函数的形状来选择权值</strong>的线性平滑滤波，适用于消除<em>高斯噪声</em>，广泛应用于图像处理的减噪过程。通俗的讲，高斯滤波就是对整幅图像进行加权平均的过程，每一个像素点的值，都由其本身和邻域内的其他像素值经过加权平均后得到。</p><p><strong>高斯噪声</strong>就是概率密度函数服从高斯分布的一类噪声。如果一个噪声，它的幅度分布服从高斯分布，而它的功率谱密度（频谱的绝对值平方？）又是均匀分布的，则称它为<strong>高斯白噪声</strong>。高斯白噪声的二阶矩不相关，一阶矩为常数，是指先后信号在时间上的相关性。</p><p>高斯滤波的空域实现步骤很简单，先获得<strong>高斯核</strong>，然后对图像做模版卷积即可（可以做padding）。关键还是在高斯核上，或者说，在于高斯核上的权值如何计算上。</p><p>当然了，<em>高斯滤波是指用高斯函数作为滤波函数</em>，具体上实现的是模糊还是锐化效果，要根据是高通还是低通来判别。高斯锐化的高斯核就是$1 - 高斯模糊卷积核$。</p><h3 id="高斯核"><a href="#高斯核" class="headerlink" title="高斯核"></a>高斯核</h3><p>一维和二维的高斯函数如下，标准差$\sigma$的大小决定了高斯函数的宽度：</p><p><img src="/assets/post_img/article67/gaussian.webp" alt="gaussian"></p><p>根据二维高斯函数图像来理解高斯滤波就非常的直观了，假设我们要实现<strong>高斯模糊</strong>（低通），在确定高斯核时，首先需要将锚点（核的中心点）作为高斯图像中的原点（山峰的中心点），然后确定一个标准差$\sigma$和核的半径$r$，根据$r$我们可以计算出高斯核中各点的坐标，通过坐标$(x,y)$和$\sigma$依次计算二维高斯函数值，并填入对应的位置，然后，还需要进行<em>归一化</em>处理，即将核中各值除去权重总和，使核的权重综合为$1$，这样就得到了最终的高斯核。</p><p>现在的问题是，如何选择$\sigma$和$r$呢？</p><p>对于标准差，可以从二维高斯函数图像来分析，当标准差越小时，图像中的“山峰”越“瘦高”，权重分布越向中心集中，平滑效果越差；当标准差越大时，“山峰”越“矮胖”，权重分布越均匀，则平滑效果越明显。（有理论说空域上的标准差和频域的标准差成负相关，以此可以进一步解释高斯核的高低通效果，但是笔者目前还没看明白，留待日后补充。）</p><p>半径的选择同样可以从图像来分析。理论上，高斯分布在定义域的所有地方都有非负值，这就需要一个无限大的核。然而钟形曲线在区间$(\mu - \sigma, \mu + \sigma)$范围内的面积占曲线下总面积的$68\%$，在区间$(\mu - 2\sigma, \mu + 2\sigma)$范围内的面积占$95\%$，在区间$(\mu - 3\sigma, \mu + 3\sigma)$范围内的面积占$99.7\%$，通常情况下$3\sigma$以外的区域所占面积已经很小，可以被忽略（认为该段分布近似包含所有情况）。所以，当半径取$3\sigma$时，就基本上可以近似使用了，此时核的大小为$(6\sigma + 1) \times (6\sigma + 1)$。</p><p>最后，对高斯核进行<em>归一化</em>处理的一个重要目的是确保卷积运算后，像素值处于$[0, 255]$的范围之内。</p><h3 id="实验-高斯模糊"><a href="#实验-高斯模糊" class="headerlink" title="实验-高斯模糊"></a>实验-高斯模糊</h3><p>光说不练，不是好文章，下面就实验一下实现高斯滤波。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gaussian_filter</span>(<span class="params">img, sigma=<span class="number">0.5</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(img.shape) == <span class="number">3</span>:</span><br><span class="line">        H, W, C = img.shape</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        img = np.expand_dims(img, axis=-<span class="number">1</span>)</span><br><span class="line">        H, W, C = img.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算半径，取4倍</span></span><br><span class="line">    radius = <span class="built_in">int</span>(sigma * <span class="number">4</span>)</span><br><span class="line">    <span class="comment"># 计算高斯核大小</span></span><br><span class="line">    n = <span class="number">2</span> * radius + <span class="number">1</span></span><br><span class="line">    <span class="comment"># padding大小</span></span><br><span class="line">    pad = n // <span class="number">2</span></span><br><span class="line">    <span class="comment"># padding的地方置为0</span></span><br><span class="line">    out = np.zeros((H + pad * <span class="number">2</span>, W + pad * <span class="number">2</span>, C), dtype=np.float64)</span><br><span class="line">    out[pad: pad + H, pad: pad + W] = img.copy().astype(np.float64)</span><br><span class="line">    <span class="comment"># 构造高斯核坐标</span></span><br><span class="line">    x_index, y_index = np.mgrid[-radius:radius + <span class="number">1</span>, -radius:radius + <span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 计算核值</span></span><br><span class="line">    k = np.exp(-(x_index ** <span class="number">2</span> + y_index ** <span class="number">2</span>) / (<span class="number">2</span> * sigma ** <span class="number">2</span>))</span><br><span class="line">    <span class="comment"># 归一化</span></span><br><span class="line">    k = k / k.<span class="built_in">sum</span>()</span><br><span class="line">    <span class="comment"># 原图像深拷贝</span></span><br><span class="line">    origin = out.copy()</span><br><span class="line">    <span class="comment"># 高斯核卷积，计算有点慢</span></span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(H):</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(W):</span><br><span class="line">            <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(C):</span><br><span class="line">                out[pad + y, pad + x, c] = np.<span class="built_in">sum</span>(k * origin[y: y + n, x: x + n, c])</span><br><span class="line">    <span class="comment"># 切除填充部分</span></span><br><span class="line">    out = out[pad: pad + H, pad: pad + W].astype(np.uint8)</span><br><span class="line">    <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 读取图像</span></span><br><span class="line">    img = cv2.imread(<span class="string">&quot;./data/bridge.jpeg&quot;</span>)</span><br><span class="line">    <span class="comment"># 滤波</span></span><br><span class="line">    result = gaussian_filter(img, sigma=<span class="number">2.5</span>)</span><br><span class="line">    <span class="comment"># 保存结果</span></span><br><span class="line">    cv2.imwrite(<span class="string">&quot;./data/bridge-blur.jpeg&quot;</span>, result)</span><br><span class="line">    <span class="comment"># 显示</span></span><br><span class="line">    cv2.imshow(<span class="string">&quot;bridge&quot;</span>, img)</span><br><span class="line">    cv2.imshow(<span class="string">&quot;bridge-blur&quot;</span>, result)</span><br><span class="line">    cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">    cv2.destroyAllWindows()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>输入（笔者在重庆随手拍的一个桥，叫什么忘记了）：<br><img src="/assets/post_img/article67/bridge.jpeg" alt="bridge"><br>输出：<br><img src="/assets/post_img/article67/bridge-blur.jpeg" alt="blur-bridge"></p><h2 id="双边滤波（Bilateral-filter）"><a href="#双边滤波（Bilateral-filter）" class="headerlink" title="双边滤波（Bilateral filter）"></a>双边滤波（Bilateral filter）</h2><p>开局先甩一个定义：</p><blockquote><p>双边滤波是一种非线性的保边滤波器，通常在计算机视觉中用作工作流中的简单降噪阶段。它将每个输出像素的强度计算为输入图像中附近像素的强度值的加权平均值。至关重要的是，权重不仅取决于当前像素和相邻像素之间的欧几里得距离，还取决于它们之间的辐射差异（例如颜色强度差异）。结果是保留了边缘，同时平滑了具有相似强度的区域。</p></blockquote><p>只看定义，好像懂了，又好像没懂，那么下面就来详细的解释一下。众所周知，图像的噪声和边缘都属于高频分量，高斯滤波在降低噪声的同时没有很好的保留边缘，而是一起“降”了，这对于人眼观看（或者特征提取）就不怎么友好了，形成的图片模糊成了一团。</p><p>这时，有人想了，能不能在降低噪声的同时，保持图像的边缘呢？这就是双边滤波干的事。双边滤波同样是基于模版运算的，他的特点在于<strong>模版的权值同时考虑了空域上距离的差别和灰度值上数值的差别</strong>。 这里后者也可以叫做<strong>灰度距离</strong>，与前者的欧式距离相呼应（没错前者实际上就是欧式距离）。</p><p>回顾高斯滤波中的高斯核，它的权值分配是按照二维高斯图像的形状分配的，这实际上是一种考虑空域上距离差别的方式，远离中心点的像素点会被分配到更少的权值。</p><p>那么为什么要考虑灰度值的差别呢？这是由于<strong>边缘的表现其实是灰度值的突变</strong>，如果想要保留边缘，则不能将高灰度值的区域与低灰度值的区域相“混合”。传统的高斯核无法考虑当前覆盖区域的灰度值信息。</p><p>由此，双边滤波的核函数是<strong>空域核</strong>与<strong>值域（灰度值域）核</strong>的综合：在图像的平坦区域，也就是灰度值变化小的区域，值域核权重趋于$1$，此时空域核权重起主要作用，与高斯模糊类似；在图像的含边缘区域，灰度值变化很大，则值域核权重会增大，从而起到保留边缘的效果。如下图：</p><p><img src="/assets/post_img/article67/bilateral-filter.png" alt="b-f"></p><p>图片左侧输入表示输入图像的灰度值形状，可以明显看到图像左侧整体灰度值低而右侧整体灰度值高，值变化很大，图像中部存在一个断层，这表示该区域属于<strong>含边缘区域</strong>。而断层两侧的部分，也有起起伏伏的小土包，这表示灰度值变化很小的区域，即<strong>平坦区域</strong>。当核移动到图中红色箭头所指位置的正上方时，空域核权重与值域核权重的情况如方框中所示，显然，该位置属于含边缘区域，值域核在形状上也表征为断层。它与空域核结合后的结果就是最终的核形状，即一个被切了一半的高斯图像，这个核的权重也很容易想象，整体上，左侧权重低而右侧权重高，起到了保留边缘的作用。</p><p>以上，应该能够理解双边滤波是如何做的了，那具体怎么做呢？接着往下看。</p><p>双边滤波器中，最终输出像素的值$g$同样依赖于领域像素值的加权组合：</p><script type="math/tex; mode=display">g(i, j)=\frac{\sum_{k, l} f(k, l) w(i, j, k, l)}{\sum_{k, l} w(i, j, k, l)}</script><p>这里，$(i,j)$是待计算位置的坐标，$(k,l)$是领域像素位置坐标，$函数f$表示计算灰度。核权重$w$就是重点了，它取决于空域核权重$s$与值域核权重$r$：</p><script type="math/tex; mode=display">s(i, j, k, l)=\exp \left(-\frac{(i-k)^2+(j-l)^2}{2 \sigma_s^2}\right), \\r(i, j, k, l)=\exp \left(- \frac{(f(i,j)-f(k,l))^2}{2 \sigma_r^2}\right)</script><p>$\sigma_s$ 和 $\sigma_r$ 分别是空域高斯（Spatial Gaussian）和值域高斯（Range Gaussian）的标准差，代表了对图片的滤波程度。$w$取决于$s$和$r$的乘积：</p><script type="math/tex; mode=display">w(i, j, k, l) = \exp \left(-\frac{(i-k)^2+(j-l)^2}{2 \sigma_s^2} - \frac{(f(i,j)-f(k,l))^2}{2 \sigma_r^2}\right)</script><p>根据该权重函数，就可以获得双边滤波的核了，然后再进行模版卷积，归一化处理即可。</p><p>正式的一个公式应该是：</p><script type="math/tex; mode=display">B F[I]_{\mathrm{p}}=\frac{1}{W_{\mathrm{p}}} \sum_{\mathbf{q} \in \mathcal{S}} G_{\sigma_s}(\|\mathbf{p}-\mathbf{q}\|) G_{\sigma_r}\left(\lvert I_{\mathbf{p}}-I_{\mathbf{q}} \rvert \right) I_{\mathbf{q}}</script><p>其中$W_{\mathrm{p}}$是一个归一化因子（normalization factor）：</p><script type="math/tex; mode=display">W_{\mathrm{p}} = \sum_{\mathbf{q} \in \mathcal{S}} G_{\sigma_s}(\|\mathbf{p}-\mathbf{q}\|) G_{\sigma_r}\left(\lvert I_{\mathbf{p}}-I_{\mathbf{q}} \rvert \right)</script><p>这里，$B F[I]_{\mathrm{p}}$为像素$p$滤波后的灰度值；$I_{\mathbf{p}}$为像素$p$原本的灰度值；$\mathrm{p}$则表示像素$p$的空间坐标；$G_{\sigma_s}$和$G_{\sigma_r}$分别为空域高斯和值域高斯；$|\mathbf{p}-\mathbf{q}|$表示取模，即欧式距离；$\lvert I_{\mathbf{p}}-I_{\mathbf{q}} \rvert$为灰度值差的绝对值；像素$q$为领域点；$\mathcal{S}$表示整个核区域内的点。</p><p>这样解读完以后，其实和一开始的公式是一样的。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]<a href="https://zhuanlan.zhihu.com/p/19763358">https://zhuanlan.zhihu.com/p/19763358</a><br>[2]<a href="https://www.zhihu.com/question/21817515/answer/472164871">https://www.zhihu.com/question/21817515/answer/472164871</a><br>[3]<a href="https://blog.csdn.net/jack__linux/article/details/99671469">https://blog.csdn.net/jack__linux/article/details/99671469</a><br>[4]<a href="https://zhuanlan.zhihu.com/p/53071745">https://zhuanlan.zhihu.com/p/53071745</a><br>[5]<a href="https://blog.csdn.net/u010430651/article/details/95340236">https://blog.csdn.net/u010430651/article/details/95340236</a><br>[6]<a href="https://zhuanlan.zhihu.com/p/40301384">https://zhuanlan.zhihu.com/p/40301384</a><br>[7]<a href="https://blog.csdn.net/jack__linux/article/details/99671469">https://blog.csdn.net/jack__linux/article/details/99671469</a><br>[8]<a href="https://blog.csdn.net/Chevy_cxw/article/details/110948262">https://blog.csdn.net/Chevy_cxw/article/details/110948262</a><br>[9]<a href="https://blog.csdn.net/dengheCSDN/article/details/78906126">https://blog.csdn.net/dengheCSDN/article/details/78906126</a><br>[10]<a href="https://www.zhihu.com/question/29246532/answer/1473209132">https://www.zhihu.com/question/29246532/answer/1473209132</a><br>[11]<a href="https://blog.csdn.net/rocketeerLi/article/details/87986751">https://blog.csdn.net/rocketeerLi/article/details/87986751</a><br>[12]<a href="https://blog.csdn.net/weixin_44479045/article/details/104948535">https://blog.csdn.net/weixin_44479045/article/details/104948535</a><br>[13]<a href="https://blog.csdn.net/silence2015/article/details/53789748">https://blog.csdn.net/silence2015/article/details/53789748</a><br>[14]<a href="https://blog.csdn.net/weixin_43135178/article/details/115453145">https://blog.csdn.net/weixin_43135178/article/details/115453145</a><br>[15]<a href="https://blog.csdn.net/jialeheyeshu/article/details/51097860">https://blog.csdn.net/jialeheyeshu/article/details/51097860</a><br>[16]<a href="https://blog.csdn.net/qq_36607894/article/details/92809731">https://blog.csdn.net/qq_36607894/article/details/92809731</a><br>[17]<a href="https://www.jianshu.com/p/fbde7bdb256d">https://www.jianshu.com/p/fbde7bdb256d</a><br>[18]<a href="https://blog.csdn.net/qimingxia/article/details/89111897">https://blog.csdn.net/qimingxia/article/details/89111897</a><br>[19]<a href="https://blog.csdn.net/qq_41603898/article/details/81674987">https://blog.csdn.net/qq_41603898/article/details/81674987</a><br>[20]<a href="https://blog.csdn.net/sunmc1204953974/article/details/50634652">https://blog.csdn.net/sunmc1204953974/article/details/50634652</a><br>[21]<a href="https://www.jianshu.com/p/73e6ccbd8f3f?u_atoken=80aa2d72-8ab8-4b03-893a-a25ecc6f099f&amp;u_asession=01etOGNYlCJWvXBvSQngE-TGud_9aeeQYAvlS2WuwpQOq_wctc1A74a7od4e-upYygX0KNBwm7Lovlpxjd_P_q4JsKWYrT3W_NKPr8w6oU7K8LeXkNNqHvfNEX4gbXYnPLg0pn3tpfEcqG8HZmzd6q3mBkFo3NEHBv0PZUm6pbxQU&amp;u_asig=05D7OEW9nUnnFJ-ggxnAnVvWOUdkfmuhIUCY6TZqOqY-eeonucHFTEdDgyfQgZ-VHHeWSFSyR-Vy_Q8xsXHLJZQp0NfuTWlhk0fCigqVK1DCFbYmyFkNL-jcmynjX_hmMoxm6kbvl94B967SOLkRJntrO6BEKNazId1s3Et_W-l5P9JS7q8ZD7Xtz2Ly-b0kmuyAKRFSVJkkdwVUnyHAIJzbYjwP9YcDA_b__QLeG_TK-M2bdIetoqxc5CZJ9nLRpI6xaDswPo-3_59so9Oh9f1-3h9VXwMyh6PgyDIVSG1W-t741JlKYWli8UYkMs4RIiQKranM7rBKraCitp69DNzOmP87UHXNQfNZeF1yHm4yNyHvomyVImJs8-nyATChPamWspDxyAEEo4kbsryBKb9Q&amp;u_aref=G%2BN03JEAOlDe%2B8ugCiZkCbuRn%2FU%3D">简单易懂的高斯滤波-简书（链接太长了）</a><br>[22]<a href="https://blog.csdn.net/weixin_42985978/article/details/126517088">https://blog.csdn.net/weixin_42985978/article/details/126517088</a><br>[23]<a href="https://blog.csdn.net/blogshinelee/article/details/82734769">https://blog.csdn.net/blogshinelee/article/details/82734769</a><br>[24]<a href="https://blog.csdn.net/lz0499/article/details/54015150">https://blog.csdn.net/lz0499/article/details/54015150</a><br>[25]<a href="https://blog.csdn.net/huachizi/article/details/88951061">https://blog.csdn.net/huachizi/article/details/88951061</a><br>[26]<a href="https://docs.nvidia.com/vpi/algo_bilat_filter.html">https://docs.nvidia.com/vpi/algo_bilat_filter.html</a><br>[27]<a href="http://www.360doc.com/content/17/0306/14/28838452_634420847.shtml">http://www.360doc.com/content/17/0306/14/28838452_634420847.shtml</a><br>[28]<a href="https://people.csail.mit.edu/sparis/bf_course/course_notes.pdf">https://people.csail.mit.edu/sparis/bf_course/course_notes.pdf</a><br>[29]<a href="https://zhuanlan.zhihu.com/p/127023952">https://zhuanlan.zhihu.com/p/127023952</a><br>[30]<a href="https://blog.csdn.net/MoFMan/article/details/77482794">https://blog.csdn.net/MoFMan/article/details/77482794</a><br>[31]<a href="https://zhuanlan.zhihu.com/p/180497579">https://zhuanlan.zhihu.com/p/180497579</a></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;学习一下高斯滤波和双边滤波，及其需要的前置知识，仅记录一下个人理解，先后逻辑比较混乱，参考请谨慎。如有错误请评论区指正，感谢。&lt;/p&gt;
&lt;p&gt;文中图片多数来自网络，我尽量擦去了水印，是为了提升观感，参考文献在文末有注明。本文的出发点，大概是在尽量用直观、快速的方式了解这些抽象概念吧。&lt;br&gt;</summary>
    
    
    
    
    <category term="计算机视觉" scheme="http://silencezheng.top/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>各种“监督”学习区分</title>
    <link href="http://silencezheng.top/2022/10/01/article66/"/>
    <id>http://silencezheng.top/2022/10/01/article66/</id>
    <published>2022-10-01T12:52:54.000Z</published>
    <updated>2022-10-01T12:53:12.630Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>区分一下各种含“监督”的学习，包括无监督、弱监督、半监督、监督、自监督。<br><span id="more"></span></p><h2 id="无监督学习-unsupervised-learning"><a href="#无监督学习-unsupervised-learning" class="headerlink" title="无监督学习(unsupervised learning)"></a>无监督学习(unsupervised learning)</h2><blockquote><p>无监督学习是机器学习的一种方法，没有给定事先标记过的训练示例，自动对输入的资料进行分类或分群。无监督学习的主要运用包含：聚类分析（cluster analysis）、关系规则（association rule）、维度缩减（dimensionality reduce）。</p></blockquote><p>可以根据特点来认识无监督学习：</p><ol><li>无监督学习是没有明确目的的训练方式，你无法提前知道结果是什么。</li><li>无监督学习<strong>不需要给数据打标签</strong>。</li><li>无监督学习几乎无法量化效果如何。</li></ol><p>无监督学习是一种机器学习的训练方式，它本质上是一个统计手段，在没有标签的数据里可以发现潜在的一些结构的一种训练方式。</p><p>无监督学习的应用，比如对用户行为进行分类，筛选异常行为用户等等。</p><h2 id="监督学习-supervised-learning"><a href="#监督学习-supervised-learning" class="headerlink" title="监督学习(supervised learning)"></a>监督学习(supervised learning)</h2><p>已知数据和其一一对应的标签，训练一个智能算法，将输入数据映射到标签的过程。监督学习是最常见的学习问题之一，例如给定一组猪的图片，并作图像级标注分类为猪，用监督学习训练一个算法可以判断新输入的图片是否是猪。</p><p>与无监督学习相比：</p><ol><li>监督学习有明确的训练目的</li><li>监督学习的训练数据必须有标签</li><li>监督学习可以量化效果</li></ol><h2 id="弱监督学习-weakly-supervised-learning"><a href="#弱监督学习-weakly-supervised-learning" class="headerlink" title="弱监督学习(weakly supervised learning)"></a>弱监督学习(weakly supervised learning)</h2><p>已知数据和其一一对应的<strong>弱标签</strong>，训练一个智能算法，将输入数据映射到一组更强的标签的过程。</p><p><strong>标签的强弱指的是标签蕴含的信息量的多少</strong>，比如相对于分割的标签来说，分类的标签就是弱标签。再比如对于弱监督目标检测，就是数据只有图像级标注，比如图片的分类，要求算法获取目标边界框的学习任务。</p><h2 id="半监督学习-semi-supervised-learning"><a href="#半监督学习-semi-supervised-learning" class="headerlink" title="半监督学习(semi-supervised learning)"></a>半监督学习(semi-supervised learning)</h2><p>已知数据和部分数据一一对应的标签，有一部分数据的标签未知，训练一个智能算法，学习已知标签和未知标签的数据，将输入数据映射到标签的过程。半监督通常是一个数据的标注非常困难，比如说医院的检查结果，医生也需要一段时间来判断健康与否，可能只有几组数据知道是健康还是非健康，其他的只有数据不知道是不是健康。那么通过有监督学习和无监督的结合的半监督学习就在这里发挥作用了。</p><p>总之，是在数据标注困难的情况下，<strong>使用少量标注数据和其他未标注数据进行学习</strong>的训练方式。</p><h2 id="自监督学习-self-supervised-learning"><a href="#自监督学习-self-supervised-learning" class="headerlink" title="自监督学习(self-supervised learning)"></a>自监督学习(self-supervised learning)</h2><p>基于监督学习当前的主要瓶颈是 标签生成和标注 的现状，人们提出了一个问题：</p><blockquote><p>我们是否可以通过特定的方式设计任务，即可以从现有图像中生成几乎无限的标签，并以此来学习特征表示？</p></blockquote><p>这道出了自监督学习的理想状态，我们希望同时拥有监督学习的明确性和无监督学习的自由性，<strong>理想的自监督学习能够在给定的无标注数据中自动生成我们需要的标签，并根据该标签和数据进行监督学习。</strong></p><p>自监督学习的核心是<strong>如何给输入数据自动生成标签</strong>。之前的很多工作都是围绕这个核心展开的。一般的套路是：首先提出一个新的自动打标签的<strong>辅助任务</strong>（pretext task），用辅助任务自动生成标签，然后做实验、测性能、发文章。每年都有新的辅助任务被提出来，自监督学习的性能也在不断提高，有的甚至已经接近监督学习的性能。总体上说，或者是提出一种完全新的辅助任务，或者是把多个旧的辅助任务组合到一起作为一个“新”的辅助任务。</p><p>自监督学习的应用，有图像着色、图像超分辨率、图像修补等等，其学习到的特征表示通常可以用于下游任务。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;区分一下各种含“监督”的学习，包括无监督、弱监督、半监督、监督、自监督。&lt;br&gt;</summary>
    
    
    
    
    <category term="机器学习" scheme="http://silencezheng.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>什么是语义分割、实例分割、全景分割</title>
    <link href="http://silencezheng.top/2022/09/29/article65/"/>
    <id>http://silencezheng.top/2022/09/29/article65/</id>
    <published>2022-09-29T13:19:15.000Z</published>
    <updated>2022-09-29T13:20:47.640Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>看见一篇好文，不得不转，<a href="https://zhuanlan.zhihu.com/p/368904941">原地址在这</a>，一文分清语义分割问题。<br><span id="more"></span></p><h2 id="图像分类（image-classification）"><a href="#图像分类（image-classification）" class="headerlink" title="图像分类（image classification）"></a>图像分类（image classification）</h2><p>识别图像中存在的内容，如下图，有人（person）、树（tree）、草地（grass）、天空（sky）。</p><p><img src="/assets/post_img/article65/classification.webp" alt="ic"></p><h2 id="目标检测（object-detection）"><a href="#目标检测（object-detection）" class="headerlink" title="目标检测（object detection）"></a>目标检测（object detection）</h2><p>识别图像中存在的内容和检测其位置，如下图，以识别和检测人（person）为例。</p><p><img src="/assets/post_img/article65/object-detection.webp" alt="od"></p><h2 id="语义分割（semantic-segmentation）"><a href="#语义分割（semantic-segmentation）" class="headerlink" title="语义分割（semantic segmentation）"></a>语义分割（semantic segmentation）</h2><p>对图像中的每个像素打上类别标签，如下图，把图像分为人（红色）、树木（深绿）、草地（浅绿）、天空（蓝色）标签。</p><p><img src="/assets/post_img/article65/semantic-segmentation.webp" alt="ss"></p><h2 id="实例分割（instance-segmentation）"><a href="#实例分割（instance-segmentation）" class="headerlink" title="实例分割（instance segmentation）"></a>实例分割（instance segmentation）</h2><p>目标检测和语义分割的结合，在图像中将目标检测出来（目标检测），然后对每个像素打上标签（语义分割）。对比上图、下图，如以人（person）为目标，语义分割不区分属于相同类别的不同实例（所有人都标为红色），实例分割区分同类的不同实例（使用不同颜色区分不同的人）。</p><p><img src="/assets/post_img/article65/instance-segmentation.webp" alt="is"></p><h2 id="全景分割（panoptic-segmentation）"><a href="#全景分割（panoptic-segmentation）" class="headerlink" title="全景分割（panoptic segmentation）"></a>全景分割（panoptic segmentation）</h2><p>语义分割和实例分割的结合，即要对所有目标都检测出来，又要区分出同个类别中的不同实例。对比上图、下图，实例分割只对图像中的目标（如上图中的人）进行检测和按像素分割，区分不同实例（使用不同颜色），而全景分割是对图中的所有物体包括背景都要进行检测和分割，区分不同实例（使用不同颜色）。</p><p><img src="/assets/post_img/article65/panoptic-segmentation.webp" alt="ps"></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;看见一篇好文，不得不转，&lt;a href=&quot;https://zhuanlan.zhihu.com/p/368904941&quot;&gt;原地址在这&lt;/a&gt;，一文分清语义分割问题。&lt;br&gt;</summary>
    
    
    
    
    <category term="机器学习" scheme="http://silencezheng.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>M系芯片配置TensorFlow环境</title>
    <link href="http://silencezheng.top/2022/09/28/article64/"/>
    <id>http://silencezheng.top/2022/09/28/article64/</id>
    <published>2022-09-28T10:09:13.000Z</published>
    <updated>2022-10-03T03:07:52.462Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>如题，需求MacOS版本12+，主要参考<a href="https://developer.apple.com/metal/tensorflow-plugin/">官方教程</a>。<br><span id="more"></span></p><h2 id="前置"><a href="#前置" class="headerlink" title="前置"></a>前置</h2><p>有一个conda环境，进入。</p><h2 id="安装TensorFlow"><a href="#安装TensorFlow" class="headerlink" title="安装TensorFlow"></a>安装TensorFlow</h2><h3 id="1-安装TensorFlow依赖"><a href="#1-安装TensorFlow依赖" class="headerlink" title="1. 安装TensorFlow依赖"></a>1. 安装TensorFlow依赖</h3><p><code>conda install -c apple tensorflow-deps</code></p><h3 id="2-安装TensorFlow"><a href="#2-安装TensorFlow" class="headerlink" title="2. 安装TensorFlow"></a>2. 安装TensorFlow</h3><p><code>python -m pip install tensorflow-macos</code></p><h3 id="3-安装tensorflow-metal插件"><a href="#3-安装tensorflow-metal插件" class="headerlink" title="3. 安装tensorflow-metal插件"></a>3. 安装tensorflow-metal插件</h3><p><code>python -m pip install tensorflow-metal</code></p><h3 id="4-安装keras"><a href="#4-安装keras" class="headerlink" title="4. 安装keras"></a>4. 安装keras</h3><p><code>python -m pip install keras</code></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;如题，需求MacOS版本12+，主要参考&lt;a href=&quot;https://developer.apple.com/metal/tensorflow-plugin/&quot;&gt;官方教程&lt;/a&gt;。&lt;br&gt;</summary>
    
    
    
    
    <category term="MacOS" scheme="http://silencezheng.top/tags/MacOS/"/>
    
  </entry>
  
</feed>
