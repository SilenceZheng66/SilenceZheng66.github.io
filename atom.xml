<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>SilenceZheng66-BLOG</title>
  
  <subtitle>log</subtitle>
  <link href="http://silencezheng.top/atom.xml" rel="self"/>
  
  <link href="http://silencezheng.top/"/>
  <updated>2022-10-22T16:33:20.039Z</updated>
  <id>http://silencezheng.top/</id>
  
  <author>
    <name>SilenceZheng66</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>语义分割常用评价指标</title>
    <link href="http://silencezheng.top/2022/10/23/article71/"/>
    <id>http://silencezheng.top/2022/10/23/article71/</id>
    <published>2022-10-22T16:26:05.000Z</published>
    <updated>2022-10-22T16:33:20.039Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>关于语义分割领域的常用评价指标进行一些个人解读，欢迎批评指正。</p><p>本来是想连目标检测的一起说了的，但是关于AP的一些东西始终有疑问，留着后面搞懂了再写一篇吧。<br><span id="more"></span></p><h2 id="前置理解"><a href="#前置理解" class="headerlink" title="前置理解"></a>前置理解</h2><p>在解读这些评价指标之前，需要对深度学习方法有一个基础认识。</p><p>首先，评价指标，是指测试阶段将预测结果与真实值进行比对得到的量化结论。<strong>评价指标（Metric）和损失函数（Loss）有联系也有区别</strong>，目前在我个人理解来说，评价指标是以实用的角度评价模型，只关心模型的结果，而损失函数是从数学的角度收敛模型，但损失往往应该与一个你最关心的评价指标相对应，通过收敛损失能够达到向指标增大的方向靠拢。并且，损失应该是容易优化的，很多时候它们对模型参数可微，甚至是凸的。下面引[10]中的例子做说明。</p><blockquote><p>假设某同学备战高考，他给自己定下了一个奋斗的方向，即每周要把自己的各科总成绩提高5分；经过多年的准备，终于在高考中取得了好成绩（710分，总分750），被北大录取。<br>分析该例子，该同学“每周要把自己的各科总成绩提高5分”这个指导原则相当于目标函数，在这个指导原则的指引下，想必该同学的总分会越来越高，即模型被训练的越来越好。<br>最终，该同学高考成绩优异，相当于模型的测试效果良好，至于用从哪个角度评价这名同学，可以用其高考总分与750分的差距来衡量，也可以用其被录取的大学的水平来衡量，这就如同模型的评估指标是多种多样的，比如分类问题中的准确率、召回率等。<br>当然，模型的评估指标多样，模型的损失函数也是多样的；该例中，该同学可以将“每周要把自己的各科总成绩提高5分”作为指导原则，也可将“每周比之前多学2个知识点”作为指导原则。<br>另外，如果该同学将“每周模拟高考总分与750分的差距”同时作为指导原则与评价角度，则类似于线性回归模型将“MSE均方误差”同时作为损失函数与评估指标。<br>该例中，备考的“指导原则”相当于“损失函数”，“评价角度”相当于“评估指标”，该同学相当于一个机器学习模型。</p></blockquote><p>其次，在多分类任务中，通常包含$n$个类别，而对于某一样本的最终预测只能是$n$个类别中的一个。但是，算法对一个样本的类别预测通常以置信度的形式表示，最终选择置信度最高的类别作为预测输出。</p><p>最后，多分类任务对于每个类来看，可以看作是一个二分类问题，以样本对于该类别预测是否正确作为区分。</p><h2 id="混淆矩阵（Confusion-Matrix）"><a href="#混淆矩阵（Confusion-Matrix）" class="headerlink" title="混淆矩阵（Confusion Matrix）"></a>混淆矩阵（Confusion Matrix）</h2><p>混淆矩阵用于直观的显示模型预测结果的情形。 混淆矩阵中的横纵轴都是类别，对于$p_{ij}$（横坐标为$i$,纵坐标为$j$处的值），其含义为属于类别$i$并被预测为类别$j$的<strong>样本数量</strong>（在语义分割中通常样本等同于像素）。也就是说，每个位置的<strong>横坐标表示模型的预测，纵坐标表示真实标签</strong>。</p><p>对于二分类问题，混淆矩阵可以表示如下：</p><p><img src="/assets/post_img/article71/confusion-matrix-2classes.jpeg" alt="cm2"></p><p>若令其中$1$表示正类，$0$表示负类，则可以定义如下四个量：</p><ul><li>TP(True Positive)：将正样本预测为正类的数量，即图中的$a$。</li><li>FN(False Negative)：将正样本预测为负类的数量，即图中的$b$。</li><li>FP(False Positive)：将负样本预测为正类的数量，即图中的$c$。</li><li>TN(True Negative)：将负样本预测为负类的数量，即图中的$d$。</li></ul><p>对于多分类问题，只是把该矩阵由$2 \times 2$变化为$n \times n$，其中$n$表示类别数量。</p><p>从混淆矩阵中我们可以获得一些基础信息，如：</p><ul><li>$i$行的和$\sum^n_{j=1}p_{ij}$表示数据集中属于类别$i$的样本个数</li><li>$j$列的和$\sum^n_{i=1}p_{ij}$表示模型预测中属于类别$j$的样本个数</li><li>矩阵中所有元素的和$\sum^n_{i=1}\sum^n_{j=1}p_{ij}$表示图像中的总样本个数</li><li>…</li></ul><h2 id="精确率（Precision）和召回率（Recall）"><a href="#精确率（Precision）和召回率（Recall）" class="headerlink" title="精确率（Precision）和召回率（Recall）"></a>精确率（Precision）和召回率（Recall）</h2><p>这两个指标都是<strong>针对某一类别</strong>而言的，是分类任务的常用评价指标。</p><p>精确率又称查准率，含义是对于模型预测中属于类别$j$的样本，预测结果正确的比例。例如对于二分类问题，正类的精确率$Precision_{positive} = \frac{TP}{TP+FP} = \frac{a}{a+c}$。</p><p>召回率又称查全率，如果说精准率是站在预测的角度看问题，那么召回率就是站在现实的角度看问题，其含义是对于数据集中属于类别$i$的样本，被正确预测的比例。例如对于二分类问题，负类的召回率$Recall_{negative} = \frac{TN}{FP+TN} = \frac{d}{c+d}$。</p><h2 id="准确率（Accuracy）"><a href="#准确率（Accuracy）" class="headerlink" title="准确率（Accuracy）"></a>准确率（Accuracy）</h2><p>准确率需要和精确率区别开，准确率是站在预测的整体角度看问题，其含义是预测正确的样本占所有样本的比例。例如对于二分类问题，预测的准确率$Accuracy_{predict} = \frac{TP+TN}{TP+FN+FP+TN} = \frac{a+d}{a+b+c+d}$。可以看出，准确率其实就是混淆矩阵对角线元素和与所有元素和的比值。</p><h2 id="F1指标（F1-Score）和F-Beta指标（F-Beta-Score）"><a href="#F1指标（F1-Score）和F-Beta指标（F-Beta-Score）" class="headerlink" title="F1指标（F1 Score）和F-Beta指标（F-Beta Score）"></a>F1指标（F1 Score）和F-Beta指标（F-Beta Score）</h2><p>单独用精确率或召回率有时不能很好的评估模型，例如在二分类问题中，模型选择对所有样本预测为正类，此时所有正类样本都被“准确”的预测了，正类召回率为$1$，但模型实际上很差。</p><p>F1指标就是用来平衡精确率和召回率的重要程度的度量指标，它被定义为两者的<strong>调和平均值</strong>，表示二者重要程度一致。F1指标的计算公式如下：</p><script type="math/tex; mode=display">F_1 = 2 \times \frac{precision \cdot recall}{precision + recall}</script><p>调和平均值的一个重要特性就是如果两者极度不平衡，调和平均值会很小，只有当两者都较高时，调和平均才会比较高。</p><p>而F-Beta指标则是更一般的形式，他的计算方式如下：</p><script type="math/tex; mode=display">F_{\beta} = (1+\beta^2) \cdot \frac{precision \cdot recall}{(\beta^2 \cdot precision) + recall}</script><p>其中参数$\beta$决定了精确率和召回率的重要程度比值，当$\beta&gt;1$时召回率比重更大，当$\beta&lt;1$时精确率比重更大。</p><h2 id="特异性（Specificity）和敏感性（Sensitivity）"><a href="#特异性（Specificity）和敏感性（Sensitivity）" class="headerlink" title="特异性（Specificity）和敏感性（Sensitivity）"></a>特异性（Specificity）和敏感性（Sensitivity）</h2><p>关于这两个指标，似乎是仅针对于二分类问题而言的，这里只谈一些个人理解。</p><p>还是参照二分类的混淆矩阵，特异性实际上指的就是负类的召回率$Recall_{negative}$，而敏感性则指的是正类的召回率$Recall_{positive}$，看了网上许多解释，都是聚焦在医疗领域，把患病作为正类，健康作为负类，说什么敏感性越高，漏诊概率越低；特异性越高，确诊概率越高。</p><p>个人理解，实际上就是召回率在特定情况下的应用吧。</p><h2 id="交并比（Intersection-over-Union，IoU）和平均交并比（mIoU）"><a href="#交并比（Intersection-over-Union，IoU）和平均交并比（mIoU）" class="headerlink" title="交并比（Intersection over Union，IoU）和平均交并比（mIoU）"></a>交并比（Intersection over Union，IoU）和平均交并比（mIoU）</h2><p>给定两个区域$A$和$B$，IoU就是两区域的交集与两区域并集的比值：</p><script type="math/tex; mode=display">IoU = \frac{A \cap B}{A \cup B}</script><p><img src="/assets/post_img/article71/IoU.png" alt="iou"></p><p>在分类任务中，可以对某一类别的预测结果和真实标签求IoU，例如对于二分类求正类的IoU如下：</p><script type="math/tex; mode=display">IoU_{positive} = \frac{TP}{TP+FP+FN} = \frac{a}{a+b+c}</script><p>也就是说，混淆矩阵中$i$行和$i$列的交集比上它们的并集。</p><p><strong>平均交并比</strong>（mean IoU）就是对每一个类别求IoU，再求和求平均得到的值。</p><p>对于目标检测，IoU还有一个重要的应用，就是判断预测框与真实框的贴合程度，两部分重合面积越大，则IoU值越大。IoU是一个比较严格的评价指标，当两区域稍微有偏差时，IoU值也可能变得相当小，于是通常认为IoU大于$0.5$时就获得了一个比较不错的预测框。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]<a href="https://zhuanlan.zhihu.com/p/111234566">https://zhuanlan.zhihu.com/p/111234566</a><br>[2]<a href="https://zhuanlan.zhihu.com/p/101566089">https://zhuanlan.zhihu.com/p/101566089</a><br>[3]<a href="https://blog.csdn.net/h1yupyp/article/details/80842172">https://blog.csdn.net/h1yupyp/article/details/80842172</a><br>[4]<a href="https://blog.csdn.net/lhxez6868/article/details/108150777">https://blog.csdn.net/lhxez6868/article/details/108150777</a><br>[5]<a href="https://zhuanlan.zhihu.com/p/371819054">https://zhuanlan.zhihu.com/p/371819054</a><br>[6]<a href="https://www.jianshu.com/p/22d947ffb71e">https://www.jianshu.com/p/22d947ffb71e</a><br>[7]<a href="https://zhuanlan.zhihu.com/p/372402161">https://zhuanlan.zhihu.com/p/372402161</a><br>[8]<a href="https://zhuanlan.zhihu.com/p/373658488">https://zhuanlan.zhihu.com/p/373658488</a><br>[9]<a href="https://zhuanlan.zhihu.com/p/373032887">https://zhuanlan.zhihu.com/p/373032887</a><br>[10]<a href="https://www.cnblogs.com/pythonfl/p/13705143.html">https://www.cnblogs.com/pythonfl/p/13705143.html</a></p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于<a href="https://silencezheng.top/">silencezheng.top</a>。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;关于语义分割领域的常用评价指标进行一些个人解读，欢迎批评指正。&lt;/p&gt;
&lt;p&gt;本来是想连目标检测的一起说了的，但是关于AP的一些东西始终有疑问，留着后面搞懂了再写一篇吧。&lt;br&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://silencezheng.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="计算机视觉" scheme="http://silencezheng.top/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>M1 Mac weasyprint安装使用</title>
    <link href="http://silencezheng.top/2022/10/19/article70/"/>
    <id>http://silencezheng.top/2022/10/19/article70/</id>
    <published>2022-10-19T15:54:48.000Z</published>
    <updated>2022-10-19T15:57:18.582Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>为了把HTML转成PDF（带很多css的），折腾了一晚上。试了一下pdfkit是基本没戏（<a href="https://pypi.org/project/pdfkit/">pdfkit css bug</a>），转头想用weasyprint，没想到适配更差，记录一下。<br><span id="more"></span></p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p><code>brew install cairo pango gdk-pixbuf libffi</code></p><p><code>pip install weasyprint</code></p><h2 id="在conda环境下使用weasyprint"><a href="#在conda环境下使用weasyprint" class="headerlink" title="在conda环境下使用weasyprint"></a>在conda环境下使用weasyprint</h2><p>本来如果在brew的python3下使用应该没什么问题，但是如果要用conda环境的解释器就会报错：<code>OSError: cannot load library &#39;gobject-2.0-0&#39;</code></p><p>解决方案如下：<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo ln -s <span class="regexp">/opt/</span>homebrew<span class="regexp">/opt/g</span>lib<span class="regexp">/lib/</span>libgobject-<span class="number">2.0</span>.<span class="number">0</span>.dylib <span class="regexp">/usr/</span>local<span class="regexp">/lib/g</span>object-<span class="number">2.0</span></span><br><span class="line">sudo ln -s <span class="regexp">/opt/</span>homebrew<span class="regexp">/opt/</span>pango<span class="regexp">/lib/</span>libpango-<span class="number">1.0</span>.dylib <span class="regexp">/usr/</span>local<span class="regexp">/lib/</span>pango-<span class="number">1.0</span></span><br><span class="line">sudo ln -s <span class="regexp">/opt/</span>homebrew<span class="regexp">/opt/</span>harfbuzz<span class="regexp">/lib/</span>libharfbuzz.dylib <span class="regexp">/usr/</span>local<span class="regexp">/lib/</span>harfbuzz</span><br><span class="line">sudo ln -s <span class="regexp">/opt/</span>homebrew<span class="regexp">/opt/</span>fontconfig<span class="regexp">/lib/</span>libfontconfig.<span class="number">1</span>.dylib <span class="regexp">/usr/</span>local<span class="regexp">/lib/</span>fontconfig-<span class="number">1</span></span><br><span class="line">sudo ln -s <span class="regexp">/opt/</span>homebrew<span class="regexp">/opt/</span>pango<span class="regexp">/lib/</span>libpangoft2-<span class="number">1.0</span>.dylib <span class="regexp">/usr/</span>local<span class="regexp">/lib/</span>pangoft2-<span class="number">1.0</span></span><br></pre></td></tr></table></figure></p><p>创建对应位置的软链接，<a href="https://github.com/Kozea/WeasyPrint/issues/1448">issue在这</a>。</p><p>这样以后在终端用是没什么问题了，<code>weasyprint url xx.pdf</code>，中文支持不佳，css部分支持不好。</p><h2 id="仍然报错"><a href="#仍然报错" class="headerlink" title="仍然报错"></a>仍然报错</h2><p>在正常Python调用中仍然会报错，<code>RuntimeError: cannot use unpack() on &lt;cdata &#39;char *&#39; NULL&gt;</code>，定位到<code>cffi/api.py</code>，一个空指针，暂时不知道怎么解决。</p><p>CFFI(C Foreign Function Interface) 是Python的C语言外部函数接口。通过CFFI，Python可以与几乎任何C语言代码进行交互。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;为了把HTML转成PDF（带很多css的），折腾了一晚上。试了一下pdfkit是基本没戏（&lt;a href=&quot;https://pypi.org/project/pdfkit/&quot;&gt;pdfkit css bug&lt;/a&gt;），转头想用weasyprint，没想到适配更差，记录一下。&lt;br&gt;</summary>
    
    
    
    
    <category term="MacOS" scheme="http://silencezheng.top/tags/MacOS/"/>
    
  </entry>
  
  <entry>
    <title>动态规划</title>
    <link href="http://silencezheng.top/2022/10/11/article69/"/>
    <id>http://silencezheng.top/2022/10/11/article69/</id>
    <published>2022-10-11T10:15:29.000Z</published>
    <updated>2022-10-11T10:20:49.606Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Dynamic Programming学习，由于动态规划问题很多，且没有固定套路，本文会随时进行增补、修改。</p><p>另外，这里写的动态规划是笔者两天速成理解的，和真实的动态规划可能相去甚远，希望大家多多指教。<br><span id="more"></span></p><h2 id="概念（随便找了一个）"><a href="#概念（随便找了一个）" class="headerlink" title="概念（随便找了一个）"></a>概念（随便找了一个）</h2><p>动态规划是研究多步决策过程<strong>最优化</strong>问题的一种数学方法。在动态规划中，为了寻找一个问题的最优解（即最优决策过程），将整个问题划分成若干个相应的阶段，并在每个阶段都根据先前所作出的决策作出当前阶段最优决策，进而得出整个问题的最优解。即<em>记住已知问题的答案，在已知的答案的基础上解决未知的问题。</em></p><h2 id="能解决的问题"><a href="#能解决的问题" class="headerlink" title="能解决的问题"></a>能解决的问题</h2><p>1、计数问题<br>例如“有多少种方式使得…”，或者“有多少种方法选出…”。</p><p>2、最值问题<br>例如经典的“最少用多少枚硬币能组合出目标面值”，或者“求最长上升子序列”。</p><p>3、存在性问题<br>例如“能不能选出k个数使得…”，或者“先手方是否必胜”。</p><p>注意，通常求所有解法的问题不能用DP方法来做，因为与最优解无关的解在动态规划中不会被全部计算。</p><h2 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h2><p>在上述问题中，可能会有许多可行解。每一个解都对应于一个值，我们希望找到具有最优值的解。动态规划算法的基本思想是将待求解问题分解成若干个子问题，先求解子问题，然后从这些子问题的解得到原问题的解。与分治法不同的是，适合于用动态规划求解的问题，经分解得到<strong>子问题往往不是互相独立的</strong>。如果能够保存已解决的子问题的答案，而在需要时再找出已求得的答案，这样就可以避免大量的重复计算，节省时间。</p><p>这种记住子问题的做法很容易联想到记忆化，但 <strong>动态规划</strong> 和 <strong>记忆化搜索</strong> 的一个重要区别是动态规划通常不使用递归实现。 另外在计算顺序方面，多数动态规划问题是自底向上的（这与状态转移有关），而记忆化搜索是自顶向下的。</p><p>通常可以用一个表来记录所有已解的子问题的答案。不管该子问题以后是否被用到，只要它被计算过，就将其最优解填入表中，方便在求解之后的子问题时可以方便调用，进而求出整个问题的最优解。这就是动态规划法的基本思路。具体的动态规划算法多种多样，但它们具有相同的填表格式。</p><p>也可以说，动态规划最核心的思想，就在于<strong>拆分子问题，记住过往，减少重复计算</strong>。</p><h2 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h2><p>通常通过动态规划求解问题由四个主要部分组成：<br>1、拆分子问题<br>2、确定状态转移方程<br>3、确定初状态和边界条件<br>4、确定计算顺序</p><p>这四个部分看起来简单易懂，但是在实践过程中却会遇到重重困难。下面分别详细的解释一下。</p><p>首先需要明确动态规划解决的是一个多步决策问题，该问题由初状态、若干中间决策和末状态组成。</p><h3 id="1、拆分子问题"><a href="#1、拆分子问题" class="headerlink" title="1、拆分子问题"></a>1、拆分子问题</h3><p>拆分子问题，即将原问题拆解成为<strong>范围更小但性质不变的子问题</strong>。</p><p>一般首先要做的是找出“最后一步”，也就是若干中间决策的最后一步，经历该决策后，问题转变到末状态。</p><p>找到“最后一步”后，我们研究“最后一步”前的状态，它应该能够构成一个子问题，i.e.，如果说“最后一步”使问题达到末状态，且这一系列决策构成问题的最优解，那么去掉“最后一步”的决策链应该是最优解的一个子集，同时“最后一步”前的状态构成一个子问题，该子集为该子问题的最优解。</p><p>这样一来，我们就从原问题中拆出了一个子问题。<strong>拆分子问题的目的是找出状态</strong>，状态是对问题各阶段的客观表述，同时需要满足<em>无后效性</em>，即当前状态之后的决策对该状态无影响。</p><h3 id="2、确定状态转移方程"><a href="#2、确定状态转移方程" class="headerlink" title="2、确定状态转移方程"></a>2、确定状态转移方程</h3><p>状态转移方程是动态规划的重中之重，一旦确定了状态转移方程，问题通常也就迎刃而解了。</p><p><strong>状态转移就是根据上一阶段的状态和之后的一次决策来导出本阶段的状态</strong>。所以如果确定了决策，状态转移方程也就可写出。 但事实上常常是反过来做，根据相邻两个阶段的状态之间的关系来确定决策方法和状态转移方程。 状态转移方程通常是一个递推公式。</p><p>同时，状态转移方程也影响着最终算法的计算顺序，通常靠后的状态会需求前方状态的信息，原问题为求到最后状态的最优解，因此动态规划问题通常都是自底向上计算的。</p><p>我们用一个数组或哈希表来记录状态的最优解，本文暂时称其为<strong>状态表</strong>。</p><h3 id="3、确定初状态和边界情况"><a href="#3、确定初状态和边界情况" class="headerlink" title="3、确定初状态和边界情况"></a>3、确定初状态和边界情况</h3><p>初状态就是问题的初始状态。 确定初状态主要是确定状态表如何初始化。</p><p>边界情况也称边界条件，即状态转移方程的边界，有时在递推公式的某一部分会出现不合理的或不属于问题范围内的项，需要通过边界条件来限制它。</p><h3 id="4、确定计算顺序"><a href="#4、确定计算顺序" class="headerlink" title="4、确定计算顺序"></a>4、确定计算顺序</h3><p>如上面所说的，根据状态转移公式来确定。</p><h2 id="案例一：摩天大楼"><a href="#案例一：摩天大楼" class="headerlink" title="案例一：摩天大楼"></a>案例一：摩天大楼</h2><p>题目如下：</p><p><img src="/assets/post_img/article69/question.jpeg" alt="skyscraper"></p><p>拿到题目我们首先考虑一下暴力法，从暴力法的实现中可以看出有没有优化的余地，下面是我手写的一个过程，以输入$[2, 5, 1, 4, 8]$为例。</p><p><img src="/assets/post_img/article69/brutal-force.png" alt="bf"></p><p>可以发现有很多重复计算，可以通过记忆化进行优化。 暴力法代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># author：SilenceZheng66</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">brutal_force</span>(<span class="params">L: <span class="built_in">list</span>[<span class="built_in">int</span>]</span>):</span></span><br><span class="line">    n = <span class="built_in">len</span>(L)</span><br><span class="line"></span><br><span class="line">    results = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> n &lt;= <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">to_top</span>(<span class="params">pos, times</span>):</span></span><br><span class="line">        <span class="keyword">if</span> pos == n - <span class="number">1</span>:</span><br><span class="line">            results.append(times)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(pos + <span class="number">1</span>, pos + L[pos] + <span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> j &lt; n:</span><br><span class="line">                    to_top(j, times + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> L[<span class="number">0</span>] &gt;= n - <span class="number">1</span>:</span><br><span class="line">        results.append(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, L[<span class="number">0</span>] + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> i &lt; n:</span><br><span class="line">                to_top(i, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">min</span>(results)</span><br></pre></td></tr></table></figure><p>下面我们开始用动态规划的解题步骤尝试去优化我们的算法。</p><h3 id="1、拆分子问题-1"><a href="#1、拆分子问题-1" class="headerlink" title="1、拆分子问题"></a>1、拆分子问题</h3><p>原问题是：找出从底层到楼顶的最少乘电梯数。我们以数组的下标$pos$来进行说明，底层的$pos = 0$，楼顶的$pos = n-1$。</p><p>假设“最后一步”前的状态为 $pos = m$ ，那么“最后一步”就是 $n-1$ 位于 $m + 1$ 和 $m + L[m]$ 之内，也可以表示为 $m + L[m] \geq n-1$。 此时原问题就被拆分成了子问题：找出从底层到$m+1$层的最少乘电梯数。 原问题的答案为该子问题答案$+1$。 </p><p>那么此时我们就可以描述状态了，$f(x) = 到达x层的最少乘电梯数$。</p><h3 id="2、确定状态转移方程-1"><a href="#2、确定状态转移方程-1" class="headerlink" title="2、确定状态转移方程"></a>2、确定状态转移方程</h3><p>第一步中我们拆分了子问题，确定了状态$f(x)$，则原问题可以描述为状态表中的最后一个状态，即$f(n-1)$。 根据“最后一步”中的推断，最理想的状态是能直接找到$f(n-1)$的前一个最优状态$f(m)$，然后再找到$f(m)$的前一最优个状态…直到正好找到$f(0)$。但是这是不可能实现的。</p><p>回到现实，还是考虑输入$[2, 5, 1, 4, 8]$，很自然的能够得出$f(4) = min(f(3), f(1)) + 1$，那么问题来了，为什么不是$min(f(3),f(2),f(1),f(0))$？很明显，我们需要考虑电梯的上升能力，即$5$层前有哪些层是能直接到达$5$层的？这些能直接到达的层里，哪些层的$f(x)$最小？找出他们，再加上$1$，就得到了$f(4)$。</p><p>基于以上分析，我们可以得出状态转移方程：</p><script type="math/tex; mode=display">f(x) = min(g(f(x-1), ... ,f(0))) + 1</script><p>其中$g(f(x-1), … ,f(0))$表示从$pos=0$到$pos=x-1$中选出那些满足$L[pos] + pos \geq x $的$f(pos)$。</p><p>得出了状态转移方程，这道题也就拿下来一多半了。</p><h3 id="3、确定初状态和边界情况-1"><a href="#3、确定初状态和边界情况-1" class="headerlink" title="3、确定初状态和边界情况"></a>3、确定初状态和边界情况</h3><p>初状态是好确定的，因为要做求最小运算，所以状态表初始化时所有元素应为系统能取到的最大正整数值。而$f(0)$应该被置为$0$，因为到达ground floor的最少乘电梯次数是$0$次。</p><p>关于边界情况，可以看到状态转移方程中有涉及到$L[pos] + pos$的运算，这可能会超出数组上界，故需要考虑进去。 同时如果大楼的层数$\leq 1$，则可以直接得出$f(n-1) = f(0) = 0$，也可以考虑进去。</p><h3 id="4、确定计算顺序-1"><a href="#4、确定计算顺序-1" class="headerlink" title="4、确定计算顺序"></a>4、确定计算顺序</h3><p>从状态转移公式可以看出，靠后的状态依赖前方状态信息，故应为自底向上。</p><p>最后就是写代码了，这里提供一个我写的示范。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># author：SilenceZheng66</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dp</span>(<span class="params">L: <span class="built_in">list</span>[<span class="built_in">int</span>]</span>):</span></span><br><span class="line">    n = <span class="built_in">len</span>(L)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 状态数组</span></span><br><span class="line">    conditions = [sys.maxsize <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始状态</span></span><br><span class="line">    conditions[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 边界情况</span></span><br><span class="line">    <span class="keyword">if</span> n &lt;= <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 自底向上</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n - <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 边界情况判断</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i + <span class="number">1</span>, i + L[i] + <span class="number">1</span>) <span class="keyword">if</span> i + L[i] + <span class="number">1</span> &lt;= n <span class="keyword">else</span> <span class="built_in">range</span>(i + <span class="number">1</span>, n):</span><br><span class="line">            conditions[j] = <span class="built_in">min</span>(conditions[i] + <span class="number">1</span>, conditions[j])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> conditions[-<span class="number">1</span>]</span><br></pre></td></tr></table></figure></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;Dynamic Programming学习，由于动态规划问题很多，且没有固定套路，本文会随时进行增补、修改。&lt;/p&gt;
&lt;p&gt;另外，这里写的动态规划是笔者两天速成理解的，和真实的动态规划可能相去甚远，希望大家多多指教。&lt;br&gt;</summary>
    
    
    
    
    <category term="数据结构与算法" scheme="http://silencezheng.top/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>复活Google翻译</title>
    <link href="http://silencezheng.top/2022/10/06/article68/"/>
    <id>http://silencezheng.top/2022/10/06/article68/</id>
    <published>2022-10-06T08:21:31.000Z</published>
    <updated>2022-10-06T08:23:51.710Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>9月末，谷歌旗下网页翻译工具谷歌翻译停止了中国区服务，现在访问<code>translate.google.cn</code>网页会指向谷歌香港站，此做法与此前谷歌搜索、谷歌地图等功能退出中国大陆时一致。</p><p>也就是说，Chrome的内置翻译也不能正常使用了。好在目前可以通过修改host文件的方式复活谷歌翻译。<br><span id="more"></span></p><h2 id="找可用IP"><a href="#找可用IP" class="headerlink" title="找可用IP"></a>找可用IP</h2><p>打开：<a href="https://ping.chinaz.com/translate.google.cn">https://ping.chinaz.com/translate.google.cn</a></p><p>随便找一个IP就行，如果想再精细一点，可以找到距离自己物理位置较近的同网络运营商的IP地址。</p><h2 id="修改host文件"><a href="#修改host文件" class="headerlink" title="修改host文件"></a>修改host文件</h2><p>把如下内容增加到host文件中，其中IP就是第一步找的IP，可以多填几个，省的后面IP不能用了再加。<br><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">220.181.174.98</span> translate.googleapis.com</span><br></pre></td></tr></table></figure></p><p>对于Windows，该文件的目录通常为<code>C:\Windows\System32\drivers\etc\hosts</code>；</p><p>对于MacOS，为<code>/private/etc/hosts</code>；</p><p>对于Linux，为<code>/etc/hosts</code>；</p><p>Android和ios也有对应方式，没啥用就不写了，详细可以看<a href="https://pangniao.net/google-translate.html">这里</a>。</p><p>修改完了source一下就可以使用了，如果不行就重启浏览器。</p><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>这个方法就是通过关联可用IP和翻译API的域名使翻译服务可用，如果没有可用IP了这个办法就行不通了。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;9月末，谷歌旗下网页翻译工具谷歌翻译停止了中国区服务，现在访问&lt;code&gt;translate.google.cn&lt;/code&gt;网页会指向谷歌香港站，此做法与此前谷歌搜索、谷歌地图等功能退出中国大陆时一致。&lt;/p&gt;
&lt;p&gt;也就是说，Chrome的内置翻译也不能正常使用了。好在目前可以通过修改host文件的方式复活谷歌翻译。&lt;br&gt;</summary>
    
    
    
    
    <category term="Chrome" scheme="http://silencezheng.top/tags/Chrome/"/>
    
  </entry>
  
  <entry>
    <title>高斯滤波与双边滤波</title>
    <link href="http://silencezheng.top/2022/10/05/article67/"/>
    <id>http://silencezheng.top/2022/10/05/article67/</id>
    <published>2022-10-05T15:14:18.000Z</published>
    <updated>2022-10-06T01:47:02.633Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>学习一下高斯滤波和双边滤波，及其需要的前置知识，仅记录一下个人理解，先后逻辑比较混乱，参考请谨慎。如有错误请评论区指正，感谢。</p><p>文中图片多数来自网络，我尽量擦去了水印，是为了提升观感，参考文献在文末有注明。本文的出发点，大概是在尽量用直观、快速的方式了解这些抽象概念吧。<br><span id="more"></span></p><h2 id="频域（Frequency-domain）、时域（Time-domain）"><a href="#频域（Frequency-domain）、时域（Time-domain）" class="headerlink" title="频域（Frequency domain）、时域（Time domain）"></a>频域（Frequency domain）、时域（Time domain）</h2><p><strong>时域</strong>（Time domain）是描述数学函数或物理信号对时间的关系。例如一个信号的时域波形可以表达信号随着时间的变化。</p><p><strong>频域</strong>（Frequency domain）是描述信号在频率方面特性时用到的一种坐标系。频域最重要的性质是：它不是真实的，而是一个数学构造。时域是惟一客观存在的域，而频域是一个遵循特定规则的数学范畴，频域也被一些学者称为上帝视角。</p><p>以上是对时域和频域下的初步定义，时域是好理解的，但要真正理解频域，还需要若干补充文字。</p><h3 id="补充一：频谱（Spectrum）"><a href="#补充一：频谱（Spectrum）" class="headerlink" title="补充一：频谱（Spectrum）"></a>补充一：频谱（Spectrum）</h3><p>频域即频率域，在频域图像（<strong>频谱</strong>）中，自变量是频率（横轴），纵轴是该频率信号的幅度（<strong>振幅</strong>）。频谱图描述了信号的频率结构 及 频率与该频率信号幅度的关系。</p><p><img src="/assets/post_img/article67/FD.png" alt="fm"></p><p>所以，以时间作为变量所进行的研究就是时域，以频率作为变量所进行的研究就是频域。下图生动的表示了这一点：</p><p><img src="/assets/post_img/article67/TDandFD.png" alt="fdandtd"></p><p>时域中的红色波形为三个正弦波相叠加的结果，频域中的三条蓝色竖线分别对应三个正弦波的频率与振幅关系。</p><h3 id="补充二：傅立叶分析（Fourier-analysis）"><a href="#补充二：傅立叶分析（Fourier-analysis）" class="headerlink" title="补充二：傅立叶分析（Fourier analysis）"></a>补充二：傅立叶分析（Fourier analysis）</h3><p>傅立叶分析是分析学中逐渐形成的一个重要分支，它研究并扩展<strong>傅立叶级数</strong>和<strong>傅立叶变换</strong>的概念，又称调和分析。</p><blockquote><p>Fourier在1807年发表的传记和1822年出版的《热分析理论》一书中指出：无论函数多么复杂，只要它是<strong>周期</strong>的，并且满足某些适度的条件，都可以表示为不同频率的正弦和（或）余弦函数之和的形式，每个正弦和（或）余弦函数都乘以不同的系数。</p></blockquote><p>我先简单说明一下傅立叶分析究竟用来干什么，仅对于图像方面。假如我现在有一个波形如下：</p><p><img src="/assets/post_img/article67/sin3sin5.webp" alt="sin35"></p><p>假如告诉你该<em>复杂周期信号</em>是由两个<em>简单周期信号</em>组成的（后面会提到这两个概念），其中一个是$\sin (3x)$，要求你把该信号从图中取出，并绘制出剩下的信号波形。显然，这个任务在时域几乎不可能完成。</p><p>但是在频域中，无非是几条竖线而已，取出其中一个再容易不过了。这是频域的意义。但是不要忘了，现实世界中只有时域，于是，<strong>傅立叶分析作为时域与频域的桥梁</strong>出现了，他允许我们从时域跨越到频域完成任务（取出$\sin (3x)$）。顺便一提，这个任务在工程上称为<strong>滤波</strong>。</p><p>傅立叶分析可以分为两块，傅立叶级数（Fourier series）用于处理连续周期函数，傅立叶变换(Fourier transformation)则可以处理连续非周期函数。这里的函数是数学上的说法，在物理上，称为<strong>信号</strong>。总而言之，傅立叶提出的两个重要观点如下：</p><ol><li>周期信号都可以表示为谐波关系的正弦信号的<strong>加权和</strong>（傅里叶级数）</li><li>非周期信号都可用正弦信号的<strong>加权积分</strong>表示（傅里叶变换）</li></ol><p>傅立叶选这个正余弦函数也是有原因的，具体可以看下面对<strong>正弦波</strong>的补充。其实信号不只能分解为正余弦信号，也可以是什么幂级数展开、泰勒展开之类的。</p><p>傅立叶变换（傅立叶级数是其特例）就是通过数学的方法反向分解复杂的信号，使之成为若干简单的正余弦信号，那么问题又来了，这么多简单的信号，每个正弦信号都在时域图上画出来也没有什么意义，那么人类只需要记住每个正弦信号的相位和角频率就可以了，记住后随时随地我们都可以制造这些简单的信号，把它们在时域上相加，就会再现相同的复杂信号。</p><p>这时，我们就需要考虑如何记录信号的相位和角频率。如我们所知，<strong>频谱</strong>只能反映角频率和振幅间的关系，但并未包含$\mathrm{A} \sin (\omega x + \phi)$中的全部信息。因此，还需要<strong>相位谱</strong>的参与。</p><h3 id="补充三：正弦波（Sine-wave）"><a href="#补充三：正弦波（Sine-wave）" class="headerlink" title="补充三：正弦波（Sine wave）"></a>补充三：正弦波（Sine wave）</h3><p>首先，正弦波就是一个圆周运动在一条直线上的投影。</p><p><img src="/assets/post_img/article67/circle-wave.gif" alt="s-w"></p><p><strong>正弦波</strong>是频域中唯一存在的波形，这是频域中最重要的规则，即正弦波是对频域的描述，因为频域中的任何波形都可用正弦波合成。这是正弦波的一个非常重要的性质。然而，它并不是正弦波的独有特性，还有许多其他的波形也有这样的性质。正弦波有四个性质使它可以有效地描述其他任一波形：</p><p>（1）频域中的任何波形都可以由正弦波的组合完全且惟一地描述。</p><p>（2）任何两个频率不同的正弦波都是<strong>正交</strong>的。如果将两个正弦波相乘并在整个时间轴上求积分，则积分值为零。这说明可以将不同的频率分量相互分离开。</p><p>（3）正弦波有精确的数学定义。</p><p>（4）正弦波及其微分值处处存在，没有上下边界。</p><p>使用正弦波作为频域中的函数形式有它特别的地方。如果变换到频域并使用正弦波描述，有时会比仅仅在时域中能更快地得到答案。</p><p>这里一个重要的意识是：<strong>正弦和余弦可以通过相位的转变相互转化</strong>。暂时把正余弦信号统称为正弦波。</p><p>正弦曲线可表示为：</p><script type="math/tex; mode=display">\mathrm{y}=\mathrm{A} \sin (\omega x + \phi)+\mathrm{k}</script><p>$k$、$\omega$ 和 $\phi$ 是常数 $(k, \omega, \phi \in R$ 且 $\omega \neq 0)$</p><p>A —— <strong>振幅</strong>，当物体作轨迹符合正弦曲线的直线往复运动时，其值为行程的 $\frac{1}{2}$ 。<br>$(\omega x + \phi)$ —— <strong>相位</strong>（Phase）， 反映变量y所处的状态。<br>$\phi$ —— 初相, $x=0$ 时的相位，反映在坐标系上则为图像的左右移动。<br>$\mathrm{k}$ —— 偏距, 反映在坐标系上则为图像的上移或下移。<br>$\omega$ —— 角速度，或<strong>角频率</strong>，控制正弦周期(单位弧度内震动的次数)。</p><p>由于正余弦信号只含有一个频率成分 $\omega$ ，当横轴上的$x$表示时间时，上式不仅是其时域描述也是其频域描述，无需进行变换，可见时域与频域是合二为一的，因此适合将其做为合成其他任意信号的基本信号。</p><h3 id="补充四：傅立叶级数与其频谱、相位谱"><a href="#补充四：傅立叶级数与其频谱、相位谱" class="headerlink" title="补充四：傅立叶级数与其频谱、相位谱"></a>补充四：傅立叶级数与其频谱、相位谱</h3><p>首先，周期信号可以分为 <strong>简单周期信号</strong> 和 <strong>复杂周期信号</strong>，两者的最大区别就是频率结构上分别是<strong>单频</strong>和<strong>多频</strong>。 对下面的例子来说，正余弦信号是单频信号，而方波信号是多频信号。</p><p>将若干个正余弦信号叠加，可得到方波信号，而通过傅立叶级数即可逆向将方波信号分解为多个正余弦信号之和。</p><p><img src="/assets/post_img/article67/F-series.gif" alt="f-series"></p><p>但若要形成标准的90度角矩形波，需要无穷多个正弦波叠加。我们需要接受一个设定：<strong>不仅仅是矩形，任何波形都是可以如此方法用正弦波叠加起来获得。</strong></p><p>下面再换一个角度看正弦波叠加：</p><p><img src="/assets/post_img/article67/rec-wave.png" alt="rec-wave"></p><p>在这几幅图中，最前面黑色的线就是所有正弦波叠加而成的总和，也就是越来越接近矩形波的那个图形。</p><p>后面依不同颜色排列而成的正弦波就是组合为矩形波的各个分量，称为<strong>频率分量</strong>。这些正弦波按照频率<em>从低到高</em>从前向后排列开来，且每一个波的振幅都是不同的。频率越高，波形越密集。</p><p>每两个正弦波之间都还有一条直线，那是<strong>振幅为0的正弦波</strong>。也就是说，为了组成特殊的曲线，有些正弦波成分是不需要的。</p><p>如果我们把第一个频率最低的频率分量看作<strong>基本单元</strong>，该正弦波的角频率为$w_0$，那么频域的基本单元就是$\omega_0$。 假设$\omega_0 = 10$，回到<a href="#补充一频谱">频谱</a>中，可以看到该频谱的基本单元正是10。</p><p>如果说频域的基本单元是“1”，那么频域的“0”就是一个周期无限长的正弦波（角频率为0），也就是一条直线，0频率也被称为<strong>直流分量</strong>。</p><p>上面已经把频域的事研究的差不多了，但如前文所述，频谱中的信息只是时域中信息的一部分，我们还需要反映相位的<strong>相位谱</strong>。那么相位谱在哪呢？请看下图，鉴于正弦波是周期的，需要设定一个用来标记正弦波位置的东西，即红色点，红点是距离频率轴最近的波峰。粉色点是红点在下平面的投影，<strong>粉点用于标注波峰距离频率轴的距离</strong>。</p><p><img src="/assets/post_img/article67/phase-detail.jpeg" alt="phase-detail"></p><p>如我们所见，如果说频谱为侧面投影的话，相位谱就是波形在下方的“投影”。但是这个投影不是直接投影，而是间接的。请看下图：</p><p><img src="/assets/post_img/article67/phase-spectrum.png" alt="phase-map"></p><p>说间接投影，就是因为<strong>相位不能直接用时间差表示</strong>，我们可以看到，时间差是粉点到时域纵轴的距离，我们记为$\varDelta t$，又可以得到当前频率分量的周期$T$，则有相位$p = \frac{\varDelta t}{T} \times 2\pi$，最终相位谱上的纵轴值（相位）是依据时间差和周期算出的。</p><p>最终的图如下所示，其实，频域图像也可以叫做<strong>幅度频谱</strong>，对应的，相位谱叫做<strong>相位频谱</strong>，因为他们的定义域都是频域，只是值域不同。</p><p><img src="/assets/post_img/article67/tfp.jpeg" alt="tfp"></p><h3 id="补充五：傅立叶变换与其频谱"><a href="#补充五：傅立叶变换与其频谱" class="headerlink" title="补充五：傅立叶变换与其频谱"></a>补充五：傅立叶变换与其频谱</h3><p>前面我们说到，傅立叶级数是傅立叶变换的一个特例，也就是说，傅立叶变换推广了傅立叶级数，这个“推广“体现在傅立叶变换可以处理<strong>时域上的非周期的连续信号</strong>，将其转换为一个在<strong>频域上的非周期连续信号</strong>。如下图：</p><p><img src="/assets/post_img/article67/Fourier-transformation.jpeg" alt="f-t"></p><p>图片右上角的<code>Frequency resolution</code>是频率分辨率，即数字信号处理系统将相距最近的两个频率分量区分开的能力，它与周期成反比，当一个连续信号是非周期信号时，可以将其周期看作无限大，则该信号近似看为周期无限大的周期连续信号。这代表<strong>非周期信号可以通过傅立叶变换分解为无限多个频率无限接近的正余弦信号的和</strong>。</p><p><img src="/assets/post_img/article67/discrete-spectrum.png" alt="ds"></p><p>试想一下，上面离散的频率分量逐渐靠近…直至连在一起，无穷无尽的分量铺满了空间。</p><p><img src="/assets/post_img/article67/continuous-spectrum.png" alt="cs"></p><p>变成了波涛汹涌的大海，这就是连续的频谱下发生的事情。那么，计算时离散频谱下分量的累加，自然也就变成了连续频谱上的<strong>积分</strong>。</p><p>至此，对于研究图像处理来说，似乎已经足够，数学上的东西，需要时再深究也不迟。</p><h3 id="补充六：虚数（Imaginary-number）-和-欧拉公式（Euler’s-formula）"><a href="#补充六：虚数（Imaginary-number）-和-欧拉公式（Euler’s-formula）" class="headerlink" title="补充六：虚数（Imaginary number） 和 欧拉公式（Euler’s formula）"></a>补充六：虚数（Imaginary number） 和 欧拉公式（Euler’s formula）</h3><p>我们知道，在实数轴上，乘$-1$其实就是使线段逆时针旋转了$180$度，那么乘一个$i$呢？显然，旋转$90$度。</p><p><img src="/assets/post_img/article67/imagine-real.jpeg" alt="i-r"></p><p>同时，我们获得了一个垂直的虚数轴。实数轴与虚数轴共同构成了一个复数的平面，也称复平面。这样我们就了解到，乘虚数i的一个功能——旋转。</p><p>欧拉公式如下（关于欧拉公式为何被称为“上帝创造的公式”此处不再赘述）：</p><script type="math/tex; mode=display">e^{i x}=\cos x+i \sin x</script><p>当$x = \pi$时，就有$e^{i x} + 1 = 0$。</p><p>这个公式的关键作用之一，是将正弦波统一成了简单的指数形式。</p><p><img src="/assets/post_img/article67/Euler.png" alt="euler"></p><p>欧拉公式所描绘的，是一个随着时间变化，在复平面上做圆周运动的点，随着时间的改变，在时间轴上就成了一条螺旋线。如果只看它的实数部分，也就是螺旋线在左侧的投影，就是一个最基础的余弦函数。而右侧的投影则是一个正弦函数。</p><p>欧拉公式的另一种形式是：</p><script type="math/tex; mode=display">\sin x=\frac{e^{i x}-e^{-i x}}{2 i}, \cos x=\frac{e^{i x}+e^{-i x}}{2}</script><h3 id="补充七：数学上的傅立叶级数和傅立叶变换"><a href="#补充七：数学上的傅立叶级数和傅立叶变换" class="headerlink" title="补充七：数学上的傅立叶级数和傅立叶变换"></a>补充七：数学上的傅立叶级数和傅立叶变换</h3><p>傅立叶级数的三角函数形式如下：</p><script type="math/tex; mode=display">f(t)=a_0+\sum_{n=1}^{\infty}\left[a_n \cos \left(n \omega t\right)+b_n \sin \left(n \omega t\right)\right], \\其中 \omega = \frac{2 \pi}{T}为基波频率, a_n和b_n为傅立叶系数，分别代表余弦分量振幅和正弦分量振幅.</script><p>傅立叶变换相关的，先放放吧。可以参考<a href="https://blog.csdn.net/Thera_qing/article/details/106154313">这</a><a href="https://blog.csdn.net/weixin_40851250/article/details/84780221">三</a><a href="https://blog.csdn.net/jack__linux/article/details/99671469">篇</a>。</p><h2 id="从信号的角度看图像"><a href="#从信号的角度看图像" class="headerlink" title="从信号的角度看图像"></a>从信号的角度看图像</h2><p>图像是信号，这是直觉上极难理解的。</p><p>首先，我们需要摆脱信号和时间“绑定”的固有直觉，信号只是由自变量和因变量构成的，自变量不一定是时间。</p><p>实际上，图像是一个沿空间分布的信号，他的定义域是图像的$x$轴或$y$轴。图像既然是沿着空间分布的信号，说明它是一个<strong>二维信号</strong>。如果一维数字信号可以看作是一组数字序列，那么二维数字信号则可以看作是一组数字阵列。图像中沿着水平方向的任何一行可以看作是像素点随着$x$轴变化的信号。而沿着竖直方向的任何一列则可以看作是像素点随着$y$轴变化的信号。单独提取一行或者是一列数据出来，按照像素值的大小随轴的变化画出来，就可以得到我们熟悉的信号波形，如下。</p><p><img src="/assets/post_img/article67/pic-signal.png" alt="pic-signal"></p><p>可以明显的看出，图片右方的波形代表黑色横线，因为除了塔尖以外像素变化不大。而下方波形则对应竖线，由上到下。</p><p>图像既然是信号，那么自然有频率、幅度和相位的特征。</p><p><strong>图像的频率对应到的是图像细节的多少</strong>，比如城市的部分频率高于天空的部分。再比如在灰度图中，图像的频率是<strong>表征图像中灰度变化剧烈程度的指标</strong>，是灰度在平面空间上的梯度。</p><p><strong>图像的幅度对应到的是图像中像素的大小</strong>，比如天空中太阳照亮的区域幅度高于其他区域。</p><p>假设有图片A和图片B，分别获取A、B的幅度频谱和相位频谱，然后将图片A的幅度谱与图片B的相位谱结合做逆傅立叶变换，最终结果的视觉效果会是图片B加上一些噪声，通过对比实验可以得到一个结论：<strong>图像的相位谱中，保留了图像的边缘以及整体结构的信息。</strong> 而错误的幅度谱看起来则像是噪声覆盖在原图上，但并没有影响图像的内容本身。</p><p>同时，和我们在一维信号中看到的一样，如果想要通过修改频域中相位或振幅的方式来间接修改原图像，则需要同时保留幅度频谱和相位频谱，来通过逆傅立叶变换得到修改后的图像。</p><h3 id="空域（Spatial-domain）"><a href="#空域（Spatial-domain）" class="headerlink" title="空域（Spatial domain）"></a>空域（Spatial domain）</h3><p>空间域也叫<strong>空域</strong>，即所说的像素域，在空域的处理就是在像素级的处理，即直接对图像上的像素值进行增加或减少。对应的另一种操作是在频域上的，空域通过傅立叶变换后，得到的是图像的频谱。表示图像的能量梯度。</p><h3 id="图像变换技术"><a href="#图像变换技术" class="headerlink" title="图像变换技术"></a>图像变换技术</h3><p>为了有效和快速地对图像进行处理和分析，需要将原定义在图像空间的图像以某种形式转换到另外的空间，利用空间的特有性质方便地进行一定的加工，最后再转换回图像空间以得到所需的效果。 如：空域与频域间的相互转化。</p><h3 id="信号分析在图像处理中的应用"><a href="#信号分析在图像处理中的应用" class="headerlink" title="信号分析在图像处理中的应用"></a>信号分析在图像处理中的应用</h3><p>很多图像处理事实上就是信号处理。比如图像去噪 (Denoise)，一般就是应用数字<strong>低通滤波器</strong>对图像进行滤波。图像增强 (Detail enhancement)，一般就是应用数字<strong>高通滤波器</strong>得到图像的高频信号，并对高频信号进行增强。对比度增强 (Contrast enhancement)，一般就是参考画面的亮暗程度 (图像的幅度)，并人为修改亮暗的一种处理。相位的概念一般会在图像的缩放 (Scaling) 中使用到。</p><h2 id="图像的灰度、亮度、强度"><a href="#图像的灰度、亮度、强度" class="headerlink" title="图像的灰度、亮度、强度"></a>图像的灰度、亮度、强度</h2><p><strong>图像灰度</strong>(Image grayscale)：把白色与黑色之间按对数关系分为若干等级，称为灰度，灰度分为256阶，从0到255。用灰度表示的图像称为<strong>灰度图</strong>，实际上，<strong>灰度表征的是单色的亮暗程度</strong>。</p><p><strong>图象亮度</strong>(Image brightness)：指画面的明亮程度，单位是 堪德拉每平米(cd/m2) 或称 nits。图象亮度是从白色表面到黑色表面的感觉连续体，由反射系数决定，亮度侧重物体，重在“反射”。在灰度图像中，亮度等于灰度，图像运算处理方式相同。但是在彩色图像中，亮度和对比度相关，即通过对RGB颜色分量的增加（增加亮度）或减少（减少亮度）相同的增量来显示，亮度的调整就是给每个分量乘以一个百分比值。</p><p><strong>图像强度</strong>(Image intensity)：表示单通道图像像素的强度（值的大小）。在灰度图像中，它是图像的灰度。在RGB颜色空间中，可以理解把它为是R通道的像素灰度值，G通道的像素灰度值，或是B通道的像素灰度值，也就是RGB中含三个图像强度。在其他颜色空间类似，也就是每个通道的图像的像素灰度值。</p><p>这里多说一下，为什么RGB也有灰度？实际上，<strong>RGB各单通道的图像也都是灰度图</strong>，只是最终成像时将他们分别放置在红、绿、蓝的单色通道中再叠加罢了，也就是说，一个RGB彩色图像，是由三个表示对应位置亮与暗程度的图像，通过放置在三个单色通道中再叠加形成的（这里属于理糙话不糙了）。</p><p><strong>认识到所有图片都是灰度图或其叠加后，很多图像处理方法的应用都可以举一反三了。</strong></p><h2 id="图像滤波"><a href="#图像滤波" class="headerlink" title="图像滤波"></a>图像滤波</h2><p>图像滤波，就是对图像的频率进行过滤。本文关注两类，即<strong>高通滤波</strong>和<strong>低通滤波</strong>。这两种滤波方式不难理解，从名字就可以看出，高通滤波就是<em>减弱或阻隔低频信号，保留高频信号</em>。而低通滤波则是<em>减弱或阻隔高频信号，保留低频信号</em>。</p><p>现在关键的问题是，图像中的高频信号和低频信号分别代表什么？结合前面所提到的，高（低）频信号也叫做高（低）频分量，图像的频率是灰度值变化剧烈程度的标准，可以推出<strong>高频信号就是灰度值变化剧烈的地方</strong>，同理，<strong>低频信号是图像灰度值变化平缓的地方</strong>。</p><p>具体些说，<em>高频信号就是相邻区域之间灰度相差很大的地方</em>，例如一个影像与背景的边缘部位，通常会有明显的差别，灰度变化很快，也就是变化频率高的部位。同样的，图像的细节处也是属于灰度值急剧变化的区域。另外，噪声（即噪点）也是这样，噪点之所以是噪点，就是因为它与附近的像素点灰度不一样了。</p><p>相对应的，<em>低频信号就是那些连续渐变的区域</em>。人眼对图像中的高频信号更为敏感，举例来说，在一张白纸上有一行字，那么人眼会直接聚焦在文字上，而不会太在意白纸本身，这里文字就是高频信号，而白纸就是低频信号。 </p><h3 id="频谱中心化"><a href="#频谱中心化" class="headerlink" title="频谱中心化"></a>频谱中心化</h3><p>研究数字图像有时需要变换到频域做处理，比如滤波等。但直接对数字图像进行二维DFT（离散傅立叶变换）得到的频谱图是<em>高频在中间，低频在四角</em>。频谱图比较亮的地方就是低频，因为<strong>图像的能量一般都是集中在低频部分</strong>。为了把能量集中起来便于使用滤波器，可以利用二维DFT的平移性质对频谱进行中心化。如下：</p><p><img src="/assets/post_img/article67/spectrum-central.png" alt="s-c"></p><p>经二维傅立叶变换的平移性质推导，结论是<strong>频谱中心化只需对数字图像的每个像素点的取值直接乘以$(-1)^{x+y}$就可以了，其中$x，y$为像素坐标</strong>。</p><h3 id="频域滤波"><a href="#频域滤波" class="headerlink" title="频域滤波"></a>频域滤波</h3><p>频域滤波的基本过程如下：<br>1、对原图像做频谱中心化处理，<br>2、对第一步的结果进行DFT，<br>3、使用某滤波器乘第二步的结果，<br>4、对第三步结果做反DFT，再做一次频谱中心化，得到滤波后图像。</p><h3 id="空域滤波"><a href="#空域滤波" class="headerlink" title="空域滤波"></a>空域滤波</h3><p><strong>空域滤波</strong>是指利用像素及像素邻域组成的空间进行图像增强的方法。之所以用“滤波”这个词，是因为借助了频域里的概念。并且，频率域的卷积与空间域的乘积存在对应关系，由卷积定理可知<strong>所有频域的滤波理论上都可以转化为空域的卷积操作</strong>。</p><p>空域滤波是在图像空间通过邻域操作完成的，邻域操作常借助<strong>模板运算</strong>来实现。由此也可以看出，空域滤波的算法比较简单，处理速度快。而频域滤波算法复杂，计算慢。（这里只是相比于做DFT来说，现在好像还有FFT可以加速运算，具体如何先不深究。）</p><p>图像的空域滤波还可以按照运算的方式分为<strong>线性滤波</strong>和<strong>非线性滤波</strong>，如果运算只是加权求和之类的操作，则为线性滤波。而若是在模版内进行求最值、绝对值等操作，则属于非线性滤波。</p><h3 id="模版运算"><a href="#模版运算" class="headerlink" title="模版运算"></a>模版运算</h3><p>模板也称为核（kernel）。模板运算的基本思路是将 某个像素的值 替换为 它本身灰度值和其相邻像素灰度值的函数值。</p><p>模板一般是$n \times n$的<strong>方阵</strong>（$n$通常是3、5、7、9等很小的<strong>奇数</strong>）。当$n$为奇数时，可以定义<em>模板的半径</em>$r = \frac{(n-1)}{2}$。模板中有一个<strong>锚点</strong>（anchor point），通常是矩阵中心点，和原图像中待计算点对应。整个模板对应的区域，就是原图像中像素点的相邻区域。一个$n \times n$的模板最多可有$n \times n$个系数，该模板的功能由这些系数的取值所决定。</p><p>以模版运算中最常用的<strong>模版卷积</strong>举例，其在空域实现的主要步骤如下：<br>1、将模板在图中漫游，并将模板锚点与图中某个像素位置重合（待计算点），<br>2、将模板上的各个系数与模板下各对应像素的灰度值相乘，<br>3、将所有乘积相加（为保持灰度范围，常将结果再除以模板的系数个数），<br>4、将上述运算结果（模板的输出响应）赋给待计算点的像素。</p><h2 id="高斯滤波（Gaussian-filter）"><a href="#高斯滤波（Gaussian-filter）" class="headerlink" title="高斯滤波（Gaussian filter）"></a>高斯滤波（Gaussian filter）</h2><p>高斯滤波是一种<strong>根据高斯函数的形状来选择权值</strong>的线性平滑滤波，适用于消除<em>高斯噪声</em>，广泛应用于图像处理的减噪过程。通俗的讲，高斯滤波就是对整幅图像进行加权平均的过程，每一个像素点的值，都由其本身和邻域内的其他像素值经过加权平均后得到。</p><p><strong>高斯噪声</strong>就是概率密度函数服从高斯分布的一类噪声。如果一个噪声，它的幅度分布服从高斯分布，而它的功率谱密度（频谱的绝对值平方？）又是均匀分布的，则称它为<strong>高斯白噪声</strong>。高斯白噪声的二阶矩不相关，一阶矩为常数，是指先后信号在时间上的相关性。</p><p>高斯滤波的空域实现步骤很简单，先获得<strong>高斯核</strong>，然后对图像做模版卷积即可（可以做padding）。关键还是在高斯核上，或者说，在于高斯核上的权值如何计算上。</p><p>当然了，<em>高斯滤波是指用高斯函数作为滤波函数</em>，具体上实现的是模糊还是锐化效果，要根据是高通还是低通来判别。高斯锐化的高斯核就是$1 - 高斯模糊卷积核$。</p><h3 id="高斯核"><a href="#高斯核" class="headerlink" title="高斯核"></a>高斯核</h3><p>一维和二维的高斯函数如下，标准差$\sigma$的大小决定了高斯函数的宽度：</p><p><img src="/assets/post_img/article67/gaussian.webp" alt="gaussian"></p><p>根据二维高斯函数图像来理解高斯滤波就非常的直观了，假设我们要实现<strong>高斯模糊</strong>（低通），在确定高斯核时，首先需要将锚点（核的中心点）作为高斯图像中的原点（山峰的中心点），然后确定一个标准差$\sigma$和核的半径$r$，根据$r$我们可以计算出高斯核中各点的坐标，通过坐标$(x,y)$和$\sigma$依次计算二维高斯函数值，并填入对应的位置，然后，还需要进行<em>归一化</em>处理，即将核中各值除去权重总和，使核的权重综合为$1$，这样就得到了最终的高斯核。</p><p>现在的问题是，如何选择$\sigma$和$r$呢？</p><p>对于标准差，可以从二维高斯函数图像来分析，当标准差越小时，图像中的“山峰”越“瘦高”，权重分布越向中心集中，平滑效果越差；当标准差越大时，“山峰”越“矮胖”，权重分布越均匀，则平滑效果越明显。（有理论说空域上的标准差和频域的标准差成负相关，以此可以进一步解释高斯核的高低通效果，但是笔者目前还没看明白，留待日后补充。）</p><p>半径的选择同样可以从图像来分析。理论上，高斯分布在定义域的所有地方都有非负值，这就需要一个无限大的核。然而钟形曲线在区间$(\mu - \sigma, \mu + \sigma)$范围内的面积占曲线下总面积的$68\%$，在区间$(\mu - 2\sigma, \mu + 2\sigma)$范围内的面积占$95\%$，在区间$(\mu - 3\sigma, \mu + 3\sigma)$范围内的面积占$99.7\%$，通常情况下$3\sigma$以外的区域所占面积已经很小，可以被忽略（认为该段分布近似包含所有情况）。所以，当半径取$3\sigma$时，就基本上可以近似使用了，此时核的大小为$(6\sigma + 1) \times (6\sigma + 1)$。</p><p>最后，对高斯核进行<em>归一化</em>处理的一个重要目的是确保卷积运算后，像素值处于$[0, 255]$的范围之内。</p><h3 id="实验-高斯模糊"><a href="#实验-高斯模糊" class="headerlink" title="实验-高斯模糊"></a>实验-高斯模糊</h3><p>光说不练，不是好文章，下面就实验一下实现高斯滤波。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gaussian_filter</span>(<span class="params">img, sigma=<span class="number">0.5</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(img.shape) == <span class="number">3</span>:</span><br><span class="line">        H, W, C = img.shape</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        img = np.expand_dims(img, axis=-<span class="number">1</span>)</span><br><span class="line">        H, W, C = img.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算半径，取4倍</span></span><br><span class="line">    radius = <span class="built_in">int</span>(sigma * <span class="number">4</span>)</span><br><span class="line">    <span class="comment"># 计算高斯核大小</span></span><br><span class="line">    n = <span class="number">2</span> * radius + <span class="number">1</span></span><br><span class="line">    <span class="comment"># padding大小</span></span><br><span class="line">    pad = n // <span class="number">2</span></span><br><span class="line">    <span class="comment"># padding的地方置为0</span></span><br><span class="line">    out = np.zeros((H + pad * <span class="number">2</span>, W + pad * <span class="number">2</span>, C), dtype=np.float64)</span><br><span class="line">    out[pad: pad + H, pad: pad + W] = img.copy().astype(np.float64)</span><br><span class="line">    <span class="comment"># 构造高斯核坐标</span></span><br><span class="line">    x_index, y_index = np.mgrid[-radius:radius + <span class="number">1</span>, -radius:radius + <span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 计算核值</span></span><br><span class="line">    k = np.exp(-(x_index ** <span class="number">2</span> + y_index ** <span class="number">2</span>) / (<span class="number">2</span> * sigma ** <span class="number">2</span>))</span><br><span class="line">    <span class="comment"># 归一化</span></span><br><span class="line">    k = k / k.<span class="built_in">sum</span>()</span><br><span class="line">    <span class="comment"># 原图像深拷贝</span></span><br><span class="line">    origin = out.copy()</span><br><span class="line">    <span class="comment"># 高斯核卷积，计算有点慢</span></span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(H):</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(W):</span><br><span class="line">            <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(C):</span><br><span class="line">                out[pad + y, pad + x, c] = np.<span class="built_in">sum</span>(k * origin[y: y + n, x: x + n, c])</span><br><span class="line">    <span class="comment"># 切除填充部分</span></span><br><span class="line">    out = out[pad: pad + H, pad: pad + W].astype(np.uint8)</span><br><span class="line">    <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 读取图像</span></span><br><span class="line">    img = cv2.imread(<span class="string">&quot;./data/bridge.jpeg&quot;</span>)</span><br><span class="line">    <span class="comment"># 滤波</span></span><br><span class="line">    result = gaussian_filter(img, sigma=<span class="number">2.5</span>)</span><br><span class="line">    <span class="comment"># 保存结果</span></span><br><span class="line">    cv2.imwrite(<span class="string">&quot;./data/bridge-blur.jpeg&quot;</span>, result)</span><br><span class="line">    <span class="comment"># 显示</span></span><br><span class="line">    cv2.imshow(<span class="string">&quot;bridge&quot;</span>, img)</span><br><span class="line">    cv2.imshow(<span class="string">&quot;bridge-blur&quot;</span>, result)</span><br><span class="line">    cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">    cv2.destroyAllWindows()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>输入（笔者在重庆随手拍的一个桥，叫什么忘记了）：<br><img src="/assets/post_img/article67/bridge.jpeg" alt="bridge"><br>输出：<br><img src="/assets/post_img/article67/bridge-blur.jpeg" alt="blur-bridge"></p><h2 id="双边滤波（Bilateral-filter）"><a href="#双边滤波（Bilateral-filter）" class="headerlink" title="双边滤波（Bilateral filter）"></a>双边滤波（Bilateral filter）</h2><p>开局先甩一个定义：</p><blockquote><p>双边滤波是一种非线性的保边滤波器，通常在计算机视觉中用作工作流中的简单降噪阶段。它将每个输出像素的强度计算为输入图像中附近像素的强度值的加权平均值。至关重要的是，权重不仅取决于当前像素和相邻像素之间的欧几里得距离，还取决于它们之间的辐射差异（例如颜色强度差异）。结果是保留了边缘，同时平滑了具有相似强度的区域。</p></blockquote><p>只看定义，好像懂了，又好像没懂，那么下面就来详细的解释一下。众所周知，图像的噪声和边缘都属于高频分量，高斯滤波在降低噪声的同时没有很好的保留边缘，而是一起“降”了，这对于人眼观看（或者特征提取）就不怎么友好了，形成的图片模糊成了一团。</p><p>这时，有人想了，能不能在降低噪声的同时，保持图像的边缘呢？这就是双边滤波干的事。双边滤波同样是基于模版运算的，他的特点在于<strong>模版的权值同时考虑了空域上距离的差别和灰度值上数值的差别</strong>。 这里后者也可以叫做<strong>灰度距离</strong>，与前者的欧式距离相呼应（没错前者实际上就是欧式距离）。</p><p>回顾高斯滤波中的高斯核，它的权值分配是按照二维高斯图像的形状分配的，这实际上是一种考虑空域上距离差别的方式，远离中心点的像素点会被分配到更少的权值。</p><p>那么为什么要考虑灰度值的差别呢？这是由于<strong>边缘的表现其实是灰度值的突变</strong>，如果想要保留边缘，则不能将高灰度值的区域与低灰度值的区域相“混合”。传统的高斯核无法考虑当前覆盖区域的灰度值信息。</p><p>由此，双边滤波的核函数是<strong>空域核</strong>与<strong>值域（灰度值域）核</strong>的综合：在图像的平坦区域，也就是灰度值变化小的区域，值域核权重趋于$1$，此时空域核权重起主要作用，与高斯模糊类似；在图像的含边缘区域，灰度值变化很大，则值域核权重会增大，从而起到保留边缘的效果。如下图：</p><p><img src="/assets/post_img/article67/bilateral-filter.png" alt="b-f"></p><p>图片左侧输入表示输入图像的灰度值形状，可以明显看到图像左侧整体灰度值低而右侧整体灰度值高，值变化很大，图像中部存在一个断层，这表示该区域属于<strong>含边缘区域</strong>。而断层两侧的部分，也有起起伏伏的小土包，这表示灰度值变化很小的区域，即<strong>平坦区域</strong>。当核移动到图中红色箭头所指位置的正上方时，空域核权重与值域核权重的情况如方框中所示，显然，该位置属于含边缘区域，值域核在形状上也表征为断层。它与空域核结合后的结果就是最终的核形状，即一个被切了一半的高斯图像，这个核的权重也很容易想象，整体上，左侧权重低而右侧权重高，起到了保留边缘的作用。</p><p>以上，应该能够理解双边滤波是如何做的了，那具体怎么做呢？接着往下看。</p><p>双边滤波器中，最终输出像素的值$g$同样依赖于领域像素值的加权组合：</p><script type="math/tex; mode=display">g(i, j)=\frac{\sum_{k, l} f(k, l) w(i, j, k, l)}{\sum_{k, l} w(i, j, k, l)}</script><p>这里，$(i,j)$是待计算位置的坐标，$(k,l)$是领域像素位置坐标，$函数f$表示计算灰度。核权重$w$就是重点了，它取决于空域核权重$s$与值域核权重$r$：</p><script type="math/tex; mode=display">s(i, j, k, l)=\exp \left(-\frac{(i-k)^2+(j-l)^2}{2 \sigma_s^2}\right), \\r(i, j, k, l)=\exp \left(- \frac{(f(i,j)-f(k,l))^2}{2 \sigma_r^2}\right)</script><p>$\sigma_s$ 和 $\sigma_r$ 分别是空域高斯（Spatial Gaussian）和值域高斯（Range Gaussian）的标准差，代表了对图片的滤波程度。$w$取决于$s$和$r$的乘积：</p><script type="math/tex; mode=display">w(i, j, k, l) = \exp \left(-\frac{(i-k)^2+(j-l)^2}{2 \sigma_s^2} - \frac{(f(i,j)-f(k,l))^2}{2 \sigma_r^2}\right)</script><p>根据该权重函数，就可以获得双边滤波的核了，然后再进行模版卷积，归一化处理即可。</p><p>正式的一个公式应该是：</p><script type="math/tex; mode=display">B F[I]_{\mathrm{p}}=\frac{1}{W_{\mathrm{p}}} \sum_{\mathbf{q} \in \mathcal{S}} G_{\sigma_s}(\|\mathbf{p}-\mathbf{q}\|) G_{\sigma_r}\left(\lvert I_{\mathbf{p}}-I_{\mathbf{q}} \rvert \right) I_{\mathbf{q}}</script><p>其中$W_{\mathrm{p}}$是一个归一化因子（normalization factor）：</p><script type="math/tex; mode=display">W_{\mathrm{p}} = \sum_{\mathbf{q} \in \mathcal{S}} G_{\sigma_s}(\|\mathbf{p}-\mathbf{q}\|) G_{\sigma_r}\left(\lvert I_{\mathbf{p}}-I_{\mathbf{q}} \rvert \right)</script><p>这里，$B F[I]_{\mathrm{p}}$为像素$p$滤波后的灰度值；$I_{\mathbf{p}}$为像素$p$原本的灰度值；$\mathrm{p}$则表示像素$p$的空间坐标；$G_{\sigma_s}$和$G_{\sigma_r}$分别为空域高斯和值域高斯；$|\mathbf{p}-\mathbf{q}|$表示取模，即欧式距离；$\lvert I_{\mathbf{p}}-I_{\mathbf{q}} \rvert$为灰度值差的绝对值；像素$q$为领域点；$\mathcal{S}$表示整个核区域内的点。</p><p>这样解读完以后，其实和一开始的公式是一样的。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]<a href="https://zhuanlan.zhihu.com/p/19763358">https://zhuanlan.zhihu.com/p/19763358</a><br>[2]<a href="https://www.zhihu.com/question/21817515/answer/472164871">https://www.zhihu.com/question/21817515/answer/472164871</a><br>[3]<a href="https://blog.csdn.net/jack__linux/article/details/99671469">https://blog.csdn.net/jack__linux/article/details/99671469</a><br>[4]<a href="https://zhuanlan.zhihu.com/p/53071745">https://zhuanlan.zhihu.com/p/53071745</a><br>[5]<a href="https://blog.csdn.net/u010430651/article/details/95340236">https://blog.csdn.net/u010430651/article/details/95340236</a><br>[6]<a href="https://zhuanlan.zhihu.com/p/40301384">https://zhuanlan.zhihu.com/p/40301384</a><br>[7]<a href="https://blog.csdn.net/jack__linux/article/details/99671469">https://blog.csdn.net/jack__linux/article/details/99671469</a><br>[8]<a href="https://blog.csdn.net/Chevy_cxw/article/details/110948262">https://blog.csdn.net/Chevy_cxw/article/details/110948262</a><br>[9]<a href="https://blog.csdn.net/dengheCSDN/article/details/78906126">https://blog.csdn.net/dengheCSDN/article/details/78906126</a><br>[10]<a href="https://www.zhihu.com/question/29246532/answer/1473209132">https://www.zhihu.com/question/29246532/answer/1473209132</a><br>[11]<a href="https://blog.csdn.net/rocketeerLi/article/details/87986751">https://blog.csdn.net/rocketeerLi/article/details/87986751</a><br>[12]<a href="https://blog.csdn.net/weixin_44479045/article/details/104948535">https://blog.csdn.net/weixin_44479045/article/details/104948535</a><br>[13]<a href="https://blog.csdn.net/silence2015/article/details/53789748">https://blog.csdn.net/silence2015/article/details/53789748</a><br>[14]<a href="https://blog.csdn.net/weixin_43135178/article/details/115453145">https://blog.csdn.net/weixin_43135178/article/details/115453145</a><br>[15]<a href="https://blog.csdn.net/jialeheyeshu/article/details/51097860">https://blog.csdn.net/jialeheyeshu/article/details/51097860</a><br>[16]<a href="https://blog.csdn.net/qq_36607894/article/details/92809731">https://blog.csdn.net/qq_36607894/article/details/92809731</a><br>[17]<a href="https://www.jianshu.com/p/fbde7bdb256d">https://www.jianshu.com/p/fbde7bdb256d</a><br>[18]<a href="https://blog.csdn.net/qimingxia/article/details/89111897">https://blog.csdn.net/qimingxia/article/details/89111897</a><br>[19]<a href="https://blog.csdn.net/qq_41603898/article/details/81674987">https://blog.csdn.net/qq_41603898/article/details/81674987</a><br>[20]<a href="https://blog.csdn.net/sunmc1204953974/article/details/50634652">https://blog.csdn.net/sunmc1204953974/article/details/50634652</a><br>[21]<a href="https://www.jianshu.com/p/73e6ccbd8f3f?u_atoken=80aa2d72-8ab8-4b03-893a-a25ecc6f099f&amp;u_asession=01etOGNYlCJWvXBvSQngE-TGud_9aeeQYAvlS2WuwpQOq_wctc1A74a7od4e-upYygX0KNBwm7Lovlpxjd_P_q4JsKWYrT3W_NKPr8w6oU7K8LeXkNNqHvfNEX4gbXYnPLg0pn3tpfEcqG8HZmzd6q3mBkFo3NEHBv0PZUm6pbxQU&amp;u_asig=05D7OEW9nUnnFJ-ggxnAnVvWOUdkfmuhIUCY6TZqOqY-eeonucHFTEdDgyfQgZ-VHHeWSFSyR-Vy_Q8xsXHLJZQp0NfuTWlhk0fCigqVK1DCFbYmyFkNL-jcmynjX_hmMoxm6kbvl94B967SOLkRJntrO6BEKNazId1s3Et_W-l5P9JS7q8ZD7Xtz2Ly-b0kmuyAKRFSVJkkdwVUnyHAIJzbYjwP9YcDA_b__QLeG_TK-M2bdIetoqxc5CZJ9nLRpI6xaDswPo-3_59so9Oh9f1-3h9VXwMyh6PgyDIVSG1W-t741JlKYWli8UYkMs4RIiQKranM7rBKraCitp69DNzOmP87UHXNQfNZeF1yHm4yNyHvomyVImJs8-nyATChPamWspDxyAEEo4kbsryBKb9Q&amp;u_aref=G%2BN03JEAOlDe%2B8ugCiZkCbuRn%2FU%3D">简单易懂的高斯滤波-简书（链接太长了）</a><br>[22]<a href="https://blog.csdn.net/weixin_42985978/article/details/126517088">https://blog.csdn.net/weixin_42985978/article/details/126517088</a><br>[23]<a href="https://blog.csdn.net/blogshinelee/article/details/82734769">https://blog.csdn.net/blogshinelee/article/details/82734769</a><br>[24]<a href="https://blog.csdn.net/lz0499/article/details/54015150">https://blog.csdn.net/lz0499/article/details/54015150</a><br>[25]<a href="https://blog.csdn.net/huachizi/article/details/88951061">https://blog.csdn.net/huachizi/article/details/88951061</a><br>[26]<a href="https://docs.nvidia.com/vpi/algo_bilat_filter.html">https://docs.nvidia.com/vpi/algo_bilat_filter.html</a><br>[27]<a href="http://www.360doc.com/content/17/0306/14/28838452_634420847.shtml">http://www.360doc.com/content/17/0306/14/28838452_634420847.shtml</a><br>[28]<a href="https://people.csail.mit.edu/sparis/bf_course/course_notes.pdf">https://people.csail.mit.edu/sparis/bf_course/course_notes.pdf</a><br>[29]<a href="https://zhuanlan.zhihu.com/p/127023952">https://zhuanlan.zhihu.com/p/127023952</a><br>[30]<a href="https://blog.csdn.net/MoFMan/article/details/77482794">https://blog.csdn.net/MoFMan/article/details/77482794</a><br>[31]<a href="https://zhuanlan.zhihu.com/p/180497579">https://zhuanlan.zhihu.com/p/180497579</a></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;学习一下高斯滤波和双边滤波，及其需要的前置知识，仅记录一下个人理解，先后逻辑比较混乱，参考请谨慎。如有错误请评论区指正，感谢。&lt;/p&gt;
&lt;p&gt;文中图片多数来自网络，我尽量擦去了水印，是为了提升观感，参考文献在文末有注明。本文的出发点，大概是在尽量用直观、快速的方式了解这些抽象概念吧。&lt;br&gt;</summary>
    
    
    
    
    <category term="计算机视觉" scheme="http://silencezheng.top/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>各种“监督”学习区分</title>
    <link href="http://silencezheng.top/2022/10/01/article66/"/>
    <id>http://silencezheng.top/2022/10/01/article66/</id>
    <published>2022-10-01T12:52:54.000Z</published>
    <updated>2022-10-01T12:53:12.630Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>区分一下各种含“监督”的学习，包括无监督、弱监督、半监督、监督、自监督。<br><span id="more"></span></p><h2 id="无监督学习-unsupervised-learning"><a href="#无监督学习-unsupervised-learning" class="headerlink" title="无监督学习(unsupervised learning)"></a>无监督学习(unsupervised learning)</h2><blockquote><p>无监督学习是机器学习的一种方法，没有给定事先标记过的训练示例，自动对输入的资料进行分类或分群。无监督学习的主要运用包含：聚类分析（cluster analysis）、关系规则（association rule）、维度缩减（dimensionality reduce）。</p></blockquote><p>可以根据特点来认识无监督学习：</p><ol><li>无监督学习是没有明确目的的训练方式，你无法提前知道结果是什么。</li><li>无监督学习<strong>不需要给数据打标签</strong>。</li><li>无监督学习几乎无法量化效果如何。</li></ol><p>无监督学习是一种机器学习的训练方式，它本质上是一个统计手段，在没有标签的数据里可以发现潜在的一些结构的一种训练方式。</p><p>无监督学习的应用，比如对用户行为进行分类，筛选异常行为用户等等。</p><h2 id="监督学习-supervised-learning"><a href="#监督学习-supervised-learning" class="headerlink" title="监督学习(supervised learning)"></a>监督学习(supervised learning)</h2><p>已知数据和其一一对应的标签，训练一个智能算法，将输入数据映射到标签的过程。监督学习是最常见的学习问题之一，例如给定一组猪的图片，并作图像级标注分类为猪，用监督学习训练一个算法可以判断新输入的图片是否是猪。</p><p>与无监督学习相比：</p><ol><li>监督学习有明确的训练目的</li><li>监督学习的训练数据必须有标签</li><li>监督学习可以量化效果</li></ol><h2 id="弱监督学习-weakly-supervised-learning"><a href="#弱监督学习-weakly-supervised-learning" class="headerlink" title="弱监督学习(weakly supervised learning)"></a>弱监督学习(weakly supervised learning)</h2><p>已知数据和其一一对应的<strong>弱标签</strong>，训练一个智能算法，将输入数据映射到一组更强的标签的过程。</p><p><strong>标签的强弱指的是标签蕴含的信息量的多少</strong>，比如相对于分割的标签来说，分类的标签就是弱标签。再比如对于弱监督目标检测，就是数据只有图像级标注，比如图片的分类，要求算法获取目标边界框的学习任务。</p><h2 id="半监督学习-semi-supervised-learning"><a href="#半监督学习-semi-supervised-learning" class="headerlink" title="半监督学习(semi-supervised learning)"></a>半监督学习(semi-supervised learning)</h2><p>已知数据和部分数据一一对应的标签，有一部分数据的标签未知，训练一个智能算法，学习已知标签和未知标签的数据，将输入数据映射到标签的过程。半监督通常是一个数据的标注非常困难，比如说医院的检查结果，医生也需要一段时间来判断健康与否，可能只有几组数据知道是健康还是非健康，其他的只有数据不知道是不是健康。那么通过有监督学习和无监督的结合的半监督学习就在这里发挥作用了。</p><p>总之，是在数据标注困难的情况下，<strong>使用少量标注数据和其他未标注数据进行学习</strong>的训练方式。</p><h2 id="自监督学习-self-supervised-learning"><a href="#自监督学习-self-supervised-learning" class="headerlink" title="自监督学习(self-supervised learning)"></a>自监督学习(self-supervised learning)</h2><p>基于监督学习当前的主要瓶颈是 标签生成和标注 的现状，人们提出了一个问题：</p><blockquote><p>我们是否可以通过特定的方式设计任务，即可以从现有图像中生成几乎无限的标签，并以此来学习特征表示？</p></blockquote><p>这道出了自监督学习的理想状态，我们希望同时拥有监督学习的明确性和无监督学习的自由性，<strong>理想的自监督学习能够在给定的无标注数据中自动生成我们需要的标签，并根据该标签和数据进行监督学习。</strong></p><p>自监督学习的核心是<strong>如何给输入数据自动生成标签</strong>。之前的很多工作都是围绕这个核心展开的。一般的套路是：首先提出一个新的自动打标签的<strong>辅助任务</strong>（pretext task），用辅助任务自动生成标签，然后做实验、测性能、发文章。每年都有新的辅助任务被提出来，自监督学习的性能也在不断提高，有的甚至已经接近监督学习的性能。总体上说，或者是提出一种完全新的辅助任务，或者是把多个旧的辅助任务组合到一起作为一个“新”的辅助任务。</p><p>自监督学习的应用，有图像着色、图像超分辨率、图像修补等等，其学习到的特征表示通常可以用于下游任务。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;区分一下各种含“监督”的学习，包括无监督、弱监督、半监督、监督、自监督。&lt;br&gt;</summary>
    
    
    
    
    <category term="机器学习" scheme="http://silencezheng.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>什么是语义分割、实例分割、全景分割</title>
    <link href="http://silencezheng.top/2022/09/29/article65/"/>
    <id>http://silencezheng.top/2022/09/29/article65/</id>
    <published>2022-09-29T13:19:15.000Z</published>
    <updated>2022-09-29T13:20:47.640Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>看见一篇好文，不得不转，<a href="https://zhuanlan.zhihu.com/p/368904941">原地址在这</a>，一文分清语义分割问题。<br><span id="more"></span></p><h2 id="图像分类（image-classification）"><a href="#图像分类（image-classification）" class="headerlink" title="图像分类（image classification）"></a>图像分类（image classification）</h2><p>识别图像中存在的内容，如下图，有人（person）、树（tree）、草地（grass）、天空（sky）。</p><p><img src="/assets/post_img/article65/classification.webp" alt="ic"></p><h2 id="目标检测（object-detection）"><a href="#目标检测（object-detection）" class="headerlink" title="目标检测（object detection）"></a>目标检测（object detection）</h2><p>识别图像中存在的内容和检测其位置，如下图，以识别和检测人（person）为例。</p><p><img src="/assets/post_img/article65/object-detection.webp" alt="od"></p><h2 id="语义分割（semantic-segmentation）"><a href="#语义分割（semantic-segmentation）" class="headerlink" title="语义分割（semantic segmentation）"></a>语义分割（semantic segmentation）</h2><p>对图像中的每个像素打上类别标签，如下图，把图像分为人（红色）、树木（深绿）、草地（浅绿）、天空（蓝色）标签。</p><p><img src="/assets/post_img/article65/semantic-segmentation.webp" alt="ss"></p><h2 id="实例分割（instance-segmentation）"><a href="#实例分割（instance-segmentation）" class="headerlink" title="实例分割（instance segmentation）"></a>实例分割（instance segmentation）</h2><p>目标检测和语义分割的结合，在图像中将目标检测出来（目标检测），然后对每个像素打上标签（语义分割）。对比上图、下图，如以人（person）为目标，语义分割不区分属于相同类别的不同实例（所有人都标为红色），实例分割区分同类的不同实例（使用不同颜色区分不同的人）。</p><p><img src="/assets/post_img/article65/instance-segmentation.webp" alt="is"></p><h2 id="全景分割（panoptic-segmentation）"><a href="#全景分割（panoptic-segmentation）" class="headerlink" title="全景分割（panoptic segmentation）"></a>全景分割（panoptic segmentation）</h2><p>语义分割和实例分割的结合，即要对所有目标都检测出来，又要区分出同个类别中的不同实例。对比上图、下图，实例分割只对图像中的目标（如上图中的人）进行检测和按像素分割，区分不同实例（使用不同颜色），而全景分割是对图中的所有物体包括背景都要进行检测和分割，区分不同实例（使用不同颜色）。</p><p><img src="/assets/post_img/article65/panoptic-segmentation.webp" alt="ps"></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;看见一篇好文，不得不转，&lt;a href=&quot;https://zhuanlan.zhihu.com/p/368904941&quot;&gt;原地址在这&lt;/a&gt;，一文分清语义分割问题。&lt;br&gt;</summary>
    
    
    
    
    <category term="机器学习" scheme="http://silencezheng.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>M系芯片配置TensorFlow环境</title>
    <link href="http://silencezheng.top/2022/09/28/article64/"/>
    <id>http://silencezheng.top/2022/09/28/article64/</id>
    <published>2022-09-28T10:09:13.000Z</published>
    <updated>2022-10-03T03:07:52.462Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>如题，需求MacOS版本12+，主要参考<a href="https://developer.apple.com/metal/tensorflow-plugin/">官方教程</a>。<br><span id="more"></span></p><h2 id="前置"><a href="#前置" class="headerlink" title="前置"></a>前置</h2><p>有一个conda环境，进入。</p><h2 id="安装TensorFlow"><a href="#安装TensorFlow" class="headerlink" title="安装TensorFlow"></a>安装TensorFlow</h2><h3 id="1-安装TensorFlow依赖"><a href="#1-安装TensorFlow依赖" class="headerlink" title="1. 安装TensorFlow依赖"></a>1. 安装TensorFlow依赖</h3><p><code>conda install -c apple tensorflow-deps</code></p><h3 id="2-安装TensorFlow"><a href="#2-安装TensorFlow" class="headerlink" title="2. 安装TensorFlow"></a>2. 安装TensorFlow</h3><p><code>python -m pip install tensorflow-macos</code></p><h3 id="3-安装tensorflow-metal插件"><a href="#3-安装tensorflow-metal插件" class="headerlink" title="3. 安装tensorflow-metal插件"></a>3. 安装tensorflow-metal插件</h3><p><code>python -m pip install tensorflow-metal</code></p><h3 id="4-安装keras"><a href="#4-安装keras" class="headerlink" title="4. 安装keras"></a>4. 安装keras</h3><p><code>python -m pip install keras</code></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;如题，需求MacOS版本12+，主要参考&lt;a href=&quot;https://developer.apple.com/metal/tensorflow-plugin/&quot;&gt;官方教程&lt;/a&gt;。&lt;br&gt;</summary>
    
    
    
    
    <category term="MacOS" scheme="http://silencezheng.top/tags/MacOS/"/>
    
  </entry>
  
  <entry>
    <title>利用VSCode编写LaTeX文件</title>
    <link href="http://silencezheng.top/2022/09/22/article63/"/>
    <id>http://silencezheng.top/2022/09/22/article63/</id>
    <published>2022-09-22T04:00:51.000Z</published>
    <updated>2022-09-22T04:04:26.932Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在MacOS上安装LaTeX环境，并利用VSCode编写LaTeX文档。<br><span id="more"></span></p><h2 id="在MacOS上安装LaTeX环境"><a href="#在MacOS上安装LaTeX环境" class="headerlink" title="在MacOS上安装LaTeX环境"></a>在MacOS上安装LaTeX环境</h2><p>有两种环境可供安装，完整的MacTeX和BasicTeX。</p><p>完整的 MacTeX-2022 安装包 包含四个部分：</p><ul><li>TeX Live 2022：包含超过 4 GB 材料的完整发行版。</li><li>GUI 应用程序：前端、实用程序和少量启动文档。</li><li>Ghostscript 9.55</li><li>Ghostscript 库 libgs，仅由 TeX Live 中的一个程序 dvisvgm 使用。</li></ul><p>BasicTeX 仅包含完整发行版的 TeX Live 片段，即编写 TeX 文档所需的所有标准工具，包括 TeX、LaTeX、pdfTeX、MetaFont、dvips、MetaPost 和 XeTeX。且包含 AMSTeX、拉丁现代字体、用于从 TeX Live 添加和更新包的 TeX Live 管理器以及 SyncTeX。</p><p>BasicTeX 不包含 GUI 程序和 Ghostscript。要在 Mac 上使用 TeX，安装 BasicTeX 和 前端就足够了。BasicTeX不会覆盖整个发行版，它安装在<code>/usr/local/texlive/2022basic</code>中。</p><p>这里选择安装BasicTeX，可以在<a href="https://tug.org/mactex/">这里</a>下载，也可以直接使用<code>brew install basictex</code>安装。</p><p>下载完了以后<code>tlmgr --help</code>有反应证明安装成功，<code>tlmgr</code>是TeX Live中的包和配置管理器，完全独立于操作系统可能提供的任何包管理器。</p><h2 id="TeX-LaTeX-XeLaTeX"><a href="#TeX-LaTeX-XeLaTeX" class="headerlink" title="TeX, LaTeX, XeLaTeX"></a>TeX, LaTeX, XeLaTeX</h2><p>LaTeX 是 TeX 中的一种格式(format) ，是建立在 TeX 基础上的宏语言，每一个 LaTeX 命令实际上最后都会被转换解释成几个甚至上百个TeX 命令。</p><p>XeLaTeX是使用LaTeX的排版引擎，命令下直接使用<code>xelatex ***.tex</code>就会产生对应的PDF文件。</p><h2 id="cls-sty"><a href="#cls-sty" class="headerlink" title=".cls, .sty"></a>.cls, .sty</h2><p><code>.cls</code>和<code>.sty</code>文件都是增加 LaTeX 功能的补足文件。它们在我们排版文章时对应的使用<code>\documentclass&#123;&#125;</code>和<code>\usepackage&#123;&#125;</code>加载，在包内部则对应的使用<code>\LoadClass, \LoadClassWithOptions</code>和<code>\RequirePackage, \RequirePackageWithOptions</code>加载。我们通常将<code>.cls</code>文件称之为类（classes）文件，将<code>.sty</code>文件称之为风格（style）文件或者包（package）。</p><p>虽然它们都可以包含任意的 TeX 和 LaTeX 代码，但它们的使用方式不同。我们必须通过<code>\documentclass&#123;&#125;</code>加载一个类文件，并且在一个 LaTeX 文件中只能出现一次，通常也是第一个出现的命令。而包是一个可选项，它可以根据我们的需求加载任意多个（在开始文档之前）。</p><h3 id="sty文件缺失问题"><a href="#sty文件缺失问题" class="headerlink" title=".sty文件缺失问题"></a>.sty文件缺失问题</h3><p>报错：LaTeX Error: File `xxx.sty‘ not found，则需要安装对应包。</p><p>自动安装：<br><code>sudo tlmgr install xxx</code></p><p>手动安装：<br>1、放置文件到<code>/usr/local/texlive/2022basic/texmf-dist/tex/latex</code>下<br>2、<code>sudo texhash</code><br>3、<code>sudo mktexlsr</code></p><h2 id="tlmgr命令"><a href="#tlmgr命令" class="headerlink" title="tlmgr命令"></a>tlmgr命令</h2><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tlmgr install <span class="symbol">&lt;packagename&gt;</span>     安装宏包</span><br><span class="line">tlmgr <span class="built_in">remove</span> <span class="symbol">&lt;packagename&gt;</span>      移除</span><br><span class="line">tlmgr <span class="keyword">update</span> --<span class="keyword">list</span>         查看所有更新的宏包指令</span><br><span class="line">tlmgr <span class="keyword">update</span> --self --<span class="keyword">all</span>   更新所有需要更新的宏包</span><br></pre></td></tr></table></figure><h2 id="配置VSCode作为前端"><a href="#配置VSCode作为前端" class="headerlink" title="配置VSCode作为前端"></a>配置VSCode作为前端</h2><h3 id="1-环境变量确认"><a href="#1-环境变量确认" class="headerlink" title="1. 环境变量确认"></a>1. 环境变量确认</h3><p>使用brew下载BasicLaTeX的话，先把TeXLive添加到环境变量，在<code>.bash_profile</code>下新增：<code>export PATH=$PATH:/usr/local/texlive/2022basic/bin/universal-darwin</code></p><p>查看是否添加成功：<code>echo $PATH</code></p><h3 id="2-安装插件"><a href="#2-安装插件" class="headerlink" title="2. 安装插件"></a>2. 安装插件</h3><p>1、<code>James-Yu.latex-workshop</code></p><h3 id="3-安装Latexmk"><a href="#3-安装Latexmk" class="headerlink" title="3. 安装Latexmk"></a>3. 安装Latexmk</h3><p>BasicTeX不含Latexmk，但插件<code>James-Yu.latex-workshop</code>需要该Perl脚本支持。 通过<code>sudo tlmgr install latexmk</code>安装。</p><p>注意：如需使用中文需要额外配置。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]<a href="https://blog.csdn.net/tsingke/article/details/105960941">https://blog.csdn.net/tsingke/article/details/105960941</a><br>[2]<a href="https://blog.csdn.net/joey_ro/article/details/123441178">https://blog.csdn.net/joey_ro/article/details/123441178</a><br>[3]<a href="https://zhuanlan.zhihu.com/p/289417922">https://zhuanlan.zhihu.com/p/289417922</a><br>[4]<a href="https://blog.csdn.net/joey_ro/article/details/123387085">https://blog.csdn.net/joey_ro/article/details/123387085</a><br>[5]<a href="https://www.cnblogs.com/ouyangsong/p/9348175.html">https://www.cnblogs.com/ouyangsong/p/9348175.html</a><br>[6]<a href="https://mg.readthedocs.io/latexmk.html">https://mg.readthedocs.io/latexmk.html</a><br>[7]<a href="https://www.jianshu.com/p/4aee83e66ab8">https://www.jianshu.com/p/4aee83e66ab8</a></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;在MacOS上安装LaTeX环境，并利用VSCode编写LaTeX文档。&lt;br&gt;</summary>
    
    
    
    
    <category term="MacOS" scheme="http://silencezheng.top/tags/MacOS/"/>
    
    <category term="VSCode" scheme="http://silencezheng.top/tags/VSCode/"/>
    
    <category term="LaTeX" scheme="http://silencezheng.top/tags/LaTeX/"/>
    
  </entry>
  
  <entry>
    <title>tesseract安装使用</title>
    <link href="http://silencezheng.top/2022/09/13/article62/"/>
    <id>http://silencezheng.top/2022/09/13/article62/</id>
    <published>2022-09-13T04:04:16.000Z</published>
    <updated>2022-09-13T04:06:14.637Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在M1 MacBook上安装使用tesseract5。<br><span id="more"></span></p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p><code>brew install tesseract</code></p><h2 id="添加中文包"><a href="#添加中文包" class="headerlink" title="添加中文包"></a>添加中文包</h2><p>1、 到<a href="https://github.com/tesseract-ocr/tessdata_best">tessdata_best</a>中去下载中文数据集。 也可以是<a href="https://github.com/tesseract-ocr/tessdata">tessdata</a>，更快但不如best精准。</p><p>2、 放置到tesseract的<code>/share/tessdata/</code>目录下，该目录可用<code>brew list tesseract</code>查看。</p><p>3、<code>tesseract 文件名 结果名 -l chi_sim</code>即可识别中文～</p><p>4、 注意待识别图片分辨率不能太低，否则报错<code>Empty page!!</code>。</p><p>中文数据集名字解释：</p><ul><li><p>chi_sim 包含了简化的常用的汉语和英文字符。</p></li><li><p>chi_tra 包含了繁体的常用汉语和英文字符。</p></li><li><p>后带_vert的数据集表示书写方向从上到下。</p></li></ul><h2 id="自有样本训练"><a href="#自有样本训练" class="headerlink" title="自有样本训练"></a>自有样本训练</h2><p>tess2 和 tess3 都可以用<a href="https://github.com/nguyenq/jTessBoxEditor">jTessBoxEditor</a> 训练，但v4之后的LSTM训练该工具不支持了（截至22.09.13）。</p><p>关于如何训练LTSM数据集，目前找到<a href="https://blog.csdn.net/watt/article/details/124099032">一篇文章</a>，没有验证，欢迎讨论。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;在M1 MacBook上安装使用tesseract5。&lt;br&gt;</summary>
    
    
    
    
    <category term="MacOS" scheme="http://silencezheng.top/tags/MacOS/"/>
    
    <category term="Tesseract" scheme="http://silencezheng.top/tags/Tesseract/"/>
    
  </entry>
  
  <entry>
    <title>PycharmCV2无法自动提示解决</title>
    <link href="http://silencezheng.top/2022/09/12/article61/"/>
    <id>http://silencezheng.top/2022/09/12/article61/</id>
    <published>2022-09-12T13:06:15.000Z</published>
    <updated>2022-09-12T13:08:39.711Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>M1芯片 MacBook 上 <strong>Pycharm无法对cv2自动代码提示</strong> 问题解决。</p><span id="more"></span><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>1、找到环境中<code>/Users/YOURNAME/miniforge3/envs/YOURENV/lib/python3.9/site-packages/cv2</code></p><p>2、复制<code>cv2.abi3.so</code>文件，放置到上一级目录中。</p><p>3、最终效果：<code>/Users/YOURNAME/miniforge3/envs/YOURENV/lib/python3.9/site-packages/cv2.abi3.so</code></p><p>4、Boom，解决。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;M1芯片 MacBook 上 &lt;strong&gt;Pycharm无法对cv2自动代码提示&lt;/strong&gt; 问题解决。&lt;/p&gt;</summary>
    
    
    
    
    <category term="MacOS" scheme="http://silencezheng.top/tags/MacOS/"/>
    
    <category term="Pycharm" scheme="http://silencezheng.top/tags/Pycharm/"/>
    
  </entry>
  
  <entry>
    <title>普通人的反向传播理解</title>
    <link href="http://silencezheng.top/2022/09/11/article60/"/>
    <id>http://silencezheng.top/2022/09/11/article60/</id>
    <published>2022-09-11T08:14:14.000Z</published>
    <updated>2022-09-11T09:02:35.535Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>一个普通人尝试理解反向传播…从上至下阅读吧，看看有什么收获。<br><span id="more"></span> </p><p>先说结论：<strong>反向传播用于快速计算神经网络中节点的梯度</strong></p><h2 id="导数（Derivative）"><a href="#导数（Derivative）" class="headerlink" title="导数（Derivative）"></a>导数（Derivative）</h2><p>对函数$f: \mathbb{R} \rightarrow \mathbb{R}$，其输入和输出都是标量。 如果$f$的导数存在，这个极限被定义为：</p><script type="math/tex; mode=display">f'(x) = \lim_{h \rightarrow 0} \frac{f(x+h) - f(x)}{h}.</script><p>如果$f’(a)$存在，则称$f$在$a$处是<strong>可微</strong>（differentiable）的。 如果$f$在一个区间的每一个点上都是可微的，那么该函数在此区间上可微。导数$f’(x)$可以解释为$f(x)$相对于$x$的瞬时（instantaneous）变化率，该变化率基于变化$h$，$h$趋近于$0$。</p><h2 id="微分（Differential）"><a href="#微分（Differential）" class="headerlink" title="微分（Differential）"></a>微分（Differential）</h2><p>给定$y = f(x)$，其中$x$和$y$分别是函数$f$的自变量和因变量，则以下表达式等价：</p><script type="math/tex; mode=display">f'(x) = y' = \frac{dy}{dx} = \frac{df}{dx} = \frac{d}{dx} f(x) = Df(x) = D_x f(x),</script><p>其中符号$\frac{d}{dx}$和$D$是微分运算符。可以使用以下规则来对常见函数求微分：</p><ul><li>$DC = 0$（$C$为常数）</li><li>$Dx^n = nx^{n-1}$（n为任意实数）</li><li>$De^x = e^x$</li><li>$D\ln(x) = 1/x$</li></ul><p>假设函数$f$和$g$可微，$C$为常数，则有如下法则：</p><p>1、 $\frac{d}{dx} [Cf(x)] = C \frac{d}{dx} f(x)$</p><p>2、 $\frac{d}{dx} [f(x) + g(x)] = \frac{d}{dx} f(x) + \frac{d}{dx} g(x)$</p><p>3、 $\frac{d}{dx} [f(x)g(x)] = f(x) \frac{d}{dx} [g(x)] + g(x) \frac{d}{dx} [f(x)]$</p><p>4、 $\frac{d}{dx} \left[\frac{f(x)}{g(x)}\right] = \frac{g(x) \frac{d}{dx} [f(x)] - f(x) \frac{d}{dx} [g(x)]}{[g(x)]^2}$</p><p>这些法则可以便于计算由常见函数组成的函数的微分，注意，<em>复合函数无法通过这些法则微分</em>。</p><h2 id="偏导数（Partial-derivative）"><a href="#偏导数（Partial-derivative）" class="headerlink" title="偏导数（Partial derivative）"></a>偏导数（Partial derivative）</h2><p>就是将微分推广到多元函数，这个应该懂得都懂。</p><p>设$y = f(x_1, x_2, \ldots, x_n)$为具有$n$个变量的函数，$y$关于$x_i$的偏导数为：</p><script type="math/tex; mode=display">\frac{\partial y}{\partial x_i} = \lim_{h \rightarrow 0} \frac{f(x_1, \ldots, x_{i-1}, x_i+h, x_{i+1}, \ldots, x_n) - f(x_1, \ldots, x_i, \ldots, x_n)}{h}.</script><p>计算时，将$x_1, \ldots, x_{i-1}, x_{i+1}, \ldots, x_n$看作常数，直接计算$y$关于$x_i$的导数即可。</p><p>同时也有以下等价表示：</p><script type="math/tex; mode=display">\frac{\partial y}{\partial x_i} = \frac{\partial f}{\partial x_i} = f_{x_i} = f_i = D_i f = D_{x_i} f.</script><h2 id="梯度（Gradient）"><a href="#梯度（Gradient）" class="headerlink" title="梯度（Gradient）"></a>梯度（Gradient）</h2><p><strong>梯度</strong>向量是连结一个多元函数对其所有变量的偏导数后得到的。</p><p>设函数$f:\mathbb{R}^n\rightarrow\mathbb{R}$的输入是一个$n$维向量$\mathbf{x}=[x_1,x_2,\ldots,x_n]^\top$，且输出是一个标量。则函数$f(\mathbf{x})$相对于$x$的梯度为：</p><script type="math/tex; mode=display">\nabla_{\mathbf{x}} f(\mathbf{x}) = \bigg[\frac{\partial f(\mathbf{x})}{\partial x_1}, \frac{\partial f(\mathbf{x})}{\partial x_2}, \ldots, \frac{\partial f(\mathbf{x})}{\partial x_n}\bigg]^\top</script><p>$\nabla_{\mathbf{x}} f(\mathbf{x})$在没有歧义的时候可以被$\nabla f(\mathbf{x})$替换。</p><p>继续假设$x$为$n$维向量，在微分多元函数时有如下法则（注意n和m的位置）：</p><p>1、对所有$\mathbf{A} \in \mathbb{R}^{m \times n}$，有$\nabla_{\mathbf{x}} \mathbf{A} \mathbf{x} = \mathbf{A}^\top$ （用于转置矩阵）</p><p>2、 对所有$\mathbf{A} \in \mathbb{R}^{n \times m}$，有$\nabla_{\mathbf{x}} \mathbf{x}^\top \mathbf{A} = \mathbf{A}$ （抵消）</p><p>3、对所有$\mathbf{A} \in \mathbb{R}^{n \times n}$，有$\nabla_{\mathbf{x}} \mathbf{x}^\top \mathbf{A} \mathbf{x} = (\mathbf{A} + \mathbf{A}^\top)\mathbf{x}$ （方阵）</p><p>4、$\nabla_{\mathbf{x}} |\mathbf{x} |^2 = \nabla_{\mathbf{x}} \mathbf{x}^\top \mathbf{x} = 2\mathbf{x}$ （函数相对于向量$x$的梯度乘以向量$x$范数的平方等于$2x$）</p><p>接第四条，对于任何矩阵$\mathbf{X}$，都有$\nabla_{\mathbf{X}} |\mathbf{X} |_F^2 = 2\mathbf{X}$。</p><h2 id="链式法则（Chain-rule）"><a href="#链式法则（Chain-rule）" class="headerlink" title="链式法则（Chain rule）"></a>链式法则（Chain rule）</h2><p>计算梯度的关键是对多元函数求导，但深度学习中多元函数通常是<em>复合</em>（composite）的，即无法通过常见函数微分法则求导。</p><p><strong>链式法则可以帮助我们对复合多元函数求导。</strong></p><p>设可微分函数$y$有变量$u_1, u_2, \ldots, u_m$，其中每个可微分函数$u_i$都有变量$x_1, x_2, \ldots, x_n$，$y$是$x_1, x_2, \ldots, x_n$的函数。对于任意$i = 1, 2, \ldots, n$，链式法则给出：</p><script type="math/tex; mode=display">\frac{dy}{dx_i} = \frac{dy}{du_1} \frac{du_1}{dx_i} + \frac{dy}{du_2} \frac{du_2}{dx_i} + \cdots + \frac{dy}{du_m} \frac{du_m}{dx_i}</script><h2 id="反向传播（Back-Propagation）"><a href="#反向传播（Back-Propagation）" class="headerlink" title="反向传播（Back Propagation）"></a>反向传播（Back Propagation）</h2><p>现在，依据上方的基础知识，我们已经知道<strong>深度学习优化算法的关键在于梯度，梯度的关键在于求导，而这个导数通常不好求，需要链式法则的参与。</strong></p><p><strong>反向传播提供了一种基于链式法则快速计算任一偏导数的方法</strong>。在深度学习框架中，框架根据我们的模型构建<strong>计算图</strong>（computational graph），计算图用于跟踪数据、操作和组合顺序，正向传播（即由输入到输出进行计算）后框架通过<strong>自动微分</strong>（automatic differentiation）反向传播梯度，跟踪整个计算图，填充关于每个参数的偏导数。</p><p>在此我不准备罗列一堆复杂的数学公式来推导<strong>反向传播</strong>的计算细节（我也不会），而是想基于应用的角度，简单的理解一下反向传播。</p><p>设有一函数$y=2\mathbf{u} + 5$，其中$u = 3\mathbf{x}^{\top}\mathbf{x}$，$y$是关于$x$的函数。</p><p>下面我们来求$y$对$x$的梯度，求梯度本质上就是求偏导。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># requires_grad为x申请存放梯度的内存</span></span><br><span class="line">x = torch.arange(<span class="number">4.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># x的转置乘x == x和x的点积</span></span><br><span class="line">u = <span class="number">3</span> * torch.dot(x, x)</span><br><span class="line"><span class="built_in">print</span>(u)</span><br><span class="line"></span><br><span class="line">y = <span class="number">2</span> * u + <span class="number">5</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">tensor(<span class="number">42.</span>, grad_fn=&lt;MulBackward0&gt;)</span><br><span class="line">tensor(<span class="number">89.</span>, grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line"><span class="literal">None</span></span><br></pre></td></tr></table></figure><p>到这里$x$还没有梯度，因为还没有进行反向传播，下面来计算梯度。 原式$y$对$u$的导数为$2$，$u$对$x$的导数为$6x$，根据链式法则，$y$对$x$的导数为$12x$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">y.backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"><span class="built_in">print</span>(<span class="number">12</span> * x == x.grad)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([ <span class="number">0.</span>, <span class="number">12.</span>, <span class="number">24.</span>, <span class="number">36.</span>])</span><br><span class="line">tensor([<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>])</span><br></pre></td></tr></table></figure><p>验证得，梯度计算正确。 这里存在一个问题，为什么不去展示中间变量$u$的梯度呢？因为出于节省内存（显存）的考虑，pytorch在反向传播的过程中只保留了计算图中的叶子结点的梯度值，而未保留中间节点的梯度。但的确可以通过一些手段获取中间变量的梯度，只是没必要，这里也不做演示了。</p><h2 id="参考文献："><a href="#参考文献：" class="headerlink" title="参考文献："></a>参考文献：</h2><p>[1]<a href="https://zh-v2.d2l.ai/chapter_preliminaries/calculus.html">https://zh-v2.d2l.ai/chapter_preliminaries/calculus.html</a><br>[2]<a href="https://blog.csdn.net/Weary_PJ/article/details/105706318">https://blog.csdn.net/Weary_PJ/article/details/105706318</a><br>[3]<a href="https://blog.csdn.net/weixin_41799019/article/details/117353078">https://blog.csdn.net/weixin_41799019/article/details/117353078</a><br>[4]<a href="https://blog.csdn.net/xierhacker/article/details/53431207">https://blog.csdn.net/xierhacker/article/details/53431207</a></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;一个普通人尝试理解反向传播…从上至下阅读吧，看看有什么收获。&lt;br&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://silencezheng.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="机器学习" scheme="http://silencezheng.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>smartmontools安装使用</title>
    <link href="http://silencezheng.top/2022/09/08/article59/"/>
    <id>http://silencezheng.top/2022/09/08/article59/</id>
    <published>2022-09-08T10:58:22.000Z</published>
    <updated>2022-09-08T11:02:14.025Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>介绍安装和使用磁盘监控工具smartmontools，以Mac为例。<br><span id="more"></span><br>smartmontools软件包包含两个实用程序（<code>smartctl</code> 和 <code>smartd</code>），使用内置在大多数现代 ATA/SATA、SCSI/SAS 和 NVMe 磁盘中的自我监控、分析和报告技术系统 (SMART) 来控制和监控存储系统。在许多情况下，这些工具将提供磁盘降级和故障的高级警告。 Smartmontools 最初源自 Linux smartsuite 包，实际上支持 ATA/SATA、SCSI/SAS 和 NVMe 磁盘以及 SCSI/SAS 磁带设备。它应该可以在任何现代 Linux、FreeBSD、NetBSD、OpenBSD、Darwin (macOS)、Solaris、Windows、Cygwin、OS/2、eComStation 或 QNX 系统上运行。 Smartmontools 也可以从许多不同的 Live CD/DVD 之一运行。</p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p><code>brew install smartmontools</code></p><h2 id="检查硬盘"><a href="#检查硬盘" class="headerlink" title="检查硬盘"></a>检查硬盘</h2><p>方式一（通用）：<br>1、打开磁盘工具，找到设备名，如<code>disk3s1s1</code><br>2、<code>smartctl -a disk3s3s1</code></p><p>方式二（仅内置）：<br><code>smartctl -a disk0</code></p><h2 id="参数解析"><a href="#参数解析" class="headerlink" title="参数解析"></a>参数解析</h2><p><strong>ID1：Critical Warning警告状态</strong><br>RAW数值显示0为正常无警告，1为过热警告，2为闪存介质引起的内部错误导致可靠性降级，3为闪存进入只读状态，4为增强型断电保护功能失效（只针对有该特性的固态硬盘）。</p><p>正常情况下ID1的RAW属性值应为0，当显示为1时代表NVMe固态硬盘已经过热，需要改善散热条件或降低工作负载。属性值为2时应考虑返修或更换新硬盘，当属性值为3时硬盘已经进入只读状态，无法正常工作，应抓紧时间备份其中的数据。家用固态硬盘通常不会配备增强型断电保护（完整断电保护），所以通常该项目不会显示为4。</p><p><strong>ID2：Temperature当前温度（十进制显示）</strong></p><p><strong>ID3：Available Spare可用冗余空间（百分比显示）</strong><br>指示当前固态硬盘可用于替换坏块的保留备用块占出厂备用块总数量的百分比。该数值从出厂时的100%随使用过程降低，直至到零。ID3归零之前就有可能产生不可预料的故障，所以不要等到该项目彻底归零才考虑更换新硬盘。</p><p><strong>ID4：Available Spare Threshold备用空间阈值</strong><br>与ID3相关，当ID3的数值低于ID4所定义的阈值之后，固态硬盘被认为达到极限状态，此时系统可能会发出可靠性警告。该项数值由厂商定义，通常为10%或0%。</p><p><strong>ID5：Percentage Used已使用的写入耐久度（百分比显示）</strong><br>该项显示已产生的写入量占厂商定义总写入寿命的百分比。该项数值为动态显示，计算结果与写入量及固态硬盘的TBW总写入量指标有关。新盘状态下该项目为0%。</p><p><strong>ID6：Data Units Read读取扇区计数（1000）</strong><br>该项数值乘以1000后即为读取的扇区（512Byte）数量统计。</p><p><strong>ID7：Data Units Write写入扇区计数（1000）</strong><br>该项数值乘以1000后即为写入的扇区（512Byte）数量统计。</p><p><strong>ID8：Host Read Commands读取命令计数</strong><br>硬盘生命周期内累计接收到的读取命令数量统计。</p><p><strong>ID9：Host Write Commands写入命令计数</strong><br>硬盘生命周期内累计接收到的写入命令数量统计。</p><p><strong>ID10：Controller Busy Time主控繁忙时间计数</strong><br>该项统计的是主控忙于处理IO命令的时间总和（单位：分钟）。当IO队列有未完成的命令时，主控即处于“忙”的状态。</p><p><strong>ID11：Power Cycles通电次数</strong></p><p><strong>ID12：Power On Hours通电时间</strong></p><p><strong>ID13：Unsafe Shut downs不安全关机次数（异常断电计数）</strong></p><p><strong>ID14：Media and Data Integrity Errors闪存和数据完整性错误</strong></p><p>主控检测到未恢复的数据完整性错误的次数。正常情况下主控不应检测到数据完整性错误（纠错应该在此之前完成），当有不可校正的ECC、CRC校验失败或者LBA标签不匹配错误发生时，该数值会增加。正常情况下ID14应保持为零。</p><p><strong>ID15：Number of Error Information Log Entries错误日志条目计数</strong></p><p>控制器使用期限内，发生的错误信息日志条目的数量统计。正常情况该项目应为零。<br>有时该条目下会有<code>Read 1 entries from Error Information Log failed: GetLogPage failed: system=0x38, sub=0x0, code=745</code>之类的信息提示。</p><p><strong>以下项目</strong>为非标准项，并非所有NVMe SSD都支持显示。<br><strong>ID16：Warning Composite Temperature Time过热警告时间</strong><br><strong>ID17：Critical Composite Temerature Time过热临界温度时间</strong><br><strong>ID18-25：Temperature Sensor X：多个温度传感器（若存在）的读数</strong></p><p>参考：<a href="https://blog.csdn.net/qq_24343177/article/details/122521952">https://blog.csdn.net/qq_24343177/article/details/122521952</a></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;介绍安装和使用磁盘监控工具smartmontools，以Mac为例。&lt;br&gt;</summary>
    
    
    
    
    <category term="MacOS" scheme="http://silencezheng.top/tags/MacOS/"/>
    
  </entry>
  
  <entry>
    <title>现代循环神经网络--《动手学深度学习》笔记0x0A</title>
    <link href="http://silencezheng.top/2022/09/07/article58/"/>
    <id>http://silencezheng.top/2022/09/07/article58/</id>
    <published>2022-09-07T14:41:53.000Z</published>
    <updated>2022-09-07T15:20:50.097Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><p>前一章中介绍了循环神经网络的基础知识，这种网络可以更好地处理序列数据。但对于当今各种各样的序列学习问题，这些技术可能并不够用。</p><p>例如，循环神经网络在实践中一个常见问题是数值不稳定性。尽管我们已经应用了梯度裁剪等技巧来缓解这个问题，但是仍需要通过设计更复杂的序列模型可以进一步处理它。比如两个广泛使用的网络：<em>门控循环单元</em>（gated recurrent units，GRU）和<em>长短期记忆网络</em>（long short-term memory，LSTM）。然后本章将基于一个单向隐藏层来扩展循环神经网络架构，描述具有多个隐藏层的深层架构，并讨论基于前向和后向循环计算的双向设计。现代循环网络经常采用这种扩展。在解释这些循环神经网络的变体时将继续利用上一章中的语言建模问题。<br><span id="more"></span><br>事实上，语言建模只揭示了序列学习能力的冰山一角。在各种序列学习问题中，如自动语音识别、文本到语音转换和机器翻译，输入和输出都是任意长度的序列。为了阐述如何拟合这种类型的数据，我们将以机器翻译为例介绍基于循环神经网络的“编码器－解码器”架构和束搜索，并用它们来生成序列。</p><h3 id="0-1-小结"><a href="#0-1-小结" class="headerlink" title="0.1. 小结"></a>0.1. 小结</h3><ul><li>门控循环神经网络可以更好地捕获时间步距离很长的序列上的依赖关系。</li><li>重置门有助于捕获序列中的短期依赖关系。</li><li>更新门有助于捕获序列中的长期依赖关系。</li><li>重置门打开时，门控循环单元包含基本循环神经网络；更新门打开时，门控循环单元可以跳过子序列。</li><li>长短期记忆网络有三种类型的门：输入门、遗忘门和输出门。</li><li>长短期记忆网络的隐藏层输出包括“隐状态”和“记忆元”。只有隐状态会传递到输出层，而记忆元完全属于内部信息。</li><li>长短期记忆网络可以缓解梯度消失和梯度爆炸。</li><li>在深度循环神经网络中，隐状态的信息被传递到当前层的下一时间步和下一层的当前时间步。</li><li>有许多不同风格的深度循环神经网络， 如长短期记忆网络、门控循环单元、或经典循环神经网络。 这些模型在深度学习框架的高级API中都有涵盖。</li><li>总体而言，深度循环神经网络需要大量的调参（如学习率和修剪） 来确保合适的收敛，模型的初始化也需要谨慎。</li><li>在双向循环神经网络中，每个时间步的隐状态由当前时间步的前后数据同时决定。</li><li>双向循环神经网络与概率图模型中的“前向-后向”算法具有相似性。</li><li>双向循环神经网络主要用于序列编码和给定双向上下文的观测估计。</li><li>由于梯度链更长，因此双向循环神经网络的训练代价非常高</li><li>机器翻译指的是将文本序列从一种语言自动翻译成另一种语言。</li><li>使用单词级词元化时的词表大小，将明显大于使用字符级词元化时的词表大小。为了缓解这一问题，我们可以将低频词元视为相同的未知词元。</li><li>通过截断和填充文本序列，可以保证所有的文本序列都具有相同的长度，以便以小批量的方式加载。</li><li>“编码器－解码器”架构可以将长度可变的序列作为输入和输出，因此适用于机器翻译等序列转换问题。</li><li>编码器将长度可变的序列作为输入，并将其转换为具有固定形状的编码状态。</li><li>解码器将具有固定形状的编码状态映射为长度可变的序列。</li><li>根据“编码器-解码器”架构的设计， 我们可以使用两个循环神经网络来设计一个序列到序列学习的模型。</li><li>在实现编码器和解码器时，我们可以使用多层循环神经网络。</li><li>可以使用屏蔽（mask）来过滤不相关的计算，例如在计算损失时。</li><li>在“编码器－解码器”训练中，强制教学方法将原始输出序列（而非预测结果）输入解码器。</li><li>BLEU是一种常用的评估方法，它通过测量预测序列和标签序列之间的元语法的匹配度来评估预测。</li><li>序列搜索策略包括贪心搜索、穷举搜索和束搜索。</li><li>贪心搜索所选取序列的计算量最小，但精度相对较低。</li><li>穷举搜索所选取序列的精度最高，但计算量最大。</li><li>束搜索通过灵活选择束宽，在正确率和计算代价之间进行权衡。</li></ul><h2 id="1-门控循环单元（GRU）"><a href="#1-门控循环单元（GRU）" class="headerlink" title="1. 门控循环单元（GRU）"></a>1. 门控循环单元（GRU）</h2><p>上一章讨论了如何在循环神经网络中计算梯度，以及矩阵连续乘积可以导致梯度消失或梯度爆炸的问题。下面简单思考一下这种梯度异常在实践中的意义：</p><ul><li>可能会遇到这样的情况：早期观测值对预测所有未来观测值具有非常重要的意义。例如一个极端情况，其中第一个观测值包含一个校验和，目标是在序列的末尾辨别校验和是否正确。在这种情况下，第一个词元的影响至关重要。我们希望有某些机制能够在一个记忆元里存储重要的早期信息。如果没有这样的机制，我们将不得不给这个观测值指定一个非常大的梯度，因为它会影响所有后续的观测值。</li><li>可能会遇到这样的情况：一些词元没有相关的观测值。例如，在对网页内容进行情感分析时，可能有一些辅助HTML代码与网页传达的情绪无关。我们希望有一些机制来<em>跳过</em>隐状态表示中的此类词元。</li><li>可能会遇到这样的情况：序列的各个部分之间存在逻辑中断。例如，书的章节之间可能会有过渡存在，或者证券的熊市和牛市之间可能会有过渡存在。在这种情况下，最好有一种方法来<em>重置</em>内部状态表示。</li></ul><p>学术界已经提出了许多方法来解决这类问题。其中最早的方法是”长短期记忆”（long-short-term memory，LSTM）[<code>Hochreiter.Schmidhuber.1997</code>]。门控循环单元（gated recurrent unit，GRU）[<code>Cho.Van-Merrienboer.Bahdanau.ea.2014</code>]是一个稍微简化的变体，通常能够提供同等的效果，并且计算[<code>Chung.Gulcehre.Cho.ea.2014</code>]的速度明显更快。由于门控循环单元更简单，本章从它开始解读。</p><h3 id="1-1-门控隐状态"><a href="#1-1-门控隐状态" class="headerlink" title="1.1. 门控隐状态"></a>1.1. 门控隐状态</h3><p>门控循环单元与普通的循环神经网络之间的关键区别在于： 前者支持隐状态的门控。 这意味着模型有专门的机制来确定应该何时更新隐状态， 以及应该何时重置隐状态。 这些机制是可学习的，并且能够解决了上面列出的问题。 例如，如果第一个词元非常重要， 模型将学会在第一次观测之后不更新隐状态。 同样，模型也可以学会跳过不相关的临时观测。 最后，模型还将学会在需要的时候重置隐状态。 下面将详细讨论各类门控。</p><h4 id="1-1-1-重置门和更新门"><a href="#1-1-1-重置门和更新门" class="headerlink" title="1.1.1. 重置门和更新门"></a>1.1.1. 重置门和更新门</h4><p>首先介绍<em>重置门</em>（reset gate）和<em>更新门</em>（update gate）。它们被设计成$(0, 1)$区间中的向量，这样就可以进行凸组合。重置门允许我们控制“可能还想记住”的过去状态的数量；更新门将允许我们控制新状态中有多少个是旧状态的副本。</p><p>凸组合<br>: 设向量 $ { x_i }, i=1,2, \ldots, n $, 如有实数 $\lambda_i \geq 0$, 且 $\sum_{i=1}^n \lambda_i=1$, 则称 $\sum_{i=1}^n \lambda_i x_i$ 为向量 $ { x_i } $ 的一个凸组合(凸线性组合)。</p><p>下图描述了门控循环单元中的重置门和更新门的输入，输入是由当前时间步的输入和前一时间步的隐状态给出。两个门的输出是由使用sigmoid激活函数的两个全连接层给出。</p><p><img src="/assets/post_img/article58/gru-1.svg" alt="在门控循环单元模型中计算重置门和更新门"></p><p>来看一下门控循环单元的数学表达。对于给定的时间步$t$，假设输入是一个小批量$\mathbf{X}_t \in \mathbb{R}^{n \times d}$（样本个数$n$，输入个数$d$），上一个时间步的隐状态是$\mathbf{H}_{t-1} \in \mathbb{R}^{n \times h}$（隐藏单元个数$h$）。那么，重置门$\mathbf{R}_t \in \mathbb{R}^{n \times h}$和更新门$\mathbf{Z}_t \in \mathbb{R}^{n \times h}$的计算如下所示：</p><script type="math/tex; mode=display">\begin{aligned}\mathbf{R}_t = \sigma(\mathbf{X}_t \mathbf{W}_{xr} + \mathbf{H}_{t-1} \mathbf{W}_{hr} + \mathbf{b}_r),\\\mathbf{Z}_t = \sigma(\mathbf{X}_t \mathbf{W}_{xz} + \mathbf{H}_{t-1} \mathbf{W}_{hz} + \mathbf{b}_z),\end{aligned}</script><p>其中$\mathbf{W}_{xr}, \mathbf{W}_{xz} \in \mathbb{R}^{d \times h}$和$\mathbf{W}_{hr}, \mathbf{W}_{hz} \in \mathbb{R}^{h \times h}$是权重参数，$\mathbf{b}_r, \mathbf{b}_z \in \mathbb{R}^{1 \times h}$是偏置参数。注意，在求和过程中会触发广播机制。使用sigmoid函数的目的是将输入值转换到区间$(0, 1)$。</p><h4 id="1-1-2-候选隐状态"><a href="#1-1-2-候选隐状态" class="headerlink" title="1.1.2. 候选隐状态"></a>1.1.2. 候选隐状态</h4><p>接下来将重置门$\mathbf{R}_t$与前一章中的常规隐状态更新机制集成，得到在时间步$t$的<em>候选隐状态</em>（candidate hidden state）$\tilde{\mathbf{H}}_t \in \mathbb{R}^{n \times h}$。</p><script type="math/tex; mode=display">\tilde{\mathbf{H}}_t = \tanh(\mathbf{X}_t \mathbf{W}_{xh} + \left(\mathbf{R}_t \odot \mathbf{H}_{t-1}\right) \mathbf{W}_{hh} + \mathbf{b}_h),</script><p>其中$\mathbf{W}_{xh} \in \mathbb{R}^{d \times h}$和$\mathbf{W}_{hh} \in \mathbb{R}^{h \times h}$是权重参数，$\mathbf{b}_h \in \mathbb{R}^{1 \times h}$是偏置项，符号$\odot$是Hadamard积（按元素乘积）运算符。这里使用tanh非线性激活函数来确保候选隐状态中的值保持在区间$(-1, 1)$中。</p><p>与常规隐状态更新机制$\mathbf{H}_t = \phi(\mathbf{X}_t \mathbf{W}_{xh} + \mathbf{H}_{t-1} \mathbf{W}_{hh}  + \mathbf{b}_h)$相比，上式中的$\mathbf{R}_t$和$\mathbf{H}_{t-1}$的元素相乘可以减少以往状态的影响。每当重置门$\mathbf{R}_t$中的项接近$1$时，会恢复一个常规隐状态更新的普通循环神经网络。对于重置门$\mathbf{R}_t$中所有接近$0$的项，候选隐状态是以$\mathbf{X}_t$作为输入的多层感知机的结果。因此，任何预先存在的隐状态都会被<em>重置</em>为默认值。下图说明了应用重置门之后的计算流程。</p><p><img src="/assets/post_img/article58/gru-2.svg" alt="在门控循环单元模型中计算候选隐状态"></p><h4 id="1-1-3-隐状态"><a href="#1-1-3-隐状态" class="headerlink" title="1.1.3. 隐状态"></a>1.1.3. 隐状态</h4><p>上述的计算结果只是候选隐状态，之后仍然需要结合更新门$\mathbf{Z}_t$的效果。这一步确定新的隐状态$\mathbf{H}_t \in \mathbb{R}^{n \times h}$在多大程度上来自旧的状态$\mathbf{H}_{t-1}$和新的候选状态$\tilde{\mathbf{H}}_t$。更新门$\mathbf{Z}_t$仅需要在$\mathbf{H}_{t-1}$和$\tilde{\mathbf{H}}_t$之间进行按元素的凸组合就可以实现这个目标。这就得出了门控循环单元的最终更新公式：</p><script type="math/tex; mode=display">\mathbf{H}_t = \mathbf{Z}_t \odot \mathbf{H}_{t-1}  + (1 - \mathbf{Z}_t) \odot \tilde{\mathbf{H}}_t.</script><p>每当更新门$\mathbf{Z}_t$接近$1$时，模型就倾向只保留旧状态。此时，来自$\mathbf{X}_t$的信息基本上被忽略，从而有效地跳过了依赖链条中的时间步$t$。相反，当$\mathbf{Z}_t$接近$0$时，新的隐状态$\mathbf{H}_t$就会接近候选隐状态$\tilde{\mathbf{H}}_t$。这些设计可以帮助我们处理循环神经网络中的梯度消失问题，并更好地捕获时间步距离很长的序列的依赖关系。例如，如果整个子序列的所有时间步的更新门都接近于$1$，则无论序列的长度如何，在序列起始时间步的旧隐状态都将很容易保留并传递到序列结束。</p><p>下图说明了更新门起作用后的计算流。</p><p><img src="/assets/post_img/article58/gru-3.svg" alt="计算门控循环单元模型中的隐状态"></p><p>总之，门控循环单元具有以下两个显著特征：</p><ul><li>重置门有助于捕获序列中的短期依赖关系；</li><li>更新门有助于捕获序列中的长期依赖关系。</li></ul><h3 id="1-2-从零实现"><a href="#1-2-从零实现" class="headerlink" title="1.2. 从零实现"></a>1.2. 从零实现</h3><p>首先读取时间机器数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br></pre></td></tr></table></figure><h4 id="1-2-1-初始化模型参数"><a href="#1-2-1-初始化模型参数" class="headerlink" title="1.2.1. 初始化模型参数"></a>1.2.1. 初始化模型参数</h4><p>下一步是初始化模型参数。从标准差为$0.01$的高斯分布中提取权重，并将偏置项设为$0$，超参数<code>num_hiddens</code>定义隐藏单元的数量，实例化与更新门、重置门、候选隐状态和输出层相关的所有权重和偏置。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_params</span>(<span class="params">vocab_size, num_hiddens, device</span>):</span></span><br><span class="line">    num_inputs = num_outputs = vocab_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">normal</span>(<span class="params">shape</span>):</span></span><br><span class="line">        <span class="keyword">return</span> torch.randn(size=shape, device=device)*<span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">three</span>():</span></span><br><span class="line">        <span class="keyword">return</span> (normal((num_inputs, num_hiddens)),</span><br><span class="line">                normal((num_hiddens, num_hiddens)),</span><br><span class="line">                torch.zeros(num_hiddens, device=device))</span><br><span class="line"></span><br><span class="line">    W_xz, W_hz, b_z = three()  <span class="comment"># 更新门参数</span></span><br><span class="line">    W_xr, W_hr, b_r = three()  <span class="comment"># 重置门参数</span></span><br><span class="line">    W_xh, W_hh, b_h = three()  <span class="comment"># 候选隐状态参数</span></span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = normal((num_hiddens, num_outputs))</span><br><span class="line">    b_q = torch.zeros(num_outputs, device=device)</span><br><span class="line">    <span class="comment"># 附加梯度</span></span><br><span class="line">    params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q]</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> params</span><br></pre></td></tr></table></figure><h4 id="1-2-2-定义模型"><a href="#1-2-2-定义模型" class="headerlink" title="1.2.2. 定义模型"></a>1.2.2. 定义模型</h4><p>现在定义隐状态的初始化函数<code>init_gru_state</code>。与上一章中从零实现RNN中定义的<code>init_rnn_state</code>函数一样，此函数返回一个形状为（批量大小，隐藏单元个数）的张量，张量的值全部为零。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_gru_state</span>(<span class="params">batch_size, num_hiddens, device</span>):</span></span><br><span class="line">    <span class="keyword">return</span> (torch.zeros((batch_size, num_hiddens), device=device), )</span><br></pre></td></tr></table></figure><p>现在定义门控循环单元模型，模型的架构与基本的循环神经网络单元是相同的，只是权重更新公式更为复杂。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gru</span>(<span class="params">inputs, state, params</span>):</span></span><br><span class="line">    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class="line">    H, = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        Z = torch.sigmoid((X @ W_xz) + (H @ W_hz) + b_z)</span><br><span class="line">        R = torch.sigmoid((X @ W_xr) + (H @ W_hr) + b_r)</span><br><span class="line">        H_tilda = torch.tanh((X @ W_xh) + ((R * H) @ W_hh) + b_h)</span><br><span class="line">        H = Z * H + (<span class="number">1</span> - Z) * H_tilda</span><br><span class="line">        Y = H @ W_hq + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">0</span>), (H,)</span><br></pre></td></tr></table></figure><h4 id="1-2-3-训练与预测"><a href="#1-2-3-训练与预测" class="headerlink" title="1.2.3. 训练与预测"></a>1.2.3. 训练与预测</h4><p>训练和预测的工作方式与上一章完全相同。 训练结束后，分别打印输出训练集的困惑度， 以及前缀“time traveler”和“traveler”的预测序列上的困惑度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vocab_size, num_hiddens, device = <span class="built_in">len</span>(vocab), <span class="number">256</span>, d2l.try_gpu()</span><br><span class="line">num_epochs, lr = <span class="number">500</span>, <span class="number">1</span></span><br><span class="line">model = d2l.RNNModelScratch(<span class="built_in">len</span>(vocab), num_hiddens, device, get_params,</span><br><span class="line">                            init_gru_state, gru)</span><br><span class="line">d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br></pre></td></tr></table></figure><h3 id="1-3-框架实现"><a href="#1-3-框架实现" class="headerlink" title="1.3. 框架实现"></a>1.3. 框架实现</h3><p>高级API包含了前文介绍的所有配置细节， 所以可以直接实例化门控循环单元模型。 这段代码的运行速度要快得多， 因为它使用的是编译好的运算符而不是Python来处理之前阐述的许多细节。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">num_inputs = vocab_size</span><br><span class="line">gru_layer = nn.GRU(num_inputs, num_hiddens)</span><br><span class="line">model = d2l.RNNModel(gru_layer, <span class="built_in">len</span>(vocab))</span><br><span class="line">model = model.to(device)</span><br><span class="line">d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br></pre></td></tr></table></figure><h2 id="2-长短期记忆网络（LSTM）"><a href="#2-长短期记忆网络（LSTM）" class="headerlink" title="2. 长短期记忆网络（LSTM）"></a>2. 长短期记忆网络（LSTM）</h2><p>长期以来，隐变量模型存在着长期信息保存和短期输入缺失的问题。解决这一问题的最早方法之一是长短期存储器（long short-term memory，LSTM）[<code>Hochreiter.Schmidhuber.1997</code>]。它有许多与门控循环单元一样的属性。长短期记忆网络的设计比门控循环单元稍微复杂一些，却比门控循环单元早诞生了近20年。</p><h3 id="2-1-门控记忆元"><a href="#2-1-门控记忆元" class="headerlink" title="2.1. 门控记忆元"></a>2.1. 门控记忆元</h3><p>长短期记忆网络的设计灵感来自于计算机的逻辑门。长短期记忆网络引入了<em>记忆元</em>（memory cell），或简称为<em>单元</em>（cell）。有些文献认为记忆元是隐状态的一种特殊类型，它们与隐状态具有相同的形状，其设计目的是用于记录附加的信息。为了控制记忆元，我们需要许多门。其中一个门用来从单元中输出条目，称其为<em>输出门</em>（output gate）。另外一个门用来决定何时将数据读入单元，称其为<em>输入门</em>（input gate）。还需要一种机制来重置单元的内容，由<em>遗忘门</em>（forget gate）来管理，这种设计的动机与门控循环单元相同，能够通过专用机制决定什么时候记忆或忽略隐状态中的输入。下面看看这在实践中是如何运作的。</p><h4 id="2-1-1-输入门、忘记门和输出门"><a href="#2-1-1-输入门、忘记门和输出门" class="headerlink" title="2.1.1. 输入门、忘记门和输出门"></a>2.1.1. 输入门、忘记门和输出门</h4><p>就如在门控循环单元中一样，当前时间步的输入和前一个时间步的隐状态作为数据送入长短期记忆网络的门中，如下图所示。它们由三个具有sigmoid激活函数的全连接层处理，以计算输入门、遗忘门和输出门的值。因此，这三个门的值都在$(0, 1)$的范围内。</p><p><img src="/assets/post_img/article58/lstm-0.svg" alt="长短期记忆模型中的输入门、遗忘门和输出门"></p><p>详细了解一下长短期记忆网络的数学表达。假设有$h$个隐藏单元，批量大小为$n$，输入数为$d$。因此，输入为$\mathbf{X}_t \in \mathbb{R}^{n \times d}$，前一时间步的隐状态为$\mathbf{H}_{t-1} \in \mathbb{R}^{n \times h}$。相应地，时间步$t$的门被定义如下：输入门是$\mathbf{I}_t \in \mathbb{R}^{n \times h}$，遗忘门是$\mathbf{F}_t \in \mathbb{R}^{n \times h}$，输出门是$\mathbf{O}_t \in \mathbb{R}^{n \times h}$。它们的计算方法如下：</p><script type="math/tex; mode=display">\begin{aligned}\mathbf{I}_t &= \sigma(\mathbf{X}_t \mathbf{W}_{xi} + \mathbf{H}_{t-1} \mathbf{W}_{hi} + \mathbf{b}_i),\\\mathbf{F}_t &= \sigma(\mathbf{X}_t \mathbf{W}_{xf} + \mathbf{H}_{t-1} \mathbf{W}_{hf} + \mathbf{b}_f),\\\mathbf{O}_t &= \sigma(\mathbf{X}_t \mathbf{W}_{xo} + \mathbf{H}_{t-1} \mathbf{W}_{ho} + \mathbf{b}_o),\end{aligned}</script><p>其中$\mathbf{W}_{xi}, \mathbf{W}_{xf}, \mathbf{W}_{xo} \in \mathbb{R}^{d \times h}$和$\mathbf{W}_{hi}, \mathbf{W}_{hf}, \mathbf{W}_{ho} \in \mathbb{R}^{h \times h}$是权重参数，$\mathbf{b}_i, \mathbf{b}_f, \mathbf{b}_o \in \mathbb{R}^{1 \times h}$是偏置参数。</p><h4 id="2-1-2-候选记忆元"><a href="#2-1-2-候选记忆元" class="headerlink" title="2.1.2. 候选记忆元"></a>2.1.2. 候选记忆元</h4><p>由于还没有指定各种门的操作，所以先介绍<em>候选记忆元</em>（candidate memory cell）$\tilde{\mathbf{C}}_t \in \mathbb{R}^{n \times h}$。它的计算与上面描述的三个门的计算类似，但是使用$\tanh$函数作为激活函数，函数的值范围为$(-1, 1)$。下面导出在时间步$t$处的方程：</p><script type="math/tex; mode=display">\tilde{\mathbf{C}}_t = \text{tanh}(\mathbf{X}_t \mathbf{W}_{xc} + \mathbf{H}_{t-1} \mathbf{W}_{hc} + \mathbf{b}_c),</script><p>其中$\mathbf{W}_{xc} \in \mathbb{R}^{d \times h}$和<br>$\mathbf{W}_{hc} \in \mathbb{R}^{h \times h}$是权重参数，$\mathbf{b}_c \in \mathbb{R}^{1 \times h}$是偏置参数。如下图所示：</p><p><img src="/assets/post_img/article58/lstm-1.svg" alt="长短期记忆模型中的候选记忆元"></p><h4 id="2-1-3-记忆元"><a href="#2-1-3-记忆元" class="headerlink" title="2.1.3. 记忆元"></a>2.1.3. 记忆元</h4><p>在门控循环单元中，有一种机制来控制输入和遗忘（或称跳过）。类似地，在长短期记忆网络中，也有两个门用于这样的目的：输入门$\mathbf{I}_t$控制采用多少来自$\tilde{\mathbf{C}}_t$的新数据，而遗忘门$\mathbf{F}_t$控制保留多少过去的记忆元$\mathbf{C}_{t-1} \in \mathbb{R}^{n \times h}$的内容。使用按元素乘法，得出：</p><script type="math/tex; mode=display">\mathbf{C}_t = \mathbf{F}_t \odot \mathbf{C}_{t-1} + \mathbf{I}_t \odot \tilde{\mathbf{C}}_t.</script><p>如果遗忘门始终为$1$且输入门始终为$0$，则过去的记忆元$\mathbf{C}_{t-1}$将随时间被保存并传递到当前时间步。引入这种设计是为了缓解梯度消失问题，并更好地捕获序列中的长距离依赖关系。<br>这样就得到了计算记忆元的流程图，如下图。</p><p><img src="/assets/post_img/article58/lstm-2.svg" alt="在长短期记忆网络模型中计算记忆元"></p><h4 id="2-1-4-隐状态"><a href="#2-1-4-隐状态" class="headerlink" title="2.1.4. 隐状态"></a>2.1.4. 隐状态</h4><p>最后需要定义如何计算隐状态$\mathbf{H}_t \in \mathbb{R}^{n \times h}$，这就是输出门发挥作用的地方。在长短期记忆网络中，它仅仅是记忆元的$\tanh$的门控版本。这就确保了$\mathbf{H}_t$的值始终在区间$(-1, 1)$内（因为输出门在0和1之间）：</p><script type="math/tex; mode=display">\mathbf{H}_t = \mathbf{O}_t \odot \tanh(\mathbf{C}_t).</script><p>只要输出门接近$1$，就能够有效地将所有记忆信息传递给预测部分，而对于输出门接近$0$，则只保留记忆元内的所有信息，而不需要更新隐状态。<br>下图提供了数据流的图形化演示。</p><p><img src="/assets/post_img/article58/lstm-3.svg" alt="在长短期记忆模型中计算隐状态"></p><h3 id="2-2-从零实现"><a href="#2-2-从零实现" class="headerlink" title="2.2. 从零实现"></a>2.2. 从零实现</h3><p>首先加载时光机器数据集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br></pre></td></tr></table></figure><h4 id="2-2-1-初始化模型参数"><a href="#2-2-1-初始化模型参数" class="headerlink" title="2.2.1. 初始化模型参数"></a>2.2.1. 初始化模型参数</h4><p>接下来需要定义和初始化模型参数。如前所述，超参数<code>num_hiddens</code>定义隐藏单元的数量。按照标准差$0.01$的高斯分布初始化权重，并将偏置项设为$0$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_lstm_params</span>(<span class="params">vocab_size, num_hiddens, device</span>):</span></span><br><span class="line">    num_inputs = num_outputs = vocab_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">normal</span>(<span class="params">shape</span>):</span></span><br><span class="line">        <span class="keyword">return</span> torch.randn(size=shape, device=device)*<span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">three</span>():</span></span><br><span class="line">        <span class="keyword">return</span> (normal((num_inputs, num_hiddens)),</span><br><span class="line">                normal((num_hiddens, num_hiddens)),</span><br><span class="line">                torch.zeros(num_hiddens, device=device))</span><br><span class="line"></span><br><span class="line">    W_xi, W_hi, b_i = three()  <span class="comment"># 输入门参数</span></span><br><span class="line">    W_xf, W_hf, b_f = three()  <span class="comment"># 遗忘门参数</span></span><br><span class="line">    W_xo, W_ho, b_o = three()  <span class="comment"># 输出门参数</span></span><br><span class="line">    W_xc, W_hc, b_c = three()  <span class="comment"># 候选记忆元参数</span></span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = normal((num_hiddens, num_outputs))</span><br><span class="line">    b_q = torch.zeros(num_outputs, device=device)</span><br><span class="line">    <span class="comment"># 附加梯度</span></span><br><span class="line">    params = [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc,</span><br><span class="line">              b_c, W_hq, b_q]</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> params</span><br></pre></td></tr></table></figure><h4 id="2-2-2-定义模型"><a href="#2-2-2-定义模型" class="headerlink" title="2.2.2. 定义模型"></a>2.2.2. 定义模型</h4><p>在初始化函数中，长短期记忆网络的隐状态需要返回一个<em>额外</em>的记忆元，单元的值为0，形状为（批量大小，隐藏单元数）。因此得到以下的状态初始化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_lstm_state</span>(<span class="params">batch_size, num_hiddens, device</span>):</span></span><br><span class="line">    <span class="keyword">return</span> (torch.zeros((batch_size, num_hiddens), device=device),</span><br><span class="line">            torch.zeros((batch_size, num_hiddens), device=device))</span><br></pre></td></tr></table></figure><p>实际模型的定义与前面讨论的一样：提供三个门和一个额外的记忆元。注意只有隐状态才会传递到输出层，而记忆元$\mathbf{C}_t$不直接参与输出计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm</span>(<span class="params">inputs, state, params</span>):</span></span><br><span class="line">    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c,</span><br><span class="line">     W_hq, b_q] = params</span><br><span class="line">    (H, C) = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        I = torch.sigmoid((X @ W_xi) + (H @ W_hi) + b_i)</span><br><span class="line">        F = torch.sigmoid((X @ W_xf) + (H @ W_hf) + b_f)</span><br><span class="line">        O = torch.sigmoid((X @ W_xo) + (H @ W_ho) + b_o)</span><br><span class="line">        C_tilda = torch.tanh((X @ W_xc) + (H @ W_hc) + b_c)</span><br><span class="line">        C = F * C + I * C_tilda</span><br><span class="line">        H = O * torch.tanh(C)</span><br><span class="line">        Y = (H @ W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">0</span>), (H, C)</span><br></pre></td></tr></table></figure><h4 id="2-2-3-训练和预测"><a href="#2-2-3-训练和预测" class="headerlink" title="2.2.3. 训练和预测"></a>2.2.3. 训练和预测</h4><p>通过实例化上一章引入的RNNModelScratch类来训练一个长短期记忆网络，就如在上一节中所做的一样。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vocab_size, num_hiddens, device = <span class="built_in">len</span>(vocab), <span class="number">256</span>, d2l.try_gpu()</span><br><span class="line">num_epochs, lr = <span class="number">500</span>, <span class="number">1</span></span><br><span class="line">model = d2l.RNNModelScratch(<span class="built_in">len</span>(vocab), num_hiddens, device, get_lstm_params,</span><br><span class="line">                            init_lstm_state, lstm)</span><br><span class="line">d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br></pre></td></tr></table></figure><h3 id="2-3-框架实现"><a href="#2-3-框架实现" class="headerlink" title="2.3. 框架实现"></a>2.3. 框架实现</h3><p>使用高级API可以直接实例化LSTM模型。 高级API封装了前文介绍的所有配置细节。 这段代码的运行速度要快得多， 因为它使用的是编译好的运算符而不是Python来处理之前阐述的许多细节。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">num_inputs = vocab_size</span><br><span class="line">lstm_layer = nn.LSTM(num_inputs, num_hiddens)</span><br><span class="line">model = d2l.RNNModel(lstm_layer, <span class="built_in">len</span>(vocab))</span><br><span class="line">model = model.to(device)</span><br><span class="line">d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br></pre></td></tr></table></figure><p>长短期记忆网络是典型的具有重要状态控制的隐变量自回归模型。 多年来已经提出了其许多变体，例如，多层、残差连接、不同类型的正则化。 然而，由于序列的长距离依赖性，训练长短期记忆网络和其他序列模型（例如门控循环单元）的成本是相当高的。 后面的内容中将讲述更高级的替代模型，如transformer。</p><h2 id="3-深度循环神经网络"><a href="#3-深度循环神经网络" class="headerlink" title="3. 深度循环神经网络"></a>3. 深度循环神经网络</h2><p>到目前为止，我们一直专注于定义由序列输入、单个隐藏 RNN 层和输出层组成的网络。尽管在任何时间步长的输入和相应的输出之间只有一个隐藏层，但这些网络在某种意义上是很深的。第一个时间步的输入可以影响最后一个时间步$T$的输出 （通常是 100 或 1000 步之后）。这些输入通过长度为$T$的时间距离，在产生最终输出之前一直应用了<em>循环层</em>。但通常人们也希望保留在某时间步长上的输入与输出之间表达复杂关系的能力。因此，我们经常构建不仅在时间方向上而且在输入到输出方向上都很深的 RNN。这正是在开发 MLP 和深度 CNN 时已经遇到的深度概念。</p><p>构建这种深度 RNN 的标准方法非常简单：可以将多层循环神经网络堆叠在一起，给定一个长度序列$T$，第一个 RNN 产生一系列输出，长度也一样为$T$. 这些又构成了下一个 RNN 层的输入。通过对几个简单层的组合，产生了一个灵活的机制。特别的是数据可能与不同层的堆叠有关。例如人们可能希望保持有关金融市场状况（熊市或牛市）的宏观数据可用，而微观数据只记录较短期的时间动态。</p><p>下图描述了一个具有$L$个隐藏层的深度循环神经网络，每个隐状态都连续地传递到当前层的下一个时间步和下一层的当前时间步。</p><p><img src="/assets/post_img/article58/deep-rnn.svg" alt="深度循环神经网络结构"></p><h3 id="3-1-函数依赖关系"><a href="#3-1-函数依赖关系" class="headerlink" title="3.1. 函数依赖关系"></a>3.1. 函数依赖关系</h3><p>可以将深度架构中的函数依赖关系形式化，这个架构是由上图中描述的$L$个隐藏层构成。后续的讨论主要集中在经典的循环神经网络模型上，但是这些讨论也适应于其他序列模型。</p><p>假设在时间步$t$有一个小批量的输入数据$\mathbf{X}_t \in \mathbb{R}^{n \times d}$（样本数：$n$，每个样本中的输入数：$d$）。同时，将$l^\mathrm{th}$隐藏层（$l=1,\ldots,L$）的隐状态设为$\mathbf{H}_t^{(l)}  \in \mathbb{R}^{n \times h}$（隐藏单元数：$h$），输出层变量设为$\mathbf{O}_t \in \mathbb{R}^{n \times q}$（输出数：$q$）。设置$\mathbf{H}_t^{(0)} = \mathbf{X}_t$，第$l$个隐藏层的隐状态使用激活函数$\phi_l$，则：</p><script type="math/tex; mode=display">\mathbf{H}_t^{(l)} = \phi_l(\mathbf{H}_t^{(l-1)} \mathbf{W}_{xh}^{(l)} + \mathbf{H}_{t-1}^{(l)} \mathbf{W}_{hh}^{(l)}  + \mathbf{b}_h^{(l)}),</script><p>其中，权重$\mathbf{W}_{xh}^{(l)} \in \mathbb{R}^{h \times h}$，$\mathbf{W}_{hh}^{(l)} \in \mathbb{R}^{h \times h}$和偏置$\mathbf{b}_h^{(l)} \in \mathbb{R}^{1 \times h}$都是第$l$个隐藏层的模型参数。</p><p>最后，输出层的计算仅基于第$l$个隐藏层最终的隐状态：</p><script type="math/tex; mode=display">\mathbf{O}_t = \mathbf{H}_t^{(L)} \mathbf{W}_{hq} + \mathbf{b}_q,</script><p>其中，权重$\mathbf{W}_{hq} \in \mathbb{R}^{h \times q}$和偏置$\mathbf{b}_q \in \mathbb{R}^{1 \times q}$都是输出层的模型参数。</p><p>与多层感知机一样，隐藏层数目$L$和隐藏单元数目$h$都是超参数，它们可以被人为调整。另外，用门控循环单元或长短期记忆网络的隐状态来代替深度循环网络中的隐状态进行计算，可以得到深度门控循环神经网络或深度长短期记忆神经网络。</p><h3 id="3-2-框架实现"><a href="#3-2-框架实现" class="headerlink" title="3.2. 框架实现"></a>3.2. 框架实现</h3><p>实现多层循环神经网络所需的许多逻辑细节在高级API中都是现成的。 简单起见这里仅示范使用此类内置函数的使用方式。 以长短期记忆网络模型为例， 该代码与上节中使用的代码非常相似，唯一的区别是我们指定了层的数量，而不是使用单一层这个默认值。从加载数据集开始。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br></pre></td></tr></table></figure><p>像选择超参数这类架构决策也跟上节中的决策非常相似。因为我们有不同的词元，所以输入和输出都选择相同数量（why？），即<code>vocab_size</code>。隐藏单元的数量仍然是$256$。唯一的区别是现在通过<code>num_layers</code>的值来设定隐藏层数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vocab_size, num_hiddens, num_layers = <span class="built_in">len</span>(vocab), <span class="number">256</span>, <span class="number">2</span></span><br><span class="line">num_inputs = vocab_size</span><br><span class="line">device = d2l.try_gpu()</span><br><span class="line">lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers)</span><br><span class="line">model = d2l.RNNModel(lstm_layer, <span class="built_in">len</span>(vocab))</span><br><span class="line">model = model.to(device)</span><br></pre></td></tr></table></figure><h3 id="3-3-训练与预测"><a href="#3-3-训练与预测" class="headerlink" title="3.3. 训练与预测"></a>3.3. 训练与预测</h3><p>由于使用了长短期记忆网络模型来实例化两个层，因此训练速度被大大降低了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, lr = <span class="number">500</span>, <span class="number">2</span></span><br><span class="line">d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br></pre></td></tr></table></figure><h2 id="4-双向循环神经网络（Bidirectional-RNN）"><a href="#4-双向循环神经网络（Bidirectional-RNN）" class="headerlink" title="4. 双向循环神经网络（Bidirectional RNN）"></a>4. 双向循环神经网络（Bidirectional RNN）</h2><p>在序列学习中，以往假设的目标是：在给定观测的情况下（例如，在时间序列的上下文中或在语言模型的上下文中），对下一个输出进行建模。虽然这是一个典型情景，但不是唯一的。还可能发生什么其它的情况呢？考虑以下三个在文本序列中填空的任务。</p><ul><li>我<code>___</code>。</li><li>我<code>___</code>饿了。</li><li>我<code>___</code>饿了，我可以吃半头猪。</li></ul><p>根据可获得的信息量，可以用不同的词填空，如“很高兴”（”happy”）、“不”（”not”）和“非常”（”very”）。很明显，每个短语的“下文”传达了重要信息（如果有的话），而这些信息关乎到选择哪个词来填空，所以无法利用这一点的序列模型将在相关任务上表现不佳。例如，如果要做好命名实体识别（例如，识别“Green”指的是“格林先生”还是绿色），不同长度的上下文范围重要性是相同的。为了获得一些解决问题的灵感，让我们先迂回到概率图模型。</p><h3 id="4-1-隐马尔可夫模型中的动态规划"><a href="#4-1-隐马尔可夫模型中的动态规划" class="headerlink" title="4.1. 隐马尔可夫模型中的动态规划"></a>4.1. 隐马尔可夫模型中的动态规划</h3><p>这一小节是用来说明动态规划问题的，具体的技术细节对于理解深度学习模型并不重要，但有助于思考为什么要使用深度学习，以及为什么要选择特定的架构。</p><p>如果想用概率图模型来解决这个问题，可以设计一个隐变量模型：在任意时间步$t$，假设存在某个隐变量$h_t$，通过概率$P(x_t \mid h_t)$控制我们观测到的$x_t$。此外，任何$h_t \to h_{t+1}$转移都是由一些状态转移概率$P(h_{t+1} \mid h_{t})$给出。这个概率图模型就是一个<em>隐马尔可夫模型</em>（hidden Markov model，HMM），如下图所示。</p><p><img src="/assets/post_img/article58/hmm.svg" alt="隐马尔可夫模型"></p><p>因此，对于有$T$个观测值的序列，在观测状态和隐状态上具有以下联合概率分布：</p><script type="math/tex; mode=display">P(x_1, \ldots, x_T, h_1, \ldots, h_T) = \prod_{t=1}^T P(h_t \mid h_{t-1}) P(x_t \mid h_t), \text{ where } P(h_1 \mid h_0) = P(h_1).</script><p>现在假设观测到了所有的$x_i$，除了$x_j$，并且我们的目标是计算$P(x_j \mid x_{-j})$，其中$x_{-j} = (x_1, \ldots, x_{j-1}, x_{j+1}, \ldots, x_{T})$。由于$P(x_j \mid x_{-j})$中没有隐变量，因此我们考虑对$h_1, \ldots, h_T$选择构成的所有可能的组合进行求和。如果任何$h_i$可以接受$k$个不同的值（有限的状态数），则意味着需要对$k^T$个项求和，这个任务显然难于登天。幸运的是，有个巧妙的解决方案：<em>动态规划</em>（dynamic programming）。</p><p>为了解动态规划的工作方式，考虑对隐变量$h_1, \ldots, h_T$的依次求和。根据上式，将得出：</p><script type="math/tex; mode=display">\begin{aligned}    &P(x_1, \ldots, x_T) \\    =& \sum_{h_1, \ldots, h_T} P(x_1, \ldots, x_T, h_1, \ldots, h_T) \\    =& \sum_{h_1, \ldots, h_T} \prod_{t=1}^T P(h_t \mid h_{t-1}) P(x_t \mid h_t) \\    =& \sum_{h_2, \ldots, h_T} \underbrace{\left[\sum_{h_1} P(h_1) P(x_1 \mid h_1) P(h_2 \mid h_1)\right]}_{\pi_2(h_2) \stackrel{\mathrm{def}}{=}}    P(x_2 \mid h_2) \prod_{t=3}^T P(h_t \mid h_{t-1}) P(x_t \mid h_t) \\    =& \sum_{h_3, \ldots, h_T} \underbrace{\left[\sum_{h_2} \pi_2(h_2) P(x_2 \mid h_2) P(h_3 \mid h_2)\right]}_{\pi_3(h_3)\stackrel{\mathrm{def}}{=}}    P(x_3 \mid h_3) \prod_{t=4}^T P(h_t \mid h_{t-1}) P(x_t \mid h_t)\\    =& \dots \\    =& \sum_{h_T} \pi_T(h_T) P(x_T \mid h_T).\end{aligned}</script><p>通常我们将<em>前向递归</em>（forward recursion）写为：</p><script type="math/tex; mode=display">\pi_{t+1}(h_{t+1}) = \sum_{h_t} \pi_t(h_t) P(x_t \mid h_t) P(h_{t+1} \mid h_t).</script><p>递归被初始化为$\pi_1(h_1) = P(h_1)$。符号简化，也可以写成$\pi_{t+1} = f(\pi_t, x_t)$，其中$f$是一些可学习的函数。这看起来就像循环神经网络中讨论的隐变量模型中的更新方程。</p><p>与前向递归（或称正向）一样，也可以使用后向递归对同一组隐变量求和。这将得到：</p><script type="math/tex; mode=display">\begin{aligned}    & P(x_1, \ldots, x_T) \\     =& \sum_{h_1, \ldots, h_T} P(x_1, \ldots, x_T, h_1, \ldots, h_T) \\    =& \sum_{h_1, \ldots, h_T} \prod_{t=1}^{T-1} P(h_t \mid h_{t-1}) P(x_t \mid h_t) \cdot P(h_T \mid h_{T-1}) P(x_T \mid h_T) \\    =& \sum_{h_1, \ldots, h_{T-1}} \prod_{t=1}^{T-1} P(h_t \mid h_{t-1}) P(x_t \mid h_t) \cdot    \underbrace{\left[\sum_{h_T} P(h_T \mid h_{T-1}) P(x_T \mid h_T)\right]}_{\rho_{T-1}(h_{T-1})\stackrel{\mathrm{def}}{=}} \\    =& \sum_{h_1, \ldots, h_{T-2}} \prod_{t=1}^{T-2} P(h_t \mid h_{t-1}) P(x_t \mid h_t) \cdot    \underbrace{\left[\sum_{h_{T-1}} P(h_{T-1} \mid h_{T-2}) P(x_{T-1} \mid h_{T-1}) \rho_{T-1}(h_{T-1}) \right]}_{\rho_{T-2}(h_{T-2})\stackrel{\mathrm{def}}{=}} \\    =& \ldots \\    =& \sum_{h_1} P(h_1) P(x_1 \mid h_1)\rho_{1}(h_{1}).\end{aligned}</script><p>因此可以将<em>后向递归</em>（backward recursion）写为：</p><script type="math/tex; mode=display">\rho_{t-1}(h_{t-1})= \sum_{h_{t}} P(h_{t} \mid h_{t-1}) P(x_{t} \mid h_{t}) \rho_{t}(h_{t}),</script><p>初始化$\rho_T(h_T) = 1$。前向和后向递归都允许我们对$T$个隐变量在$\mathcal{O}(kT)$（线性而不是指数）时间内对$(h_1, \ldots, h_T)$的所有值求和。这是使用图模型进行概率推理的巨大好处之一。它也是通用消息传递算法[<code>Aji.McEliece.2000</code>]的一个非常特殊的例子。结合前向和后向递归，我们能够计算</p><script type="math/tex; mode=display">P(x_j \mid x_{-j}) \propto \sum_{h_j} \pi_j(h_j) \rho_j(h_j) P(x_j \mid h_j).</script><blockquote><p>∝，数学符号，表示与某个量成正比例。A∝B也可表示有一个从 𝐴 到 𝐵 的多项式变换，当A、B为集合或表示$\text { set }{(a, b) \in A \times B: a=k b} \text { for some constant } k$</p></blockquote><p>因为符号简化的需要，后向递归也可以写为$\rho_{t-1} = g(\rho_t, x_t)$，其中$g$是一个可以学习的函数。这同样看起来像一个更新方程，只是不像在循环神经网络中看到的那样前向运算，而是后向计算。事实上，知道未来数据何时可用对隐马尔可夫模型是有益的。信号处理学家将是否知道未来观测这两种情况区分为内插和外推，有关更多详细信息，请参阅 [<code>Doucet.De-Freitas.Gordon.2001</code>]。</p><h3 id="4-2-双向模型"><a href="#4-2-双向模型" class="headerlink" title="4.2. 双向模型"></a>4.2. 双向模型</h3><p>如果我们希望在循环神经网络中拥有一种机制，使之能够提供与隐马尔可夫模型类似的前瞻能力，就需要修改循环神经网络的设计。这在概念上很容易，只需要增加一个“从最后一个词元开始从后向前运行”的循环神经网络，而不是只有一个在前向模式下“从第一个词元开始运行”的循环神经网络。<em>双向循环神经网络</em>（bidirectional RNNs）添加了反向传递信息的隐藏层，以便更灵活地处理此类信息。下图描述了具有单个隐藏层的双向循环神经网络的架构。</p><p><img src="/assets/post_img/article58/birnn.svg" alt="双向循环神经网络架构"></p><p>事实上这与隐马尔可夫模型中的动态规划的前向和后向递归没有太大区别。主要区别是在隐马尔可夫模型中的方程具有特定的统计意义。双向循环神经网络没有这样容易理解的解释，我们只能把它们当作通用的、可学习的函数。这种转变集中体现了现代深度网络的设计原则：首先使用经典统计模型的函数依赖类型，然后将其参数化为通用形式。（就是模仿结构但是没有数学理论支持？）</p><h4 id="4-2-1-定义"><a href="#4-2-1-定义" class="headerlink" title="4.2.1. 定义"></a>4.2.1. 定义</h4><p>双向循环神经网络是由[<code>Schuster.Paliwal.1997</code>]提出的，关于各种架构的详细讨论请参阅[<code>Graves.Schmidhuber.2005</code>]。来看看这样一个网络的细节。</p><p>对于任意时间步$t$，给定一个小批量的输入数据$\mathbf{X}_t \in \mathbb{R}^{n \times d}$（样本数$n$，每个示例中的输入数$d$），并且令隐藏层激活函数为$\phi$。在双向架构中，设该时间步的前向和反向隐状态分别为$\overrightarrow{\mathbf{H}}_t  \in \mathbb{R}^{n \times h}$和$\overleftarrow{\mathbf{H}}_t  \in \mathbb{R}^{n \times h}$，其中$h$是隐藏单元的数目。前向和反向隐状态的更新如下：</p><script type="math/tex; mode=display">\begin{aligned}\overrightarrow{\mathbf{H}}_t &= \phi(\mathbf{X}_t \mathbf{W}_{xh}^{(f)} + \overrightarrow{\mathbf{H}}_{t-1} \mathbf{W}_{hh}^{(f)}  + \mathbf{b}_h^{(f)}),\\\overleftarrow{\mathbf{H}}_t &= \phi(\mathbf{X}_t \mathbf{W}_{xh}^{(b)} + \overleftarrow{\mathbf{H}}_{t+1} \mathbf{W}_{hh}^{(b)}  + \mathbf{b}_h^{(b)}),\end{aligned}</script><p>其中，权重$\mathbf{W}_{xh}^{(f)} \in \mathbb{R}^{d \times h}, \mathbf{W}_{hh}^{(f)} \in \mathbb{R}^{h \times h}, \mathbf{W}_{xh}^{(b)} \in \mathbb{R}^{d \times h}, \mathbf{W}_{hh}^{(b)} \in \mathbb{R}^{h \times h}$和偏置$\mathbf{b}_h^{(f)} \in \mathbb{R}^{1 \times h}, \mathbf{b}_h^{(b)} \in \mathbb{R}^{1 \times h}$都是模型参数。</p><p>接下来，将前向隐状态$\overrightarrow{\mathbf{H}}_t$和反向隐状态$\overleftarrow{\mathbf{H}}_t$连接起来，获得需要送入输出层的隐状态$\mathbf{H}_t \in \mathbb{R}^{n \times 2h}$。在具有多个隐藏层的深度双向循环神经网络中，该信息作为输入传递到下一个双向层。最后，输出层计算得到的输出为$\mathbf{O}_t \in \mathbb{R}^{n \times q}$（$q$是输出单元的数目）：</p><script type="math/tex; mode=display">\mathbf{O}_t = \mathbf{H}_t \mathbf{W}_{hq} + \mathbf{b}_q.</script><p>这里，权重矩阵$\mathbf{W}_{hq} \in \mathbb{R}^{2h \times q}$和偏置$\mathbf{b}_q \in \mathbb{R}^{1 \times q}$是输出层的模型参数。事实上，这两个方向可以拥有不同数量的隐藏单元。</p><h4 id="4-2-2-模型的计算代价及其应用"><a href="#4-2-2-模型的计算代价及其应用" class="headerlink" title="4.2.2. 模型的计算代价及其应用"></a>4.2.2. 模型的计算代价及其应用</h4><p>双向循环神经网络的一个关键特性是：使用来自序列两端的信息来估计输出。也就是说，我们使用来自过去和未来的观测信息来预测当前的观测。但是在对下一个词元进行预测的情况中，这样的模型并不是我们所需的。因为在预测下一个词元时我们无法知道下一个词元的下文是什么，所以将不会得到很好的精度。具体地说，在训练期间，我们能够利用过去和未来的数据来估计现在空缺的词；而在测试期间，我们只有过去的数据，因此精度将会很差。下面的实验将说明这一点。</p><p>另一个严重问题是，双向循环神经网络的计算速度非常慢。其主要原因是网络的前向传播需要在双向层中进行前向和后向递归，并且网络的反向传播还依赖于前向传播的结果。因此，梯度求解将有一个非常长的链。</p><p>双向层的使用在实践中非常少，并且仅仅应用于部分场合。例如，填充缺失的单词、词元注释（例如，用于命名实体识别）以及作为序列处理流水线中的一个步骤对序列进行编码（例如，用于机器翻译）。本书未来将介绍如何使用双向循环神经网络编码文本序列。</p><h3 id="4-3-双向循环神经网络的错误应用"><a href="#4-3-双向循环神经网络的错误应用" class="headerlink" title="4.3. 双向循环神经网络的错误应用"></a>4.3. 双向循环神经网络的错误应用</h3><p>由于双向循环神经网络使用了过去的和未来的数据，所以不能盲目地将这一语言模型应用于任何预测任务。 尽管模型产出的困惑度是合理的，该模型预测未来词元的能力却可能存在严重缺陷。 这里用下面的示例代码引以为戒，以防在错误的环境中使用它们。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line">batch_size, num_steps, device = <span class="number">32</span>, <span class="number">35</span>, d2l.try_gpu()</span><br><span class="line">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br><span class="line"><span class="comment"># 通过设置“bidirective=True”来定义双向LSTM模型</span></span><br><span class="line">vocab_size, num_hiddens, num_layers = <span class="built_in">len</span>(vocab), <span class="number">256</span>, <span class="number">2</span></span><br><span class="line">num_inputs = vocab_size</span><br><span class="line">lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers, bidirectional=<span class="literal">True</span>)</span><br><span class="line">model = d2l.RNNModel(lstm_layer, <span class="built_in">len</span>(vocab))</span><br><span class="line">model = model.to(device)</span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">num_epochs, lr = <span class="number">500</span>, <span class="number">1</span></span><br><span class="line">d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">perplexity <span class="number">1.1</span>, <span class="number">130005.5</span> tokens/sec on cuda:<span class="number">0</span></span><br><span class="line">time travellerererererererererererererererererererererererererer</span><br><span class="line">travellerererererererererererererererererererererererererer</span><br></pre></td></tr></table></figure><p>最终结果虽然困惑度降低了，但预测效果非常差。关于如何更有效地使用双向循环神经网络的讨论，请参阅情感分类应用。</p><h2 id="5-机器翻译与数据集"><a href="#5-机器翻译与数据集" class="headerlink" title="5. 机器翻译与数据集"></a>5. 机器翻译与数据集</h2><p>语言模型是自然语言处理的关键，而<em>机器翻译</em>是语言模型最成功的基准测试。因为机器翻译正是将输入序列转换成输出序列的<em>序列转换模型</em>（sequence transduction）的核心问题。序列转换模型在各类现代人工智能应用中发挥着至关重要的作用，因此将其做为本章剩余部分和下一章的重点。本节将介绍机器翻译问题及其后文需要使用的数据集。</p><p><em>机器翻译</em>（machine translation）指的是将序列从一种语言自动翻译成另一种语言。这个研究领域可以追溯到数字计算机发明后不久的20世纪40年代，特别是在第二次世界大战中使用计算机破解语言编码。几十年来，在使用神经网络进行端到端学习的兴起之前，统计学方法在这一领域一直占据主导地位。因为<em>统计机器翻译</em>（statisticalmachine translation）涉及了翻译模型和语言模型等组成部分的统计分析，因此基于神经网络的方法通常被称为<em>神经机器翻译</em>（neuralmachine translation），用于将两种翻译模型区分开来。</p><p>本书的关注点是神经网络机器翻译方法，强调的是端到端的学习。与上一章中语料库是单一语言的语言模型问题存在不同，机器翻译的数据集是由源语言和目标语言的文本序列对组成的。因此需要一种完全不同的方法来预处理机器翻译数据集，而不是复用语言模型的预处理程序。下面看一下如何将预处理后的数据加载到小批量中用于训练。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><h3 id="5-1-下载和预处理数据集"><a href="#5-1-下载和预处理数据集" class="headerlink" title="5.1. 下载和预处理数据集"></a>5.1. 下载和预处理数据集</h3><p>首先，下载一个由<a href="http://www.manythings.org/anki/">Tatoeba项目的双语句子对</a>组成的“英－法”数据集，数据集中的每一行都是制表符分隔的文本序列对，序列对由英文文本序列和翻译后的法语文本序列组成。请注意，每个文本序列可以是一个句子，也可以是包含多个句子的一个段落。在这个将英语翻译成法语的机器翻译问题中，英语是<em>源语言</em>（source language），法语是<em>目标语言</em>（target language）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">d2l.DATA_HUB[<span class="string">&#x27;fra-eng&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;fra-eng.zip&#x27;</span>,</span><br><span class="line">                           <span class="string">&#x27;94646ad1522d915e7b0f9296181140edcf86a4f5&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_data_nmt</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;载入“英语－法语”数据集&quot;&quot;&quot;</span></span><br><span class="line">    data_dir = d2l.download_extract(<span class="string">&#x27;fra-eng&#x27;</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(data_dir, <span class="string">&#x27;fra.txt&#x27;</span>), <span class="string">&#x27;r&#x27;</span>,</span><br><span class="line">             encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">return</span> f.read()</span><br><span class="line"></span><br><span class="line">raw_text = read_data_nmt()</span><br><span class="line"><span class="built_in">print</span>(raw_text[:<span class="number">75</span>])</span><br></pre></td></tr></table></figure><p>下载数据集后，原始文本数据需要经过几个预处理步骤。例如:用空格代替<em>不间断空格</em>（non-breaking space），使用小写字母替换大写字母，并在单词和标点符号之间插入空格。</p><p>关于空格的种类（带u的是unicode编码）：<br>1、半角空格：<code>\0x20</code>，占位符为一个半角字符，日常英文数学和代码编写使用。<br>2、全角空格：<code>\u3000</code>，中文输入空格，两个半角空格。<br>3、不间断空格：<code>\u00A0</code>或<code>\xa0</code>，在word、html等中大量使用。<br>4、零宽度空格：<code>\u200B</code>，不可见非打印字符，可以替换html中的<code>&lt;wbr/&gt;</code>标签。<br>5、零宽度非中断空格：<code>\u2060</code>，结合了 non-breaking space 和 零宽度空格的特点。既会自动换行，宽度又是0。<br>6、还有一些html宽度度量下的其他空格字符，如<code>\u202f</code>就是一种窄不间断空格，不同语种中不太一样。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_nmt</span>(<span class="params">text</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;预处理“英语－法语”数据集&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">no_space</span>(<span class="params">char, prev_char</span>):</span></span><br><span class="line">        <span class="keyword">return</span> char <span class="keyword">in</span> <span class="built_in">set</span>(<span class="string">&#x27;,.!?&#x27;</span>) <span class="keyword">and</span> prev_char != <span class="string">&#x27; &#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用空格替换不间断空格</span></span><br><span class="line">    <span class="comment"># 使用小写字母替换大写字母</span></span><br><span class="line">    text = text.replace(<span class="string">&#x27;\u202f&#x27;</span>, <span class="string">&#x27; &#x27;</span>).replace(<span class="string">&#x27;\xa0&#x27;</span>, <span class="string">&#x27; &#x27;</span>).lower()</span><br><span class="line">    <span class="comment"># 在单词和标点符号之间插入空格</span></span><br><span class="line">    out = [<span class="string">&#x27; &#x27;</span> + char <span class="keyword">if</span> i &gt; <span class="number">0</span> <span class="keyword">and</span> no_space(char, text[i - <span class="number">1</span>]) <span class="keyword">else</span> char</span><br><span class="line">           <span class="keyword">for</span> i, char <span class="keyword">in</span> <span class="built_in">enumerate</span>(text)]</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;&#x27;</span>.join(out)</span><br><span class="line"></span><br><span class="line">text = preprocess_nmt(raw_text)</span><br><span class="line"><span class="built_in">print</span>(text[:<span class="number">80</span>])</span><br></pre></td></tr></table></figure><h3 id="5-2-词元化"><a href="#5-2-词元化" class="headerlink" title="5.2. 词元化"></a>5.2. 词元化</h3><p>与上一章中的字符级词元化不同，在机器翻译中，更喜欢做单词级词元化（最先进的模型可能使用更高级的词元化技术）。下面的<code>tokenize_nmt</code>函数对前<code>num_examples</code>个文本序列对进行词元，其中每个词元要么是一个词，要么是一个标点符号。此函数返回两个词元列表：<code>source</code>和<code>target</code>：<code>source[i]</code>是源语言（这里是英语）第$i$个文本序列的词元列表，<code>target[i]</code>是目标语言（这里是法语）第$i$个文本序列的词元列表。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize_nmt</span>(<span class="params">text, num_examples=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;词元化“英语－法语”数据数据集&quot;&quot;&quot;</span></span><br><span class="line">    source, target = [], []</span><br><span class="line">    <span class="keyword">for</span> i, line <span class="keyword">in</span> <span class="built_in">enumerate</span>(text.split(<span class="string">&#x27;\n&#x27;</span>)):</span><br><span class="line">        <span class="keyword">if</span> num_examples <span class="keyword">and</span> i &gt; num_examples:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        parts = line.split(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(parts) == <span class="number">2</span>:</span><br><span class="line">            source.append(parts[<span class="number">0</span>].split(<span class="string">&#x27; &#x27;</span>))</span><br><span class="line">            target.append(parts[<span class="number">1</span>].split(<span class="string">&#x27; &#x27;</span>))</span><br><span class="line">    <span class="keyword">return</span> source, target</span><br><span class="line"></span><br><span class="line">source, target = tokenize_nmt(text)</span><br></pre></td></tr></table></figure><p>绘制每个文本序列所包含的词元数量的直方图。在这个简单的“英－法”数据集中，大多数文本序列的词元数量少于$20$个。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_list_len_pair_hist</span>(<span class="params">legend, xlabel, ylabel, xlist, ylist</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;绘制列表长度对的直方图&quot;&quot;&quot;</span></span><br><span class="line">    d2l.set_figsize()</span><br><span class="line">    _, _, patches = d2l.plt.hist(</span><br><span class="line">        [[<span class="built_in">len</span>(l) <span class="keyword">for</span> l <span class="keyword">in</span> xlist], [<span class="built_in">len</span>(l) <span class="keyword">for</span> l <span class="keyword">in</span> ylist]])</span><br><span class="line">    d2l.plt.xlabel(xlabel)</span><br><span class="line">    d2l.plt.ylabel(ylabel)</span><br><span class="line">    <span class="keyword">for</span> patch <span class="keyword">in</span> patches[<span class="number">1</span>].patches:</span><br><span class="line">        patch.set_hatch(<span class="string">&#x27;/&#x27;</span>)</span><br><span class="line">    d2l.plt.legend(legend)</span><br><span class="line"></span><br><span class="line">show_list_len_pair_hist([<span class="string">&#x27;source&#x27;</span>, <span class="string">&#x27;target&#x27;</span>], <span class="string">&#x27;# tokens per sequence&#x27;</span>,</span><br><span class="line">                        <span class="string">&#x27;count&#x27;</span>, source, target);</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article58/output_machine-translation-and-dataset_887557_54_0.svg" alt="tokens per sequence"></p><h3 id="5-3-词表"><a href="#5-3-词表" class="headerlink" title="5.3. 词表"></a>5.3. 词表</h3><p>由于机器翻译数据集由语言对组成，则可以分别为源语言和目标语言构建两个词表。使用单词级词元化时，词表大小将明显大于使用字符级词元化时的词表大小。为了缓解这一问题，这里将出现次数少于2次的低频率词元视为相同的未知（“&lt;unk&gt;”）词元。除此之外，还指定了额外的特定词元，例如在小批量时用于将序列填充到相同长度的填充词元（“&lt;pad&gt;”），以及序列的开始词元（“&lt;bos&gt;”）和结束词元（“&lt;eos&gt;”）。这些特殊词元在自然语言处理任务中比较常用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">src_vocab = d2l.Vocab(source, min_freq=<span class="number">2</span>,</span><br><span class="line">                      reserved_tokens=[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>, <span class="string">&#x27;&lt;bos&gt;&#x27;</span>, <span class="string">&#x27;&lt;eos&gt;&#x27;</span>])</span><br><span class="line"><span class="built_in">len</span>(src_vocab)</span><br></pre></td></tr></table></figure><h3 id="5-4-加载数据集"><a href="#5-4-加载数据集" class="headerlink" title="5.4. 加载数据集"></a>5.4. 加载数据集</h3><p>语言模型中的序列样本都有一个固定的长度，无论这个样本是一个句子的一部分还是跨越了多个句子的一个片断。这个固定长度是上一章中第3节中的<code>num_steps</code>（时间步数或词元数量）参数指定的。在机器翻译中，每个样本都是由源和目标组成的文本序列对，其中的每个文本序列可能具有不同的长度。</p><p>为了提高计算效率，我们仍然可以通过<em>截断</em>（truncation）和<em>填充</em>（padding）方式实现一次只处理一个小批量的文本序列。假设同一个小批量中的每个序列都应该具有相同的长度<code>num_steps</code>，那么如果文本序列的词元数目少于<code>num_steps</code>时，我们将继续在其末尾添加特定的“&lt;pad&gt;”词元，直到其长度达到<code>num_steps</code>；反之，我们将截断文本序列时，只取其前<code>num_steps</code> 个词元，并且丢弃剩余的词元。这样，每个文本序列将具有相同的长度，以便以相同形状的小批量进行加载。</p><p>下面的<code>truncate_pad</code>函数将截断或填充文本序列。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">truncate_pad</span>(<span class="params">line, num_steps, padding_token</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;截断或填充文本序列&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(line) &gt; num_steps:</span><br><span class="line">        <span class="keyword">return</span> line[:num_steps]  <span class="comment"># 截断</span></span><br><span class="line">    <span class="keyword">return</span> line + [padding_token] * (num_steps - <span class="built_in">len</span>(line))  <span class="comment"># 填充</span></span><br><span class="line"></span><br><span class="line">truncate_pad(src_vocab[source[<span class="number">0</span>]], <span class="number">10</span>, src_vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>])</span><br></pre></td></tr></table></figure><p>现在定义一个函数，可以将文本序列转换成小批量数据集用于训练。我们将特定的“&lt;eos&gt;”词元添加到所有序列的末尾，用于表示序列的结束。当模型通过一个词元接一个词元地生成序列进行预测时，生成的“&lt;eos&gt;”词元说明完成了序列输出工作。此外，我们还记录了每个文本序列的长度，统计长度时排除了填充词元，之后介绍的一些模型会需要这个长度信息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_array_nmt</span>(<span class="params">lines, vocab, num_steps</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;将机器翻译的文本序列转换成小批量&quot;&quot;&quot;</span></span><br><span class="line">    lines = [vocab[l] <span class="keyword">for</span> l <span class="keyword">in</span> lines]</span><br><span class="line">    lines = [l + [vocab[<span class="string">&#x27;&lt;eos&gt;&#x27;</span>]] <span class="keyword">for</span> l <span class="keyword">in</span> lines]</span><br><span class="line">    array = torch.tensor([truncate_pad(</span><br><span class="line">        l, num_steps, vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]) <span class="keyword">for</span> l <span class="keyword">in</span> lines])</span><br><span class="line">    valid_len = (array != vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]).<span class="built_in">type</span>(torch.int32).<span class="built_in">sum</span>(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> array, valid_len</span><br></pre></td></tr></table></figure><h3 id="5-5-训练模型"><a href="#5-5-训练模型" class="headerlink" title="5.5. 训练模型"></a>5.5. 训练模型</h3><p>最后定义<code>load_data_nmt</code>函数来返回数据迭代器，以及源语言和目标语言的两种词表。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_nmt</span>(<span class="params">batch_size, num_steps, num_examples=<span class="number">600</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回翻译数据集的迭代器和词表&quot;&quot;&quot;</span></span><br><span class="line">    text = preprocess_nmt(read_data_nmt())</span><br><span class="line">    source, target = tokenize_nmt(text, num_examples)</span><br><span class="line">    src_vocab = d2l.Vocab(source, min_freq=<span class="number">2</span>,</span><br><span class="line">                          reserved_tokens=[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>, <span class="string">&#x27;&lt;bos&gt;&#x27;</span>, <span class="string">&#x27;&lt;eos&gt;&#x27;</span>])</span><br><span class="line">    tgt_vocab = d2l.Vocab(target, min_freq=<span class="number">2</span>,</span><br><span class="line">                          reserved_tokens=[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>, <span class="string">&#x27;&lt;bos&gt;&#x27;</span>, <span class="string">&#x27;&lt;eos&gt;&#x27;</span>])</span><br><span class="line">    src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)</span><br><span class="line">    tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)</span><br><span class="line">    data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)</span><br><span class="line">    data_iter = d2l.load_array(data_arrays, batch_size)</span><br><span class="line">    <span class="keyword">return</span> data_iter, src_vocab, tgt_vocab</span><br></pre></td></tr></table></figure><p>下面读出“英语－法语”数据集中的第一个小批量数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">train_iter, src_vocab, tgt_vocab = load_data_nmt(batch_size=<span class="number">2</span>, num_steps=<span class="number">8</span>)</span><br><span class="line"><span class="keyword">for</span> X, X_valid_len, Y, Y_valid_len <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;X:&#x27;</span>, X.<span class="built_in">type</span>(torch.int32))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;X的有效长度:&#x27;</span>, X_valid_len)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Y:&#x27;</span>, Y.<span class="built_in">type</span>(torch.int32))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Y的有效长度:&#x27;</span>, Y_valid_len)</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">X: tensor([[ <span class="number">9</span>,  <span class="number">4</span>,  <span class="number">3</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>],</span><br><span class="line">        [<span class="number">87</span>, <span class="number">22</span>,  <span class="number">4</span>,  <span class="number">3</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>]], dtype=torch.int32)</span><br><span class="line">X的有效长度: tensor([<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">Y: tensor([[ <span class="number">16</span>,   <span class="number">5</span>,   <span class="number">3</span>,   <span class="number">1</span>,   <span class="number">1</span>,   <span class="number">1</span>,   <span class="number">1</span>,   <span class="number">1</span>],</span><br><span class="line">        [<span class="number">175</span>, <span class="number">176</span>,   <span class="number">4</span>,   <span class="number">3</span>,   <span class="number">1</span>,   <span class="number">1</span>,   <span class="number">1</span>,   <span class="number">1</span>]], dtype=torch.int32)</span><br><span class="line">Y的有效长度: tensor([<span class="number">3</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure><h2 id="6-编码器-解码器架构"><a href="#6-编码器-解码器架构" class="headerlink" title="6. 编码器-解码器架构"></a>6. 编码器-解码器架构</h2><p>机器翻译是序列转换模型的一个核心问题，其输入和输出都是长度可变的序列。为了处理这种类型的输入和输出，可以设计一个包含两个主要组件的架构：第一个组件是一个<em>编码器</em>（encoder）：它接受一个长度可变的序列作为输入，并将其转换为具有固定形状的编码状态。第二个组件是<em>解码器</em>（decoder）：它将固定形状的编码状态映射到长度可变的序列。这被称为<em>编码器-解码器</em>（encoder-decoder）架构，如下图所示。</p><p><img src="/assets/post_img/article58/encoder-decoder.svg" alt="编码器-解码器架构"></p><p>以英语到法语的机器翻译为例：给定一个英文的输入序列：“They”“are”“watching”“.”。首先，这种“编码器－解码器”架构将长度可变的输入序列编码成一个“状态”，然后对该状态进行解码，一个词元接着一个词元地生成翻译后的序列作为输出：“Ils”“regordent”“.”。由于“编码器－解码器”架构是形成后续章节中不同序列转换模型的基础，因此本节将把这个架构转换为接口方便后面的代码实现。</p><h3 id="6-1-编码器"><a href="#6-1-编码器" class="headerlink" title="6.1. 编码器"></a>6.1. 编码器</h3><p>在编码器接口中，我们只指定长度可变的序列作为编码器的输入<code>X</code>。<br>任何继承这个<code>Encoder</code>基类的模型将完成代码实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;编码器-解码器架构的基本编码器接口&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, *args</span>):</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure><p>这里解析一下<code>super(Encoder, self).__init__()</code>这个写法。self指的是实例（instance）本身，Python的类中规定：<strong>类中的方法的第一个参数一定要是self，而且不能省略。</strong> 所以构造函数也就是<code>__init__ ()</code>方法必须包含一个self参数，而且要是第一个参数。</p><p><code>super()</code>是Python的内置函数，用于调用父类。在Python3中我们通常使用<code>super().xxx</code>代替<code>super(Class, self).xxx</code>。<code>super(Encoder, self).__init__()</code>的工作原理是首先找到Encoder的父类nn.Module，然后把实例self转化为父类的对象，再去调用该实例的构造方法，实际上也就是调用了父类的构造方法。</p><h3 id="6-2-解码器"><a href="#6-2-解码器" class="headerlink" title="6.2. 解码器"></a>6.2. 解码器</h3><p>在下面的解码器接口中，新增一个<code>init_state</code>函数，用于将编码器的输出（<code>enc_outputs</code>）转换为编码后的状态。注意，此步骤可能需要额外的输入，例如：输入序列的有效长度。为了逐个地生成长度可变的词元序列，解码器在每个时间步都会将输入（例如：在前一时间步生成的词元）和编码后的状态映射成当前时间步的输出词元。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;编码器-解码器架构的基本解码器接口&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_state</span>(<span class="params">self, enc_outputs, *args</span>):</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, state</span>):</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure><h3 id="6-3-合并编码器和解码器"><a href="#6-3-合并编码器和解码器" class="headerlink" title="6.3. 合并编码器和解码器"></a>6.3. 合并编码器和解码器</h3><p>总而言之，“编码器-解码器”架构包含了一个编码器和一个解码器， 并且还拥有可选的额外的参数。 在前向传播中，编码器的输出用于生成编码状态， 这个状态又被解码器作为其输入的一部分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderDecoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;编码器-解码器架构的基类&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, encoder, decoder, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(EncoderDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, enc_X, dec_X, *args</span>):</span></span><br><span class="line">        enc_outputs = self.encoder(enc_X, *args)</span><br><span class="line">        dec_state = self.decoder.init_state(enc_outputs, *args)</span><br><span class="line">        <span class="keyword">return</span> self.decoder(dec_X, dec_state)</span><br></pre></td></tr></table></figure><p>“编码器－解码器”体系架构中的术语<em>状态</em>会启发人们使用具有状态的神经网络来实现该架构。下一节将学习如何应用循环神经网络，来设计基于“编码器－解码器”架构的序列转换模型。</p><h2 id="7-序列到序列学习（seq2seq）"><a href="#7-序列到序列学习（seq2seq）" class="headerlink" title="7. 序列到序列学习（seq2seq）"></a>7. 序列到序列学习（seq2seq）</h2><p>机器翻译中的输入序列和输出序列都是长度可变的。为了解决这类问题，我们设计了一个通用的”编码器－解码器“架构。本节将使用两个循环神经网络的编码器和解码器，并将其应用于<em>序列到序列</em>（sequence to sequence，seq2seq）类的学习任务[<code>Sutskever.Vinyals.Le.2014,Cho.Van-Merrienboer.Gulcehre.ea.2014</code>]。</p><p>遵循编码器－解码器架构的设计原则，循环神经网络编码器使用长度可变的序列作为输入，将其转换为固定形状的隐状态。换言之，输入序列的信息被<em>编码</em>到循环神经网络编码器的隐状态中。为了连续生成输出序列的词元，独立的循环神经网络解码器是基于输入序列的编码信息和输出序列已经看见的或者生成的词元来预测下一个词元。下图演示了如何在机器翻译中使用两个循环神经网络进行序列到序列学习。</p><p><img src="/assets/post_img/article58/seq2seq.svg" alt="使用循环神经网络编码器和循环神经网络解码器的序列到序列学习"></p><p>图中，特定的“&lt;eos&gt;”表示序列结束词元。一旦输出序列生成此词元，模型就会停止预测。在循环神经网络解码器的初始化时间步，有两个特殊的设计：第一，特定的“&lt;bos&gt;”表示序列开始词元，它是解码器的输入序列的第一个词元。第二，使用循环神经网络编码器最终的隐状态来初始化解码器的隐状态。例如，在[<code>Sutskever.Vinyals.Le.2014</code>]的设计中，正是基于这种设计将输入序列的编码信息送入到解码器中来生成输出序列的。在其他一些设计中[<code>Cho.Van-Merrienboer.Gulcehre.ea.2014</code>]，如上图所示，<em>编码器最终的隐状态在每一个时间步都作为解码器的输入序列的一部分</em>。类似于上一章中语言模型的训练，可以允许标签成为原始的输出序列，从源序列词元“&lt;bos&gt;”“Ils”“regardent”“.”到新序列词元“Ils”“regardent”“.”“&lt;eos&gt;”来移动预测的位置。</p><p>下面来动手构建以上的设计，并将基于“英－法”数据集来训练这个机器翻译模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><h3 id="7-1-编码器"><a href="#7-1-编码器" class="headerlink" title="7.1. 编码器"></a>7.1. 编码器</h3><p>从技术上讲，编码器将长度可变的输入序列转换成形状固定的上下文变量$\mathbf{c}$，并且将输入序列的信息在该上下文变量中进行编码。如前图所示，可以使用循环神经网络来设计编码器。</p><p>对于由一个序列组成的样本（批量大小是$1$）。假设输入序列是$x_1, \ldots, x_T$，其中$x_t$是输入文本序列中的第$t$个词元。在时间步$t$，循环神经网络将词元$x_t$的输入特征向量$\mathbf{x}_t$和$\mathbf{h} _{t-1}$（即上一时间步的隐状态）转换为$\mathbf{h}_t$（即当前步的隐状态）。使用一个函数$f$来描述循环神经网络的循环层所做的变换：</p><script type="math/tex; mode=display">\mathbf{h}_t = f(\mathbf{x}_t, \mathbf{h}_{t-1}).</script><p>而编码器通过选定的函数$q$，将所有时间步的隐状态转换为上下文变量：</p><script type="math/tex; mode=display">\mathbf{c} =  q(\mathbf{h}_1, \ldots, \mathbf{h}_T).</script><p>比如在上面的图中，指定$q(\mathbf{h}_1, \ldots, \mathbf{h}_T) = \mathbf{h}_T$后，上下文变量则仅是输入序列在最后时间步的隐状态$\mathbf{h}_T$。</p><p>目前为止，我们使用的是一个单向循环神经网络来设计编码器，其中隐状态只依赖于输入子序列，这个子序列是由输入序列的开始位置到隐状态所在的时间步的位置（包括隐状态所在的时间步）组成。当然也可以使用双向循环神经网络构造编码器，其中隐状态依赖于两个输入子序列，两个子序列是由隐状态所在的时间步的位置之前的序列 和 之后的序列（包括隐状态所在的时间步），因此隐状态对整个序列的信息都进行了编码。</p><p>现在来实现循环神经网络编码器。注意这里使用了<em>嵌入层</em>（embedding layer）来获得输入序列中每个词元的特征向量。嵌入层的权重是一个矩阵，其行数等于输入词表的大小（<code>vocab_size</code>），其列数等于特征向量的维度（<code>embed_size</code>）。对于任意输入词元的索引$i$，嵌入层获取权重矩阵的第$i$行（从$0$开始）以返回其特征向量。另外，本文选择了一个多层门控循环单元来实现编码器。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Seq2SeqEncoder</span>(<span class="params">Encoder</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;用于序列到序列学习的循环神经网络编码器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embed_size, num_hiddens, num_layers,</span></span></span><br><span class="line"><span class="params"><span class="function">                 dropout=<span class="number">0</span>, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Seq2SeqEncoder, self).__init__(**kwargs)</span><br><span class="line">        <span class="comment"># 嵌入层</span></span><br><span class="line">        <span class="comment"># Embedding的作用简单来说就是为单词编码，将单词编码成为向量。</span></span><br><span class="line">        <span class="comment"># 嵌入层比独热编码更节约空间，能方便运算。</span></span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        <span class="comment"># 嵌入层size等于特征向量维数作为input_size</span></span><br><span class="line">        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers,</span><br><span class="line">                          dropout=dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, *args</span>):</span></span><br><span class="line">        <span class="comment"># 输出&#x27;X&#x27;的形状：(batch_size,num_steps,embed_size)</span></span><br><span class="line">        <span class="comment"># batch_size,num_steps来自输入，经嵌入层后会增加一维，因为原本的元素（单词）被向量化。</span></span><br><span class="line">        X = self.embedding(X)</span><br><span class="line">        <span class="comment"># 在循环神经网络模型中，第一个轴对应于时间步</span></span><br><span class="line">        <span class="comment"># permute就是维度变换，其中0、1、2指第一维、第二维、第三维</span></span><br><span class="line">        <span class="comment"># 这里就是调换两个维度，把形状(batch_size,num_steps,embed_size)</span></span><br><span class="line">        <span class="comment"># 转换为(num_steps,batch_size,embed_size)</span></span><br><span class="line">        X = X.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 如果未提及状态，则默认为0</span></span><br><span class="line">        output, state = self.rnn(X)</span><br><span class="line">        <span class="comment"># output的形状:(num_steps,batch_size,num_hiddens)</span></span><br><span class="line">        <span class="comment"># state的形状:(num_layers,batch_size,num_hiddens)</span></span><br><span class="line">        <span class="keyword">return</span> output, state</span><br></pre></td></tr></table></figure><p>循环层返回变量的说明可以参考上一章“循环神经网络的框架实现”。</p><p>下面实例化上述编码器的实现：使用一个两层门控循环单元编码器，其隐藏单元数为$16$。给定一小批量的输入序列<code>X</code>（批量大小为$4$，时间步为$7$）。在完成所有时间步后，最后一层的隐状态的输出是一个张量（<code>output</code>由编码器的循环层返回），其形状为（时间步数，批量大小，隐藏单元数）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">encoder = Seq2SeqEncoder(vocab_size=<span class="number">10</span>, embed_size=<span class="number">8</span>, num_hiddens=<span class="number">16</span>,</span><br><span class="line">                         num_layers=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 评估模式，测试时一定要使用，对Dropout和BatchNorm等有作用。</span></span><br><span class="line">encoder.<span class="built_in">eval</span>()</span><br><span class="line">X = torch.zeros((<span class="number">4</span>, <span class="number">7</span>), dtype=torch.long)</span><br><span class="line">output, state = encoder(X)</span><br><span class="line">output.shape, state.shape</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">(torch.Size([<span class="number">7</span>, <span class="number">4</span>, <span class="number">16</span>]), torch.Size([<span class="number">2</span>, <span class="number">4</span>, <span class="number">16</span>]))</span><br></pre></td></tr></table></figure><p>由于这里使用的是门控循环单元，所以在最后一个时间步的多层隐状态的形状是 （隐藏层的数量，批量大小，隐藏单元的数量）。 如果使用长短期记忆网络，state中还将包含记忆单元信息。</p><h3 id="7-2-解码器"><a href="#7-2-解码器" class="headerlink" title="7.2. 解码器"></a>7.2. 解码器</h3><p>如上文所说，编码器输出的上下文变量$\mathbf{c}$对整个输入序列$x_1, \ldots, x_T$进行编码。来自训练数据集的输出序列$y_1, y_2, \ldots, y_{T’}$，对于每个时间步$t’$（与输入序列或编码器的时间步$t$不同），解码器输出$y_{t’}$的概率取决于先前的输出子序列$y_1, \ldots, y_{t’-1}$和上下文变量$\mathbf{c}$，即$P(y_{t’} \mid y_1, \ldots, y_{t’-1}, \mathbf{c})$。</p><p>为了在序列上模型化这种条件概率，可以使用另一个循环神经网络作为解码器。在输出序列上的任意时间步$t^\prime$，循环神经网络将来自上一时间步的输出$y_{t^\prime-1}$和上下文变量$\mathbf{c}$作为其输入，然后在当前时间步将它们和上一隐状态$\mathbf{s}_{t^\prime-1}$转换为隐状态$\mathbf{s}_{t^\prime}$。可以使用函数$g$来表示解码器的隐藏层的变换：</p><script type="math/tex; mode=display">\mathbf{s}_{t^\prime} = g(y_{t^\prime-1}, \mathbf{c}, \mathbf{s}_{t^\prime-1}).</script><p>在获得解码器的隐状态之后，我们可以使用输出层和softmax操作来计算在时间步$t^\prime$时输出$y_{t^\prime}$的条件概率分布$P(y_{t^\prime} \mid y_1, \ldots, y_{t^\prime-1}, \mathbf{c})$。</p><p>根据一开始给出的图，当实现解码器时，我们直接使用编码器最后一个时间步的隐状态来初始化解码器的隐状态。这就要求使用循环神经网络实现的编码器和解码器具有相同数量的层和隐藏单元（批量大小是一样的）。为了进一步包含经过编码的输入序列的信息，上下文变量在所有的时间步与解码器的输入进行拼接（concatenate）。为了预测输出词元的概率分布，在循环神经网络解码器的最后一层使用全连接层来变换隐状态。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Seq2SeqDecoder</span>(<span class="params">Decoder</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;用于序列到序列学习的循环神经网络解码器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embed_size, num_hiddens, num_layers,</span></span></span><br><span class="line"><span class="params"><span class="function">                 dropout=<span class="number">0</span>, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Seq2SeqDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers,</span><br><span class="line">                          dropout=dropout)</span><br><span class="line">        self.dense = nn.Linear(num_hiddens, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_state</span>(<span class="params">self, enc_outputs, *args</span>):</span></span><br><span class="line">        <span class="keyword">return</span> enc_outputs[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, state</span>):</span></span><br><span class="line">        <span class="comment"># 输出&#x27;X&#x27;的形状：(batch_size,num_steps,embed_size)</span></span><br><span class="line">        X = self.embedding(X).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 广播context，使其具有与X相同的num_steps</span></span><br><span class="line">        <span class="comment"># PyTorch中的repeat()函数可以对张量进行重复扩充。</span></span><br><span class="line">        <span class="comment"># 三个参数分别是：通道数的重复倍数，列的重复倍数，行的重复倍数。</span></span><br><span class="line">        <span class="comment"># 这里表示第三维扩充batch_size倍，其他不变</span></span><br><span class="line">        context = state[-<span class="number">1</span>].repeat(X.shape[<span class="number">0</span>], <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        X_and_context = torch.cat((X, context), <span class="number">2</span>)</span><br><span class="line">        output, state = self.rnn(X_and_context, state)</span><br><span class="line">        output = self.dense(output).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># output的形状:(batch_size,num_steps,vocab_size)</span></span><br><span class="line">        <span class="comment"># state的形状:(num_layers,batch_size,num_hiddens)</span></span><br><span class="line">        <span class="keyword">return</span> output, state</span><br></pre></td></tr></table></figure><p>下面用与前面提到的编码器中相同的超参数来实例化解码器。解码器的输出形状变为（批量大小，时间步数，词表大小）， 其中张量的最后一个维度存储预测的词元分布。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">decoder = Seq2SeqDecoder(vocab_size=<span class="number">10</span>, embed_size=<span class="number">8</span>, num_hiddens=<span class="number">16</span>,</span><br><span class="line">                         num_layers=<span class="number">2</span>)</span><br><span class="line">decoder.<span class="built_in">eval</span>()</span><br><span class="line">state = decoder.init_state(encoder(X))</span><br><span class="line">output, state = decoder(X, state)</span><br><span class="line">output.shape, state.shape</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">(torch.Size([<span class="number">4</span>, <span class="number">7</span>, <span class="number">10</span>]), torch.Size([<span class="number">2</span>, <span class="number">4</span>, <span class="number">16</span>]))</span><br></pre></td></tr></table></figure><p>上述循环神经网络“编码器－解码器”模型中的各层如下图所示：</p><p><img src="/assets/post_img/article58/seq2seq-details.svg" alt="循环神经网络编码器-解码器模型中的层"></p><h3 id="7-3-损失函数"><a href="#7-3-损失函数" class="headerlink" title="7.3. 损失函数"></a>7.3. 损失函数</h3><p>在每个时间步，解码器预测了输出词元的概率分布。类似于语言模型，可以使用softmax来获得分布，并通过计算交叉熵损失函数来进行优化。<a href="#5-机器翻译与数据集">第五节</a>中，特定的填充词元被添加到序列的末尾，因此不同长度的序列可以以相同形状的小批量加载。但是，我们应该将填充词元的预测排除在损失函数的计算之外。</p><p>为此可以使用下面的<code>sequence_mask</code>函数通过零值化屏蔽不相关的项，以便后面任何不相关预测的计算都是与零的乘积，结果都等于零。例如，如果两个序列的有效长度（不包括填充词元）分别为$1$和$2$，则第一个序列的第一项和第二个序列的前两项之后的剩余项将被清除为零。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sequence_mask</span>(<span class="params">X, valid_len, value=<span class="number">0</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;在序列中屏蔽不相关的项&quot;&quot;&quot;</span></span><br><span class="line">    maxlen = X.size(<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 在[]中加入None表示维度扩充，第二维用:表示在第二维放入全部元素</span></span><br><span class="line">    <span class="comment"># 例如原本的张量为：tensor([0., 1., 2.])</span></span><br><span class="line">    <span class="comment"># 则tensor([0., 1., 2.])[None, :] = tensor([[0., 1., 2.]])</span></span><br><span class="line">    <span class="comment"># 即第一维扩充了，第二维放入原本张量的所有元素</span></span><br><span class="line">    <span class="comment"># [:, None]则会使torch.tensor([1, 2]) 变为 tensor([[1], [2]])</span></span><br><span class="line">    <span class="comment"># 后面的 &lt; 比较符号应用广播机制，使得mask数组变为布尔值</span></span><br><span class="line">    mask = torch.arange((maxlen), dtype=torch.float32,</span><br><span class="line">                        device=X.device)[<span class="literal">None</span>, :] &lt; valid_len[:, <span class="literal">None</span>]</span><br><span class="line">    <span class="comment"># ～表示按位取反，对于布尔值就是T变F，F变T</span></span><br><span class="line">    <span class="comment"># 在一个张量的索引中放入另一个布尔值张量，会进行按位比对，将True对应的元素提取出来，对这些元素所做的更改会体现在原张量中。</span></span><br><span class="line">    X[~mask] = value</span><br><span class="line">    <span class="keyword">return</span> X</span><br><span class="line"></span><br><span class="line">X = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">sequence_mask(X, torch.tensor([<span class="number">1</span>, <span class="number">2</span>]))</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>, <span class="number">0</span>]])</span><br></pre></td></tr></table></figure><p>可以使用此函数屏蔽最后几个轴上的所有项。也可以通过指定<code>value</code>参数使用非零值来替换这些项。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">X = torch.ones(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">sequence_mask(X, torch.tensor([<span class="number">1</span>, <span class="number">2</span>]), value=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">输出:</span><br><span class="line">tensor([[[ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">         [-<span class="number">1.</span>, -<span class="number">1.</span>, -<span class="number">1.</span>, -<span class="number">1.</span>],</span><br><span class="line">         [-<span class="number">1.</span>, -<span class="number">1.</span>, -<span class="number">1.</span>, -<span class="number">1.</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">         [ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">         [-<span class="number">1.</span>, -<span class="number">1.</span>, -<span class="number">1.</span>, -<span class="number">1.</span>]]])</span><br></pre></td></tr></table></figure><p>现在可以通过扩展softmax交叉熵损失函数来遮蔽不相关的预测。最初，所有预测词元的掩码都设置为1。一旦给定了有效长度，与填充词元对应的掩码将被设置为0。最后，将所有词元的损失乘以掩码，以过滤掉损失中填充词元产生的不相关预测。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MaskedSoftmaxCELoss</span>(<span class="params">nn.CrossEntropyLoss</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;带屏蔽的softmax交叉熵损失函数&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># pred的形状：(batch_size,num_steps,vocab_size)</span></span><br><span class="line">    <span class="comment"># label的形状：(batch_size,num_steps)</span></span><br><span class="line">    <span class="comment"># valid_len的形状：(batch_size,)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, pred, label, valid_len</span>):</span></span><br><span class="line">        weights = torch.ones_like(label)</span><br><span class="line">        weights = sequence_mask(weights, valid_len)</span><br><span class="line">        self.reduction=<span class="string">&#x27;none&#x27;</span></span><br><span class="line">        <span class="comment"># 原本的交叉熵</span></span><br><span class="line">        unweighted_loss = <span class="built_in">super</span>(MaskedSoftmaxCELoss, self).forward(</span><br><span class="line">            pred.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>), label)</span><br><span class="line">        <span class="comment"># 除去屏蔽掉的填充词元</span></span><br><span class="line">        weighted_loss = (unweighted_loss * weights).mean(dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> weighted_loss</span><br></pre></td></tr></table></figure><p>可以创建三个相同的序列来进行代码健全性检查，然后分别指定这些序列的有效长度为$4$、$2$和$0$。结果就是，第一个序列的损失应为第二个序列的两倍，而第三个序列的损失应为零。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">loss = MaskedSoftmaxCELoss()</span><br><span class="line">loss(torch.ones(<span class="number">3</span>, <span class="number">4</span>, <span class="number">10</span>), torch.ones((<span class="number">3</span>, <span class="number">4</span>), dtype=torch.long),</span><br><span class="line">     torch.tensor([<span class="number">4</span>, <span class="number">2</span>, <span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([<span class="number">2.3026</span>, <span class="number">1.1513</span>, <span class="number">0.0000</span>])</span><br></pre></td></tr></table></figure><h3 id="7-4-训练"><a href="#7-4-训练" class="headerlink" title="7.4. 训练"></a>7.4. 训练</h3><p>在下面的循环训练过程中，如<a href="#7-序列到序列学习seq2seq">前图</a>所示，特定的序列开始词元（“&lt;bos&gt;”）和原始的输出序列（不包括序列结束词元“&lt;eos&gt;”）拼接在一起作为解码器的输入。这被称为<em>强制教学</em>（teacher forcing），因为原始的输出序列（词元的标签）被送入解码器。或者，将来自上一个时间步的<em>预测</em>得到的词元作为解码器的当前输入。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_seq2seq</span>(<span class="params">net, data_iter, lr, num_epochs, tgt_vocab, device</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练序列到序列模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">xavier_init_weights</span>(<span class="params">m</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">            nn.init.xavier_uniform_(m.weight)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.GRU:</span><br><span class="line">            <span class="keyword">for</span> param <span class="keyword">in</span> m._flat_weights_names:</span><br><span class="line">                <span class="keyword">if</span> <span class="string">&quot;weight&quot;</span> <span class="keyword">in</span> param:</span><br><span class="line">                    nn.init.xavier_uniform_(m._parameters[param])</span><br><span class="line"></span><br><span class="line">    net.apply(xavier_init_weights)</span><br><span class="line">    net.to(device)</span><br><span class="line">    optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">    loss = MaskedSoftmaxCELoss()</span><br><span class="line">    net.train()</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>,</span><br><span class="line">                     xlim=[<span class="number">10</span>, num_epochs])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        timer = d2l.Timer()</span><br><span class="line">        metric = d2l.Accumulator(<span class="number">2</span>)  <span class="comment"># 训练损失总和，词元数量</span></span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> data_iter:</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            X, X_valid_len, Y, Y_valid_len = [x.to(device) <span class="keyword">for</span> x <span class="keyword">in</span> batch]</span><br><span class="line">            bos = torch.tensor([tgt_vocab[<span class="string">&#x27;&lt;bos&gt;&#x27;</span>]] * Y.shape[<span class="number">0</span>],</span><br><span class="line">                          device=device).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            dec_input = torch.cat([bos, Y[:, :-<span class="number">1</span>]], <span class="number">1</span>)  <span class="comment"># 强制教学</span></span><br><span class="line">            Y_hat, _ = net(X, dec_input, X_valid_len)</span><br><span class="line">            l = loss(Y_hat, Y, Y_valid_len)</span><br><span class="line">            l.<span class="built_in">sum</span>().backward()      <span class="comment"># 损失函数的标量进行“反向传播”</span></span><br><span class="line">            d2l.grad_clipping(net, <span class="number">1</span>) <span class="comment"># 梯度裁剪</span></span><br><span class="line">            num_tokens = Y_valid_len.<span class="built_in">sum</span>()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                metric.add(l.<span class="built_in">sum</span>(), num_tokens)</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>, (metric[<span class="number">0</span>] / metric[<span class="number">1</span>],))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;loss <span class="subst">&#123;metric[<span class="number">0</span>] / metric[<span class="number">1</span>]:<span class="number">.3</span>f&#125;</span>, <span class="subst">&#123;metric[<span class="number">1</span>] / timer.stop():<span class="number">.1</span>f&#125;</span> &#x27;</span></span><br><span class="line">        <span class="string">f&#x27;tokens/sec on <span class="subst">&#123;<span class="built_in">str</span>(device)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>现在可以在 机器翻译数据集 上创建和训练一个循环神经网络“编码器－解码器”模型用于序列到序列的学习。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">embed_size, num_hiddens, num_layers, dropout = <span class="number">32</span>, <span class="number">32</span>, <span class="number">2</span>, <span class="number">0.1</span></span><br><span class="line">batch_size, num_steps = <span class="number">64</span>, <span class="number">10</span></span><br><span class="line">lr, num_epochs, device = <span class="number">0.005</span>, <span class="number">300</span>, d2l.try_gpu()</span><br><span class="line"></span><br><span class="line">train_iter, src_vocab, tgt_vocab = load_data_nmt(batch_size, num_steps)</span><br><span class="line">encoder = Seq2SeqEncoder(<span class="built_in">len</span>(src_vocab), embed_size, num_hiddens, num_layers,</span><br><span class="line">                        dropout)</span><br><span class="line">decoder = Seq2SeqDecoder(<span class="built_in">len</span>(tgt_vocab), embed_size, num_hiddens, num_layers,</span><br><span class="line">                        dropout)</span><br><span class="line">net = EncoderDecoder(encoder, decoder)</span><br><span class="line">train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)</span><br></pre></td></tr></table></figure><h3 id="7-5-预测"><a href="#7-5-预测" class="headerlink" title="7.5. 预测"></a>7.5. 预测</h3><p>为了采用一个接着一个词元的方式预测输出序列，每个解码器当前时间步的输入都将来自于前一时间步的预测词元。与训练类似，序列开始词元（“&lt;bos&gt;”）在初始时间步被输入到解码器中。该预测过程如下图所示，当输出序列的预测遇到序列结束词元（“&lt;eos&gt;”）时，预测就结束了。</p><p><img src="/assets/post_img/article58/seq2seq-predict.svg" alt="使用循环神经网络编码器-解码器逐词元地预测输出序列。"></p><p>下一节中将介绍不同的序列生成策略。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_seq2seq</span>(<span class="params">net, src_sentence, src_vocab, tgt_vocab, num_steps,</span></span></span><br><span class="line"><span class="params"><span class="function">                    device, save_attention_weights=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;序列到序列模型的预测&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 在预测时将net设置为评估模式</span></span><br><span class="line">    net.<span class="built_in">eval</span>()</span><br><span class="line">    src_tokens = src_vocab[src_sentence.lower().split(<span class="string">&#x27; &#x27;</span>)] + [src_vocab[<span class="string">&#x27;&lt;eos&gt;&#x27;</span>]]</span><br><span class="line">    enc_valid_len = torch.tensor([<span class="built_in">len</span>(src_tokens)], device=device)</span><br><span class="line">    src_tokens = truncate_pad(src_tokens, num_steps, src_vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>])</span><br><span class="line">    <span class="comment"># 添加批量轴</span></span><br><span class="line">    <span class="comment"># unsqueeze()函数起升维的作用,参数表示在哪个地方加一个维度。</span></span><br><span class="line">    enc_X = torch.unsqueeze(torch.tensor(src_tokens, dtype=torch.long, device=device), dim=<span class="number">0</span>)</span><br><span class="line">    enc_outputs = net.encoder(enc_X, enc_valid_len)</span><br><span class="line">    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)</span><br><span class="line">    <span class="comment"># 添加批量轴</span></span><br><span class="line">    dec_X = torch.unsqueeze(torch.tensor([tgt_vocab[<span class="string">&#x27;&lt;bos&gt;&#x27;</span>]], dtype=torch.long, device=device), dim=<span class="number">0</span>)</span><br><span class="line">    output_seq, attention_weight_seq = [], []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_steps):</span><br><span class="line">        Y, dec_state = net.decoder(dec_X, dec_state)</span><br><span class="line">        <span class="comment"># 使用具有预测最高可能性的词元，作为解码器在下一时间步的输入</span></span><br><span class="line">        dec_X = Y.argmax(dim=<span class="number">2</span>)</span><br><span class="line">        pred = dec_X.squeeze(dim=<span class="number">0</span>).<span class="built_in">type</span>(torch.int32).item()</span><br><span class="line">        <span class="comment"># 保存注意力权重（稍后讨论）</span></span><br><span class="line">        <span class="keyword">if</span> save_attention_weights:</span><br><span class="line">            attention_weight_seq.append(net.decoder.attention_weights)</span><br><span class="line">        <span class="comment"># 一旦序列结束词元被预测，输出序列的生成就完成了</span></span><br><span class="line">        <span class="keyword">if</span> pred == tgt_vocab[<span class="string">&#x27;&lt;eos&gt;&#x27;</span>]:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        output_seq.append(pred)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27; &#x27;</span>.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq</span><br></pre></td></tr></table></figure><h3 id="7-6-预测序列的评估"><a href="#7-6-预测序列的评估" class="headerlink" title="7.6. 预测序列的评估"></a>7.6. 预测序列的评估</h3><p>我们可以通过与真实的标签序列进行比较来评估预测序列。虽然[<code>Papineni.Roukos.Ward.ea.2002</code>]提出的BLEU（bilingual evaluation understudy）最先是用于评估机器翻译的结果，但现在它已经被广泛用于测量许多应用的输出序列的质量。原则上说，对于预测序列中的任意$n$元语法（n-grams），BLEU的评估都是这个$n$元语法是否出现在标签序列中。</p><p>我们将BLEU定义为：</p><script type="math/tex; mode=display">\exp\left(\min\left(0, 1 - \frac{\mathrm{len}_{\text{label}}}{\mathrm{len}_{\text{pred}}}\right)\right) \prod_{n=1}^k p_n^{1/2^n},</script><p>其中$\mathrm{len}_{\text{label}}$表示标签序列中的词元数和$\mathrm{len}_{\text{pred}}$表示预测序列中的词元数，$k$是用于匹配的最长的$n$元语法。另外，用$p_n$表示$n$元语法的精确度，它是两个数量的比值：第一个是预测序列与标签序列中匹配的$n$元语法的数量，第二个是预测序列中$n$元语法的数量的比率。具体地说，给定标签序列$A$、$B$、$C$、$D$、$E$、$F$和预测序列$A$、$B$、$B$、$C$、$D$，我们有$p_1 = 4/5$、$p_2 = 3/4$、$p_3 = 1/3$和$p_4 = 0$。</p><p>这里解释一下，先说$p_1$，首先明确一点，序列是有方向有顺序的，我们把序列都从左向右看，那么预测序列中的1元语法分别为：$P(A)$、$P(B)$、$P(B)$、$P(C)$、$P(D)$，标签序列同理。可知预测标签与标签序列中匹配的1元语法数量为4，分别为：$P(A)$、$P(B)$、$P(C)$、$P(D)$，注意这里是一一对应，所以要去重。而预测序列中1元语法的数量为5，故有$p_1 = 4/5$。对于$p_2$，预测序列中的2元语法分别为：$P(B \mid A)$、$P(B \mid B)$、$P(C \mid B)$、$P(D \mid C)$，后面同理。</p><p>根据上述BLEU的定义，当预测序列与标签序列完全相同时，BLEU为$1$。此外，由于$n$元语法越长则匹配难度越大，所以BLEU为更长的$n$元语法的精确度分配更大的权重。具体来说，当$p_n$固定时，$p_n^{1/2^n}$会随着$n$的增长而增加（原始论文使用$p_n^{1/n}$）。而且由于预测的序列越短获得的$p_n$值越高，所以上式中乘法项之前的系数 $\exp\left(\min\left(0, 1 - \frac{\mathrm{len}_{\text{label}}}{\mathrm{len}_{\text{pred}}}\right)\right)$ 用于惩罚较短的预测序列。例如，当$k=2$时，给定标签序列$A$、$B$、$C$、$D$、$E$、$F$和预测序列$A$、$B$，尽管$p_1 = p_2 = 1$，惩罚因子$\exp(1-6/2) \approx 0.14$会降低BLEU。BLEU的代码实现如下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bleu</span>(<span class="params">pred_seq, label_seq, k</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算BLEU&quot;&quot;&quot;</span></span><br><span class="line">    pred_tokens, label_tokens = pred_seq.split(<span class="string">&#x27; &#x27;</span>), label_seq.split(<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">    len_pred, len_label = <span class="built_in">len</span>(pred_tokens), <span class="built_in">len</span>(label_tokens)</span><br><span class="line">    score = math.exp(<span class="built_in">min</span>(<span class="number">0</span>, <span class="number">1</span> - len_label / len_pred))</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, k + <span class="number">1</span>):</span><br><span class="line">        num_matches, label_subs = <span class="number">0</span>, collections.defaultdict(<span class="built_in">int</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(len_label - n + <span class="number">1</span>):</span><br><span class="line">            label_subs[<span class="string">&#x27; &#x27;</span>.join(label_tokens[i: i + n])] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(len_pred - n + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> label_subs[<span class="string">&#x27; &#x27;</span>.join(pred_tokens[i: i + n])] &gt; <span class="number">0</span>:</span><br><span class="line">                num_matches += <span class="number">1</span></span><br><span class="line">                label_subs[<span class="string">&#x27; &#x27;</span>.join(pred_tokens[i: i + n])] -= <span class="number">1</span></span><br><span class="line">        score *= math.<span class="built_in">pow</span>(num_matches / (len_pred - n + <span class="number">1</span>), math.<span class="built_in">pow</span>(<span class="number">0.5</span>, n))</span><br><span class="line">    <span class="keyword">return</span> score</span><br></pre></td></tr></table></figure><p>最后，利用训练好的循环神经网络“编码器－解码器”模型， 将几个英语句子翻译成法语，并计算BLEU的最终结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">engs = [<span class="string">&#x27;go .&#x27;</span>, <span class="string">&quot;i lost .&quot;</span>, <span class="string">&#x27;he\&#x27;s calm .&#x27;</span>, <span class="string">&#x27;i\&#x27;m home .&#x27;</span>]</span><br><span class="line">fras = [<span class="string">&#x27;va !&#x27;</span>, <span class="string">&#x27;j\&#x27;ai perdu .&#x27;</span>, <span class="string">&#x27;il est calme .&#x27;</span>, <span class="string">&#x27;je suis chez moi .&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> eng, fra <span class="keyword">in</span> <span class="built_in">zip</span>(engs, fras):</span><br><span class="line">    translation, attention_weight_seq = predict_seq2seq(</span><br><span class="line">        net, eng, src_vocab, tgt_vocab, num_steps, device)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;eng&#125;</span> =&gt; <span class="subst">&#123;translation&#125;</span>, bleu <span class="subst">&#123;bleu(translation, fra, k=<span class="number">2</span>):<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">go . =&gt; va !, bleu <span class="number">1.000</span></span><br><span class="line">i lost . =&gt; j<span class="string">&#x27;ai perdu ?, bleu 0.687</span></span><br><span class="line"><span class="string">he&#x27;</span>s calm . =&gt; soyez &lt;unk&gt; !, bleu <span class="number">0.000</span></span><br><span class="line">i<span class="string">&#x27;m home . =&gt; je suis chez moi qui l&#x27;</span>ai vu ., bleu <span class="number">0.640</span></span><br></pre></td></tr></table></figure><p><strong>第一次阅读，只对我注意到的细节部分做一些解释，实际上在更大的层面上，我完全是一知半解，甚至一窍不通，由于没有决定是否选择NLP方向，所以并没有追求一次性完全弄懂训练、预测过程中的所有细节，留待以后回看吧。 — SilenceZheng于22.09.07</strong></p><h2 id="8-束搜索"><a href="#8-束搜索" class="headerlink" title="8. 束搜索"></a>8. 束搜索</h2><p>上节中，我们逐个预测输出序列，直到预测序列中出现特定的序列结束词元“&lt;eos&gt;”。本节将首先介绍<em>贪心搜索</em>（greedy search）策略，并探讨其存在的问题，然后对比其他替代策略：<em>穷举搜索</em>（exhaustive search）和<em>束搜索</em>（beam search）。</p><p>在正式介绍贪心搜索之前，使用与上节中相同的数学符号定义搜索问题。在任意时间步$t’$，解码器输出$y_{t’}$的概率取决于时间步$t’$之前的输出子序列$y_1, \ldots, y_{t’-1}$和对输入序列的信息进行编码得到的上下文变量$\mathbf{c}$。为了量化计算代价，用$\mathcal{Y}$表示输出词表，其中包含“&lt;eos&gt;”，所以这个词汇集合的基数$\left|\mathcal{Y}\right|$就是词表的大小。再将输出序列的最大词元数指定为$T’$。则我们的目标是从所有$\mathcal{O}(\left|\mathcal{Y}\right|^{T’})$个可能的输出序列中寻找理想的输出。这种计算方式略微高估了可能输出的数量，因为对于所有输出序列，在“&lt;eos&gt;”之后的部分将在实际输出中丢弃。但大体上这个数字反应了搜索空间的大小。</p><h3 id="8-1-贪心搜索"><a href="#8-1-贪心搜索" class="headerlink" title="8.1. 贪心搜索"></a>8.1. 贪心搜索</h3><p>首先看一个简单的策略：<em>贪心搜索</em>，该策略已用于<a href="#76-预测序列的评估">上节</a>的序列预测。对于输出序列的每一时间步$t’$，我们都将基于贪心搜索从$\mathcal{Y}$中找到具有最高条件概率的词元，即：</p><script type="math/tex; mode=display">y_{t'} = \operatorname*{argmax}_{y \in \mathcal{Y}} P(y \mid y_1, \ldots, y_{t'-1}, \mathbf{c})</script><p>一旦输出序列包含了“&lt;eos&gt;”或者达到其最大长度$T’$，则输出完成。</p><p><img src="/assets/post_img/article58/s2s-prob1.svg" alt="在每个时间步，贪心搜索选择具有最高条件概率的词元"></p><p>如图，假设输出中有四个词元“A”“B”“C”和“&lt;eos&gt;”。每个时间步下的四个数字分别表示在该时间步生成“A”“B”“C”和“&lt;eos&gt;”的条件概率。在每个时间步，贪心搜索选择具有最高条件概率的词元。因此图中预测输出序列为“A”“B”“C”和“&lt;eos&gt;”。这个输出序列的条件概率是$0.5\times0.4\times0.4\times0.6 = 0.048$。</p><p>那么贪心搜索存在的问题是什么呢？现实中，<em>最优序列</em>（optimal sequence）应该是最大化$\prod_{t’=1}^{T’} P(y_{t’} \mid y_1, \ldots, y_{t’-1}, \mathbf{c})$值的输出序列，这是基于输入序列生成输出序列的条件概率。贪心搜索无法保证得到最优序列。</p><p><img src="/assets/post_img/article58/s2s-prob2.svg" alt="在时间步2，选择具有第二高条件概率的词元“C”（而非最高条件概率的词元）"></p><p>上图中的另一个例子阐述了这个问题。与第一种情况不同，在时间步$2$中，我们选择词元“C”，它具有<em>第二</em>高的条件概率。由于时间步$3$所基于的时间步$1$和$2$处的输出子序列已从 第一种情况中的“A”和“B”改变为上图中的“A”和“C”，因此时间步$3$处的每个词元的条件概率也在上图中改变。假设我们在时间步$3$选择词元“B”，于是当前的时间步$4$基于前三个时间步的输出子序列“A”“C”和“B”为条件，这与第一种情况中的“A”“B”和“C”不同。此时上图中的时间步$4$生成每个词元的条件概率也不同于第一种情况中的条件概率。结果，上图中的输出序列“A”“C”“B”和“&lt;eos&gt;”的条件概率为$0.5\times0.3 \times0.6\times0.6=0.054$，大于第一种情况中的贪心搜索的条件概率。这个例子说明：贪心搜索获得的输出序列“A”“B”“C”和“&lt;eos&gt;”不一定是最佳序列。</p><h3 id="8-2-穷举搜索"><a href="#8-2-穷举搜索" class="headerlink" title="8.2. 穷举搜索"></a>8.2. 穷举搜索</h3><p>如果目标是获得最优序列，可以考虑使用<em>穷举搜索</em>（exhaustive search）：穷举地列举所有可能的输出序列及其条件概率，然后计算输出条件概率最高的一个。</p><p>虽然我们可以使用穷举搜索来获得最优序列，但其计算量$\mathcal{O}(\left|\mathcal{Y}\right|^{T’})$高的惊人。例如，当$|\mathcal{Y}|=10000$和$T’=10$时，我们需要评估$10000^{10} = 10^{40}$序列，这是一个极大的数，现有的计算机几乎不可能计算它。然而贪心搜索的计算量$\mathcal{O}(\left|\mathcal{Y}\right|T’)$要显著地小于穷举搜索。例如，当$|\mathcal{Y}|=10000$和$T’=10$时，我们只需要评估$10000\times10=10^5$个序列。</p><h3 id="8-3-束搜索"><a href="#8-3-束搜索" class="headerlink" title="8.3. 束搜索"></a>8.3. 束搜索</h3><p>那么该选取哪种序列搜索策略呢？如果精度最重要，则显然是穷举搜索。如果计算成本最重要，则显然是贪心搜索。而束搜索的实际应用则介于这两个极端之间。</p><p><em>束搜索</em>（beam search）是贪心搜索的一个改进版本。它有一个超参数，名为<em>束宽</em>（beam size）$k$。在时间步$1$，我们选择具有最高条件概率的$k$个词元。这$k$个词元将分别是$k$个候选输出序列的第一个词元。在随后的每个时间步，基于上一时间步的$k$个候选输出序列，我们将继续从$k\left|\mathcal{Y}\right|$个可能的选择中挑出具有最高条件概率的$k$个候选输出序列。</p><p><img src="/assets/post_img/article58/beam-search.svg" alt="束搜索过程（束宽：2，输出序列的最大长度：3）。候选输出序列是$A$、$C$、$AB$、$CE$、$ABD$和$CED$"></p><p>上图演示了束搜索的过程。假设输出的词表只包含五个元素：$\mathcal{Y} = {A, B, C, D, E}$，其中有一个是“&lt;eos&gt;”。设置束宽为$2$，输出序列的最大长度为$3$。在时间步$1$，假设具有最高条件概率$P(y_1 \mid \mathbf{c})$的词元是$A$和$C$。在时间步$2$，我们计算所有$y_2 \in \mathcal{Y}$为：</p><script type="math/tex; mode=display">\begin{aligned}P(A, y_2 \mid \mathbf{c}) = P(A \mid \mathbf{c})P(y_2 \mid A, \mathbf{c}),\\ P(C, y_2 \mid \mathbf{c}) = P(C \mid \mathbf{c})P(y_2 \mid C, \mathbf{c}),\end{aligned}</script><p>从这十个值中选择最大的两个，比如$P(A, B \mid \mathbf{c})$和$P(C, E \mid \mathbf{c})$。然后在时间步$3$，我们计算所有$y_3 \in \mathcal{Y}$为：</p><script type="math/tex; mode=display">\begin{aligned}P(A, B, y_3 \mid \mathbf{c}) = P(A, B \mid \mathbf{c})P(y_3 \mid A, B, \mathbf{c}),\\P(C, E, y_3 \mid \mathbf{c}) = P(C, E \mid \mathbf{c})P(y_3 \mid C, E, \mathbf{c}),\end{aligned}</script><p>从这十个值中选择最大的两个，即$P(A, B, D \mid \mathbf{c})$和$P(C, E, D \mid  \mathbf{c})$，我们会得到六个候选输出序列：</p><p>（1）$A$；（2）$C$；（3）$A,B$；（4）$C,E$；（5）$A,B,D$；（6）$C,E,D$。</p><p>最后，基于这六个序列（例如，丢弃包括“&lt;eos&gt;”和之后的部分），我们获得最终候选输出序列集合。然后我们选择其中条件概率乘积最高的序列作为输出序列：</p><script type="math/tex; mode=display">\frac{1}{L^\alpha} \log P(y_1, \ldots, y_{L}\mid \mathbf{c}) = \frac{1}{L^\alpha} \sum_{t'=1}^L \log P(y_{t'} \mid y_1, \ldots, y_{t'-1}, \mathbf{c}),</script><p>其中$L$是最终候选序列的长度，$\alpha$通常设置为$0.75$。因为一个较长的序列在上式的求和中会有更多的对数项，因此分母中的$L^\alpha$用于惩罚长序列。</p><p>束搜索的计算量为$\mathcal{O}(k\left|\mathcal{Y}\right|T’)$，这个结果介于贪心搜索和穷举搜索之间。实际上，贪心搜索可以看作一种束宽为$1$的特殊类型的束搜索。通过灵活地选择束宽，束搜索可以在正确率和计算代价之间进行权衡。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;p&gt;前一章中介绍了循环神经网络的基础知识，这种网络可以更好地处理序列数据。但对于当今各种各样的序列学习问题，这些技术可能并不够用。&lt;/p&gt;
&lt;p&gt;例如，循环神经网络在实践中一个常见问题是数值不稳定性。尽管我们已经应用了梯度裁剪等技巧来缓解这个问题，但是仍需要通过设计更复杂的序列模型可以进一步处理它。比如两个广泛使用的网络：&lt;em&gt;门控循环单元&lt;/em&gt;（gated recurrent units，GRU）和&lt;em&gt;长短期记忆网络&lt;/em&gt;（long short-term memory，LSTM）。然后本章将基于一个单向隐藏层来扩展循环神经网络架构，描述具有多个隐藏层的深层架构，并讨论基于前向和后向循环计算的双向设计。现代循环网络经常采用这种扩展。在解释这些循环神经网络的变体时将继续利用上一章中的语言建模问题。&lt;br&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://silencezheng.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>循环神经网络--《动手学深度学习》笔记0x09</title>
    <link href="http://silencezheng.top/2022/08/31/article57/"/>
    <id>http://silencezheng.top/2022/08/31/article57/</id>
    <published>2022-08-30T17:40:50.000Z</published>
    <updated>2022-08-30T17:46:56.151Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><p>目前为止仅接触到两种类型的数据：表格数据和图像数据。 对于图像数据，可以设计专门的卷积神经网络架构来为这类特殊的数据结构建模。 对于一张图像，我们需要有效地利用其像素位置，假若对图像中的像素位置进行重排，就会对图像中内容的推断造成极大的困难。</p><p>最重要的是，到目前为止我们默认数据都来自于某种分布， 并且所有样本都是独立同分布的 （independently and identically distributed，i.i.d.）。 然而，大多数的数据并非如此。 例如，文章中的单词是按顺序写的，如果顺序被随机地重排，就很难理解文章原始的意思。 同样，视频中的图像帧、对话中的音频信号以及网站上的浏览行为都是有顺序的。 因此，针对此类数据而设计特定模型，可能效果会更好。<br><span id="more"></span><br>另一个问题是：我们不仅可以接收一个序列作为输入，而且还可能期望继续猜测这个序列的后续。 例如，一个任务可以是继续预测$2, 4, 6, 8, 10, \ldots$。 这在时间序列分析中是相当常见的，可以用来预测股市的波动、 患者的体温曲线或者赛车所需的加速度。 我们需要能够处理这些数据的特定模型。</p><p>简言之，如果说卷积神经网络可以有效地处理空间信息， 那么本章介绍的<em>循环神经网络</em>（recurrent neural network，RNN）则可以更好地处理序列信息。 循环神经网络通过引入状态变量存储过去的信息和当前的输入，从而可以确定当前的输出。</p><p>许多使用循环网络的例子都是基于文本数据的，因此本章重点介绍语言模型。 主要内容包括对序列数据的详细探讨，文本预处理的实用技术，语言模型的基本概念，循环神经网络的设计方法。 最后会解析循环神经网络的梯度计算方法，以探讨训练此类网络时可能遇到的问题。</p><p>对应实践：<a href="https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x09.ipynb">https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x09.ipynb</a></p><h3 id="0-1-结论"><a href="#0-1-结论" class="headerlink" title="0.1. 结论"></a>0.1. 结论</h3><ul><li>内插法（在现有观测值之间进行估计）和外推法（对超出已知观测范围进行预测）在实践的难度上差别很大。因此对于所拥有的序列数据，在训练时始终要尊重其时间顺序，即最好不要基于未来的数据进行训练。</li><li>序列模型的估计需要专门的统计工具，两种较流行的选择是自回归模型和隐变量自回归模型。</li><li>对于时间是向前推进的因果模型，正向估计通常比反向估计更容易。</li><li>对于直到时间步$t$的观测序列，其在时间步$t+k$的预测输出是“$k$步预测”。随着对预测时间$k$值的增加，会造成误差的快速累积和预测质量的极速下降。</li><li>文本是序列数据的一种最常见的形式之一。</li><li>为了对文本进行预处理，通常将文本拆分为词元，构建词表将词元字符串映射为数字索引，并将文本数据转换为词元索引以供模型操作。</li><li>语言模型是自然语言处理的关键。</li><li>$n$元语法通过截断相关性，为处理长序列提供了一种实用的模型。</li><li>长序列存在一个问题：很少出现或者从不出现。</li><li>齐普夫定律支配着单词的分布，这个分布不仅适用于一元语法，还适用于其他元语法。</li><li>通过拉普拉斯平滑法可以有效地处理结构丰富而频率不足的低频词词组。</li><li>读取长序列的主要方式是随机采样和顺序分区。在迭代过程中，后者可以保证来自两个相邻的小批量中的子序列在原始序列上也是相邻的。</li><li>对隐状态使用循环计算的神经网络称为循环神经网络（RNN）。</li><li>循环神经网络的隐状态可以捕获直到当前时间步序列的历史信息。</li><li>循环神经网络模型的参数数量不会随着时间步的增加而增加。</li><li>可以使用循环神经网络创建字符级语言模型。</li><li>可以使用困惑度来评价语言模型的质量。</li><li>可以训练一个基于循环神经网络的字符级语言模型，根据用户提供的文本的前缀生成后续文本。</li><li>一个简单的循环神经网络语言模型包括输入编码、循环神经网络模型和输出生成。</li><li>循环神经网络模型在训练以前需要初始化状态，不过随机抽样和顺序采样使用初始化方法不同。</li><li>当使用顺序采样时，我们需要分离梯度以减少计算量。</li><li>在进行任何预测之前，模型通过预热期进行自我更新（例如，获得比初始值更好的隐状态）。</li><li>梯度裁剪可以防止梯度爆炸，但不能应对梯度消失。</li><li>深度学习框架的高级API提供了循环神经网络层的实现。</li><li>高级API的循环神经网络层返回一个输出和一个更新后的隐状态，我们还需要计算整个模型的输出层。</li><li>相比从零实现的循环神经网络，使用高级API实现可以加速训练。</li><li>“通过时间反向传播”仅仅适用于反向传播在具有隐状态的序列模型。</li><li>截断是计算方便性和数值稳定性的需要。截断包括：规则截断和随机截断。</li><li>矩阵的高次幂可能导致神经网络特征值的发散或消失，将以梯度爆炸或梯度消失的形式表现。</li><li>为了计算的效率，“通过时间反向传播”在计算期间会缓存中间值。</li></ul><h2 id="1-序列模型"><a href="#1-序列模型" class="headerlink" title="1. 序列模型"></a>1. 序列模型</h2><p>想象一下有人正在看网飞（Netflix）上的电影。一名忠实的用户会对每一部电影都给出评价，毕竟一部好电影需要更多的支持和认可。然而事实上随着时间的推移，人们对电影的看法会发生很大的变化。心理学家对这些现象起了名字：</p><ul><li><em>锚定</em>（anchoring）效应：基于其他人的意见做出评价。例如，奥斯卡颁奖后，受到关注的电影的评分会上升，尽管它还是原来那部电影。这种影响将持续几个月，直到人们忘记了这部电影曾经获得的奖项。结果表明，这种效应会使评分提高半个百分点以上。</li><li><em>享乐适应</em>（hedonic adaption）：人们迅速接受并且适应一种更好或者更坏的情况作为新的常态。例如，在看了很多好电影之后，人们会强烈期望下部电影会更好。因此在看过许多精彩电影后，一部普通的电影也可能被认为是糟糕的。</li><li><em>季节性</em>（seasonality）：少有观众喜欢在八月看圣诞老人的电影。</li><li>有时，电影会由于导演或演员在制作中的不当行为变得不受欢迎。</li><li>有些电影因为其极度糟糕只能成为小众电影。</li></ul><p>简而言之，电影评分决不是固定不变的。因此，使用时间动力学可以得到更准确的电影推荐。当然，序列数据不仅仅是关于电影评分的。下面给出了更多的场景。</p><ul><li>在使用程序时，许多用户都有很强的特定习惯。例如，在学生放学后社交媒体应用更受欢迎。在市场开放时股市交易软件更常用。</li><li>预测明天的股价要比过去的股价更困难，尽管两者都只是估计一个数字。在统计学中，前者（对超出已知观测范围进行预测）称为<em>外推法</em>（extrapolation），而后者（在现有观测值之间进行估计）称为<em>内插法</em>（interpolation）。</li><li>在本质上，音乐、语音、文本和视频都是连续的。如果它们的序列被重排，那么就会失去原有的意义。比如，一个文本标题“狗咬人”远没有“人咬狗”那么令人惊讶，尽管组成两句话的字完全相同。</li><li>地震具有很强的相关性，即大地震发生后，很可能会有几次小余震，这些余震的强度比非大地震后的余震要大得多。事实上，地震是时空相关的，即余震通常发生在很短的时间跨度和很近的距离内。</li><li>人类之间的互动也是连续的，这可以从微博上的争吵和辩论中看出。</li></ul><h3 id="1-1-统计工具"><a href="#1-1-统计工具" class="headerlink" title="1.1. 统计工具"></a>1.1. 统计工具</h3><p>处理序列数据需要统计工具和新的深度神经网络架构。 为了简单起见，以下图所示的股票价格（富时100指数）为例。</p><p><img src="/assets/post_img/article57/数据分析.png" alt="近30年的富时100指数"></p><p>其中，用$x_t$表示价格，即在<em>时间步</em>（time step）$t \in \mathbb{Z}^+$时，观察到的价格$x_t$。注意$t$对于本文中的序列通常是离散的，并在整数或其子集上变化。假设一个交易员想在$t$日的股市中表现良好，于是通过以下途径预测$x_t$：</p><script type="math/tex; mode=display">x_t \sim P(x_t \mid x_{t-1}, \ldots, x_1)</script><h4 id="1-1-1-自回归模型"><a href="#1-1-1-自回归模型" class="headerlink" title="1.1.1. 自回归模型"></a>1.1.1. 自回归模型</h4><p>为了实现这个预测，交易员可以使用回归模型(例如最简单的线性回归）。这里仅有一个主要问题：输入数据的数量，输入$x_{t-1}, \ldots, x_1$本身因$t$而异。也就是说，输入数据的数量这个数字将会随着我们遇到的数据量的增加而增加，因此需要一个近似方法来使这个计算变得容易处理。本章后面的大部分内容将围绕着如何有效估计$P(x_t \mid x_{t-1}, \ldots, x_1)$展开。简单地说，它归结为以下两种策略。</p><p>第一种策略，假设在现实情况下相当长的序列$x_{t-1}, \ldots, x_1$可能是不必要的，则只需要满足某个长度为$\tau$的时间跨度，即使用观测序列$x_{t-1}, \ldots, x_{t-\tau}$。当下获得的最直接的好处就是参数的数量总是不变的，至少在$t &gt; \tau$时如此，这就使我们能够训练一个上述的深度网络。这种模型被称为<em>自回归模型</em>（autoregressive models），因为它们是对自己执行回归。</p><p>第二种策略，如下图所示，是保留一些对过去观测的总结$h_t$，并且同时更新预测$\hat{x}_t$和总结$h_t$。这就产生了基于$\hat{x}_t = P(x_t \mid h_{t})$估计$x_t$，以及公式$h_t = g(h_{t-1}, x_{t-1})$更新的模型。由于$h_t$从未被观测到，这类模型也被称为<em>隐变量自回归模型</em>（latent autoregressive models）。</p><p><img src="/assets/post_img/article57/sequence-model.svg" alt="隐变量自回归模型"></p><p>这两种策略有一个显而易见的问题：如何生成训练数据？一个经典方法是使用历史观测来预测下一个未来观测。我们并不指望时间会停滞不前，但一个常见的假设是虽然特定值$x_t$可能会改变，但是序列本身的动力学不会改变。这样的假设是合理的，因为新的动力学一定受新的数据影响，而人们不可能用目前所掌握的数据来预测新的动力学。统计学家称不变的动力学为<em>静止的</em>（stationary）。因此，整个序列的估计值都将通过以下的方式获得：</p><script type="math/tex; mode=display">P(x_1, \ldots, x_T) = \prod_{t=1}^T P(x_t \mid x_{t-1}, \ldots, x_1).</script><p>注意，如果我们处理的是离散的对象（如单词），而不是连续的数字，则上述的考虑仍然有效。唯一的差别是，对于离散的对象，需要使用分类器而不是回归模型来估计条件概率$P(x_t \mid  x_{t-1}, \ldots, x_1)$。</p><h4 id="1-1-2-马尔可夫模型"><a href="#1-1-2-马尔可夫模型" class="headerlink" title="1.1.2. 马尔可夫模型"></a>1.1.2. 马尔可夫模型</h4><p>在自回归模型的近似法中使用$x_{t-1}, \ldots, x_{t-\tau}$而不是$x_{t-1}, \ldots, x_1$来估计$x_t$。只要这种是近似精确的，我们就说序列满足<em>马尔可夫条件</em>（Markov condition）。特别是，如果$\tau = 1$，得到一个<em>一阶马尔可夫模型</em>（first-order Markov model），$P(x)$由下式给出：</p><script type="math/tex; mode=display">P(x_1, \ldots, x_T) = \prod_{t=1}^T P(x_t \mid x_{t-1}) \text{ 当 } P(x_1 \mid x_0) = P(x_1).</script><p>当假设$x_t$仅是离散值时，这样的模型特别棒，因为在这种情况下，使用动态规划可以沿着马尔可夫链精确地计算结果。例如可以高效地计算$P(x_{t+1} \mid x_{t-1})$：</p><script type="math/tex; mode=display">\begin{aligned}P(x_{t+1} \mid x_{t-1})&= \frac{\sum_{x_t} P(x_{t+1}, x_t, x_{t-1})}{P(x_{t-1})}\\&= \frac{\sum_{x_t} P(x_{t+1} \mid x_t, x_{t-1}) P(x_t, x_{t-1})}{P(x_{t-1})}\\&= \sum_{x_t} P(x_{t+1} \mid x_t) P(x_t \mid x_{t-1})\end{aligned}</script><p>利用这一事实，我们只需要考虑过去观察中的一个非常短的历史片段：$P(x_{t+1} \mid x_t, x_{t-1}) = P(x_{t+1} \mid x_t)$。隐马尔可夫模型中的动态规划超出了本节的范围，而动态规划这些计算工具已经在控制算法和强化学习算法广泛使用。</p><h4 id="1-1-3-因果关系"><a href="#1-1-3-因果关系" class="headerlink" title="1.1.3. 因果关系"></a>1.1.3. 因果关系</h4><p>原则上，将$P(x_1, \ldots, x_T)$倒序展开也没什么问题。毕竟，基于条件概率公式总是可以写出：</p><script type="math/tex; mode=display">P(x_1, \ldots, x_T) = \prod_{t=T}^1 P(x_t \mid x_{t+1}, \ldots, x_T).</script><p>事实上，如果基于一个马尔可夫模型，我们还可以得到一个反向的条件概率分布。但在许多情况下，数据存在一个自然的方向，即在时间上是前进的。未来的事件不能影响过去。因此，如果我们改变$x_t$，可能会影响未来发生的事情$x_{t+1}$，但不能反过来。也就是说，如果我们改变$x_t$，基于过去事件得到的分布不会改变。因此，解释$P(x_{t+1} \mid x_t)$应该比解释$P(x_t \mid x_{t+1})$更容易。例如在某些情况下，对于某些可加性噪声$\epsilon$，我们可以找到$x_{t+1} = f(x_t) + \epsilon$，而反之则不行。这个向前推进的方向恰好也是比较有用的方向。彼得斯等人对该主题的更多内容做了详尽的解释。</p><h3 id="1-2-训练"><a href="#1-2-训练" class="headerlink" title="1.2. 训练"></a>1.2. 训练</h3><p>在实践中尝试一下上述统计工具。首先生成一些数据：使用正弦函数和一些可加性噪声来生成序列数据，时间步为$1, 2, \ldots, 1000$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">T = <span class="number">1000</span>  <span class="comment"># 总共产生1000个点</span></span><br><span class="line">time = torch.arange(<span class="number">1</span>, T + <span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">x = torch.sin(<span class="number">0.01</span> * time) + torch.normal(<span class="number">0</span>, <span class="number">0.2</span>, (T,))</span><br><span class="line"><span class="comment"># d2l.plot</span></span><br><span class="line">plot(time, [x], <span class="string">&#x27;time&#x27;</span>, <span class="string">&#x27;x&#x27;</span>, xlim=[<span class="number">1</span>, <span class="number">1000</span>], figsize=(<span class="number">6</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article57/output_sequence.svg" alt="正弦函数和一些可加性噪声"></p><p>接下来将这个序列转换为模型的<em>特征－标签</em>（feature-label）对。基于嵌入维度$\tau$将数据映射为数据对$y_t = x_t$和$\mathbf{x}_t = [x_{t-\tau}, \ldots, x_{t-1}]$。这比我们提供的数据样本少了$\tau$个，因为我们没有足够的历史记录来描述前$\tau$个数据样本。一个简单的解决办法是：如果拥有足够长的序列就丢弃这几项；另一个方法是用零填充序列。这里我们仅使用前600个“特征－标签”对进行训练。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 嵌入维度 τ，决定了特征向量的维度（特征数和样本个数）。</span></span><br><span class="line">tau = <span class="number">4</span></span><br><span class="line">features = torch.zeros((T - tau, tau))</span><br><span class="line"><span class="comment"># 对每个特征（每一列），用x中的数据填入，每次循环x向后推进一位</span></span><br><span class="line"><span class="comment"># 最终特征向量中的最后一列样本为x中的后996个值，样本数为996</span></span><br><span class="line"><span class="comment"># 这样整理后，样本的起始时刻为5，前四个时刻的数据作为第一个样本的特征，以此类推</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(tau):</span><br><span class="line">    features[:, i] = x[i: T - tau + i]</span><br><span class="line"><span class="comment"># 标签也整理为（996，1）</span></span><br><span class="line">labels = x[tau:].reshape((-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">batch_size, n_train = <span class="number">16</span>, <span class="number">600</span></span><br><span class="line"><span class="comment"># 只有前n_train个样本用于训练</span></span><br><span class="line"><span class="comment"># d2l.load_array</span></span><br><span class="line">train_iter = load_array((features[:n_train], labels[:n_train]),</span><br><span class="line">                            batch_size, is_train=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>这里使用一个相当简单的架构训练模型： 一个拥有两个全连接层的多层感知机，ReLU激活函数和平方损失。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化网络权重的函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_weights</span>(<span class="params">m</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.xavier_uniform_(m.weight)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 一个简单的多层感知机</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_net</span>():</span></span><br><span class="line">    net = nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">10</span>),</span><br><span class="line">                        nn.ReLU(),</span><br><span class="line">                        nn.Linear(<span class="number">10</span>, <span class="number">1</span>))</span><br><span class="line">    net.apply(init_weights)</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line"><span class="comment"># 平方损失。注意：MSELoss计算平方误差时不带系数1/2</span></span><br><span class="line">loss = nn.MSELoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br></pre></td></tr></table></figure><p>现在训练模型，实现下面的训练代码的方式与前面几节中的循环训练基本相同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">net, train_iter, loss, epochs, lr</span>):</span></span><br><span class="line">    trainer = torch.optim.Adam(net.parameters(), lr)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            trainer.zero_grad()</span><br><span class="line">            l = loss(net(X), y)</span><br><span class="line">            l.<span class="built_in">sum</span>().backward()</span><br><span class="line">            trainer.step()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, &#x27;</span></span><br><span class="line">        <span class="comment"># d2l.evaluate_loss</span></span><br><span class="line">              <span class="string">f&#x27;loss: <span class="subst">&#123;evaluate_loss(net, train_iter, loss):f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">net = get_net()</span><br><span class="line">train(net, train_iter, loss, <span class="number">5</span>, <span class="number">0.01</span>)</span><br></pre></td></tr></table></figure><h3 id="1-3-预测"><a href="#1-3-预测" class="headerlink" title="1.3. 预测"></a>1.3. 预测</h3><p>前面训练的损失很小，则可以期望模型有很好的工作效果。首先是检查模型预测下一个时间步的能力， 也就是单步预测（one-step-ahead prediction）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">onestep_preds = net(features)</span><br><span class="line"><span class="comment"># d2l.plot</span></span><br><span class="line">plot([time, time[tau:]],</span><br><span class="line">         [x.detach().numpy(), onestep_preds.detach().numpy()], <span class="string">&#x27;time&#x27;</span>,</span><br><span class="line">         <span class="string">&#x27;x&#x27;</span>, legend=[<span class="string">&#x27;data&#x27;</span>, <span class="string">&#x27;1-step preds&#x27;</span>], xlim=[<span class="number">1</span>, <span class="number">1000</span>],</span><br><span class="line">         figsize=(<span class="number">6</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article57/output_sequence_predict.svg" alt="predict_output"></p><p>可以看到确实单步预测效果不错。即使这些预测的时间步超过了$600+4$（<code>n_train + tau</code>），其结果看起来仍然是可信的。然而有一个小问题：如果数据观察序列的时间步只到$604$，我们需要一步一步地向前迈进：</p><script type="math/tex; mode=display">\hat{x}_{605} = f(x_{601}, x_{602}, x_{603}, x_{604}), \\\hat{x}_{606} = f(x_{602}, x_{603}, x_{604}, \hat{x}_{605}), \\\hat{x}_{607} = f(x_{603}, x_{604}, \hat{x}_{605}, \hat{x}_{606}),\\\hat{x}_{608} = f(x_{604}, \hat{x}_{605}, \hat{x}_{606}, \hat{x}_{607}),\\\hat{x}_{609} = f(\hat{x}_{605}, \hat{x}_{606}, \hat{x}_{607}, \hat{x}_{608}),\\\ldots</script><p>通常，对于直到$x_t$的观测序列，其在时间步$t+k$处的预测输出$\hat{x}_{t+k}$称为$k$<em>步预测</em>（$k$-step-ahead-prediction）。由于观察已经到了$x_{604}$，它的$k$步预测是$\hat{x}_{604+k}$。则我们必须使用自己的预测（而不是原始数据）来进行多步预测，看效果如何：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">multistep_preds = torch.zeros(T)</span><br><span class="line"><span class="comment"># 把向量前面的数值替换为x中604前的数值</span></span><br><span class="line">multistep_preds[: n_train + tau] = x[: n_train + tau]</span><br><span class="line"><span class="comment"># 用自己的预测结果预测后面的时间步</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_train + tau, T):</span><br><span class="line">    multistep_preds[i] = net(</span><br><span class="line">        multistep_preds[i - tau:i].reshape((<span class="number">1</span>, -<span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># d2l.plot</span></span><br><span class="line">plot([time, time[tau:], time[n_train + tau:]],</span><br><span class="line">         [x.detach().numpy(), onestep_preds.detach().numpy(),</span><br><span class="line">          multistep_preds[n_train + tau:].detach().numpy()], <span class="string">&#x27;time&#x27;</span>,</span><br><span class="line">         <span class="string">&#x27;x&#x27;</span>, legend=[<span class="string">&#x27;data&#x27;</span>, <span class="string">&#x27;1-step preds&#x27;</span>, <span class="string">&#x27;multistep preds&#x27;</span>],</span><br><span class="line">         xlim=[<span class="number">1</span>, <span class="number">1000</span>], figsize=(<span class="number">6</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article57/output_sequence_k.svg" alt="k——step"></p><p>如上图所示，绿线的预测显然并不理想。经过几个预测步骤之后，预测的结果很快就会衰减到一个常数。这个算法效果如此差是由于错误的累积：假设在步骤$1$之后，我们积累了一些错误$\epsilon_1 = \bar\epsilon$。于是，步骤$2$的输入被扰动了$\epsilon_1$，结果积累的误差是依照次序的$\epsilon_2 = \bar\epsilon + c \epsilon_1$，其中$c$为某个常数，后面的预测误差依此类推。因此误差可能会相当快地偏离真实的观测结果。例如，未来$24$小时的天气预报往往相当准确，但超过这一点，精度就会迅速下降。本章及后续章节中将讨论如何改进这一点。</p><p>基于$k = 1, 4, 16, 64$，通过对整个序列预测的计算，来更仔细地看一下$k$步预测的困难。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">max_steps = <span class="number">64</span></span><br><span class="line"></span><br><span class="line">features = torch.zeros((T - tau - max_steps + <span class="number">1</span>, tau + max_steps))</span><br><span class="line"><span class="comment"># 列i（i&lt;tau）是来自x的观测，其时间步从（i）到（i+T-tau-max_steps+1）</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(tau):</span><br><span class="line">    features[:, i] = x[i: i + T - tau - max_steps + <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列i（i&gt;=tau）是来自（i-tau+1）步的预测，其时间步从（i）到（i+T-tau-max_steps+1）</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(tau, tau + max_steps):</span><br><span class="line">    features[:, i] = net(features[:, i - tau:i]).reshape(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">steps = (<span class="number">1</span>, <span class="number">4</span>, <span class="number">16</span>, <span class="number">64</span>)</span><br><span class="line"><span class="comment"># d2l.plot</span></span><br><span class="line">plot([time[tau + i - <span class="number">1</span>: T - max_steps + i] <span class="keyword">for</span> i <span class="keyword">in</span> steps],</span><br><span class="line">         [features[:, (tau + i - <span class="number">1</span>)].detach().numpy() <span class="keyword">for</span> i <span class="keyword">in</span> steps], <span class="string">&#x27;time&#x27;</span>, <span class="string">&#x27;x&#x27;</span>,</span><br><span class="line">         legend=[<span class="string">f&#x27;<span class="subst">&#123;i&#125;</span>-step preds&#x27;</span> <span class="keyword">for</span> i <span class="keyword">in</span> steps], xlim=[<span class="number">5</span>, <span class="number">1000</span>],</span><br><span class="line">         figsize=(<span class="number">6</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article57/output_sequence_kstep.svg" alt="kstep"></p><p>上图清楚地说明了当试图预测更远的未来时，预测的质量是如何变化的。 虽然“4步预测”看起来仍然不错，但超过这个跨度的任何预测几乎都是无用的。</p><h2 id="2-文本预处理"><a href="#2-文本预处理" class="headerlink" title="2. 文本预处理"></a>2. 文本预处理</h2><p>对于序列数据处理问题，上节中评估了所需的统计工具和预测时面临的挑战。 这样的数据存在许多种形式，文本是最常见例子之一。 例如，一篇文章可以被简单地看作是一串单词序列，甚至是一串字符序列。 本节将解析文本的常见预处理步骤。 这些步骤通常包括：</p><ol><li>将文本作为字符串加载到内存中。</li><li>将字符串拆分为词元（如单词和字符）。</li><li>建立一个词表，将拆分的词元映射到数字索引。</li><li>将文本转换为数字索引序列，方便模型操作。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> re</span><br></pre></td></tr></table></figure><h3 id="2-1-读取数据集"><a href="#2-1-读取数据集" class="headerlink" title="2.1. 读取数据集"></a>2.1. 读取数据集</h3><p>首先从H.G.Well的<a href="https://www.gutenberg.org/ebooks/35">《时光机器》</a>中加载文本。 这是一个相当小的语料库，只有30000多个单词， 而现实中的文档集合可能会包含数十亿个单词。 下面的函数将数据集读取到由多条文本行组成的列表中，其中每条文本行都是一个字符串。 为简单起见这里忽略了标点符号和字母大写。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># d2l.DATA_HUB、d2l.DATA_URL</span></span><br><span class="line">DATA_HUB[<span class="string">&#x27;time_machine&#x27;</span>] = (DATA_URL + <span class="string">&#x27;timemachine.txt&#x27;</span>,</span><br><span class="line">                                <span class="string">&#x27;090b5e7e70c295757f55df93cb0a180b9691891a&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_time_machine</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;将时间机器数据集加载到文本行的列表中&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># d2l.download</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(download(<span class="string">&#x27;time_machine&#x27;</span>), <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        lines = f.readlines()</span><br><span class="line">    <span class="comment"># regular expression substitute（替换）</span></span><br><span class="line">    <span class="comment"># 这里[^A-Za-z]+的含义是匹配除了字母A-Z和a-z外的字符，一次到多次，替换为空格</span></span><br><span class="line">    <span class="comment"># 然后strip移除字符串头尾的空格，化为小写后完毕。</span></span><br><span class="line">    <span class="keyword">return</span> [re.sub(<span class="string">&#x27;[^A-Za-z]+&#x27;</span>, <span class="string">&#x27; &#x27;</span>, line).strip().lower() <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 无法获取数据集可能是DNS问题导致</span></span><br><span class="line">lines = read_time_machine()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;# 文本总行数: <span class="subst">&#123;<span class="built_in">len</span>(lines)&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(lines[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(lines[<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line"><span class="comment"># 文本总行数: 3221</span></span><br><span class="line">the time machine by h g wells</span><br><span class="line">twinkled <span class="keyword">and</span> his usually pale face was flushed <span class="keyword">and</span> animated the</span><br></pre></td></tr></table></figure><h3 id="2-2-词元化"><a href="#2-2-词元化" class="headerlink" title="2.2. 词元化"></a>2.2. 词元化</h3><p><code>tokenize</code>函数将文本行列表（<code>lines</code>）作为输入，列表中的每个元素是一个文本序列（如一条文本行）。每个文本序列又被拆分成一个词元列表，<em>词元</em>（token）是文本的基本单位（单词或字母）。最后，返回一个由词元列表组成的列表，其中的每个词元都是一个字符串（string）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize</span>(<span class="params">lines, token=<span class="string">&#x27;word&#x27;</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;将文本行拆分为单词或字符词元&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> token == <span class="string">&#x27;word&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> [line.split() <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">    <span class="keyword">elif</span> token == <span class="string">&#x27;char&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> [<span class="built_in">list</span>(line) <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;错误：未知词元类型：&#x27;</span> + token)</span><br><span class="line"></span><br><span class="line">tokens = tokenize(lines)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">11</span>):</span><br><span class="line">    <span class="built_in">print</span>(tokens[i])</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">[<span class="string">&#x27;the&#x27;</span>, <span class="string">&#x27;time&#x27;</span>, <span class="string">&#x27;machine&#x27;</span>, <span class="string">&#x27;by&#x27;</span>, <span class="string">&#x27;h&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;wells&#x27;</span>]</span><br><span class="line">[]</span><br><span class="line">[]</span><br><span class="line">[]</span><br><span class="line">[]</span><br><span class="line">[<span class="string">&#x27;i&#x27;</span>]</span><br><span class="line">[]</span><br><span class="line">[]</span><br><span class="line">[<span class="string">&#x27;the&#x27;</span>, <span class="string">&#x27;time&#x27;</span>, <span class="string">&#x27;traveller&#x27;</span>, <span class="string">&#x27;for&#x27;</span>, <span class="string">&#x27;so&#x27;</span>, <span class="string">&#x27;it&#x27;</span>, <span class="string">&#x27;will&#x27;</span>, <span class="string">&#x27;be&#x27;</span>, <span class="string">&#x27;convenient&#x27;</span>, <span class="string">&#x27;to&#x27;</span>, <span class="string">&#x27;speak&#x27;</span>, <span class="string">&#x27;of&#x27;</span>, <span class="string">&#x27;him&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;was&#x27;</span>, <span class="string">&#x27;expounding&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;recondite&#x27;</span>, <span class="string">&#x27;matter&#x27;</span>, <span class="string">&#x27;to&#x27;</span>, <span class="string">&#x27;us&#x27;</span>, <span class="string">&#x27;his&#x27;</span>, <span class="string">&#x27;grey&#x27;</span>, <span class="string">&#x27;eyes&#x27;</span>, <span class="string">&#x27;shone&#x27;</span>, <span class="string">&#x27;and&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;twinkled&#x27;</span>, <span class="string">&#x27;and&#x27;</span>, <span class="string">&#x27;his&#x27;</span>, <span class="string">&#x27;usually&#x27;</span>, <span class="string">&#x27;pale&#x27;</span>, <span class="string">&#x27;face&#x27;</span>, <span class="string">&#x27;was&#x27;</span>, <span class="string">&#x27;flushed&#x27;</span>, <span class="string">&#x27;and&#x27;</span>, <span class="string">&#x27;animated&#x27;</span>, <span class="string">&#x27;the&#x27;</span>]</span><br></pre></td></tr></table></figure><h3 id="2-3-词表"><a href="#2-3-词表" class="headerlink" title="2.3. 词表"></a>2.3. 词表</h3><p>词元的类型是字符串，而模型需要的输入是数字，所以我们需要构建一个字典，通常也叫做<em>词表</em>（vocabulary），用来将字符串类型的词元映射到从$0$开始的数字索引中。首先将训练集中的所有文档合并在一起，对它们的唯一词元进行统计，得到的统计结果称之为<em>语料</em>（corpus）。然后根据每个唯一词元的出现频率，为其分配一个数字索引。很少出现的词元通常被移除，这可以降低复杂性。另外，语料库中不存在或已删除的任何词元都将映射到一个特定的未知词元“&lt;unk&gt;”。我们可以选择增加一个列表，用于保存那些被保留的词元，例如：填充词元（“&lt;pad&gt;”）；序列开始词元（“&lt;bos&gt;”）；序列结束词元（“&lt;eos&gt;”）。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Vocab</span>:</span> </span><br><span class="line">    <span class="string">&quot;&quot;&quot;文本词表&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, tokens=<span class="literal">None</span>, min_freq=<span class="number">0</span>, reserved_tokens=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> tokens <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            tokens = []</span><br><span class="line">        <span class="keyword">if</span> reserved_tokens <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            reserved_tokens = []</span><br><span class="line">        <span class="comment"># 获取词元展平后的Counter对象。</span></span><br><span class="line">        counter = count_corpus(tokens)</span><br><span class="line">        <span class="comment"># 私有变量，词元频率（事实上是伪私有）</span></span><br><span class="line">        <span class="comment"># 按出现频率排序，sorted() 函数用于对所有可迭代的对象进行排序操作。</span></span><br><span class="line">        <span class="comment"># counter.items()等同于字典的items()函数，返回一个可迭代的集合数据结构</span></span><br><span class="line">        <span class="comment"># 参数key是用来进行比较的元素，指定可迭代对象中的一个元素来进行排序，这里指的是每一个tuple中的第二个元素，即频率</span></span><br><span class="line">        <span class="comment"># reverse参数：True为降序，False为升序，默认False  </span></span><br><span class="line">        self._token_freqs = <span class="built_in">sorted</span>(counter.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>],</span><br><span class="line">                                   reverse=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 生成词元列表</span></span><br><span class="line">        <span class="comment"># 左加使未知词元的索引为0, 如：[&#x27;&lt;unk&gt;&#x27;, ...]</span></span><br><span class="line">        self.idx_to_token = [<span class="string">&#x27;&lt;unk&gt;&#x27;</span>] + reserved_tokens</span><br><span class="line">        <span class="comment"># 生成词元与索引对应的字典</span></span><br><span class="line">        self.token_to_idx = &#123;token: idx</span><br><span class="line">                             <span class="keyword">for</span> idx, token <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.idx_to_token)&#125;</span><br><span class="line">        <span class="keyword">for</span> token, freq <span class="keyword">in</span> self._token_freqs:</span><br><span class="line">            <span class="keyword">if</span> freq &lt; min_freq:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="comment"># 发现新词，在词元列表中加入该词，然后在词元-索引字典中添加该词及其索引</span></span><br><span class="line">            <span class="keyword">if</span> token <span class="keyword">not</span> <span class="keyword">in</span> self.token_to_idx:</span><br><span class="line">                self.idx_to_token.append(token)</span><br><span class="line">                self.token_to_idx[token] = <span class="built_in">len</span>(self.idx_to_token) - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.idx_to_token)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义该方法使得Vocab类可以以 p[key] 的方式取值</span></span><br><span class="line">    <span class="comment"># 此处‘key’的格式可以为单个词、list或tuple</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, tokens</span>):</span></span><br><span class="line">        <span class="comment"># 单个词</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(tokens, (<span class="built_in">list</span>, <span class="built_in">tuple</span>)):</span><br><span class="line">            <span class="comment"># 如果查找不到则返回频率0</span></span><br><span class="line">            <span class="keyword">return</span> self.token_to_idx.get(tokens, self.unk)</span><br><span class="line">        <span class="comment"># 可遍历对象，返回一个频率列表</span></span><br><span class="line">        <span class="keyword">return</span> [self.__getitem__(token) <span class="keyword">for</span> token <span class="keyword">in</span> tokens]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 接受索引返回词元</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">to_tokens</span>(<span class="params">self, indices</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(indices, (<span class="built_in">list</span>, <span class="built_in">tuple</span>)):</span><br><span class="line">            <span class="keyword">return</span> self.idx_to_token[indices]</span><br><span class="line">        <span class="keyword">return</span> [self.idx_to_token[index] <span class="keyword">for</span> index <span class="keyword">in</span> indices]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># property装饰器，可以直接通过方法名来访问方法，不需要在方法名后添加圆括号“()”</span></span><br><span class="line">    <span class="comment"># 如：vacab.unk、vacab.token_freqs</span></span><br><span class="line">    <span class="comment"># 相当于getter方法，访问私有成员的接口</span></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">unk</span>(<span class="params">self</span>):</span>  <span class="comment"># 未知词元的索引为0</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">token_freqs</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self._token_freqs</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_corpus</span>(<span class="params">tokens</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;统计词元的频率&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 这里的tokens是1D列表或2D列表</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(tokens) == <span class="number">0</span> <span class="keyword">or</span> <span class="built_in">isinstance</span>(tokens[<span class="number">0</span>], <span class="built_in">list</span>):</span><br><span class="line">        <span class="comment"># 将词元列表展平成一个列表</span></span><br><span class="line">        tokens = [token <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line">    <span class="comment"># 返回一个Counter对象，该对象是一个高性能的容器数据类型，有许多作用</span></span><br><span class="line">    <span class="comment"># 对于取频率这件事，直接dict（counter）就可以得到频率字典。</span></span><br><span class="line">    <span class="keyword">return</span> collections.Counter(tokens)</span><br></pre></td></tr></table></figure></p><p>使用时光机器数据集作为语料库来构建词表，然后打印前几个高频词元及其索引：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vocab = Vocab(tokens)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(vocab.token_to_idx.items())[:<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">[(<span class="string">&#x27;&lt;unk&gt;&#x27;</span>, <span class="number">0</span>), (<span class="string">&#x27;the&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;i&#x27;</span>, <span class="number">2</span>), (<span class="string">&#x27;and&#x27;</span>, <span class="number">3</span>), (<span class="string">&#x27;of&#x27;</span>, <span class="number">4</span>), (<span class="string">&#x27;a&#x27;</span>, <span class="number">5</span>), (<span class="string">&#x27;to&#x27;</span>, <span class="number">6</span>), (<span class="string">&#x27;was&#x27;</span>, <span class="number">7</span>), (<span class="string">&#x27;in&#x27;</span>, <span class="number">8</span>), (<span class="string">&#x27;that&#x27;</span>, <span class="number">9</span>)]</span><br></pre></td></tr></table></figure></p><p>可以将每一条文本行转换成一个数字索引列表：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">0</span>, <span class="number">10</span>]:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;文本:&#x27;</span>, tokens[i])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;索引:&#x27;</span>, vocab[tokens[i]])</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">文本: [<span class="string">&#x27;the&#x27;</span>, <span class="string">&#x27;time&#x27;</span>, <span class="string">&#x27;machine&#x27;</span>, <span class="string">&#x27;by&#x27;</span>, <span class="string">&#x27;h&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;wells&#x27;</span>]</span><br><span class="line">索引: [<span class="number">1</span>, <span class="number">19</span>, <span class="number">50</span>, <span class="number">40</span>, <span class="number">2183</span>, <span class="number">2184</span>, <span class="number">400</span>]</span><br><span class="line">文本: [<span class="string">&#x27;twinkled&#x27;</span>, <span class="string">&#x27;and&#x27;</span>, <span class="string">&#x27;his&#x27;</span>, <span class="string">&#x27;usually&#x27;</span>, <span class="string">&#x27;pale&#x27;</span>, <span class="string">&#x27;face&#x27;</span>, <span class="string">&#x27;was&#x27;</span>, <span class="string">&#x27;flushed&#x27;</span>, <span class="string">&#x27;and&#x27;</span>, <span class="string">&#x27;animated&#x27;</span>, <span class="string">&#x27;the&#x27;</span>]</span><br><span class="line">索引: [<span class="number">2186</span>, <span class="number">3</span>, <span class="number">25</span>, <span class="number">1044</span>, <span class="number">362</span>, <span class="number">113</span>, <span class="number">7</span>, <span class="number">1421</span>, <span class="number">3</span>, <span class="number">1045</span>, <span class="number">1</span>]</span><br></pre></td></tr></table></figure><h3 id="2-4-功能整合"><a href="#2-4-功能整合" class="headerlink" title="2.4. 功能整合"></a>2.4. 功能整合</h3><p>将所有功能打包到load_corpus_time_machine函数中， 该函数返回corpus（词元索引列表）和vocab（时光机器语料库的词表）。有两点需要注意：</p><ol><li>为了简化后面章节中的训练，使用字符（而不是单词）实现文本词元化；</li><li>时光机器数据集中的每个文本行不一定是一个句子或一个段落，还可能是一个单词，因此返回的corpus展平为一维列表，而不是由多词元列表构成的一个列表。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_corpus_time_machine</span>(<span class="params">max_tokens=-<span class="number">1</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回时光机器数据集的词元索引列表和词表&quot;&quot;&quot;</span></span><br><span class="line">    lines = read_time_machine()</span><br><span class="line">    tokens = tokenize(lines, <span class="string">&#x27;char&#x27;</span>)</span><br><span class="line">    vocab = Vocab(tokens)</span><br><span class="line">    <span class="comment"># 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，</span></span><br><span class="line">    <span class="comment"># 所以将所有文本行展平到一个列表中</span></span><br><span class="line">    corpus = [vocab[token] <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line">    <span class="keyword">if</span> max_tokens &gt; <span class="number">0</span>:</span><br><span class="line">        corpus = corpus[:max_tokens]</span><br><span class="line">    <span class="keyword">return</span> corpus, vocab</span><br><span class="line"></span><br><span class="line">corpus, vocab = load_corpus_time_machine()</span><br><span class="line"><span class="built_in">len</span>(corpus), <span class="built_in">len</span>(vocab)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">(<span class="number">170580</span>, <span class="number">28</span>)</span><br></pre></td></tr></table></figure><h2 id="3-语言模型和数据集"><a href="#3-语言模型和数据集" class="headerlink" title="3. 语言模型和数据集"></a>3. 语言模型和数据集</h2><p>假设长度为$T$的文本序列中的词元依次为$x_1, x_2, \ldots, x_T$。则$x_t$（$1 \leq t \leq T$）可以被认为是文本序列在时间步$t$处的观测或标签。在给定这样的文本序列时，<em>语言模型</em>（language model）的目标是估计序列的联合概率：</p><script type="math/tex; mode=display">P(x_1, x_2, \ldots, x_T)</script><p>例如，一个理想的语言模型只需一次抽取一个词元$x_t \sim P(x_t \mid x_{t-1}, \ldots, x_1)$就能够基于模型本身生成自然文本。从这样的模型中提取的文本都将作为自然语言来传递。只需要基于前面的对话片断中的文本，就足以生成一个有意义的对话。显然，我们离设计出这样的系统还很遥远，因为它需要“理解”文本，而不仅仅是生成语法合理的内容。</p><p>尽管如此，语言模型依然是非常有用的。例如，短语“to recognize speech”和“to wreck a nice beach”读音上听起来非常相似。这种相似性会导致语音识别中的歧义，但是这很容易通过语言模型来解决，因为第二句的语义很奇怪。同样，在文档摘要生成算法中，“狗咬人”比“人咬狗”出现的频率要高得多，或者“我想吃奶奶”是一个相当匪夷所思的语句，而“我想吃，奶奶”则要正常得多。</p><h3 id="3-1-学习语言模型"><a href="#3-1-学习语言模型" class="headerlink" title="3.1. 学习语言模型"></a>3.1. 学习语言模型</h3><p>我们面对的问题是如何对一个文档，甚至是一个词元序列进行建模。假设在单词级别对文本数据进行词元化，可以依靠<a href="#1-序列模型">之前</a>对序列模型的分析，从基本概率规则开始：</p><script type="math/tex; mode=display">P(x_1, x_2, \ldots, x_T) = \prod_{t=1}^T P(x_t  \mid  x_1, \ldots, x_{t-1})</script><p>例如，包含了四个单词的一个文本序列的概率是：</p><script type="math/tex; mode=display">P(\text{deep}, \text{learning}, \text{is}, \text{fun}) =  P(\text{deep}) P(\text{learning}  \mid  \text{deep}) P(\text{is}  \mid  \text{deep}, \text{learning}) P(\text{fun}  \mid  \text{deep}, \text{learning}, \text{is})</script><p>为训练语言模型，需要计算单词的概率，以及给定前面几个单词后出现某个单词的条件概率。这些概率本质上就是语言模型的参数。</p><p>这里假设训练数据集是一个大型的文本语料库。比如维基百科的所有条目或者所有发布在网络上的文本。训练数据集中<em>词的概率</em>可以根据给定词的相对词频来计算。比如可以将估计值$\hat{P}(\text{deep})$计算为任何以单词“deep”开头的句子的概率。另一种（不太精确的）方法是统计单词“deep”在数据集中的出现次数，然后将其除以整个语料库中的单词总数。这种方法效果不错，特别是对于频繁出现的单词。<br>接下来可以尝试估计：</p><script type="math/tex; mode=display">\hat{P}(\text{learning} \mid \text{deep}) = \frac{n(\text{deep, learning})}{n(\text{deep})},</script><p>其中$n(x)$和$n(x, x’)$分别是单个单词和连续单词对的出现次数。由于连续单词对“deep learning”的出现频率要低得多，所以估计这类单词正确的概率要困难得多。特别是对于一些不常见的单词组合，要想找到足够的出现次数来获得准确的估计并不容易。而对于三个或者更多的单词组合，情况会变得更糟。许多合理的三个单词组合可能是存在的，但是在数据集中却找不到。除非有某种策略，来将这些单词组合指定为非零计数，否则将无法在语言模型中使用它们。如果数据集很小，或者单词非常罕见，那么这类单词出现一次的机会可能都找不到。</p><p>一种常见的策略是执行某种形式的<em>拉普拉斯平滑</em>（Laplace smoothing），具体方法是在所有计数中添加一个小常量。用$n$表示训练集中的单词总数，用$m$表示唯一单词的数量。此解决方案有助于处理单元素问题，例如通过：</p><script type="math/tex; mode=display">\begin{aligned}    \hat{P}(x) & = \frac{n(x) + \epsilon_1/m}{n + \epsilon_1}, \\    \hat{P}(x' \mid x) & = \frac{n(x, x') + \epsilon_2 \hat{P}(x')}{n(x) + \epsilon_2}, \\    \hat{P}(x'' \mid x,x') & = \frac{n(x, x',x'') + \epsilon_3 \hat{P}(x'')}{n(x, x') + \epsilon_3}.\end{aligned}</script><p>其中，$\epsilon_1,\epsilon_2$和$\epsilon_3$是超参数。以$\epsilon_1$为例：当$\epsilon_1 = 0$时，不应用平滑；当$\epsilon_1$接近正无穷大时，$\hat{P}(x)$接近均匀概率分布$1/m$（可能是常数都忽略，然后上下消掉超参数）。上面的公式是<a href="https://zh-v2.d2l.ai/chapter_references/zreferences.html#id185">文章</a>的一个相当原始的变形。</p><p>但这样的模型很容易变得无效，原因如下：<br>1、模型需要存储所有的计数；<br>2、模型完全忽略了单词的意思。例如，“猫”（cat）和“猫科动物”（feline）可能出现在相关的上下文中，但是想根据上下文调整这类模型其实是相当困难的。<br>3、长单词序列大部分是没出现过的，因此一个模型如果只是简单地统计先前“看到”的单词序列频率，那么模型面对这种问题肯定是表现不佳的。</p><h3 id="3-2-马尔可夫模型与-n-元语法"><a href="#3-2-马尔可夫模型与-n-元语法" class="headerlink" title="3.2. 马尔可夫模型与$n$元语法"></a>3.2. 马尔可夫模型与$n$元语法</h3><p>回想一下马尔可夫模型，并且将其应用于语言建模。如果$P(x_{t+1} \mid x_t, \ldots, x_1) = P(x_{t+1} \mid x_t)$，则序列上的分布满足一阶马尔可夫性质。阶数越高，对应的依赖关系就越长。这种性质推导出了许多可以应用于序列建模的近似公式：</p><script type="math/tex; mode=display">\begin{aligned}P(x_1, x_2, x_3, x_4) &=  P(x_1) P(x_2) P(x_3) P(x_4),\\P(x_1, x_2, x_3, x_4) &=  P(x_1) P(x_2  \mid  x_1) P(x_3  \mid  x_2) P(x_4  \mid  x_3),\\P(x_1, x_2, x_3, x_4) &=  P(x_1) P(x_2  \mid  x_1) P(x_3  \mid  x_1, x_2) P(x_4  \mid  x_2, x_3).\end{aligned}</script><p>这里说一下我个人的理解，第一行的含义为“各个时刻的数据独立，与之前发生的事无关”，这与通过计数统计和平滑来建模单词的想法是一致的；第二行满足一阶马尔可夫性质，含义为“t时刻发生的概率或许可仅用t前一个时刻发生的事来断定”；第三行依赖关系更长，也就是满足二阶马尔可夫性质，含义为“t时刻发生的概率或许可仅用t前两个时刻发生的事来断定”。</p><p>事实上，上面的三个式子分别对应一、二、三元语法，涉及一个、两个和三个变量的概率公式分别被称为<em>一元语法</em>（unigram）、<em>二元语法</em>（bigram）和<em>三元语法</em>（trigram）模型。下面将学习如何去设计更好的模型。</p><h3 id="3-3-自然语言统计"><a href="#3-3-自然语言统计" class="headerlink" title="3.3. 自然语言统计"></a>3.3. 自然语言统计</h3><p>学习在真实数据上如何进行自然语言统计。根据时光机器数据集构建词表，并打印前$10$个最常用的（频率最高的）单词。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">tokens = tokenize(read_time_machine())</span><br><span class="line"><span class="comment"># 因为每个文本行不一定是一个句子或一个段落，因此把所有文本行拼接到一起</span></span><br><span class="line">corpus = [token <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line">vocab = Vocab(corpus)</span><br><span class="line">vocab.token_freqs[:<span class="number">10</span>]</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">[(<span class="string">&#x27;the&#x27;</span>, <span class="number">2261</span>),</span><br><span class="line"> (<span class="string">&#x27;i&#x27;</span>, <span class="number">1267</span>),</span><br><span class="line"> (<span class="string">&#x27;and&#x27;</span>, <span class="number">1245</span>),</span><br><span class="line"> (<span class="string">&#x27;of&#x27;</span>, <span class="number">1155</span>),</span><br><span class="line"> (<span class="string">&#x27;a&#x27;</span>, <span class="number">816</span>),</span><br><span class="line"> (<span class="string">&#x27;to&#x27;</span>, <span class="number">695</span>),</span><br><span class="line"> (<span class="string">&#x27;was&#x27;</span>, <span class="number">552</span>),</span><br><span class="line"> (<span class="string">&#x27;in&#x27;</span>, <span class="number">541</span>),</span><br><span class="line"> (<span class="string">&#x27;that&#x27;</span>, <span class="number">443</span>),</span><br><span class="line"> (<span class="string">&#x27;my&#x27;</span>, <span class="number">440</span>)]</span><br></pre></td></tr></table></figure><p>最流行的词看起来很无聊，这些词被称为<strong>停用词</strong>（stop words），因此可以被过滤掉。但它们本身仍然是有意义的，我们仍然会在模型中使用它们。另一个明显的问题是词频衰减的速度非常快，第$10$个还不到第$1$个的$1/5$。为了更好地理解，可以画出的词频图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">freqs = [freq <span class="keyword">for</span> token, freq <span class="keyword">in</span> vocab.token_freqs]</span><br><span class="line"><span class="comment"># d2l. </span></span><br><span class="line">plot(freqs, xlabel=<span class="string">&#x27;token: x&#x27;</span>, ylabel=<span class="string">&#x27;frequency: n(x)&#x27;</span>,</span><br><span class="line">         xscale=<span class="string">&#x27;log&#x27;</span>, yscale=<span class="string">&#x27;log&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article57/output_language-models-and-dataset_789d14_18_0.svg" alt="output"></p><p>可以发现：词频以一种明确的方式迅速衰减。将前几个单词作为例外消除后，剩余的所有单词大致遵循双对数坐标图上的一条直线。这意味着单词的频率满足<em>齐普夫定律</em>（Zipf’s law），即第$i$个最常用单词的频率$n_i$为：</p><script type="math/tex; mode=display">n_i \propto \frac{1}{i^\alpha},</script><p>等价于</p><script type="math/tex; mode=display">\log n_i = -\alpha \log i + c,</script><p>其中$\alpha$是刻画分布的指数，$c$是常数。这说明想要通过计数统计和平滑来建模单词是不可行的，这样建模的结果会大大高估尾部单词的频率，也就是所谓的不常用单词。那么其他的词元组合，比如二元语法、三元语法等等，又会如何呢？来看看二元语法的频率是否与一元语法的频率表现出相同的行为方式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一个去尾，一个掐头，然后zip在一起，读tuple出来</span></span><br><span class="line"><span class="comment"># 这样以后，每个tuple都是t时刻和t-1时刻的组合</span></span><br><span class="line">bigram_tokens = [pair <span class="keyword">for</span> pair <span class="keyword">in</span> <span class="built_in">zip</span>(corpus[:-<span class="number">1</span>], corpus[<span class="number">1</span>:])]</span><br><span class="line">bigram_vocab = Vocab(bigram_tokens)</span><br><span class="line">bigram_vocab.token_freqs[:<span class="number">10</span>]</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">[((<span class="string">&#x27;of&#x27;</span>, <span class="string">&#x27;the&#x27;</span>), <span class="number">309</span>),</span><br><span class="line"> ((<span class="string">&#x27;in&#x27;</span>, <span class="string">&#x27;the&#x27;</span>), <span class="number">169</span>),</span><br><span class="line"> ((<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;had&#x27;</span>), <span class="number">130</span>),</span><br><span class="line"> ((<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;was&#x27;</span>), <span class="number">112</span>),</span><br><span class="line"> ((<span class="string">&#x27;and&#x27;</span>, <span class="string">&#x27;the&#x27;</span>), <span class="number">109</span>),</span><br><span class="line"> ((<span class="string">&#x27;the&#x27;</span>, <span class="string">&#x27;time&#x27;</span>), <span class="number">102</span>),</span><br><span class="line"> ((<span class="string">&#x27;it&#x27;</span>, <span class="string">&#x27;was&#x27;</span>), <span class="number">99</span>),</span><br><span class="line"> ((<span class="string">&#x27;to&#x27;</span>, <span class="string">&#x27;the&#x27;</span>), <span class="number">85</span>),</span><br><span class="line"> ((<span class="string">&#x27;as&#x27;</span>, <span class="string">&#x27;i&#x27;</span>), <span class="number">78</span>),</span><br><span class="line"> ((<span class="string">&#x27;of&#x27;</span>, <span class="string">&#x27;a&#x27;</span>), <span class="number">73</span>)]</span><br></pre></td></tr></table></figure><p>这里值得注意：在十个最频繁的词对中，有九个是由两个停用词组成的， 只有一个与“the time”有关。 下面再进一步看看三元语法的频率是否表现出相同的行为方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 去尾两个、掐头去尾、掐头两个，然后zip在一起</span></span><br><span class="line"><span class="comment"># 对应t、t-1、t-2</span></span><br><span class="line">trigram_tokens = [triple <span class="keyword">for</span> triple <span class="keyword">in</span> <span class="built_in">zip</span>(</span><br><span class="line">    corpus[:-<span class="number">2</span>], corpus[<span class="number">1</span>:-<span class="number">1</span>], corpus[<span class="number">2</span>:])]</span><br><span class="line">trigram_vocab = Vocab(trigram_tokens)</span><br><span class="line">trigram_vocab.token_freqs[:<span class="number">10</span>]</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">[((<span class="string">&#x27;the&#x27;</span>, <span class="string">&#x27;time&#x27;</span>, <span class="string">&#x27;traveller&#x27;</span>), <span class="number">59</span>),</span><br><span class="line"> ((<span class="string">&#x27;the&#x27;</span>, <span class="string">&#x27;time&#x27;</span>, <span class="string">&#x27;machine&#x27;</span>), <span class="number">30</span>),</span><br><span class="line"> ((<span class="string">&#x27;the&#x27;</span>, <span class="string">&#x27;medical&#x27;</span>, <span class="string">&#x27;man&#x27;</span>), <span class="number">24</span>),</span><br><span class="line"> ((<span class="string">&#x27;it&#x27;</span>, <span class="string">&#x27;seemed&#x27;</span>, <span class="string">&#x27;to&#x27;</span>), <span class="number">16</span>),</span><br><span class="line"> ((<span class="string">&#x27;it&#x27;</span>, <span class="string">&#x27;was&#x27;</span>, <span class="string">&#x27;a&#x27;</span>), <span class="number">15</span>),</span><br><span class="line"> ((<span class="string">&#x27;here&#x27;</span>, <span class="string">&#x27;and&#x27;</span>, <span class="string">&#x27;there&#x27;</span>), <span class="number">15</span>),</span><br><span class="line"> ((<span class="string">&#x27;seemed&#x27;</span>, <span class="string">&#x27;to&#x27;</span>, <span class="string">&#x27;me&#x27;</span>), <span class="number">14</span>),</span><br><span class="line"> ((<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;did&#x27;</span>, <span class="string">&#x27;not&#x27;</span>), <span class="number">14</span>),</span><br><span class="line"> ((<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;saw&#x27;</span>, <span class="string">&#x27;the&#x27;</span>), <span class="number">13</span>),</span><br><span class="line"> ((<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;began&#x27;</span>, <span class="string">&#x27;to&#x27;</span>), <span class="number">13</span>)]</span><br></pre></td></tr></table></figure><p>直观地对比三种模型中的词元频率：一元语法、二元语法和三元语法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bigram_freqs = [freq <span class="keyword">for</span> token, freq <span class="keyword">in</span> bigram_vocab.token_freqs]</span><br><span class="line">trigram_freqs = [freq <span class="keyword">for</span> token, freq <span class="keyword">in</span> trigram_vocab.token_freqs]</span><br><span class="line"><span class="comment"># d2l. </span></span><br><span class="line">plot([freqs, bigram_freqs, trigram_freqs], xlabel=<span class="string">&#x27;token: x&#x27;</span>,</span><br><span class="line">         ylabel=<span class="string">&#x27;frequency: n(x)&#x27;</span>, xscale=<span class="string">&#x27;log&#x27;</span>, yscale=<span class="string">&#x27;log&#x27;</span>,</span><br><span class="line">         legend=[<span class="string">&#x27;unigram&#x27;</span>, <span class="string">&#x27;bigram&#x27;</span>, <span class="string">&#x27;trigram&#x27;</span>])</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article57/u-b-t-output.svg" alt="output"></p><p>这张图非常令人振奋！可以得到以下结论：<br>1、除了一元语法词，单词序列似乎也遵循齐普夫定律，尽管其中的指数$\alpha$更小（指数的大小受序列长度的影响）；<br>2、词表中$n$元组的数量并没有那么大，这说明语言中存在相当多的结构，这些结构给了我们应用模型的希望；<br>3、很多$n$元组很少出现，这使得拉普拉斯平滑非常不适合语言建模。作为代替，我们将使用<em>基于深度学习的模型</em>。</p><h3 id="3-4-读取长序列数据"><a href="#3-4-读取长序列数据" class="headerlink" title="3.4. 读取长序列数据"></a>3.4. 读取长序列数据</h3><p>由于序列数据本质上是连续的，在处理数据时需要解决这个问题。在<a href="#1-序列模型">文章的开头</a>我们以一种相当特别的方式做到了这一点：当序列变得太长而不能被模型一次性全部处理时，可能去拆分这样的序列方便模型读取。</p><p>介绍模型前说一下总体策略：假设使用神经网络来训练语言模型，模型中的网络一次处理具有预定义长度（例如$n$个时间步）的一个小批量序列。现在的问题是<em>如何随机生成一个小批量数据的特征和标签以供读取</em>。</p><p>首先，由于文本序列可以是任意长的（如整本《时光机器》），则任意长的序列可以被划分为具有相同时间步数的子序列。当训练神经网络时，这样的小批量子序列将被输入到模型中。假设网络一次只处理具有$n$个时间步的子序列。 下图画出了从原始文本序列获得子序列的所有不同的方式，其中$n=5$，并且每个时间步的词元对应一个字符。我们可以选择任意偏移量来指示初始位置，所以有相当大的自由度。</p><p><img src="/assets/post_img/article57/timemachine-5gram.svg" alt="分割文本时，不同的偏移量会导致不同的子序列"></p><p>那应该从图中选择哪一个呢？事实上他们都一样好。但如果我们只选择一个偏移量，那么用于训练网络的、所有可能的子序列的覆盖范围将是有限的。因此可以从随机偏移量开始划分序列，以同时获得<em>覆盖性</em>（coverage）和<em>随机性</em>（randomness）。下面将描述如何实现<em>随机采样</em>（random sampling）和<em>顺序分区</em>（sequential partitioning）策略。</p><h4 id="3-4-1-随机采样"><a href="#3-4-1-随机采样" class="headerlink" title="3.4.1. 随机采样"></a>3.4.1. 随机采样</h4><p>在随机采样中，每个样本都是在原始的长序列上任意捕获的子序列。 在迭代过程中，来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻。 对于语言建模，目标是基于到目前为止我们看到的词元来预测下一个词元， 因此标签是移位了一个词元的原始序列（右移，或者说后移）。</p><p>下面的代码每次可以从数据中随机生成一个小批量，参数batch_size指定了每个小批量中子序列样本的数目， 参数num_steps是每个子序列中预定义的时间步数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">seq_data_iter_random</span>(<span class="params">corpus, batch_size, num_steps</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;从数据中随机生成一个小批量&quot;&quot;&quot;</span> </span><br><span class="line">    <span class="comment"># 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1</span></span><br><span class="line">    corpus = corpus[random.randint(<span class="number">0</span>, num_steps - <span class="number">1</span>):]</span><br><span class="line">    <span class="comment"># 减去1，是因为我们需要考虑标签</span></span><br><span class="line">    <span class="comment"># 词数除去时间步长得到子序列的总数量</span></span><br><span class="line">    num_subseqs = (<span class="built_in">len</span>(corpus) - <span class="number">1</span>) // num_steps</span><br><span class="line">    <span class="comment"># 长度为num_steps的各个子序列的起始索引</span></span><br><span class="line">    initial_indices = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">0</span>, num_subseqs * num_steps, num_steps))</span><br><span class="line">    <span class="comment"># 在随机抽样的迭代过程中，</span></span><br><span class="line">    <span class="comment"># 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻</span></span><br><span class="line">    <span class="comment"># 也就是打乱起始索引列表</span></span><br><span class="line">    random.shuffle(initial_indices)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">data</span>(<span class="params">pos</span>):</span></span><br><span class="line">        <span class="comment"># 返回从pos位置开始的长度为num_steps的序列</span></span><br><span class="line">        <span class="comment"># 注意corpus是存储词对应索引的列表，并非字符串</span></span><br><span class="line">        <span class="keyword">return</span> corpus[pos: pos + num_steps]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对子序列分批，子列数量除去批量大小得到批次数量</span></span><br><span class="line">    num_batches = num_subseqs // batch_size</span><br><span class="line">    <span class="comment"># 这里的range方式与之前相同，就是获取批次的起始索引</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, batch_size * num_batches, batch_size):</span><br><span class="line">        <span class="comment"># initial_indices包含子序列的随机起始索引</span></span><br><span class="line">        <span class="comment"># 获取当前批次对应的子序列起始索引</span></span><br><span class="line">        initial_indices_per_batch = initial_indices[i: i + batch_size]</span><br><span class="line">        <span class="comment"># 训练数据</span></span><br><span class="line">        X = [data(j) <span class="keyword">for</span> j <span class="keyword">in</span> initial_indices_per_batch]</span><br><span class="line">        <span class="comment"># 训练数据后移一个词元得到标签</span></span><br><span class="line">        Y = [data(j + <span class="number">1</span>) <span class="keyword">for</span> j <span class="keyword">in</span> initial_indices_per_batch]</span><br><span class="line">        <span class="comment"># 生成器</span></span><br><span class="line">        <span class="keyword">yield</span> torch.tensor(X), torch.tensor(Y)</span><br></pre></td></tr></table></figure><p>下面生成一个从$0$到$34$的序列。假设批量大小为$2$，时间步数为$5$，则可以生成$\lfloor (35 - 1) / 5 \rfloor= 6$个“特征－标签”子序列对。如果设置小批量大小为$2$，就只能得到$3$个小批量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">my_seq = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">35</span>))</span><br><span class="line"><span class="keyword">for</span> X, Y <span class="keyword">in</span> seq_data_iter_random(my_seq, batch_size=<span class="number">2</span>, num_steps=<span class="number">5</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;X: &#x27;</span>, X, <span class="string">&#x27;\nY:&#x27;</span>, Y)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">X:  tensor([[<span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>],</span><br><span class="line">        [ <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>]])</span><br><span class="line">Y: tensor([[<span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>],</span><br><span class="line">        [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>]])</span><br><span class="line">X:  tensor([[<span class="number">19</span>, <span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>],</span><br><span class="line">        [<span class="number">29</span>, <span class="number">30</span>, <span class="number">31</span>, <span class="number">32</span>, <span class="number">33</span>]])</span><br><span class="line">Y: tensor([[<span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>, <span class="number">24</span>],</span><br><span class="line">        [<span class="number">30</span>, <span class="number">31</span>, <span class="number">32</span>, <span class="number">33</span>, <span class="number">34</span>]])</span><br><span class="line">X:  tensor([[ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>],</span><br><span class="line">        [<span class="number">24</span>, <span class="number">25</span>, <span class="number">26</span>, <span class="number">27</span>, <span class="number">28</span>]])</span><br><span class="line">Y: tensor([[ <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>],</span><br><span class="line">        [<span class="number">25</span>, <span class="number">26</span>, <span class="number">27</span>, <span class="number">28</span>, <span class="number">29</span>]])</span><br></pre></td></tr></table></figure><h4 id="3-4-2-顺序分区"><a href="#3-4-2-顺序分区" class="headerlink" title="3.4.2. 顺序分区"></a>3.4.2. 顺序分区</h4><p>在迭代过程中，除了对原始序列可以随机抽样外，还可以保证两个相邻的小批量中的子序列在原始序列上也是相邻的。 这种策略在基于小批量的迭代过程中保留了拆分的子序列的顺序，因此称为顺序分区。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">seq_data_iter_sequential</span>(<span class="params">corpus, batch_size, num_steps</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用顺序分区生成一个小批量子序列&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 从随机偏移量开始划分序列</span></span><br><span class="line">    offset = random.randint(<span class="number">0</span>, num_steps)</span><br><span class="line">    <span class="comment"># 可用词元的数量</span></span><br><span class="line">    num_tokens = ((<span class="built_in">len</span>(corpus) - offset - <span class="number">1</span>) // batch_size) * batch_size</span><br><span class="line">    <span class="comment"># 所有特征</span></span><br><span class="line">    Xs = torch.tensor(corpus[offset: offset + num_tokens])</span><br><span class="line">    <span class="comment"># 所有标签</span></span><br><span class="line">    Ys = torch.tensor(corpus[offset + <span class="number">1</span>: offset + <span class="number">1</span> + num_tokens])</span><br><span class="line">    <span class="comment"># 分批，第二维中对应各批次的数据（其实是索引）</span></span><br><span class="line">    Xs, Ys = Xs.reshape(batch_size, -<span class="number">1</span>), Ys.reshape(batch_size, -<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 这里其实写的有些奇怪，批次数量num_batches和批次大小事实上都是</span></span><br><span class="line">    <span class="comment"># 相对于子列数量而言的，而上一步直接在原始数据和批次大小间建立联系</span></span><br><span class="line">    <span class="comment"># 不过无所谓，最终只是要获取批次数量的值而已，该值的公式：</span></span><br><span class="line">    <span class="comment"># 批次数量 = 子列数量//批次大小</span></span><br><span class="line">    <span class="comment"># 子列数量 = 原始数据//时间步长</span></span><br><span class="line">    num_batches = Xs.shape[<span class="number">1</span>] // num_steps</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_steps * num_batches, num_steps):</span><br><span class="line">        X = Xs[:, i: i + num_steps]</span><br><span class="line">        Y = Ys[:, i: i + num_steps]</span><br><span class="line">        <span class="keyword">yield</span> X, Y</span><br></pre></td></tr></table></figure><p>基于相同的设置，通过顺序分区读取每个小批量的子序列的特征X和标签Y。 可以看到迭代期间来自两个相邻的小批量中的子序列在原始序列中确实是相邻的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> X, Y <span class="keyword">in</span> seq_data_iter_sequential(my_seq, batch_size=<span class="number">2</span>, num_steps=<span class="number">5</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;X: &#x27;</span>, X, <span class="string">&#x27;\nY:&#x27;</span>, Y)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">X:  tensor([[ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>],</span><br><span class="line">        [<span class="number">19</span>, <span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>]])</span><br><span class="line">Y: tensor([[ <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>],</span><br><span class="line">        [<span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>, <span class="number">24</span>]])</span><br><span class="line">X:  tensor([[ <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>],</span><br><span class="line">        [<span class="number">24</span>, <span class="number">25</span>, <span class="number">26</span>, <span class="number">27</span>, <span class="number">28</span>]])</span><br><span class="line">Y: tensor([[<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>],</span><br><span class="line">        [<span class="number">25</span>, <span class="number">26</span>, <span class="number">27</span>, <span class="number">28</span>, <span class="number">29</span>]])</span><br><span class="line">X:  tensor([[<span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>],</span><br><span class="line">        [<span class="number">29</span>, <span class="number">30</span>, <span class="number">31</span>, <span class="number">32</span>, <span class="number">33</span>]])</span><br><span class="line">Y: tensor([[<span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>],</span><br><span class="line">        [<span class="number">30</span>, <span class="number">31</span>, <span class="number">32</span>, <span class="number">33</span>, <span class="number">34</span>]])</span><br></pre></td></tr></table></figure><p>将上面的两个采样函数包装到一个类中， 以便稍后可以将其用作数据迭代器：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SeqDataLoader</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;加载序列数据的迭代器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, batch_size, num_steps, use_random_iter, max_tokens</span>):</span></span><br><span class="line">        <span class="keyword">if</span> use_random_iter:</span><br><span class="line">            self.data_iter_fn = seq_data_iter_random</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.data_iter_fn = seq_data_iter_sequential</span><br><span class="line">        self.corpus, self.vocab = load_corpus_time_machine(max_tokens)</span><br><span class="line">        self.batch_size, self.num_steps = batch_size, num_steps</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义 __iter__ 表示这个类是一个迭代器，只在迭代开始的时候运行一次</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)</span><br></pre></td></tr></table></figure><p>最后定义一个函数load_data_time_machine， 它同时返回数据迭代器和词表， 因此可以与其他带有load_data前缀的函数 （如d2l.load_data_fashion_mnist）类似地使用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_time_machine</span>(<span class="params">batch_size, num_steps, </span></span></span><br><span class="line"><span class="params"><span class="function">                           use_random_iter=<span class="literal">False</span>, max_tokens=<span class="number">10000</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回时光机器数据集的迭代器和词表&quot;&quot;&quot;</span></span><br><span class="line">    data_iter = SeqDataLoader(</span><br><span class="line">        batch_size, num_steps, use_random_iter, max_tokens)</span><br><span class="line">    <span class="keyword">return</span> data_iter, data_iter.vocab</span><br></pre></td></tr></table></figure><h2 id="4-循环神经网络"><a href="#4-循环神经网络" class="headerlink" title="4. 循环神经网络"></a>4. 循环神经网络</h2><p>上节中介绍了$n$元语法模型，其中单词$x_t$在时间步$t$的条件概率仅取决于前面$n-1$个单词。对于时间步$t-(n-1)$之前的单词，如果想将其可能产生的影响合并到$x_t$上，需要增加$n$的值，此时模型参数的数量也会随之呈指数增长，因为词表$\mathcal{V}$需要存储$|\mathcal{V}|^n$个数字，因此与其将$P(x_t \mid x_{t-1}, \ldots, x_{t-n+1})$模型化，不如使用隐变量模型：</p><script type="math/tex; mode=display">P(x_t \mid x_{t-1}, \ldots, x_1) \approx P(x_t \mid h_{t-1}),</script><p>其中$h_{t-1}$是<em>隐状态</em>（hidden state），也称为<em>隐藏变量</em>（hidden variable），它存储了到时间步$t-1$的序列信息。通常我们可以基于当前输入$x_{t}$和先前隐状态$h_{t-1}$来计算时间步$t$处的任何时间的隐状态：</p><script type="math/tex; mode=display">h_t = f(x_{t}, h_{t-1})</script><p>对于函数$f$，隐变量模型不是近似值（这句没看懂）。$h_t$可以存储到目前为止观察到的所有数据，但这样的操作可能会使计算和存储的代价提高。</p><p>值得注意的是，具有隐藏单元的隐藏层和隐状态是两个截然不同的概念。隐藏层是在从输入到输出的路径上（主要以观测角度来理解，实际上也是层）的隐藏的层，而隐状态则是在给定步骤所做的任何事情（是以技术角度来定义，存在精心设计）的<em>输入</em>，并且这些状态只能通过先前时间步的数据来计算。</p><p><em>循环神经网络</em>（recurrent neural networks，RNNs）是具有隐状态的神经网络。在介绍循环神经网络模型之前，先回顾一下多层感知机模型。</p><h3 id="4-1-无隐状态的神经网络"><a href="#4-1-无隐状态的神经网络" class="headerlink" title="4.1. 无隐状态的神经网络"></a>4.1. 无隐状态的神经网络</h3><p>对单隐藏层的多层感知机。设隐藏层的激活函数为$\phi$，给定一个小批量样本$\mathbf{X} \in \mathbb{R}^{n \times d}$，其中批量大小为$n$，输入维度为$d$，则隐藏层的输出$\mathbf{H} \in \mathbb{R}^{n \times h}$通过下式计算：</p><script type="math/tex; mode=display">\mathbf{H} = \phi(\mathbf{X} \mathbf{W}_{xh} + \mathbf{b}_h)</script><p>上式中，隐藏层权重参数为$\mathbf{W}_{xh} \in \mathbb{R}^{d \times h}$，偏置参数为$\mathbf{b}_h \in \mathbb{R}^{1 \times h}$，以及隐藏单元的数目为$h$。因此求和时可以应用广播机制。接下来，将隐藏变量$\mathbf{H}$用作输出层的输入。输出层由下式给出：</p><script type="math/tex; mode=display">\mathbf{O} = \mathbf{H} \mathbf{W}_{hq} + \mathbf{b}_q,</script><p>其中，$\mathbf{O} \in \mathbb{R}^{n \times q}$是输出变量，$\mathbf{W}_{hq} \in \mathbb{R}^{h \times q}$是权重参数，$\mathbf{b}_q \in \mathbb{R}^{1 \times q}$是输出层的偏置参数。如果是分类问题，可以用$\text{softmax}(\mathbf{O})$来计算输出类别的概率分布。</p><p>对于这种网络，只要可以随机选择“特征-标签”对，并且通过自动微分和随机梯度下降能够学习网络参数就可以了。</p><h3 id="4-2-有隐状态的循环神经网络"><a href="#4-2-有隐状态的循环神经网络" class="headerlink" title="4.2. 有隐状态的循环神经网络"></a>4.2. 有隐状态的循环神经网络</h3><p>有了隐状态后，情况就完全不同了。假设在时间步$t$有小批量输入$\mathbf{X}_t \in \mathbb{R}^{n \times d}$。也就是对$n$个序列样本的小批量，$\mathbf{X}_t$的每一行对应于来自该序列的时间步$t$处的一个样本（$n \times d$)。接下来，用$\mathbf{H}_t  \in \mathbb{R}^{n \times h}$表示时间步$t$的隐藏变量。与多层感知机不同的是，这里保存了前一个时间步的隐藏变量$\mathbf{H}_{t-1}$，并引入了一个新的权重参数$\mathbf{W}_{hh} \in \mathbb{R}^{h \times h}$，来描述如何在当前时间步中使用前一个时间步的隐藏变量。当前时间步隐藏变量由 当前时间步的输入 与 前一个时间步的隐藏变量 一起计算得出：</p><script type="math/tex; mode=display">\mathbf{H}_t = \phi(\mathbf{X}_t \mathbf{W}_{xh} + \mathbf{H}_{t-1} \mathbf{W}_{hh}  + \mathbf{b}_h)</script><p>与无隐状态的情况相比，上式多添加了一项$\mathbf{H}_{t-1} \mathbf{W}_{hh}$，从而实例化了开头提到的公式：$h_t = f(x_{t}, h_{t-1})$。从相邻时间步的隐藏变量$\mathbf{H}_t$和$\mathbf{H}_{t-1}$之间的关系可知，这些变量捕获并保留了（通过作为参数参与下一次计算的方式）序列直到其当前时间步的历史信息，如同当前时间步下神经网络的状态或记忆，因此这些隐藏变量被称为<em>隐状态</em>（hidden state）。由于在当前时间步中，隐状态使用的定义与前一个时间步中使用的定义相同，所以上式中的计算是<em>循环的</em>（recurrent）。基于循环计算的隐状态神经网络被命名为<em>循环神经网络</em>（recurrent neural network）。在循环神经网络中执行上式隐状态计算的层称为<em>循环层</em>（recurrent layer）。</p><p>有许多不同的方法可以构建循环神经网络，上式定义的隐状态的循环神经网络是其中常见的一种。对于时间步$t$，输出层的输出类似于多层感知机中的计算：</p><script type="math/tex; mode=display">\mathbf{O}_t = \mathbf{H}_t \mathbf{W}_{hq} + \mathbf{b}_q</script><p>循环神经网络的参数包括隐藏层的权重$\mathbf{W}_{xh} \in \mathbb{R}^{d \times h}, \mathbf{W}_{hh} \in \mathbb{R}^{h \times h}$和偏置$\mathbf{b}_h \in \mathbb{R}^{1 \times h}$，以及输出层的权重$\mathbf{W}_{hq} \in \mathbb{R}^{h \times q}$和偏置$\mathbf{b}_q \in \mathbb{R}^{1 \times q}$。在不同的时间步上，循环神经网络也使用这些相同模型参数。因此，<strong>循环神经网络的参数开销不会随着时间步的增加而增加</strong>。</p><p>下图展示了循环神经网络在三个相邻时间步的计算逻辑。在任意时间步$t$，隐状态的计算可以被视为：</p><ol><li>拼接当前时间步$t$的输入$\mathbf{X}_t$和前一时间步$t-1$的隐状态$\mathbf{H}_{t-1}$；</li><li>将拼接的结果送入带有激活函数$\phi$的全连接层。全连接层的输出是当前时间步$t$的隐状态$\mathbf{H}_t$。</li></ol><p>图中，模型参数是$\mathbf{W}_{xh}$和$\mathbf{W}_{hh}$的拼接（concat），以及$\mathbf{b}_h$的偏置。当前时间步$t$的隐状态$\mathbf{H}_t$将参与计算下一时间步$t+1$的隐状态$\mathbf{H}_{t+1}$。而且$\mathbf{H}_t$还将送入全连接输出层，用于计算当前时间步$t$的输出$\mathbf{O}_t$。</p><p><img src="/assets/post_img/article57/rnn.svg" alt="具有隐状态的循环神经网络"></p><p>刚才提到，隐状态中$\mathbf{X}_t \mathbf{W}_{xh} + \mathbf{H}_{t-1} \mathbf{W}_{hh}$的计算，相当于$\mathbf{X}_t$和$\mathbf{H}_{t-1}$的拼接与$\mathbf{W}_{xh}$和$\mathbf{W}_{hh}$的拼接的矩阵乘法。这个性质可以通过数学证明，下面使用一个简单的代码来说明一下。</p><p>首先定义矩阵<code>X</code>、<code>W_xh</code>、<code>H</code>和<code>W_hh</code>，它们的形状分别为$(3，1)$、$(1，4)$、$(3，4)$和$(4，4)$。分别将<code>X</code>乘以<code>W_xh</code>，将<code>H</code>乘以<code>W_hh</code>，然后将这两个乘法相加，得到一个形状为$(3，4)$的矩阵。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">X, W_xh = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">3</span>, <span class="number">1</span>)), torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">1</span>, <span class="number">4</span>))</span><br><span class="line">H, W_hh = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">3</span>, <span class="number">4</span>)), torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">torch.matmul(X, W_xh) + torch.matmul(H, W_hh)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[-<span class="number">1.4258</span>,  <span class="number">1.8849</span>, -<span class="number">1.2227</span>, -<span class="number">3.2763</span>],</span><br><span class="line">        [ <span class="number">0.5912</span>, -<span class="number">0.8081</span>,  <span class="number">0.6962</span>, -<span class="number">0.0819</span>],</span><br><span class="line">        [-<span class="number">3.9654</span>,  <span class="number">1.2145</span>, -<span class="number">4.2720</span>, -<span class="number">0.6869</span>]])</span><br></pre></td></tr></table></figure><p>现在沿列（轴1）拼接矩阵<code>X</code>和<code>H</code>，沿行（轴0）拼接矩阵<code>W_xh</code>和<code>W_hh</code>。这两个拼接分别产生形状$(3, 5)$和形状$(5, 4)$的矩阵。再将这两个拼接的矩阵相乘，可得到与上面相同形状$(3, 4)$的输出矩阵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">torch.matmul(torch.cat((X, H), <span class="number">1</span>), torch.cat((W_xh, W_hh), <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[-<span class="number">1.4258</span>,  <span class="number">1.8849</span>, -<span class="number">1.2227</span>, -<span class="number">3.2763</span>],</span><br><span class="line">        [ <span class="number">0.5912</span>, -<span class="number">0.8081</span>,  <span class="number">0.6962</span>, -<span class="number">0.0819</span>],</span><br><span class="line">        [-<span class="number">3.9654</span>,  <span class="number">1.2145</span>, -<span class="number">4.2720</span>, -<span class="number">0.6869</span>]])</span><br></pre></td></tr></table></figure><h3 id="4-3-基于循环神经网络的字符级语言模型"><a href="#4-3-基于循环神经网络的字符级语言模型" class="headerlink" title="4.3. 基于循环神经网络的字符级语言模型"></a>4.3. 基于循环神经网络的字符级语言模型</h3><p>回想一下<a href="#3-语言模型和数据集">3</a>中的语言模型，我们的目标是根据过去的和当前的词元预测下一个词元，因此将原始序列移位一个词元作为标签。Bengio等人首先提出使用神经网络进行语言建模。下面看一下如何使用循环神经网络来构建语言模型。设小批量大小为1，批量中的文本序列为“machine”。为简化后续的训练，此处使用<em>字符级语言模型</em>（character-level language model），将文本词元化为字符而不是单词。 下图演示了如何通过基于字符级语言建模的循环神经网络，使用当前的和先前的字符预测下一个字符。</p><p><img src="/assets/post_img/article57/rnn-train.svg" alt="基于循环神经网络的字符级语言模型：输入序列和标签序列分别为“machin”和“achine”"></p><p>在训练过程中，我们对每个时间步的输出层的输出进行softmax操作，然后利用交叉熵损失计算模型输出和标签之间的误差。由于隐藏层中隐状态的循环计算，上图中的第$3$个时间步的输出$\mathbf{O}_3$由文本序列“m”“a”和“c”确定。由于训练数据中这个文本序列的下一个字符是“h”，因此第$3$个时间步的损失将取决于下一个字符的概率分布，而下一个字符是基于特征序列“m”“a”“c”和这个时间步的标签“h”生成的。<br>（Personal Statement：这里的意思应该是损失是由与标签对比后计算得到的，而损失会影响模型参数的更新，进一步影响模型的预测结果，而不是说标签会直接影响，因为模型的根本目的是预测下一字符是什么。正确的说法应该是“下一个字符是基于特征序列‘m’‘a’‘c’和损失值生成的”）</p><p>在实践中使用的批量大小$n&gt;1$，每个词元都由一个$d$维向量表示。因此，在时间步$t$输入$\mathbf X_t$将是一个$n\times d$矩阵，这与在<a href="#42-有隐状态的循环神经网络">4.2</a>中的讨论相同，也与<a href="#12-训练">1.2</a>中近似。</p><h3 id="4-4-困惑度（Perplexity）"><a href="#4-4-困惑度（Perplexity）" class="headerlink" title="4.4. 困惑度（Perplexity）"></a>4.4. 困惑度（Perplexity）</h3><p>现在来讨论如何度量语言模型的质量，这将用于评估基于循环神经网络的模型。一个好的语言模型能够用高度准确的词元来预测我们接下来会看到什么。请看下列由不同的语言模型给出的对“It is raining …”的续写：</p><ol><li>“It is raining outside”（外面下雨了）；</li><li>“It is raining banana tree”（香蕉树下雨了）；</li><li>“It is raining piouw;kcj pwepoiut”。</li></ol><p>就质量而言，例$1$显然是最合乎情理、在逻辑上最连贯的。虽然这个模型可能没有很准确地反映出后续词的语义，比如，“It is raining in San Francisco”（旧金山下雨了）和“It is raining in winter”（冬天下雨了）可能才是更完美的合理扩展，但该模型已经能够捕捉到跟在后面的是哪类单词。例$2$则要糟糕得多，因为其产生了一个无意义的续写。尽管如此，至少该模型已经学会了如何拼写单词，以及单词之间的某种程度的相关性。最后，例$3$表明了训练不足的模型是无法正确地拟合数据的。</p><p>或许可以通过计算序列的似然概率来度量模型的质量，但这是一个难以理解、难以比较的数字。较短的序列比较长的序列更有可能出现，因此评估模型产生长篇巨著的可能性会比产生中篇小说的要小得多。</p><p>信息论这时可以派上用场了。前文在引入softmax回归时定义了熵、交叉熵，并在<a href="https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/information-theory.html">信息论在线附录</a>中讨论了更多的信息论知识（还没看）。如果想要压缩文本，我们可以根据当前词元集预测的下一个词元。一个更好的语言模型应该能更准确地预测下一个词元。因此，它应该允许我们在压缩序列时花费更少的比特。所以可以通过一个序列中所有的$n$个词元的交叉熵损失的平均值来衡量：</p><script type="math/tex; mode=display">\frac{1}{n} \sum_{t=1}^n -\log P(x_t \mid x_{t-1}, \ldots, x_1)</script><p>其中$P$由语言模型给出，$x_t$是在时间步$t$从该序列中观察到的实际词元。这使得不同长度的文档的性能具有了可比性。由于历史原因，自然语言处理的科学家更喜欢使用<em>困惑度</em>（perplexity）来表达，它是上式的指数：</p><script type="math/tex; mode=display">\exp\left(-\frac{1}{n} \sum_{t=1}^n \log P(x_t \mid x_{t-1}, \ldots, x_1)\right)</script><p>困惑度（perplexity）的基本思想是：给测试集的句子赋予较高概率值的语言模型较好,当语言模型训练完之后，测试集中的句子都是正常的句子，那么训练好的模型就是在测试集上的概率越高越好，此概率越高，困惑度越小，越“收敛”。</p><p>困惑度是“下一个词元的实际选择数的调和平均数”，请看下方案例。</p><ul><li>在最好的情况下，模型总是完美地估计标签词元的概率为1。在这种情况下，模型的困惑度为1。</li><li>在最坏的情况下，模型总是预测标签词元的概率为0。在这种情况下，困惑度是正无穷大。</li><li>在基准线情况下，模型的预测是词表的所有可用词元上的均匀分布。这种情况下，困惑度等于词表中唯一词元的数量。如果在没有任何压缩的情况下存储序列，这将是我们能做的最好的编码方式。因此这种方式提供了一个重要的下限，任何实际模型都必须超越这个下限。</li></ul><p>调和平均数<br>: 调和平均数（harmonic mean）又称倒数平均数，是总体各统计变量倒数的算术平均数的倒数。简单调和平均数的公式为：<script type="math/tex">H_{n}=\frac{1}{\frac{1}{n} \sum_{i=1}^{n} \frac{1}{x_{i}}}=\frac{n}{\sum_{i=1}^{n} \frac{1}{x_{i}}}</script></p><p>关于基准线情况的补充，有种说法是这样的:</p><blockquote><p>在看到一个语言模型报告其perplexity是109时，我们可以直观的理解为，平均情况下，这个语言模型预测下一个词时，其认为有109个词等可能地可以作为下一个词的合理选择。</p></blockquote><h2 id="5-循环神经网络的从零实现"><a href="#5-循环神经网络的从零实现" class="headerlink" title="5. 循环神经网络的从零实现"></a>5. 循环神经网络的从零实现</h2><p>从头开始基于循环神经网络实现字符级语言模型，将在H.G.Wells的时光机器数据集上训练。详细代码实现参见对应实践。</p><h3 id="5-1-独热编码"><a href="#5-1-独热编码" class="headerlink" title="5.1. 独热编码"></a>5.1. 独热编码</h3><p>在train_iter中，每个词元都表示为一个数字索引， 将这些索引直接输入神经网络可能会使学习变得困难。 通常将每个词元表示为更具表现力的特征向量。 最简单的表示为独热编码（one-hot encoding）。</p><p>简言之，将每个索引映射为相互不同的单位向量：假设词表中不同词元的数目为$N$（即<code>len(vocab)</code>），词元索引的范围为$0$到$N-1$。如果词元的索引是整数$i$，那么我们将创建一个长度为$N$的全$0$向量，并将第$i$处的元素设置为$1$。此向量是原始词元的一个独热向量。</p><p>每次采样的小批量数据形状是二维张量： （批量大小，时间步数）。 one_hot函数将这样一个小批量数据转换成三维张量， 张量的最后一个维度等于词表大小（len(vocab)）。 通常会转换输入的维度以获得形状为（时间步数，批量大小，词表大小）的输出。 这将使我们能够更方便地通过最外层的维度， 一步一步地更新小批量数据的隐状态。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">X = torch.arange(<span class="number">10</span>).reshape((<span class="number">2</span>, <span class="number">5</span>))</span><br><span class="line">F.one_hot(X.T, <span class="number">28</span>).shape</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">torch.Size([<span class="number">5</span>, <span class="number">2</span>, <span class="number">28</span>])</span><br></pre></td></tr></table></figure><h3 id="5-2-初始化模型参数"><a href="#5-2-初始化模型参数" class="headerlink" title="5.2. 初始化模型参数"></a>5.2. 初始化模型参数</h3><p>初始化循环神经网络模型的模型参数。 隐藏单元数num_hiddens是一个可调的超参数。 当训练语言模型时，输入和输出来自相同的词表。 因此，它们具有相同的维度，即词表的大小。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_params</span>(<span class="params">vocab_size, num_hiddens, device</span>):</span></span><br><span class="line">    num_inputs = num_outputs = vocab_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">normal</span>(<span class="params">shape</span>):</span></span><br><span class="line">        <span class="keyword">return</span> torch.randn(size=shape, device=device) * <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 隐藏层参数</span></span><br><span class="line">    W_xh = normal((num_inputs, num_hiddens))</span><br><span class="line">    W_hh = normal((num_hiddens, num_hiddens))</span><br><span class="line">    b_h = torch.zeros(num_hiddens, device=device)</span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = normal((num_hiddens, num_outputs))</span><br><span class="line">    b_q = torch.zeros(num_outputs, device=device)</span><br><span class="line">    <span class="comment"># 附加梯度</span></span><br><span class="line">    params = [W_xh, W_hh, b_h, W_hq, b_q]</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> params</span><br></pre></td></tr></table></figure><h3 id="5-3-循环神经网络模型"><a href="#5-3-循环神经网络模型" class="headerlink" title="5.3. 循环神经网络模型"></a>5.3. 循环神经网络模型</h3><p>定义循环神经网络模型， 首先需要一个init_rnn_state函数在初始化时返回隐状态。 这个函数的返回是一个张量，张量全用0填充，形状为（批量大小，隐藏单元数）。 后面的章节中会遇到隐状态包含多个变量的情况，使用元组可以更容易地处理些。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_rnn_state</span>(<span class="params">batch_size, num_hiddens, device</span>):</span></span><br><span class="line">    <span class="keyword">return</span> (torch.zeros((batch_size, num_hiddens), device=device), )</span><br></pre></td></tr></table></figure><p>下面的<code>rnn</code>函数定义了如何在一个时间步内计算隐状态和输出。循环神经网络模型通过<code>inputs</code>最外层的维度实现循环，以便逐时间步更新小批量数据的隐状态<code>H</code>。此外，这里使用$\tanh$函数作为激活函数。当元素在实数上满足均匀分布时，$\tanh$函数的平均值为0。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn</span>(<span class="params">inputs, state, params</span>):</span></span><br><span class="line">    <span class="comment"># inputs的形状：(时间步数量，批量大小，词表大小)</span></span><br><span class="line">    W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class="line">    H, = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="comment"># X的形状：(批量大小，词表大小)</span></span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)</span><br><span class="line">        Y = torch.mm(H, W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">0</span>), (H,)</span><br></pre></td></tr></table></figure><p>创建一个类来包装这些函数， 并存储从零开始实现的循环神经网络模型的参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNNModelScratch</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;从零开始实现的循环神经网络模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, num_hiddens, device,</span></span></span><br><span class="line"><span class="params"><span class="function">                 get_params, init_state, forward_fn</span>):</span></span><br><span class="line">        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens</span><br><span class="line">        self.params = get_params(vocab_size, num_hiddens, device)</span><br><span class="line">        self.init_state, self.forward_fn = init_state, forward_fn</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, X, state</span>):</span></span><br><span class="line">        X = F.one_hot(X.T, self.vocab_size).<span class="built_in">type</span>(torch.float32)</span><br><span class="line">        <span class="keyword">return</span> self.forward_fn(X, state, self.params)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">begin_state</span>(<span class="params">self, batch_size, device</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.init_state(batch_size, self.num_hiddens, device)</span><br></pre></td></tr></table></figure><h3 id="5-4-预测"><a href="#5-4-预测" class="headerlink" title="5.4. 预测"></a>5.4. 预测</h3><p>首先定义预测函数来生成<code>prefix</code>之后的新字符，<code>prefix</code>是一个用户提供的包含多个字符的字符串。在循环遍历<code>prefix</code>中的开始字符时，我们不断地将隐状态传递到下一个时间步，但是不生成任何输出。这被称为<em>预热</em>（warm-up）期，因为在此期间模型会自我更新（例如，更新隐状态），但不会进行预测。预热结束后，隐状态的值通常比刚开始的初始值更适合预测，从而预测字符并输出它们。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_ch8</span>(<span class="params">prefix, num_preds, net, vocab, device</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;在prefix后面生成新字符&quot;&quot;&quot;</span></span><br><span class="line">    state = net.begin_state(batch_size=<span class="number">1</span>, device=device)</span><br><span class="line">    outputs = [vocab[prefix[<span class="number">0</span>]]]</span><br><span class="line">    get_input = <span class="keyword">lambda</span>: torch.tensor([outputs[-<span class="number">1</span>]], device=device).reshape((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> prefix[<span class="number">1</span>:]:  <span class="comment"># 预热期</span></span><br><span class="line">        _, state = net(get_input(), state)</span><br><span class="line">        outputs.append(vocab[y])</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_preds):  <span class="comment"># 预测num_preds步</span></span><br><span class="line">        y, state = net(get_input(), state)</span><br><span class="line">        outputs.append(<span class="built_in">int</span>(y.argmax(dim=<span class="number">1</span>).reshape(<span class="number">1</span>)))</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;&#x27;</span>.join([vocab.idx_to_token[i] <span class="keyword">for</span> i <span class="keyword">in</span> outputs])</span><br></pre></td></tr></table></figure><h3 id="5-5-梯度裁剪"><a href="#5-5-梯度裁剪" class="headerlink" title="5.5. 梯度裁剪"></a>5.5. 梯度裁剪</h3><p>对于上述的循环神经网络，不仅有纵向的深度（输入到输出），还有横向的“深度”（从第一个时间步到末时间步），对于长度为$T$的序列，在迭代中计算这$T$个时间步上的梯度，将会在反向传播过程中产生长度为$\mathcal{O}(T)$的矩阵乘法链。当$T$较大时，它可能导致数值不稳定，例如梯度爆炸或梯度消失。因此，循环神经网络模型往往需要额外的方式来支持稳定训练。</p><p>一般在通过梯度下降解决优化问题（优化某个目标）时，采用迭代方式更新模型参数，更新操作是对参数向量$\mathbf{x}$，将其推向负梯度$\mathbf{g}$的方向上（在随机梯度下降中该梯度在随机抽样的小批量中计算）。例如，使用$\eta &gt; 0$作为学习率时，在一次迭代中，我们将$\mathbf{x}$更新为$\mathbf{x} - \eta \mathbf{g}$。此时进一步假设目标函数$f$表现良好，即函数$f$在常数$L$下是<em>利普希茨连续的</em>（Lipschitz continuous）。也就是说，对于任意$\mathbf{x}$和$\mathbf{y}$有：</p><script type="math/tex; mode=display">|f(\mathbf{x}) - f(\mathbf{y})| \leq L \|\mathbf{x} - \mathbf{y}\|</script><p>如果我们通过$\eta \mathbf{g}$更新参数向量，则目标值的变化取决于学习率、梯度的范数和$L$：</p><script type="math/tex; mode=display">|f(\mathbf{x}) - f(\mathbf{x} - \eta\mathbf{g})| \leq L \eta\|\mathbf{g}\|</script><p>这意味着目标的变化不会超过$L \eta |\mathbf{g}|$。这个上限的值较小既是坏事也是好事。坏的方面，它限制了取得进展的速度；好的方面，它限制了事情变糟的程度，尤其当我们朝着错误的方向前进时。</p><p>有时梯度可能很大（梯度爆炸），优化算法可能无法收敛，可以通过降低$\eta$的学习率来解决这个问题。但如果很少得到大的梯度时，不可能对所有情况采取降低学习率的方式，这个做法会减缓我们在所有步骤中的进展，只是为了处理罕见的梯度爆炸事件。一个流行的替代方案是通过将梯度$\mathbf{g}$投影回给定半径（例如$\theta$）的球来<strong>裁剪梯度</strong>$\mathbf{g}$。如下式：</p><script type="math/tex; mode=display">\mathbf{g} \leftarrow \min\left(1, \frac{\theta}{\|\mathbf{g}\|}\right) \mathbf{g}</script><p>这样做后，梯度范数永远不会超过$\theta$，并且更新后的梯度完全与$\mathbf{g}$的原始方向对齐。它还有一个值得拥有的副作用，即限制任何给定的小批量数据（以及其中任何给定的样本）对参数向量的影响，这赋予了模型一定程度的稳定性。梯度裁剪提供了一个快速修复梯度爆炸的方法，虽然它并不能完全解决问题，但它是众多有效的技术之一。</p><p>下面定义一个函数来裁剪模型的梯度，在此计算了所有模型参数的梯度的范数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad_clipping</span>(<span class="params">net, theta</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;裁剪梯度&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module):</span><br><span class="line">        params = [p <span class="keyword">for</span> p <span class="keyword">in</span> net.parameters() <span class="keyword">if</span> p.requires_grad]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        params = net.params</span><br><span class="line">    <span class="comment"># 范数，平方和</span></span><br><span class="line">    norm = torch.sqrt(<span class="built_in">sum</span>(torch.<span class="built_in">sum</span>((p.grad ** <span class="number">2</span>)) <span class="keyword">for</span> p <span class="keyword">in</span> params))</span><br><span class="line">    <span class="keyword">if</span> norm &gt; theta:</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            param.grad[:] *= theta / norm</span><br></pre></td></tr></table></figure><h3 id="5-6-训练"><a href="#5-6-训练" class="headerlink" title="5.6. 训练"></a>5.6. 训练</h3><p>在训练模型之前，定义一个函数在一个迭代周期内训练模型。这与之前讲到的训练模型的方式有三个不同之处。</p><ol><li>序列数据的不同采样方法（随机采样和顺序分区）将导致隐状态初始化的差异。</li><li>在更新模型参数之前会裁剪梯度，这样操作的目的是，即使训练过程中某个点上发生了梯度爆炸，也能保证模型不会发散。</li><li>用困惑度来评价模型，这样的度量确保了不同长度的序列具有可比性。</li></ol><p>当使用顺序采样（顺序分区）时，我们只在每个迭代周期的开始位置初始化隐状态。由于下一个小批量数据中的第$i$个子序列样本与当前第$i$个子序列样本相邻，因此当前小批量数据最后一个样本的隐状态，将用于初始化下一个小批量数据第一个样本的隐状态。这样，存储在隐状态中的序列的历史信息可以在一个迭代周期内流经相邻的子序列。然而，在任何一点隐状态的计算，都依赖于同一迭代周期中前面所有的小批量数据，这使得梯度计算变得复杂。为了降低计算量，在处理任何一个小批量数据之前，通常先分离梯度，使得隐状态的梯度计算总是限制在一个小批量数据的时间步内。</p><p>当使用随机抽样时，因为每个样本都是在一个随机位置抽样的，因此需要为每个迭代周期重新初始化隐状态。与之前章节中的<code>train_epoch_ch3</code>函数相同，<code>updater</code>是更新模型参数的常用函数。它既可以是从头开始实现的<code>d2l.sgd</code>函数，也可以是深度学习框架中内置的优化函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_epoch_ch8</span>(<span class="params">net, train_iter, loss, updater, device, use_random_iter</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练网络一个迭代周期（定义见第8章）&quot;&quot;&quot;</span></span><br><span class="line">    state, timer = <span class="literal">None</span>, Timer()</span><br><span class="line">    metric = Accumulator(<span class="number">2</span>)  <span class="comment"># 训练损失之和,词元数量</span></span><br><span class="line">    <span class="keyword">for</span> X, Y <span class="keyword">in</span> train_iter:</span><br><span class="line">        <span class="keyword">if</span> state <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> use_random_iter:</span><br><span class="line">            <span class="comment"># 在第一次迭代或使用随机抽样时初始化state</span></span><br><span class="line">            state = net.begin_state(batch_size=X.shape[<span class="number">0</span>], device=device)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 已有隐状态，在处理小批量前先分离梯度。</span></span><br><span class="line">            <span class="comment"># tensor.detach_()将一个tensor从创建它的图中分离，并把它设置成叶子tensor。是对tensor本身的更改。</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module) <span class="keyword">and</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(state, <span class="built_in">tuple</span>):</span><br><span class="line">                <span class="comment"># state对于nn.GRU是个张量</span></span><br><span class="line">                state.detach_()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># state对于nn.LSTM或对于我们从零开始实现的模型是个张量</span></span><br><span class="line">                <span class="keyword">for</span> s <span class="keyword">in</span> state:</span><br><span class="line">                    s.detach_()</span><br><span class="line">        y = Y.T.reshape(-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 加载到显存</span></span><br><span class="line">        X, y = X.to(device), y.to(device)</span><br><span class="line">        y_hat, state = net(X, state)</span><br><span class="line">        l = loss(y_hat, y.long()).mean()</span><br><span class="line">        <span class="comment"># grad_clipping-梯度裁剪</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(updater, torch.optim.Optimizer):</span><br><span class="line">            updater.zero_grad()</span><br><span class="line">            l.backward()</span><br><span class="line">            grad_clipping(net, <span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 开始优化一次</span></span><br><span class="line">            updater.step()</span><br><span class="line">        <span class="comment"># 如果优化函数是非框架的实现</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            l.backward()</span><br><span class="line">            grad_clipping(net, <span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 因为已经调用了mean函数</span></span><br><span class="line">            updater(batch_size=<span class="number">1</span>)</span><br><span class="line">        metric.add(l * y.numel(), y.numel())</span><br><span class="line">    <span class="comment"># 返回 困惑度 和 速度</span></span><br><span class="line">    <span class="keyword">return</span> math.exp(metric[<span class="number">0</span>] / metric[<span class="number">1</span>]), metric[<span class="number">1</span>] / timer.stop()</span><br></pre></td></tr></table></figure><p>训练函数，既支持从零开始实现， 也可以使用高级API来实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_ch8</span>(<span class="params">net, train_iter, vocab, lr, num_epochs, device,</span></span></span><br><span class="line"><span class="params"><span class="function">              use_random_iter=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练模型（定义见第8章）&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 定义损失函数，交叉熵</span></span><br><span class="line">    loss = nn.CrossEntropyLoss()</span><br><span class="line">    animator = Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;perplexity&#x27;</span>,</span><br><span class="line">                            legend=[<span class="string">&#x27;train&#x27;</span>], xlim=[<span class="number">10</span>, num_epochs])</span><br><span class="line">    <span class="comment"># 初始化，设定参数优化器（小批量随机梯度下降），指定参数表和学习率。</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module):</span><br><span class="line">        updater = torch.optim.SGD(net.parameters(), lr)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        updater = <span class="keyword">lambda</span> batch_size: sgd(net.params, lr, batch_size)</span><br><span class="line">    predict = <span class="keyword">lambda</span> prefix: predict_ch8(prefix, <span class="number">50</span>, net, vocab, device)</span><br><span class="line">    <span class="comment"># 训练和预测</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        ppl, speed = train_epoch_ch8(</span><br><span class="line">            net, train_iter, loss, updater, device, use_random_iter)</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(predict(<span class="string">&#x27;time traveller&#x27;</span>))</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>, [ppl])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;困惑度 <span class="subst">&#123;ppl:<span class="number">.1</span>f&#125;</span>, <span class="subst">&#123;speed:<span class="number">.1</span>f&#125;</span> 词元/秒 <span class="subst">&#123;<span class="built_in">str</span>(device)&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(predict(<span class="string">&#x27;time traveller&#x27;</span>))</span><br><span class="line">    <span class="built_in">print</span>(predict(<span class="string">&#x27;traveller&#x27;</span>))</span><br></pre></td></tr></table></figure><p>然后开始训练即可，因为在数据集中只使用了10000个词元， 所以模型需要更多的迭代周期来更好地收敛。</p><h2 id="6-循环神经网络的框架实现"><a href="#6-循环神经网络的框架实现" class="headerlink" title="6. 循环神经网络的框架实现"></a>6. 循环神经网络的框架实现</h2><p>使用深度学习框架的高级API提供的函数更有效地实现相同的语言模型。</p><h3 id="6-1-读取数据集"><a href="#6-1-读取数据集" class="headerlink" title="6.1. 读取数据集"></a>6.1. 读取数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">train_iter, vocab = load_data_time_machine(batch_size, num_steps)</span><br></pre></td></tr></table></figure><h3 id="6-2-定义模型"><a href="#6-2-定义模型" class="headerlink" title="6.2. 定义模型"></a>6.2. 定义模型</h3><p>构造一个具有256个隐藏单元的单隐藏层的循环神经网络层rnn_layer。虽然目前还没有讨论多层循环神经网络的意义。目前仅需要将多层理解为一层循环神经网络的输出被用作下一层循环神经网络的输入就足够了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens = <span class="number">256</span></span><br><span class="line">rnn_layer = nn.RNN(<span class="built_in">len</span>(vocab), num_hiddens)</span><br></pre></td></tr></table></figure><p>使用张量来初始化隐状态，它的形状是（隐藏层数，批量大小，隐藏单元数）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">state = torch.zeros((<span class="number">1</span>, batch_size, num_hiddens))</span><br><span class="line">state.shape</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">32</span>, <span class="number">256</span>])</span><br></pre></td></tr></table></figure><p>通过一个隐状态和一个输入，就可以用更新后的隐状态计算输出。 需要强调的是，rnn_layer的“输出”（Y）不涉及输出层的计算： 它是指每个时间步的隐状态，这些隐状态可以用作后续输出层的输入。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(size=(num_steps, batch_size, <span class="built_in">len</span>(vocab)))</span><br><span class="line">Y, state_new = rnn_layer(X, state)</span><br><span class="line">Y.shape, state_new.shape</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">(torch.Size([<span class="number">35</span>, <span class="number">32</span>, <span class="number">256</span>]), torch.Size([<span class="number">1</span>, <span class="number">32</span>, <span class="number">256</span>]))</span><br></pre></td></tr></table></figure><p>为一个完整的循环神经网络模型定义了一个RNNModel类。 由于rnn_layer只包含隐藏的循环层，我们还需要创建一个单独的输出层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNNModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;循环神经网络模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, rnn_layer, vocab_size, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(RNNModel, self).__init__(**kwargs)</span><br><span class="line">        self.rnn = rnn_layer</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.num_hiddens = self.rnn.hidden_size</span><br><span class="line">        <span class="comment"># 如果RNN是双向的，num_directions应该是2，否则应该是1</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.rnn.bidirectional:</span><br><span class="line">            self.num_directions = <span class="number">1</span></span><br><span class="line">            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.num_directions = <span class="number">2</span></span><br><span class="line">            self.linear = nn.Linear(self.num_hiddens * <span class="number">2</span>, self.vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs, state</span>):</span></span><br><span class="line">        X = F.one_hot(inputs.T.long(), self.vocab_size)</span><br><span class="line">        X = X.to(torch.float32)</span><br><span class="line">        Y, state = self.rnn(X, state)</span><br><span class="line">        <span class="comment"># 全连接层首先将Y的形状改为(时间步数*批量大小,隐藏单元数)</span></span><br><span class="line">        <span class="comment"># 它的输出形状是(时间步数*批量大小,词表大小)。</span></span><br><span class="line">        output = self.linear(Y.reshape((-<span class="number">1</span>, Y.shape[-<span class="number">1</span>])))</span><br><span class="line">        <span class="keyword">return</span> output, state</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">begin_state</span>(<span class="params">self, device, batch_size=<span class="number">1</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(self.rnn, nn.LSTM):</span><br><span class="line">            <span class="comment"># nn.GRU以张量作为隐状态</span></span><br><span class="line">            <span class="keyword">return</span>  torch.zeros((self.num_directions * self.rnn.num_layers,</span><br><span class="line">                                batch_size, self.num_hiddens),</span><br><span class="line">                                device=device)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># nn.LSTM以元组作为隐状态</span></span><br><span class="line">            <span class="keyword">return</span> (torch.zeros((</span><br><span class="line">                self.num_directions * self.rnn.num_layers,</span><br><span class="line">                batch_size, self.num_hiddens), device=device),</span><br><span class="line">                    torch.zeros((</span><br><span class="line">                        self.num_directions * self.rnn.num_layers,</span><br><span class="line">                        batch_size, self.num_hiddens), device=device))</span><br></pre></td></tr></table></figure><h3 id="6-3-训练与预测"><a href="#6-3-训练与预测" class="headerlink" title="6.3. 训练与预测"></a>6.3. 训练与预测</h3><p>在训练模型之前，基于一个具有随机权重的模型进行预测:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&#x27;mps&#x27;</span>)</span><br><span class="line">net = RNNModel(rnn_layer, vocab_size=<span class="built_in">len</span>(vocab))</span><br><span class="line">net = net.to(device)</span><br><span class="line">predict_ch8(<span class="string">&#x27;time traveller&#x27;</span>, <span class="number">10</span>, net, vocab, device)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line"><span class="string">&#x27;time travellerskhsskkkhs&#x27;</span></span><br></pre></td></tr></table></figure><p>很明显，这种模型根本不能输出好的结果。<br>下面开始使用预备好的超参数进行训练，结果会好的多。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, lr = <span class="number">500</span>, <span class="number">1</span></span><br><span class="line">train_ch8(net, train_iter, vocab, lr, num_epochs, device)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">perplexity <span class="number">1.3</span>, <span class="number">288014.3</span> tokens/sec on cuda:<span class="number">0</span></span><br><span class="line">time travellerit s against reason <span class="keyword">and</span> of the inghero mad alove a</span><br><span class="line">travellereat so largattematiche ture copethi thele we the i</span><br></pre></td></tr></table></figure><p>由于深度学习框架的高级API对代码进行了更多的优化，该模型相比从零实现在较短的时间内达到了较低的困惑度。</p><p>在M1芯片的机器上训练如果使用GPU可能会训练不出结果，估计是显存问题。</p><h2 id="7-通过时间反向传播"><a href="#7-通过时间反向传播" class="headerlink" title="7. 通过时间反向传播"></a>7. 通过时间反向传播</h2><p>本节会更深入地探讨序列模型反向传播的细节， 以及相关的数学原理。之前实践中遇到的“梯度爆炸”、“梯度消失”、对循环神经网络“分离梯度”等概念也会得到充分的解释。</p><p>通过时间反向传播（backpropagation through time，BPTT）实际上是循环神经网络中反向传播技术的一个特定应用。它要求我们将循环神经网络的计算图一次展开一个时间步， 以获得模型变量和参数之间的依赖关系。 然后，基于链式法则，应用反向传播来计算和存储梯度。 由于序列可能相当长，因此依赖关系也可能相当长。 例如，某个1000个字符的序列， 其第一个词元可能会对最后位置的词元产生重大影响。 这在计算上是不可行的（它需要的时间和内存都太多了）， 并且还需要超过1000个矩阵的乘积才能得到非常难以捉摸的梯度。 这个过程充满了计算与统计的不确定性。 下面将阐述过程中会发生什么以及如何在实践中解决它们。</p><h3 id="7-1-循环神经网络的梯度分析"><a href="#7-1-循环神经网络的梯度分析" class="headerlink" title="7.1. 循环神经网络的梯度分析"></a>7.1. 循环神经网络的梯度分析</h3><p>从一个描述循环神经网络工作原理的简化模型开始，此模型忽略了隐状态的特性及其更新方式的细节。这里的数学表示没有明确地区分标量、向量和矩阵，因为这些细节对于分析并不重要，反而只会使本小节中的符号变得混乱。</p><p>这个简化模型中将时间步$t$的隐状态表示为$h_t$，输入表示为$x_t$，输出表示为$o_t$。前面提到过，输入和隐状态可以拼接后与隐藏层中的一个权重变量相乘。这里分别使用$w_h$和$w_o$来表示隐藏层和输出层的权重。每个时间步的隐状态和输出可以写为：</p><script type="math/tex; mode=display">\begin{aligned}h_t &= f(x_t, h_{t-1}, w_h),\\o_t &= g(h_t, w_o),\end{aligned}</script><p>其中$f$和$g$分别是隐藏层和输出层的变换。则可以获得一个链${\ldots, (x_{t-1}, h_{t-1}, o_{t-1}), (x_{t}, h_{t}, o_t), \ldots}$，它们通过循环计算彼此依赖。在这个序列上正向传播相当简单，一次一个时间步的遍历三元组$(x_t, h_t, o_t)$，然后通过一个目标函数在所有$T$个时间步内评估输出$o_t$和对应的标签$y_t$之间的差异：</p><script type="math/tex; mode=display">L(x_1, \ldots, x_T, y_1, \ldots, y_T, w_h, w_o) = \frac{1}{T}\sum_{t=1}^T l(y_t, o_t)</script><p>而反向传播则相对棘手，特别是在计算目标函数$L$关于参数$w_h$的梯度时。按照链式法则：</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial L}{\partial w_h}  & = \frac{1}{T}\sum_{t=1}^T \frac{\partial l(y_t, o_t)}{\partial w_h}  \\& = \frac{1}{T}\sum_{t=1}^T \frac{\partial l(y_t, o_t)}{\partial o_t} \frac{\partial g(h_t, w_o)}{\partial h_t}  \frac{\partial h_t}{\partial w_h}.\end{aligned}</script><p>上式中乘积的第一项和第二项很容易计算，而第三项$\partial h_t/\partial w_h$是使事情变得棘手的地方，因为这一项需要循环地计算参数$w_h$对$h_t$的影响。根据$ h_t = f(x_t, h_{t-1}, w_h) $产生的递归计算，$h_t$既依赖于$h_{t-1}$又依赖于$w_h$，其中$h_{t-1}$的计算也依赖于$w_h$。因此，使用链式法则产生：</p><script type="math/tex; mode=display">\frac{\partial h_t}{\partial w_h}= \frac{\partial f(x_{t},h_{t-1},w_h)}{\partial w_h} +\frac{\partial f(x_{t},h_{t-1},w_h)}{\partial h_{t-1}} \frac{\partial h_{t-1}}{\partial w_h}</script><p>为导出梯度$\frac{\partial h_t}{\partial w_h}$的计算通式，设有三个序列${a_{t}},{b_{t}},{c_{t}}$，当$t=1,2,\ldots$时，序列满足$a_{0}=0$且$a_{t}=b_{t}+c_{t}a_{t-1}$。对于$t\geq 1$，很容易得出：</p><script type="math/tex; mode=display">a_{t}=b_{t}+\sum_{i=1}^{t-1}\left(\prod_{j=i+1}^{t}c_{j}\right)b_{i}</script><p>对应替换$a_t$、$b_t$和$c_t$：</p><script type="math/tex; mode=display">\begin{aligned}a_t &= \frac{\partial h_t}{\partial w_h},\\b_t &= \frac{\partial f(x_{t},h_{t-1},w_h)}{\partial w_h}, \\c_t &= \frac{\partial f(x_{t},h_{t-1},w_h)}{\partial h_{t-1}},\end{aligned}</script><p>由于前面计算公式中的梯度计算满足$a_{t}=b_{t}+c_{t}a_{t-1}$，则可以得到：</p><script type="math/tex; mode=display">\frac{\partial h_t}{\partial w_h}=\frac{\partial f(x_{t},h_{t-1},w_h)}{\partial w_h}+\sum_{i=1}^{t-1}\left(\prod_{j=i+1}^{t} \frac{\partial f(x_{j},h_{j-1},w_h)}{\partial h_{j-1}} \right) \frac{\partial f(x_{i},h_{i-1},w_h)}{\partial w_h}</script><p>虽然可以使用链式法则递归地计算$\partial h_t/\partial w_h$，但当$t$很大时这个链就会变得很长，我们需要想办法来处理这一问题。</p><h4 id="7-1-1-完全计算"><a href="#7-1-1-完全计算" class="headerlink" title="7.1.1. 完全计算"></a>7.1.1. 完全计算</h4><p>我们可以简单的递归计算上式中的全部总和，但这样的计算非常缓慢，并且可能会发生梯度爆炸， 因为初始条件的微小变化就可能会对结果产生巨大的影响，这类似于蝴蝶效应，即初始条件的很小变化就会导致结果发生不成比例的变化。 这对于我们追求的 能很好泛化高稳定性模型的预测器 是背道而驰的。 因此实践中这种方法几乎从未使用过。</p><h4 id="7-1-2-截断时间步"><a href="#7-1-2-截断时间步" class="headerlink" title="7.1.2. 截断时间步"></a>7.1.2. 截断时间步</h4><p>然后我们想到或许可以在$\tau$步后截断上式中的求和计算。这会带来真实梯度的<em>近似</em>，只需将求和终止为$\partial h_{t-\tau}/\partial w_h$。在实践中这种方法很凑效，它通常被称为截断的通过时间反向传播。这样做会导致该模型主要侧重于短期影响，而不是长期影响。这种截断是可取的，因为它会将估计值偏向更简单和更稳定的模型。</p><h4 id="7-1-3-随机截断"><a href="#7-1-3-随机截断" class="headerlink" title="7.1.3. 随机截断"></a>7.1.3. 随机截断</h4><p>在普通的“固定截断”上继续发展，则或许可以用一个随机变量替换$\partial h_t/\partial w_h$，该随机变量在预期中是正确的，但是会截断序列。这个随机变量是通过使用序列$\xi_t$来实现的，该序列预定义了$0 \leq \pi_t \leq 1$，其中$P(\xi_t = 0) = 1-\pi_t$且$P(\xi_t = \pi_t^{-1}) = \pi_t$，则有$E[\xi_t] = 1$。使用它来替换前式中的梯度$\partial h_t/\partial w_h$得到：</p><script type="math/tex; mode=display">z_t= \frac{\partial f(x_{t},h_{t-1},w_h)}{\partial w_h} +\xi_t \frac{\partial f(x_{t},h_{t-1},w_h)}{\partial h_{t-1}} \frac{\partial h_{t-1}}{\partial w_h}</script><p>从$\xi_t$的定义中推导出来$E[z_t] = \partial h_t/\partial w_h$。每当$\xi_t = 0$时，递归计算终止在这个$t$时间步。这导致了不同长度序列的加权和，其中长序列出现的很少，所以将适当地加大权重。这个想法是由塔莱克和奥利维尔提出的。</p><h4 id="7-1-4-比较上述计算策略"><a href="#7-1-4-比较上述计算策略" class="headerlink" title="7.1.4. 比较上述计算策略"></a>7.1.4. 比较上述计算策略</h4><p><img src="/assets/post_img/article57/truncated-bptt.svg" alt="比较RNN中计算梯度的策略，3行自上而下分别为：随机截断、常规截断、完整计算"><br>上图说明了基于循环神经网络使用通过时间反向传播分析《时间机器》书中前几个字符的三种策略：</p><ul><li>第一行采用随机截断，方法是将文本划分为不同长度的片断；</li><li>第二行采用常规截断，方法是将文本分解为相同长度的子序列。这也是我们在循环神经网络实验中一直在做的；</li><li>第三行采用通过时间的完全反向传播，结果是产生了在计算上不可行的表达式。</li></ul><p>遗憾的是，虽然随机截断在理论上具有吸引力，但很可能是由于多种因素在实践中并不比常规截断更好。首先，在对过去若干个时间步经过反向传播后，观测结果足以捕获实际的依赖关系。其次，增加的方差抵消了时间步数越多梯度越精确的事实。第三，我们真正想要的是只有短范围交互的模型。因此，模型需要的正是截断的通过时间反向传播方法所具备的轻度正则化效果。</p><h3 id="7-2-通过时间反向传播的细节"><a href="#7-2-通过时间反向传播的细节" class="headerlink" title="7.2. 通过时间反向传播的细节"></a>7.2. 通过时间反向传播的细节</h3><p>在讨论一般性原则之后，来看一下通过时间反向传播问题的细节。与上一小节中的分析不同，本小节将展示如何计算目标函数相对于所有分解模型参数的梯度。为保持简单，设有一个没有偏置参数的循环神经网络，其在隐藏层中的激活函数使用恒等映射（$\phi(x)=x$）。对于时间步$t$，设单个样本的输入及其对应的标签分别为$\mathbf{x}_t \in \mathbb{R}^d$和$y_t$。计算隐状态$\mathbf{h}_t \in \mathbb{R}^h$和输出$\mathbf{o}_t \in \mathbb{R}^q$的方式为：</p><script type="math/tex; mode=display">\begin{aligned}\mathbf{h}_t &= \mathbf{W}_{hx} \mathbf{x}_t + \mathbf{W}_{hh} \mathbf{h}_{t-1},\\\mathbf{o}_t &= \mathbf{W}_{qh} \mathbf{h}_{t},\end{aligned}</script><p>其中权重参数为$\mathbf{W}_{hx} \in \mathbb{R}^{h \times d}$、$\mathbf{W}_{hh} \in \mathbb{R}^{h \times h}$和$\mathbf{W}_{qh} \in \mathbb{R}^{q \times h}$。用$l(\mathbf{o}_t, y_t)$表示时间步$t$处（即从序列开始起的超过$T$个时间步）的损失函数，则目标函数的总体损失是：</p><script type="math/tex; mode=display">L = \frac{1}{T} \sum_{t=1}^T l(\mathbf{o}_t, y_t)</script><p>为了在循环神经网络的计算过程中可视化模型变量和参数之间的依赖关系，可以为模型绘制一个计算图，如下图所示。利用计算图可以看到很多东西，比如时间步3的隐状态$\mathbf{h}_3$的计算取决于模型参数$\mathbf{W}_{hx}$和$\mathbf{W}_{hh}$，以及最终时间步的隐状态$\mathbf{h}_2$以及当前时间步的输入$\mathbf{x}_3$。</p><p><img src="/assets/post_img/article57/rnn-bptt.svg" alt="上图表示具有三个时间步的循环神经网络模型依赖关系的计算图。未着色的方框表示变量，着色的方框表示参数，圆表示运算符"></p><p>上中的模型参数是$\mathbf{W}_{hx}$、$\mathbf{W}_{hh}$和$\mathbf{W}_{qh}$。通常，训练该模型需要对这些参数进行梯度计算：$\partial L/\partial \mathbf{W}_{hx}$、$\partial L/\partial \mathbf{W}_{hh}$和$\partial L/\partial \mathbf{W}_{qh}$。根据依赖关系，我们可以沿箭头的相反方向遍历计算图，依次计算和存储梯度。为灵活地表示链式法则中不同形状的矩阵、向量和标量的乘法，继续采用$\text{prod}$运算符表示参数乘法以及一些必要操作。</p><p>首先，在任意时间步$t$，目标函数关于模型输出的微分计算是相当简单的：</p><script type="math/tex; mode=display">\frac{\partial L}{\partial \mathbf{o}_t} =  \frac{\partial l (\mathbf{o}_t, y_t)}{T \cdot \partial \mathbf{o}_t} \in \mathbb{R}^q</script><p>现在可以计算目标函数关于输出层中参数$\mathbf{W}_{qh}$的梯度：$\partial L/\partial \mathbf{W}_{qh} \in \mathbb{R}^{q \times h}$。基于计算图，目标函数$L$通过$\mathbf{o}_1, \ldots, \mathbf{o}_T$依赖于$\mathbf{W}_{qh}$。依据链式法则，得到</p><script type="math/tex; mode=display">\frac{\partial L}{\partial \mathbf{W}_{qh}}= \sum_{t=1}^T \text{prod}\left(\frac{\partial L}{\partial \mathbf{o}_t}, \frac{\partial \mathbf{o}_t}{\partial \mathbf{W}_{qh}}\right)= \sum_{t=1}^T \frac{\partial L}{\partial \mathbf{o}_t} \mathbf{h}_t^\top,</script><p>其中$\partial L/\partial \mathbf{o}_t$是由第一步给出的。</p><p>接下来，由计算图知在最后的时间步$T$，目标函数$L$仅通过$\mathbf{o}_T$依赖于隐状态$\mathbf{h}_T$。使用链式法可以很容易地得到梯度$\partial L/\partial \mathbf{h}_T \in \mathbb{R}^h$：</p><script type="math/tex; mode=display">\frac{\partial L}{\partial \mathbf{h}_T} = \text{prod}\left(\frac{\partial L}{\partial \mathbf{o}_T}, \frac{\partial \mathbf{o}_T}{\partial \mathbf{h}_T} \right) = \mathbf{W}_{qh}^\top \frac{\partial L}{\partial \mathbf{o}_T}</script><p>当目标函数$L$通过$\mathbf{h}_{t+1}$和$\mathbf{o}_t$依赖$\mathbf{h}_t$时，对任意时间步$t &lt; T$来说都变得更加棘手。根据链式法则，隐状态的梯度$\partial L/\partial \mathbf{h}_t \in \mathbb{R}^h$在任何时间步骤$t &lt; T$时都可以递归地计算为：</p><script type="math/tex; mode=display">\frac{\partial L}{\partial \mathbf{h}_t} = \text{prod}\left(\frac{\partial L}{\partial \mathbf{h}_{t+1}}, \frac{\partial \mathbf{h}_{t+1}}{\partial \mathbf{h}_t} \right) + \text{prod}\left(\frac{\partial L}{\partial \mathbf{o}_t}, \frac{\partial \mathbf{o}_t}{\partial \mathbf{h}_t} \right) = \mathbf{W}_{hh}^\top \frac{\partial L}{\partial \mathbf{h}_{t+1}} + \mathbf{W}_{qh}^\top \frac{\partial L}{\partial \mathbf{o}_t}</script><p>为了进行分析，对于任何时间步$1 \leq t \leq T$展开递归计算得：</p><script type="math/tex; mode=display">\frac{\partial L}{\partial \mathbf{h}_t}= \sum_{i=t}^T {\left(\mathbf{W}_{hh}^\top\right)}^{T-i} \mathbf{W}_{qh}^\top \frac{\partial L}{\partial \mathbf{o}_{T+t-i}}</script><p>我们可以从上式中看到，这个简单的线性例子已经展现了长序列模型的一些关键问题：它陷入到$\mathbf{W}_{hh}^\top$的潜在的非常大的幂。在这个幂中，小于1的特征值将会消失，大于1的特征值将会发散。这在数值上是不稳定的，表现形式为梯度消失或梯度爆炸。解决此问题的一种方法是按照计算方便的需要截断指定大小的时间步长。实践中这种截断是通过在给定数量的时间步之后分离梯度来实现的。后面将学习更复杂的序列模型（如长短期记忆模型）是如何进一步缓解这一问题的。</p><p>最后，计算图表明：目标函数$L$通过隐状态$\mathbf{h}_1, \ldots, \mathbf{h}_T$依赖于隐藏层中的模型参数$\mathbf{W}_{hx}$和$\mathbf{W}_{hh}$。为了计算有关这些参数的梯度$\partial L / \partial \mathbf{W}_{hx} \in \mathbb{R}^{h \times d}$和$\partial L / \partial \mathbf{W}_{hh} \in \mathbb{R}^{h \times h}$，可以应用链式规则得：</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial L}{\partial \mathbf{W}_{hx}}&= \sum_{t=1}^T \text{prod}\left(\frac{\partial L}{\partial \mathbf{h}_t}, \frac{\partial \mathbf{h}_t}{\partial \mathbf{W}_{hx}}\right)= \sum_{t=1}^T \frac{\partial L}{\partial \mathbf{h}_t} \mathbf{x}_t^\top,\\\frac{\partial L}{\partial \mathbf{W}_{hh}}&= \sum_{t=1}^T \text{prod}\left(\frac{\partial L}{\partial \mathbf{h}_t}, \frac{\partial \mathbf{h}_t}{\partial \mathbf{W}_{hh}}\right)= \sum_{t=1}^T \frac{\partial L}{\partial \mathbf{h}_t} \mathbf{h}_{t-1}^\top,\end{aligned}</script><p>其中$\partial L/\partial \mathbf{h}_t$是由$ \frac{\partial L}{\partial \mathbf{h}_T} = \text{prod}\left(\frac{\partial L}{\partial \mathbf{o}_T}, \frac{\partial \mathbf{o}_T}{\partial \mathbf{h}_T} \right) = \mathbf{W}_{qh}^\top \frac{\partial L}{\partial \mathbf{o}_T} $和$ \frac{\partial L}{\partial \mathbf{h}_t} = \text{prod}\left(\frac{\partial L}{\partial \mathbf{h}_{t+1}}, \frac{\partial \mathbf{h}_{t+1}}{\partial \mathbf{h}_t} \right) + \text{prod}\left(\frac{\partial L}{\partial \mathbf{o}_t}, \frac{\partial \mathbf{o}_t}{\partial \mathbf{h}_t} \right) = \mathbf{W}_{hh}^\top \frac{\partial L}{\partial \mathbf{h}_{t+1}} + \mathbf{W}_{qh}^\top \frac{\partial L}{\partial \mathbf{o}_t} $递归计算得到的，是影响数值稳定性的关键量。</p><p>由于BPTT是反向传播在循环神经网络中的应用方式，所以训练循环神经网络交替使用正向传播和BPTT。BPTT依次计算并存储上述梯度。具体而言，存储的中间值会被重复使用，以避免重复计算，例如存储$\partial L/\partial \mathbf{h}_t$，以便在计算$\partial L / \partial \mathbf{W}_{hx}$和$\partial L / \partial \mathbf{W}_{hh}$时使用。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;p&gt;目前为止仅接触到两种类型的数据：表格数据和图像数据。 对于图像数据，可以设计专门的卷积神经网络架构来为这类特殊的数据结构建模。 对于一张图像，我们需要有效地利用其像素位置，假若对图像中的像素位置进行重排，就会对图像中内容的推断造成极大的困难。&lt;/p&gt;
&lt;p&gt;最重要的是，到目前为止我们默认数据都来自于某种分布， 并且所有样本都是独立同分布的 （independently and identically distributed，i.i.d.）。 然而，大多数的数据并非如此。 例如，文章中的单词是按顺序写的，如果顺序被随机地重排，就很难理解文章原始的意思。 同样，视频中的图像帧、对话中的音频信号以及网站上的浏览行为都是有顺序的。 因此，针对此类数据而设计特定模型，可能效果会更好。&lt;br&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://silencezheng.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>什么是Mixin</title>
    <link href="http://silencezheng.top/2022/08/26/article56/"/>
    <id>http://silencezheng.top/2022/08/26/article56/</id>
    <published>2022-08-26T10:30:40.000Z</published>
    <updated>2022-08-26T10:32:25.266Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在开发程序的过程中，尤其是在使用各个成熟的工具时，经常会接触到<strong>Mixin</strong>的概念。初见时完全不理解是什么含义，在网上查找了一些资料，总结一下，如有错误请评论指正。<br><span id="more"></span></p><h2 id="什么是Mixin？"><a href="#什么是Mixin？" class="headerlink" title="什么是Mixin？"></a>什么是Mixin？</h2><p>Mixin即“混入”，也可以是MixIn（和迷信区分开）或Mix-in，在笔者当前的理解来看，Mixin的根本目的有三：解耦、灵活、简单。</p><p>从Python的角度看，Mixin和多继承是密不可分的。在讲述Mixin的逻辑前，先来明确两个主体：A类 和 B类，我们假定A类为<em>Mixin类</em>，B类为<em>需要使用A功能的类</em>。</p><p>注意这里说的多继承与多重继承还有些许区别，强调广度而不是深度。</p><p>设计Mixin类的目的首先是为需要功能的类（如B类）提供功能，获得功能的方法就是继承。例如在程序设计中需要设计C类时，先考虑通过多继承来组合多个Mixin类来实现C类的功能，而不是依赖传统的多重继承。这样做有很多好处，例如使类的组织结构扁平化，抛弃了多重继承链的复杂性，以及减少子类功能冗余等等。</p><p>在Vue中同样使用了Mixin的思想，官方的说法是这样的：</p><blockquote><p>Mixin（混入）提供了一种非常灵活的方式，来分发 Vue 组件中的可复用功能。一个混入对象可以包含任意组件选项。当组件使用混入对象时，所有混入对象的选项将被“混合”进入该组件本身的选项。</p></blockquote><p>通俗来讲，Mixin有些像组件的组件，将组件的公共逻辑或者配置提取出来，哪个组件需要用到时，直接将提取的这部分混入到组件内部。</p><p>不难看出，虽然在不同的语言中实现的方式略有不同，但Mixin的基本思想是一致的，Mixin类本质上是一种功能抽象。</p><h2 id="Mixin规范"><a href="#Mixin规范" class="headerlink" title="Mixin规范"></a>Mixin规范</h2><p>基于以上Mixin类的概念，可以发现Mixin类实际上还要遵守一些“隐性”规范，说“隐性”是因为Mixin并不是Python中的一种关键字，很多要求是语义上允许的，但当我们决定使用Mixin时，不得不去思考如何构建一个成功的Mixin模式，于是产生了这些规范。</p><p>引用一个很好的说法：</p><blockquote><p>从某种程度上来说，继承强调 I am，Mixin 强调 I can。</p></blockquote><p>这句话简单明了的说明了Mixin与传统继承的区别：Mixin类直接包含功能的默认实现，虽然使用继承的方式引入，但真正做到开箱即用。</p><p>笔者认为需要再次强调明确一下“继承”（指含义上的继承，并非语义上的继承）和“引入”的概念，假设有两种类：Mixin类和普通类，那么普通类继承普通类是“继承”，普通类继承Mixin类是“引入”，Mixin类继承Mixin类是“继承”，Mixin类继承普通类是不被允许的。</p><p>Mixin类需具备的特征（”隐性“要求）：<br>1、不能单独产生实例，只能作为“功能模块”被其他可产生实例的类引入，因为Mixin类是抽象类。<br>2、不能继承普通类，只能继承其他Mixin类。<br>3、能独立实现功能，仅写出方法内容和方法所需的变量名。<br>4、普通类引入Mixin类时，不覆盖所继承Mixin的属性和方法，不需要调用super()去取Mixin中的方法。<br>5、可被同时继承的Mixin类之间没有重复的方法或属性，因此不用关心继承的顺序，强调隔离性。</p><p>之所以称上述特征为”隐性“要求，还是因为语义上对这些规范都没有明令禁止，但这些特征却实际上是Mixin奏效的灵魂所在。</p><p>举一个Mixin类的例子（取自Flask-Login）：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UserMixin</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    This provides default implementations for the methods </span></span><br><span class="line"><span class="string">    that Flask-Login expects user objects to have.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Python 3 implicitly set __hash__ to None if we override __eq__</span></span><br><span class="line">    <span class="comment"># We set it back to its default implementation</span></span><br><span class="line">    __hash__ = <span class="built_in">object</span>.__hash__</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">is_active</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">is_authenticated</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.is_active</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">is_anonymous</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_id</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">str</span>(self.<span class="built_in">id</span>)</span><br><span class="line">        <span class="keyword">except</span> AttributeError:</span><br><span class="line">            <span class="keyword">raise</span> NotImplementedError(<span class="string">&quot;No `id` attribute - override `get_id`&quot;</span>) <span class="keyword">from</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__eq__</span>(<span class="params">self, other</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Checks the equality of two `UserMixin` objects using `get_id`.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(other, UserMixin):</span><br><span class="line">            <span class="keyword">return</span> self.get_id() == other.get_id()</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">NotImplemented</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__ne__</span>(<span class="params">self, other</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Checks the inequality of two `UserMixin` objects using `get_id`.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        equal = self.__eq__(other)</span><br><span class="line">        <span class="keyword">if</span> equal <span class="keyword">is</span> <span class="literal">NotImplemented</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">NotImplemented</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">not</span> equal</span><br></pre></td></tr></table></figure></p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>关于Mixin，网络上也是众说纷纭，相关的内容有组合模式（Composite Pattern），Duck Typing等等，但笔者认为仅依靠上述解释已经可以初步理解 Mixin的含义 及 Mixin的使用目的。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;在开发程序的过程中，尤其是在使用各个成熟的工具时，经常会接触到&lt;strong&gt;Mixin&lt;/strong&gt;的概念。初见时完全不理解是什么含义，在网上查找了一些资料，总结一下，如有错误请评论指正。&lt;br&gt;</summary>
    
    
    
    
    <category term="Python" scheme="http://silencezheng.top/tags/Python/"/>
    
    <category term="编程思想" scheme="http://silencezheng.top/tags/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/"/>
    
  </entry>
  
  <entry>
    <title>远程调试代码的几种方式</title>
    <link href="http://silencezheng.top/2022/08/16/article55/"/>
    <id>http://silencezheng.top/2022/08/16/article55/</id>
    <published>2022-08-16T14:45:20.000Z</published>
    <updated>2022-08-16T14:50:18.990Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>针对远程运行深度学习程序以及其他代码调试探索了几种方法，依次介绍一下。此次用个人电脑作为服务器实验，Windows系统作为服务器还是比较麻烦的。</p><p>服务器配置要求：SSH、Conda<br><span id="more"></span><br>方式主要有以下几种：<br>1、远程使用jupyter<br>2、Pycharm远程开发<br>3、VSCode远程开发</p><h2 id="远程使用jupyter"><a href="#远程使用jupyter" class="headerlink" title="远程使用jupyter"></a>远程使用jupyter</h2><p>原理：在服务器上运行jupyter，开发机用ssh进行端口映射，在本地浏览器上使用远端的jupyter进行开发。</p><h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><p>1、ssh连接服务器。<br>2、启动jupyter：<br><figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#</span> --<span class="comment">no</span><span class="literal">-</span><span class="comment">browser表示不需要寻找浏览器</span></span><br><span class="line"><span class="comment">jupyter</span> <span class="comment">notebook</span> --<span class="comment">no</span><span class="literal">-</span><span class="comment">browser</span> --<span class="comment">port=6666</span></span><br></pre></td></tr></table></figure><br>启动后会给一个token（口令），后面用的到。<br>3、ssh端口映射：<br><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># -N：ssh没有远程命令需要执行</span></span><br><span class="line"><span class="meta"># -f：ssh在后台执行</span></span><br><span class="line"><span class="meta"># -L：指定本地端口</span></span><br><span class="line"><span class="meta"># -g：允许服务器连接此端口，否则将只允许开发机使用该端口</span></span><br><span class="line">ssh -N -f -g -L <span class="number">6688</span>:localhost:<span class="number">6666</span> username@remote_ip</span><br></pre></td></tr></table></figure><br>输入密码后，本地6688端口会完成映射。<br>4、登录jupyter：<br>浏览器通过地址<code>localhost:6688</code>即可访问服务器jupyter，需要输入token或密码。<br>5、若要取消端口映射：<br>首先查看端口使用情况<code>lsof -i:6688</code>，然后<code>kill 进程ID</code>。<br>6、关闭jupyter：<br>直接在步骤1的连接中<code>ctrl+c</code>即可关闭。</p><h3 id="可能遇到的问题"><a href="#可能遇到的问题" class="headerlink" title="可能遇到的问题"></a>可能遇到的问题</h3><p>1、步骤1连接后，需要先使用conda激活指定的环境，然后在环境下启动jupyter，否则可能发生找不到包的问题。<br>2、当服务器为windows系统时，可以使用<code>Call conda.bat</code>代替<code>conda</code>。<br>3、步骤1连接后，可以先cd移动到想去的目录，再在该目录下启动jupyter。</p><h2 id="Pycharm远程开发"><a href="#Pycharm远程开发" class="headerlink" title="Pycharm远程开发"></a>Pycharm远程开发</h2><p>先说下结论：截至2022.08.16，Pycharm不支持在Windows服务器上进行远程开发。<br>原理：同样是基于SSH，只不过进行了封装。</p><h3 id="步骤："><a href="#步骤：" class="headerlink" title="步骤："></a>步骤：</h3><p>1、到<code>Preferences | Build, Execution, Deployment | Deployment</code> 中添加一个SFTP服务器。测试连接，该服务是可以工作的。<br>2、到<code>Preferences | Project: name | Python Interpreter</code> 中添加一个Python解释器，选择SSH解释器，添加一个SSH配置，下一步填入服务器Python解释器路径以及希望当前项目在服务器上的同步目录，示例如下：<br><figure class="highlight taggerscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">C:<span class="symbol">\u</span>ser<span class="symbol">\A</span>pps<span class="symbol">\A</span>naconda<span class="symbol">\e</span>nvs<span class="symbol">\D</span>eepLearning<span class="symbol">\p</span>ython.exe</span><br><span class="line"></span><br><span class="line">C:<span class="symbol">\u</span>ser<span class="symbol">\P</span>rojs<span class="symbol">\r</span>emote</span><br></pre></td></tr></table></figure></p><p>这里会遇到问题，无法显示服务器文件目录：<br><img src="/assets/post_img/article55/error1.jpg" alt="显示服务器文件目录报错"><br>该报错的官方issue在<a href="https://youtrack.jetbrains.com/issue/PY-38097">这里</a>，民间也有很多<a href="https://bbs.csdn.net/topics/397093590">反馈</a>，目前没有解决办法。<br>3、虽然无法显示文件树，但可以自行填入以上两个目录，这样以后会自动开始refresh skeletons。这里会遇到最致命的问题，报错<code>Couldn&#39;t refresh skeletons for remote interpreter failed to run generator3/__main__.py for sftp...</code>。尝试过删除服务器上的<code>.pycharm_helpers</code>目录，重新添加python解释器，也没办法解决。怀疑是上一个issue的延续，或者是文件编码问题，因为报错的后半段会有乱码。<br>4、Anyway，目前无法解决，Pycharm不能使用Windows服务器远程开发。</p><h2 id="VSCode远程开发"><a href="#VSCode远程开发" class="headerlink" title="VSCode远程开发"></a>VSCode远程开发</h2><p>原理：同上</p><h3 id="步骤-1"><a href="#步骤-1" class="headerlink" title="步骤"></a>步骤</h3><p>1、VSCode安装Remote-SSH、Remote-Containers插件。Remote-SSH负责建立SSH连接、Remote-Containers负责访问服务器目录。<br>2、安装完毕会左侧工具栏会出现图标，点击后如下：<br><img src="/assets/post_img/article55/remote-ssh.jpg" alt="remote-ssh"><br>在此新建一个SSH会话，第一次连接时的信息，会被写入根目录的<code>.ssh/config</code>文件下。<br>3、右击服务器图标即可连接，连接成功后左下角会显示：<br><img src="/assets/post_img/article55/left-corner.jpg" alt="lc"><br>第一次连接成功会先在用户目录下安装<code>vscode-server</code>，大概190MB，形成<code>.vscode-server目录</code>。<br>4、此时左侧文件目录处即为远端服务器目录，可以随心所欲的编辑代码了～<br>5、在服务器上跑代码的话，只需要在服务器上安装对应插件即可，通常训练模型需要Python和Jupyter这两个，安装过后可以自动识别服务器上的conda环境，右上角就可以直接选内核运行了，十分方便。<br>6、第一次连接跑代码时，可能出现绘图不显示的问题，关闭VSCode重新打开就解决了，动图也可以正常显示，效果比直接使用jupyter远程更加好。</p><p>下面是我使用的扩展：<br><img src="/assets/post_img/article55/extensions.jpg" alt="extensions"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇所有测试基于以下平台：<br>开发机：MacOS 12.5<br>服务器：Windows 10 Professional<br>VSCode：1.70.1<br>Pycharm Professional Edition：2021.3.3</p><p>总的来说，目前使用VSCode进行远程开发与训练效果最好。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;针对远程运行深度学习程序以及其他代码调试探索了几种方法，依次介绍一下。此次用个人电脑作为服务器实验，Windows系统作为服务器还是比较麻烦的。&lt;/p&gt;
&lt;p&gt;服务器配置要求：SSH、Conda&lt;br&gt;</summary>
    
    
    
    
    <category term="VSCode" scheme="http://silencezheng.top/tags/VSCode/"/>
    
  </entry>
  
  <entry>
    <title>现代卷积神经网络--《动手学深度学习》笔记0x08</title>
    <link href="http://silencezheng.top/2022/08/06/article54/"/>
    <id>http://silencezheng.top/2022/08/06/article54/</id>
    <published>2022-08-05T18:43:56.000Z</published>
    <updated>2022-08-11T05:45:55.323Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><p>介绍现代的卷积神经网络架构，本章中的每一个模型都曾一度占据主导地位，其中许多模型都是ImageNet竞赛的优胜者。ImageNet竞赛自2010年以来，一直是计算机视觉中监督学习进展的指向标。</p><span id="more"></span><p>这些模型包括：<br>AlexNet，第一个在大规模视觉竞赛中击败传统计算机视觉模型的大型神经网络；</p><p>使用重复块的网络（VGG），它利用许多重复的神经网络块；</p><p>网络中的网络（NiN），它重复使用由卷积层和$1 \times 1$卷积层（用来代替全连接层）来构建深层网络;</p><p>含并行连结的网络（GoogLeNet），它使用并行连结的网络，通过不同窗口大小的卷积层和最大池化层来并行抽取信息；</p><p>残差网络（ResNet），它通过残差块构建跨层的数据通道，是计算机视觉中最流行的体系架构；</p><p>稠密连接网络（DenseNet），它的计算成本很高，但带来了更好的效果。</p><p>虽然深度神经网络的概念非常简单——将神经网络堆叠在一起。但由于不同的网络架构和超参数选择，这些神经网络的性能会发生很大变化。 本章介绍的神经网络是将人类直觉和相关数学见解结合后，经过大量研究试错后的结晶。 按时间顺序介绍这些模型是很好的，能够加深对模型是如何被创造出来的理解。 例如，批量规范化（batch normalization）和残差网络（ResNet）为设计和训练深度神经网络提供了重要思想指导。</p><p>顺便一提，本章的训练用M1芯片算起来费劲😄。</p><p>对应实践：<a href="https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x08.ipynb">https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x08.ipynb</a></p><h3 id="0-1-结论"><a href="#0-1-结论" class="headerlink" title="0.1. 结论"></a>0.1. 结论</h3><ul><li>AlexNet的架构与LeNet相似，但使用了更多的卷积层和更多的参数来拟合大规模的ImageNet数据集。</li><li>今天，AlexNet已经被更有效的架构所超越，但它是从浅层网络到深层网络的关键一步。</li><li>尽管AlexNet的代码只比LeNet多出几行，但学术界花了很多年才接受深度学习这一概念，并应用其出色的实验结果。这也是由于缺乏有效的计算工具</li><li>Dropout、ReLU和预处理是提升计算机视觉任务性能的其他关键步骤。</li><li>VGG-11使用可复用的卷积块构造网络。不同的VGG模型可通过每个块中卷积层数量和输出通道数量的差异来定义。</li><li>块的使用导致网络定义的非常简洁。使用块可以有效地设计复杂的网络。</li><li>在VGG论文中，Simonyan和Ziserman尝试了各种架构。特别是他们发现深层且窄的卷积（即$3\times3$）比较浅层且宽的卷积更有效。</li><li>NiN使用由一个$1 \times 1$卷积层和多个卷积层组成的块。该块可以在卷积神经网络中使用，以允许更多的每像素非线性。</li><li>NiN去除了容易造成过拟合的全连接层，将它们替换为全局平均池化层（即在所有位置上进行求和）。该池化层通道数量为所需的输出数量（例如，Fashion-MNIST的输出为10）。</li><li>移除全连接层可减少过拟合，同时显著减少NiN的参数。</li><li>NiN的设计影响了许多后续卷积神经网络的设计。</li><li>Inception块相当于一个有4条路径的子网络。它通过不同窗口形状的卷积层和最大池化层来并行抽取信息，并使用$1 \times 1$卷积层减少每像素级别上的通道维数从而降低模型复杂度。</li><li>GoogLeNet将多个设计精细的Inception块与其他层（卷积层、全连接层）串联起来。其中Inception块的通道数分配之比是在ImageNet数据集上通过大量的实验得来的。</li><li>GoogLeNet和它的后继者们一度是ImageNet上最有效的模型之一：它以较低的计算复杂度提供了类似的测试精度。</li><li>在模型训练过程中，批量规范化利用小批量的均值和标准差，不断调整神经网络的中间输出，使整个神经网络各层的中间输出值更加稳定。</li><li>批量规范化在全连接层和卷积层的使用略有不同。</li><li>批量规范化层和暂退层一样，在训练模式和预测模式下计算不同。</li><li>批量规范化有许多有益的副作用，主要是正则化。另一方面，”减少内部协变量偏移“的原始动机似乎不是一个有效的解释。</li><li>学习嵌套函数（nested function）是训练神经网络的理想情况。在深层神经网络中，学习另一层作为恒等映射（identity function）较容易（尽管这是一个极端情况）。</li><li>残差映射可以更容易地学习同一函数，例如将权重层中的参数近似为零。</li><li>利用残差块（residual blocks）可以训练出一个有效的深层神经网络：输入可以通过层间的残余连接更快地向前传播。</li><li>残差网络（ResNet）对随后的深层神经网络设计产生了深远影响。</li><li>在跨层连接上，不同于ResNet中将输入与输出相加，稠密连接网络（DenseNet）在通道维上连结输入与输出。</li><li>DenseNet的主要构建模块是稠密块和过渡层。</li><li>在构建DenseNet时，需要通过添加过渡层来控制网络的维数，从而再次减少通道的数量。</li></ul><h2 id="1-深度卷积神经网络（AlexNet）"><a href="#1-深度卷积神经网络（AlexNet）" class="headerlink" title="1. 深度卷积神经网络（AlexNet）"></a>1. 深度卷积神经网络（AlexNet）</h2><p>事实上，在上世纪90年代初到2012年之间的大部分时间里，神经网络往往被其他机器学习方法超越，如支持向量机（support vector machines）。</p><p>在计算机视觉中，直接将神经网络与其他机器学习方法进行比较也许不公平。这是因为，卷积神经网络的输入是由原始像素值或是经过简单预处理（例如居中、缩放）的像素值组成的。但在使用传统机器学习方法时，从业者永远不会将原始像素作为输入。在传统机器学习方法中，计算机视觉流水线是由经过人的手工精心设计的特征流水线组成的。对于这些传统方法，大部分的进展都来自于对特征有了更聪明的想法，并且学习到的算法往往归于事后的解释。</p><p>虽然上世纪90年代就有了一些神经网络加速卡，但仅靠它们还不足以开发出有大量参数的深层多通道多层卷积神经网络。此外，当时的数据集仍然相对较小。除了这些障碍，训练神经网络的一些关键技巧仍然缺失，包括启发式参数初始化、随机梯度下降的变体、非挤压激活函数和有效的正则化技术。</p><p>因此，与训练<em>端到端</em>（从像素到分类结果）系统不同，经典机器学习的流水线看起来更像下面这样：</p><ol><li>获取一个有趣的数据集。</li><li>根据光学、几何学、其他知识以及偶然的发现，手工对特征数据集进行预处理。</li><li>通过标准的特征提取算法，如SIFT（尺度不变特征变换）和SURF（加速鲁棒特征）或其他手动调整的流水线来输入数据。</li><li>将提取的特征送入最喜欢的分类器中（例如线性模型或其它核方法），以训练分类器。</li></ol><p>机器学习研究人员相信机器学习既重要又美丽：优雅的理论去证明各种模型的性质。机器学习是一个正在蓬勃发展、严谨且非常有用的领域。然而计算机视觉研究人员会告诉你图像识别的诡异事实: <em>推动领域进步的是数据特征，而不是学习算法</em>。计算机视觉研究人员相信，从对最终模型精度的影响来说，更大或更干净的数据集、或是稍微改进的特征提取，比任何学习算法带来的进步要大得多。</p><h3 id="1-1-学习表征"><a href="#1-1-学习表征" class="headerlink" title="1.1. 学习表征"></a>1.1. 学习表征</h3><p>在2012年前，图像特征都是机械地计算出来的。事实上，设计一套新的特征函数、改进结果，并撰写论文是盛极一时的潮流。SIFT、SURF、HOG（定向梯度直方图）、<a href="https://en.wikipedia.org/wiki/Bag-of-words_model_in_computer_vision">bags of visual words</a>和类似的特征提取方法占据了主导地位。</p><p>深度学习从业人员想法则与众不同：他们认为特征本身应该被学习。此外，他们还认为，在合理地复杂性前提下，特征应该由多个共同学习的神经网络层组成，每个层都有可学习的参数。在机器视觉中，最底层可能检测边缘、颜色和纹理。事实上，Alex Krizhevsky、Ilya Sutskever和Geoff Hinton提出了一种新的卷积神经网络变体<em>AlexNet</em>。在2012年ImageNet挑战赛中取得了轰动一时的成绩。AlexNet以Alex Krizhevsky的名字命名，他是AlexNet论文的第一作者。</p><p>有趣的是，在网络的最底层，模型学习到了一些类似于传统滤波器的特征抽取器。 下图是从AlexNet论文复制的，描述了底层图像特征。<br><img src="/assets/post_img/article54/filters.png" alt="AlexNet第一层学习到的特征抽取器"></p><p>AlexNet的更高层建立在这些底层表示的基础上，以表示更大的特征，如眼睛、鼻子、草叶等等。而更高的层可以检测整个物体，如人、飞机、狗或飞盘。最终的隐藏神经元可以学习图像的综合表示，从而使属于不同类别的数据易于区分。尽管一直有一群执着的研究者不断钻研，试图学习视觉数据的逐级表征，然而很长一段时间里这些尝试都未有突破。深度卷积神经网络的突破出现在2012年。突破可归因于两个关键因素：<em>数据</em> 和 <em>硬件</em></p><h4 id="1-1-1-数据"><a href="#1-1-1-数据" class="headerlink" title="1.1.1. 数据"></a>1.1.1. 数据</h4><p>包含许多特征的深度模型需要大量的有标签数据，才能显著优于基于凸优化的传统方法（如线性方法和核方法）。<br>然而，限于早期计算机有限的存储和90年代有限的研究预算，大部分研究只基于小的公开数据集。例如，不少研究论文基于加州大学欧文分校（UCI）提供的若干个公开数据集，其中许多数据集只有几百至几千张在非自然环境下以低分辨率拍摄的图像。这一状况在2010年前后兴起的大数据浪潮中得到改善。<br>2009年，ImageNet数据集发布，并发起ImageNet挑战赛：要求研究人员从100万个样本中训练模型，以区分1000个不同类别的对象。ImageNet数据集由斯坦福教授李飞飞小组的研究人员开发，利用谷歌图像搜索（Google Image Search）对每一类图像进行预筛选，并利用亚马逊众包（Amazon Mechanical Turk）来标注每张图片的相关类别。这种规模是前所未有的。这项被称为ImageNet的挑战赛推动了计算机视觉和机器学习研究的发展，挑战研究人员确定哪些模型能够在更大的数据规模下表现最好。</p><h4 id="1-1-2-硬件"><a href="#1-1-2-硬件" class="headerlink" title="1.1.2. 硬件"></a>1.1.2. 硬件</h4><p>深度学习对计算资源要求很高，训练可能需要数百个迭代轮数，每次迭代都需要通过代价高昂的许多线性代数层传递数据。这也是为什么在20世纪90年代至21世纪初，优化凸目标的简单算法是研究人员的首选。然而，用GPU训练神经网络改变了这一格局。<em>图形处理器</em>（Graphics Processing Unit，GPU）早年用来加速图形处理，使电脑游戏玩家受益。GPU可优化高吞吐量的$4 \times 4$矩阵和向量乘法，从而服务于基本的图形任务。这些数学运算与卷积层的计算惊人地相似！由此，英伟达（NVIDIA）和ATI已经开始为通用计算操作优化gpu，甚至把它们作为<em>通用GPU</em>（general-purpose GPUs，GPGPU）来销售。</p><p>在解释GPU比CPU“强”之前，先来深度理解一下中央处理器（Central Processing Unit，CPU）的<em>核心</em>。<br>CPU的每个核心都拥有高时钟频率的运行能力，和高达数MB的三级缓存（L3Cache）。它们非常适合执行各种指令，具有分支预测器、深层流水线和其他使CPU能够运行各种程序的功能。<br>然而，这种明显的优势也是它的致命弱点：通用核心的制造成本非常高。<br>它们需要大量的芯片面积、复杂的支持结构（内存接口、内核之间的缓存逻辑、高速互连等等），而且它们在任何单个任务上的性能都相对较差。</p><p>相比于CPU，GPU由$100 \sim 1000$个小的处理单元组成（NVIDIA、ATI、ARM和其他芯片供应商之间的细节稍有不同），通常被分成更大的组（NVIDIA称之为warps）。<br>虽然每个GPU核心都相对较弱，有时甚至以低于1GHz的时钟频率运行，但庞大的核心数量使GPU比CPU快几个数量级。<br>例如，NVIDIA最近一代的Ampere GPU架构为每个芯片提供了高达312 TFlops的浮点性能，而CPU的浮点性能到目前为止还没有超过1 TFlops。<br>之所以有如此大的差距，原因其实很简单：首先，功耗往往会随时钟频率呈二次方增长。对于一个CPU核心，假设它的运行速度比GPU快4倍，你可以使用16个GPU内核取代，那么GPU的综合性能就是CPU的$16 \times 1/4 = 4$倍。其次，GPU内核要简单得多，这使得它们更节能。同时深度学习中的许多操作需要相对较高的内存带宽，而GPU拥有10倍于CPU的带宽。</p><p>回到2012年的重大突破，当Alex Krizhevsky和Ilya Sutskever实现了可以在GPU硬件上运行的深度卷积神经网络时，一个重大突破出现了。他们意识到卷积神经网络中的计算瓶颈：卷积和矩阵乘法，都是可以在硬件上并行化的操作。于是，他们使用两个显存为3GB的NVIDIA GTX580 GPU实现了快速卷积运算。他们的创新<a href="https://code.google.com/archive/p/cuda-convnet/">cuda-convnet</a>几年来它一直是行业标准，并推动了深度学习热潮。</p><h3 id="1-2-AlexNet"><a href="#1-2-AlexNet" class="headerlink" title="1.2. AlexNet"></a>1.2. AlexNet</h3><p>2012年，AlexNet横空出世。它首次证明了学习到的特征可以超越手工设计的特征，一举打破了计算机视觉研究的现状。<br>AlexNet使用了8层卷积神经网络，并以很大的优势赢得了2012年ImageNet图像识别挑战赛。</p><p>AlexNet和LeNet的架构非常相似，如下图所示。<br>注意，这里我们提供了一个稍微精简版本的AlexNet，去除了当年需要两个小型GPU同时运算的设计特点。</p><p><img src="/assets/post_img/article54/alexnet.svg" alt="从LeNet（左）到AlexNet（右）"></p><p>AlexNet和LeNet的设计理念非常相似，但也存在显著差异。<br>首先，AlexNet比相对较小的LeNet5要深得多。AlexNet由八层组成：五个卷积层、两个全连接隐藏层和一个全连接输出层。<br>其次，AlexNet使用ReLU而不是sigmoid作为其激活函数。</p><p>下面深入研究AlexNet的细节。</p><h4 id="1-2-1-模型设计"><a href="#1-2-1-模型设计" class="headerlink" title="1.2.1. 模型设计"></a>1.2.1. 模型设计</h4><p>在AlexNet的第一层，卷积窗口的形状是$11\times11$。由于ImageNet中大多数图像的宽和高比MNIST图像的多10倍以上，因此，需要一个更大的卷积窗口来捕获目标。<br>第二层中的卷积窗口形状被缩减为$5\times5$，然后是$3\times3$。<br>此外，在第一层、第二层和第五层卷积层之后，加入窗口形状为$3\times3$、步幅为2的最大池化层。而且，AlexNet的卷积通道数目是LeNet的10倍。</p><p>在最后一个卷积层后有两个全连接层，分别有4096个输出。这两个巨大的全连接层拥有将近1GB的模型参数。由于早期GPU显存有限，原版的AlexNet采用了双数据流设计，使得每个GPU只负责存储和计算模型的一半参数。<br>现在GPU显存相对充裕，所以现在很少需要跨GPU分解模型（因此这里的AlexNet模型在这方面与原始论文稍有不同）。</p><h4 id="1-2-2-激活函数"><a href="#1-2-2-激活函数" class="headerlink" title="1.2.2. 激活函数"></a>1.2.2. 激活函数</h4><p>此外，AlexNet将sigmoid激活函数改为更简单的ReLU激活函数。一方面，ReLU激活函数的计算更简单，它不需要如sigmoid激活函数那般复杂的求幂运算。另一方面，当使用不同的参数初始化方法时，ReLU激活函数使训练模型更加容易。<br>当sigmoid激活函数的输出非常接近于0或1时，这些区域的梯度几乎为0，因此反向传播无法继续更新一些模型参数。相反，ReLU激活函数在正区间的梯度总是1。<br>也就是说，如果模型参数没有正确初始化，sigmoid函数可能在正区间内得到几乎为0的梯度，从而使模型无法得到有效的训练。</p><h4 id="1-2-3-容量控制和预处理"><a href="#1-2-3-容量控制和预处理" class="headerlink" title="1.2.3. 容量控制和预处理"></a>1.2.3. 容量控制和预处理</h4><p>AlexNet通过暂退法控制全连接层的模型复杂度，而LeNet只使用了权重衰减。 为了进一步扩充数据，AlexNet在训练时增加了大量的图像增强数据，如翻转、裁切和变色。 这使得模型更健壮，更大的样本量有效地减少了过拟合。有关数据扩增的内容会在<em>计算机视觉</em>章节中讲到。</p><p>下面来构造一下AlexNet：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    <span class="comment"># 使用一个11*11的更大卷积窗口来捕捉对象。</span></span><br><span class="line">    <span class="comment"># 同时，步幅为4，以减少输出的高度和宽度。</span></span><br><span class="line">    <span class="comment"># 输出通道的数目（96）远大于LeNet</span></span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">96</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    <span class="comment"># 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数</span></span><br><span class="line">    nn.Conv2d(<span class="number">96</span>, <span class="number">256</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    <span class="comment"># 使用三个连续的卷积层和较小的卷积窗口。</span></span><br><span class="line">    <span class="comment"># 除了最后的卷积层，输出通道的数量进一步增加。</span></span><br><span class="line">    <span class="comment"># 在前两个卷积层之后，池化层不用于减少输入的高度和宽度</span></span><br><span class="line">    nn.Conv2d(<span class="number">256</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">384</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">384</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    <span class="comment"># 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合</span></span><br><span class="line">    nn.Linear(<span class="number">6400</span>, <span class="number">4096</span>), nn.ReLU(),</span><br><span class="line">    nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">    nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>), nn.ReLU(),</span><br><span class="line">    nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">    <span class="comment"># 最后是输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000</span></span><br><span class="line">    nn.Linear(<span class="number">4096</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure><br>构造一个高度和宽度都为224的单通道数据，来观察每一层输出的形状。 它与前面图的AlexNet架构相匹配。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">X = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X=layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__,<span class="string">&#x27;output shape:\t&#x27;</span>,X.shape)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">Conv2d output shape:         torch.Size([<span class="number">1</span>, <span class="number">96</span>, <span class="number">54</span>, <span class="number">54</span>])</span><br><span class="line">ReLU output shape:   torch.Size([<span class="number">1</span>, <span class="number">96</span>, <span class="number">54</span>, <span class="number">54</span>])</span><br><span class="line">MaxPool2d output shape:      torch.Size([<span class="number">1</span>, <span class="number">96</span>, <span class="number">26</span>, <span class="number">26</span>])</span><br><span class="line">Conv2d output shape:         torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">26</span>, <span class="number">26</span>])</span><br><span class="line">ReLU output shape:   torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">26</span>, <span class="number">26</span>])</span><br><span class="line">MaxPool2d output shape:      torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br><span class="line">Conv2d output shape:         torch.Size([<span class="number">1</span>, <span class="number">384</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br><span class="line">ReLU output shape:   torch.Size([<span class="number">1</span>, <span class="number">384</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br><span class="line">Conv2d output shape:         torch.Size([<span class="number">1</span>, <span class="number">384</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br><span class="line">ReLU output shape:   torch.Size([<span class="number">1</span>, <span class="number">384</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br><span class="line">Conv2d output shape:         torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br><span class="line">ReLU output shape:   torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br><span class="line">MaxPool2d output shape:      torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">5</span>, <span class="number">5</span>])</span><br><span class="line">Flatten output shape:        torch.Size([<span class="number">1</span>, <span class="number">6400</span>])</span><br><span class="line">Linear output shape:         torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">ReLU output shape:   torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">Dropout output shape:        torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">Linear output shape:         torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">ReLU output shape:   torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">Dropout output shape:        torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">Linear output shape:         torch.Size([<span class="number">1</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure></p><h3 id="1-3-读取数据集"><a href="#1-3-读取数据集" class="headerlink" title="1.3. 读取数据集"></a>1.3. 读取数据集</h3><p>在这里使用的是Fashion-MNIST数据集。因为即使在现代GPU上，训练ImageNet模型，同时使其收敛可能需要数小时或数天的时间。<br>将AlexNet直接应用于Fashion-MNIST的一个问题是，Fashion-MNIST图像的分辨率（$28 \times 28$像素）低于ImageNet图像。为了解决这个问题，这里将它们增加到$224 \times 224$（通常来讲这不是一个明智的做法，只是为了匹配AlexNet）。使用<code>d2l.load_data_fashion_mnist</code>函数中的<code>resize</code>参数执行此调整。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">128</span></span><br><span class="line"><span class="comment"># d2l包中的函数，省略其实现，后续训练过程同上</span></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=<span class="number">224</span>)</span><br></pre></td></tr></table></figure></p><h3 id="1-4-训练AlexNet"><a href="#1-4-训练AlexNet" class="headerlink" title="1.4. 训练AlexNet"></a>1.4. 训练AlexNet</h3><p>与LeNet相比，这里的主要变化是使用更小的学习速率训练，这是因为网络更深更广、图像分辨率更高，训练卷积神经网络就更昂贵。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lr, num_epochs = <span class="number">0.01</span>, <span class="number">10</span></span><br><span class="line"><span class="comment"># d2l包中的函数，省略其实现，后续训练过程同上</span></span><br><span class="line"><span class="comment"># 修改设备为mps，适配M1芯片设备</span></span><br><span class="line">train_ch6(net, train_iter, test_iter, num_epochs, lr, torch.device(<span class="string">&#x27;mps&#x27;</span>))</span><br></pre></td></tr></table></figure></p><h2 id="2-使用块的网络（VGG）"><a href="#2-使用块的网络（VGG）" class="headerlink" title="2. 使用块的网络（VGG）"></a>2. 使用块的网络（VGG）</h2><p>虽然AlexNet证明深层神经网络卓有成效，但它没有提供一个通用的模板来指导后续的研究人员设计新的网络。 下面将介绍一些常用于设计深层神经网络的启发式概念。</p><p>与芯片设计中工程师从放置晶体管到逻辑元件再到逻辑块的过程类似，神经网络架构的设计也逐渐变得更加抽象。研究人员开始从单个神经元的角度思考问题，发展到整个层，现在又转向块，重复层的模式。</p><p>使用块的想法首先出现在牛津大学的视觉几何组（visualgeometry group）的VGG网络中。通过使用循环和子程序，可以很容易地在任何现代深度学习框架的代码中实现这些重复的架构。</p><h3 id="2-1-VGG块"><a href="#2-1-VGG块" class="headerlink" title="2.1. VGG块"></a>2.1. VGG块</h3><p>经典卷积神经网络的基本组成部分是下面的这个序列：</p><ol><li>带填充以保持分辨率的卷积层；</li><li>非线性激活函数，如ReLU；</li><li>池化层，如最大池化层。</li></ol><p>而一个VGG块与之类似，由一系列卷积层组成，后面再加上用于空间下采样（缩小图像）的最大池化层。在最初的VGG论文中，作者使用了带有$3 \times 3$卷积核、填充为1（保持高度和宽度）的卷积层，和带有$2 \times 2$池化窗口、步幅为2（每个块后的分辨率减半）的最大池化层。在下面的代码中定义了一个名为<code>vgg_block</code>的函数来实现一个VGG块。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 该函数有三个参数，分别对应于卷积层的数量num_convs、</span></span><br><span class="line"><span class="comment"># 输入通道的数量in_channels 和输出通道的数量out_channels.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vgg_block</span>(<span class="params">num_convs, in_channels, out_channels</span>):</span></span><br><span class="line">    layers = []</span><br><span class="line">    <span class="comment"># for _ in range(n) 一般仅仅用于循环n次，不用设置变量，用 _ 指代临时变量，只在这个语句中使用一次。</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_convs):</span><br><span class="line">        layers.append(nn.Conv2d(in_channels, out_channels,</span><br><span class="line">                                kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">        layers.append(nn.ReLU())</span><br><span class="line">        in_channels = out_channels</span><br><span class="line">    layers.append(nn.MaxPool2d(kernel_size=<span class="number">2</span>,stride=<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*layers)</span><br></pre></td></tr></table></figure></p><h3 id="2-2-VGG网络"><a href="#2-2-VGG网络" class="headerlink" title="2.2. VGG网络"></a>2.2. VGG网络</h3><p>与AlexNet、LeNet一样，VGG网络可以分为两部分：第一部分主要由卷积层和池化层组成，第二部分由全连接层组成。</p><p><img src="/assets/post_img/article54/vgg.svg" alt="从AlexNet到VGG，本质上都是块设计"></p><p>VGG神经网络连接上图中的几个VGG块。其中有超参数变量<code>conv_arch</code>。该变量指定了每个VGG块里卷积层个数和输出通道数。全连接模块则与AlexNet中的相同。</p><p>原始VGG网络有5个卷积块，其中前两个块各有一个卷积层，后三个块各包含两个卷积层。第一个模块有64个输出通道，每个后续模块将输出通道数量翻倍，直到该数字达到512。由于该网络使用8个卷积层和3个全连接层，因此它通常被称为VGG-11。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conv_arch = ((<span class="number">1</span>, <span class="number">64</span>), (<span class="number">1</span>, <span class="number">128</span>), (<span class="number">2</span>, <span class="number">256</span>), (<span class="number">2</span>, <span class="number">512</span>), (<span class="number">2</span>, <span class="number">512</span>))</span><br></pre></td></tr></table></figure></p><p>下面的代码实现了VGG-11。可以通过在conv_arch上执行for循环来简单实现。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vgg</span>(<span class="params">conv_arch</span>):</span></span><br><span class="line">    conv_blks = []</span><br><span class="line">    in_channels = <span class="number">1</span></span><br><span class="line">    <span class="comment"># 卷积层部分</span></span><br><span class="line">    <span class="keyword">for</span> (num_convs, out_channels) <span class="keyword">in</span> conv_arch:</span><br><span class="line">        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))</span><br><span class="line">        in_channels = out_channels</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        <span class="comment"># 星号变量</span></span><br><span class="line">        *conv_blks, nn.Flatten(),</span><br><span class="line">        <span class="comment"># 全连接层部分</span></span><br><span class="line">        nn.Linear(out_channels * <span class="number">7</span> * <span class="number">7</span>, <span class="number">4096</span>), nn.ReLU(), nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">        nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>), nn.ReLU(), nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">        nn.Linear(<span class="number">4096</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">net = vgg(conv_arch)</span><br></pre></td></tr></table></figure></p><p>构建一个高度和宽度为224的单通道数据样本，以观察每个层输出的形状。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">X = torch.randn(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line"><span class="keyword">for</span> blk <span class="keyword">in</span> net:</span><br><span class="line">    X = blk(X)</span><br><span class="line">    <span class="built_in">print</span>(blk.__class__.__name__,<span class="string">&#x27;output shape:\t&#x27;</span>,X.shape)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">64</span>, <span class="number">112</span>, <span class="number">112</span>])</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">128</span>, <span class="number">56</span>, <span class="number">56</span>])</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">512</span>, <span class="number">14</span>, <span class="number">14</span>])</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">512</span>, <span class="number">7</span>, <span class="number">7</span>])</span><br><span class="line">Flatten output shape:        torch.Size([<span class="number">1</span>, <span class="number">25088</span>])</span><br><span class="line">Linear output shape:         torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">ReLU output shape:   torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">Dropout output shape:        torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">Linear output shape:         torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">ReLU output shape:   torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">Dropout output shape:        torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">Linear output shape:         torch.Size([<span class="number">1</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure></p><p>在每个块的高度和宽度减半，最终高度和宽度都为7。最后再展平表示，送入全连接层处理。</p><h3 id="2-3-训练模型"><a href="#2-3-训练模型" class="headerlink" title="2.3. 训练模型"></a>2.3. 训练模型</h3><p>VGG-11比AlexNet计算量更大，因此构建了一个通道数较少的网络，足够用于训练Fashion-MNIST数据集。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ratio = <span class="number">4</span></span><br><span class="line">small_conv_arch = [(pair[<span class="number">0</span>], pair[<span class="number">1</span>] // ratio) <span class="keyword">for</span> pair <span class="keyword">in</span> conv_arch]</span><br><span class="line">net = vgg(small_conv_arch)</span><br></pre></td></tr></table></figure></p><p>除了使用略高的学习率外，模型训练过程与之前的AlexNet类似。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lr, num_epochs, batch_size = <span class="number">0.05</span>, <span class="number">10</span>, <span class="number">128</span></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=<span class="number">224</span>)</span><br><span class="line">train_ch6(net, train_iter, test_iter, num_epochs, lr, torch.device(<span class="string">&#x27;mps&#x27;</span>))</span><br></pre></td></tr></table></figure></p><h2 id="3-网络中的网络（NiN）"><a href="#3-网络中的网络（NiN）" class="headerlink" title="3. 网络中的网络（NiN）"></a>3. 网络中的网络（NiN）</h2><p>LeNet、AlexNet和VGG都有一个共同的设计模式：通过一系列的卷积层与池化层来提取空间结构特征；然后通过全连接层对特征的表征进行处理。 AlexNet和VGG对LeNet的改进主要在于如何扩大和加深这两个模块。 然而如果在网络的早期使用全连接层，则可能会完全放弃表征的空间结构。 网络中的网络（NiN）提供了一个非常简单的解决方案：在每个像素的通道上分别使用多层感知机。</p><h3 id="3-1-NiN块"><a href="#3-1-NiN块" class="headerlink" title="3.1. NiN块"></a>3.1. NiN块</h3><p>卷积层的输入和输出由四维张量组成，张量的每个轴分别对应样本、通道、高度和宽度。另外，全连接层的输入和输出通常是分别对应于样本和特征的二维张量。</p><p>NiN的想法是在每个像素位置（针对每个高度和宽度）应用一个全连接层。如果将权重连接到每个空间位置，则可以将其视为$1 \times 1$卷积层，或作为在每个像素位置上独立作用的全连接层。从另一个角度看，即将空间维度中的每个像素视为单个样本，将通道维度视为不同特征。</p><p>NiN块以一个普通卷积层开始，后面是两个$1 \times 1$的卷积层。这两个$1 \times 1$卷积层充当带有ReLU激活函数的逐像素全连接层。第一层的卷积窗口形状通常由用户设置。随后的卷积窗口形状固定为$1 \times 1$。</p><p><img src="/assets/post_img/article54/nin.svg" alt="VGG和NiN及他们的块之间主要架构差异"></p><p>NiN块函数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nin_block</span>(<span class="params">in_channels, out_channels, kernel_size, strides, padding</span>):</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">        nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>), nn.ReLU())</span><br></pre></td></tr></table></figure></p><h3 id="3-2-NiN模型"><a href="#3-2-NiN模型" class="headerlink" title="3.2. NiN模型"></a>3.2. NiN模型</h3><p>最初的NiN网络从AlexNet中得到了一些启示。NiN使用窗口形状为$11\times11$、$5\times5$和$3\times3$的卷积层，输出通道数量与AlexNet中的相同。每个NiN块后有一个最大池化层，池化窗口形状为$3\times3$，步幅为2。</p><p>NiN和AlexNet之间的一个显著区别是NiN完全取消了全连接层。相反，NiN使用一个NiN块，其输出通道数等于标签类别的数量。最后放一个<em>全局平均池化层</em>（global average pooling layer），生成一个对数几率 （logits）。NiN设计的一个优点是，它显著减少了模型所需参数的数量。然而在实践中，这种设计有时会增加训练模型的时间。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    nin_block(<span class="number">1</span>, <span class="number">96</span>, kernel_size=<span class="number">11</span>, strides=<span class="number">4</span>, padding=<span class="number">0</span>),</span><br><span class="line">    <span class="comment"># 最大池化，第一个参数是池化窗口的大小</span></span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">96</span>, <span class="number">256</span>, kernel_size=<span class="number">5</span>, strides=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">256</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">    <span class="comment"># 标签类别数是10</span></span><br><span class="line">    nin_block(<span class="number">384</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    <span class="comment"># 自适应平均池化，参数为输出的形状</span></span><br><span class="line">    nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">    <span class="comment"># 将四维的输出转成二维的输出，其形状为(批量大小,10)</span></span><br><span class="line">    nn.Flatten())</span><br></pre></td></tr></table></figure></p><p>创建一个数据样本来查看每个块的输出形状：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__,<span class="string">&#x27;output shape:\t&#x27;</span>, X.shape)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">96</span>, <span class="number">54</span>, <span class="number">54</span>])</span><br><span class="line">MaxPool2d output shape:      torch.Size([<span class="number">1</span>, <span class="number">96</span>, <span class="number">26</span>, <span class="number">26</span>])</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">26</span>, <span class="number">26</span>])</span><br><span class="line">MaxPool2d output shape:      torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">384</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br><span class="line">MaxPool2d output shape:      torch.Size([<span class="number">1</span>, <span class="number">384</span>, <span class="number">5</span>, <span class="number">5</span>])</span><br><span class="line">Dropout output shape:        torch.Size([<span class="number">1</span>, <span class="number">384</span>, <span class="number">5</span>, <span class="number">5</span>])</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">10</span>, <span class="number">5</span>, <span class="number">5</span>])</span><br><span class="line">AdaptiveAvgPool2d output shape:      torch.Size([<span class="number">1</span>, <span class="number">10</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">Flatten output shape:        torch.Size([<span class="number">1</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure></p><h3 id="3-3-训练模型"><a href="#3-3-训练模型" class="headerlink" title="3.3. 训练模型"></a>3.3. 训练模型</h3><p>使用Fashion-MNIST来训练模型。训练NiN与训练AlexNet、VGG时相似。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lr, num_epochs, batch_size = <span class="number">0.1</span>, <span class="number">10</span>, <span class="number">128</span></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=<span class="number">224</span>)</span><br><span class="line">train_ch6(net, train_iter, test_iter, num_epochs, lr, torch.device(<span class="string">&#x27;mps&#x27;</span>))</span><br></pre></td></tr></table></figure></p><h2 id="4-含并行连结的网络（GoogLeNet）"><a href="#4-含并行连结的网络（GoogLeNet）" class="headerlink" title="4. 含并行连结的网络（GoogLeNet）"></a>4. 含并行连结的网络（GoogLeNet）</h2><p>在2014年的ImageNet图像识别挑战赛中，一个名叫<em>GoogLeNet</em>的网络架构大放异彩。GoogLeNet吸收了NiN中串联网络的思想，并在此基础上做了改进。这篇论文的一个重点是解决了什么样大小的卷积核最合适的问题。<br>毕竟，以前流行的网络使用小到$1\times1$，大到$11\times11$的卷积核。该文的一个观点是，有时使用不同大小的卷积核组合是有利的。本节将介绍一个稍微简化的GoogLeNet版本：省略了一些为稳定训练而添加的特殊特性，现在有了更好的训练方法，这些特性不是必要的。</p><h3 id="4-1-Inception块"><a href="#4-1-Inception块" class="headerlink" title="4.1. Inception块"></a>4.1. Inception块</h3><p>在GoogLeNet中，基本的卷积块被称为<em>Inception块</em>（Inception block）。这很可能得名于电影《盗梦空间》（Inception），因为电影中的一句话“我们需要走得更深”（“We need to go deeper”）。</p><p><img src="/assets/post_img/article54/inception.svg" alt="Inception块的架构"></p><p>如上图所示，Inception块由四条并行路径组成。前三条路径使用窗口大小为$1\times1$、$3\times3$和$5\times5$的卷积层，从不同空间大小中提取信息。中间的两条路径在输入上执行$1\times1$卷积，以减少通道数，从而降低模型的复杂性。第四条路径使用$3\times3$最大池化层，然后使用$1\times1$卷积层来改变通道数。</p><p>这四条路径都使用合适的填充来使输入与输出的高和宽一致，最后会将每条线路的输出在通道维度上连结，并构成Inception块的输出。在Inception块中，通常调整的超参数是<em>每层输出通道数</em>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Inception</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="comment"># c1--c4是每条路径的输出通道数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels, c1, c2, c3, c4, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Inception, self).__init__(**kwargs)</span><br><span class="line">        <span class="comment"># 线路1，单1x1卷积层</span></span><br><span class="line">        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 线路2，1x1卷积层后接3x3卷积层</span></span><br><span class="line">        self.p2_1 = nn.Conv2d(in_channels, c2[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p2_2 = nn.Conv2d(c2[<span class="number">0</span>], c2[<span class="number">1</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 线路3，1x1卷积层后接5x5卷积层</span></span><br><span class="line">        self.p3_1 = nn.Conv2d(in_channels, c3[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p3_2 = nn.Conv2d(c3[<span class="number">0</span>], c3[<span class="number">1</span>], kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 线路4，3x3最大池化层后接1x1卷积层</span></span><br><span class="line">        self.p4_1 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        p1 = F.relu(self.p1_1(x))</span><br><span class="line">        <span class="comment"># 为什么中间两条加激活函数呢？</span></span><br><span class="line">        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))</span><br><span class="line">        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))</span><br><span class="line">        p4 = F.relu(self.p4_2(self.p4_1(x)))</span><br><span class="line">        <span class="comment"># 在通道维度上连结输出</span></span><br><span class="line">        <span class="keyword">return</span> torch.cat((p1, p2, p3, p4), dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p><p>为什么GoogLeNet如此有效呢？首要原因是滤波器（也就是不同大小的卷积核，或者说卷积层）的组合，它们可以用各种滤波器尺寸探索图像，这意味着不同大小的滤波器可以有效地识别不同范围的图像细节。同时，我们可以为不同的滤波器分配不同数量的参数。</p><h3 id="4-2-GoogLeNet模型"><a href="#4-2-GoogLeNet模型" class="headerlink" title="4.2. GoogLeNet模型"></a>4.2. GoogLeNet模型</h3><p>如下图所示，GoogLeNet一共使用9个Inception块和全局平均池化层的堆叠来生成其估计值。Inception块之间的最大池化层可降低维度。第一个模块类似于AlexNet和LeNet，Inception块的组合从VGG继承，全局平均池化层避免了在最后使用全连接层。</p><p><img src="/assets/post_img/article54/inception-full.svg" alt="GoogleNet架构"></p><p>下面逐一实现GoogLeNet的每个模块。第一个模块使用64个通道、$7\times7$卷积层。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">b1 = nn.Sequential(nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure></p><p>第二个模块使用两个卷积层：第一个卷积层是64个通道、$1\times1$卷积层；第二个卷积层使用将通道数量增加三倍的$3\times3$卷积层。这对应于Inception块中的第二条路径。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">b2 = nn.Sequential(nn.Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">1</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.Conv2d(<span class="number">64</span>, <span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure></p><p>第三个模块串联两个完整的Inception块。<br>第一个Inception块的输出通道数为$64+128+32+32=256$，四个路径之间的输出通道数量比为$64:128:32:32=2:4:1:1$。第二个和第三个路径首先将输入通道的数量分别减少到$96/192=1/2$和$16/192=1/12$，然后连接第二个卷积层。第二个Inception块的输出通道数增加到$128+192+96+64=480$，四个路径之间的输出通道数量比为$128:192:96:64 = 4:6:3:2$。第二条和第三条路径首先将输入通道的数量分别减少到$128/256=1/2$和$32/256=1/8$。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">b3 = nn.Sequential(Inception(<span class="number">192</span>, <span class="number">64</span>, (<span class="number">96</span>, <span class="number">128</span>), (<span class="number">16</span>, <span class="number">32</span>), <span class="number">32</span>),</span><br><span class="line">                   Inception(<span class="number">256</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">192</span>), (<span class="number">32</span>, <span class="number">96</span>), <span class="number">64</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><br>第四模块更加复杂，它串联了5个Inception块，其输出通道数分别是$192+208+48+64=512$、$160+224+64+64=512$、$128+256+64+64=512$、$112+288+64+64=528$和$256+320+128+128=832$。这些路径的通道数分配和第三模块中的类似，首先是含$3×3$卷积层的第二条路径输出最多通道，其次是仅含$1×1$卷积层的第一条路径，之后是含$5×5$卷积层的第三条路径和含$3×3$最大汇聚层的第四条路径。其中第二、第三条路径都会先按比例减小通道数。这些比例在各个Inception块中都略有不同。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">b4 = nn.Sequential(Inception(<span class="number">480</span>, <span class="number">192</span>, (<span class="number">96</span>, <span class="number">208</span>), (<span class="number">16</span>, <span class="number">48</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">160</span>, (<span class="number">112</span>, <span class="number">224</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">256</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">112</span>, (<span class="number">144</span>, <span class="number">288</span>), (<span class="number">32</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">528</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><br>第五模块包含输出通道数为$256+320+128+128=832$和$384+384+128+128=1024$的两个Inception块。其中每条路径通道数的分配思路和第三、第四模块中的一致，只是在具体数值上有所不同。需要注意的是，第五模块的后面紧跟输出层，该模块同NiN一样使用全局平均池化层，将每个通道的高和宽变成1。最后将输出变成二维数组，再接上一个输出个数为标签类别数的全连接层。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">b5 = nn.Sequential(Inception(<span class="number">832</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   Inception(<span class="number">832</span>, <span class="number">384</span>, (<span class="number">192</span>, <span class="number">384</span>), (<span class="number">48</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>)),</span><br><span class="line">                   nn.Flatten())</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(<span class="number">1024</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure></p><p>GoogLeNet模型的计算复杂，而且不如VGG那样便于修改通道数。 为了使Fashion-MNIST上的训练短小精悍，这里将输入的高和宽从224降到96，简化了计算。下面演示各个模块输出的形状变化。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">96</span>, <span class="number">96</span>))</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__,<span class="string">&#x27;output shape:\t&#x27;</span>, X.shape)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">64</span>, <span class="number">24</span>, <span class="number">24</span>])</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">192</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">480</span>, <span class="number">6</span>, <span class="number">6</span>])</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">832</span>, <span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">1024</span>])</span><br><span class="line">Linear output shape:         torch.Size([<span class="number">1</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure></p><h3 id="4-3-训练模型"><a href="#4-3-训练模型" class="headerlink" title="4.3. 训练模型"></a>4.3. 训练模型</h3><p>使用Fashion-MNIST数据集来训练我们的模型。在训练之前将图片转换为$96×96$分辨率。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lr, num_epochs, batch_size = <span class="number">0.1</span>, <span class="number">10</span>, <span class="number">128</span></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=<span class="number">96</span>)</span><br><span class="line">train_ch6(net, train_iter, test_iter, num_epochs, lr, torch.device(<span class="string">&#x27;mps&#x27;</span>))</span><br></pre></td></tr></table></figure></p><h2 id="5-批量规范化（batch-normalization）"><a href="#5-批量规范化（batch-normalization）" class="headerlink" title="5. 批量规范化（batch normalization）"></a>5. 批量规范化（batch normalization）</h2><p>训练深层神经网络是十分困难的，特别是在较短的时间内使他们收敛更加棘手。本节中将介绍<em>批量规范化</em>（batch normalization），这是一种流行且有效的技术，可持续加速深层网络的收敛速度。再结合下节将介绍的残差块，批量规范化使得研究人员能够训练100层以上的网络。</p><h3 id="5-1-训练深层网络"><a href="#5-1-训练深层网络" class="headerlink" title="5.1. 训练深层网络"></a>5.1. 训练深层网络</h3><p>对于批量规范化层的需求源于人们在训练神经网络时遇到的一些实际挑战。<br>首先，数据预处理的方式通常会对最终结果产生巨大影响。以应用多层感知机来预测房价为例，使用真实数据时，第一步是标准化输入特征，使其平均值为0，方差为1。直观地说，这种标准化可以很好地与优化器配合使用，因为它可以将参数的量级进行统一。</p><p>第二，对于典型的多层感知机或卷积神经网络。当训练时，中间层中的变量（例如，多层感知机中的仿射变换输出）可能具有更广的变化范围：不论是沿着从输入到输出的层，跨同一层中的单元，或是随着时间的推移，模型参数的随着训练更新变幻莫测。<br>批量规范化的发明者非正式地假设，这些变量分布中的这种偏移可能会阻碍网络的收敛。也就是我们猜想，如果一个层的可变值是另一层的100倍，则可能需要对学习率进行补偿调整。</p><p>第三，更深层的网络很复杂，容易过拟合。这意味着正则化变得更加重要。批量规范化应用于单个可选层（也可以应用到所有层），其原理如下：在每次训练迭代中，我们首先规范化输入，即通过减去其均值并除以其标准差，这两步操作均基于当前小批量处理。然后应用比例系数和比例偏移。正是由于这个基于<em>批量</em>统计的标准化，才有了<em>批量规范化</em>的名称。</p><p>注意如果尝试使用大小为1的小批量应用批量规范化，将无法学到任何东西。这是因为在减去均值之后，每个隐藏单元将为0。所以，只有使用足够大的小批量，批量规范化这种方法才是有效且稳定的。并且在应用批量规范化时，批量大小的选择可能比没有批量规范化时更重要。</p><p>从形式上来说，用$\mathbf{x} \in \mathcal{B}$表示一个来自小批量$\mathcal{B}$的输入，批量规范化$\mathrm{BN}$根据以下表达式转换$\mathbf{x}$：</p><script type="math/tex; mode=display">\mathrm{BN}(\mathbf{x}) = \boldsymbol{\gamma} \odot \frac{\mathbf{x} - \hat{\boldsymbol{\mu}}_\mathcal{B}}{\hat{\boldsymbol{\sigma}}_\mathcal{B}} + \boldsymbol{\beta}</script><p>在上式中，$\hat{\boldsymbol{\mu}}_\mathcal{B}$是小批量$\mathcal{B}$的样本均值，$\hat{\boldsymbol{\sigma}}_\mathcal{B}$是小批量$\mathcal{B}$的样本标准差。应用标准化后，生成的小批量的平均值为0和单位方差为1。由于单位方差（与其他一些魔法数）是一个主观的选择，因此我们通常包含<em>拉伸参数</em>（scale）$\boldsymbol{\gamma}$和<em>偏移参数</em>（shift）$\boldsymbol{\beta}$，它们的形状与$\mathbf{x}$相同。$\boldsymbol{\gamma}$和$\boldsymbol{\beta}$是需要与其他模型参数一起学习的参数。</p><p>由于在训练过程中，中间层的变化幅度不能过于剧烈，而批量规范化将每一层主动居中，并将它们重新调整为给定的平均值和大小（通过$\hat{\boldsymbol{\mu}}_\mathcal{B}$和${\hat{\boldsymbol{\sigma}}_\mathcal{B}}$）。</p><p>从形式上来看，我们计算出上式中的$\hat{\boldsymbol{\mu}}_\mathcal{B}$和${\hat{\boldsymbol{\sigma}}_\mathcal{B}}$，如下所示：</p><script type="math/tex; mode=display">\begin{aligned} \hat{\boldsymbol{\mu}}_\mathcal{B} &= \frac{1}{|\mathcal{B}|} \sum_{\mathbf{x} \in \mathcal{B}} \mathbf{x},\\\hat{\boldsymbol{\sigma}}_\mathcal{B}^2 &= \frac{1}{|\mathcal{B}|} \sum_{\mathbf{x} \in \mathcal{B}} (\mathbf{x} - \hat{\boldsymbol{\mu}}_{\mathcal{B}})^2 + \epsilon\end{aligned}</script><p>我们在方差估计值中添加一个小的常量$\epsilon &gt; 0$，以确保我们永远不会尝试除以零，即使在经验方差估计值可能消失的情况下也是如此。估计值$\hat{\boldsymbol{\mu}}_\mathcal{B}$和${\hat{\boldsymbol{\sigma}}_\mathcal{B}}$通过使用平均值和方差的噪声（常量）估计来抵消缩放问题。这种噪声事实上是有益的。</p><p>由于某些无法用理论解释的原因，优化中的各种噪声源通常会促使更快的训练和较少的过拟合：这种变化似乎是正则化的一种形式。在一些初步研究中，将批量规范化的性质与贝叶斯先验相关联。这些理论揭示了为什么批量规范化最适应$50 \sim 100$范围中的中等批量大小的难题。</p><p>另外，批量规范化层在”训练模式“（通过小批量统计数据规范化）和“预测模式”（通过数据集统计规范化）中的功能不同。在训练过程中，我们无法得知使用整个数据集来估计平均值和方差，所以只能根据每个小批次的平均值和方差不断训练模型。而在预测模式下，可以根据整个数据集精确计算批量规范化所需的平均值和方差。</p><p>下面，了解一下批量规范化在实践中是如何工作的。</p><h3 id="5-2-批量规范化层"><a href="#5-2-批量规范化层" class="headerlink" title="5.2. 批量规范化层"></a>5.2. 批量规范化层</h3><p>批量规范化和其他层之间的一个关键区别是，由于批量规范化在完整的小批量上运行，因此不能像以前在引入其他层时那样忽略批量大小。 下面讨论两种情况：全连接层和卷积层，他们的批量规范化实现略有不同。</p><h4 id="5-2-1-全连接层"><a href="#5-2-1-全连接层" class="headerlink" title="5.2.1. 全连接层"></a>5.2.1. 全连接层</h4><p>通常会将批量规范化层置于全连接层中的仿射变换和激活函数之间。设全连接层的输入为x，权重参数和偏置参数分别为$\mathbf{W}$和$\mathbf{b}$，激活函数为$\phi$，批量规范化的运算符为$\mathrm{BN}$。那么，使用批量规范化的全连接层的输出的计算详情如下：</p><script type="math/tex; mode=display">\mathbf{h} = \phi(\mathrm{BN}(\mathbf{W}\mathbf{x} + \mathbf{b}) )</script><p>均值和方差是在应用变换的”相同”小批量上计算的。</p><h4 id="5-2-2-卷积层"><a href="#5-2-2-卷积层" class="headerlink" title="5.2.2. 卷积层"></a>5.2.2. 卷积层</h4><p>同样，对于卷积层，我们可以在卷积层之后和非线性激活函数之前应用批量规范化。当卷积有多个输出通道时，则需要对这些通道的“每个”输出执行批量规范化，每个通道都有自己的拉伸（scale）和偏移（shift）参数，这两个参数都是标量。假设当前小批量包含$m$个样本，并且对于每个通道，卷积的输出具有高度$p$和宽度$q$。那么对于卷积层，在每个输出通道的$m \cdot p \cdot q$个元素上同时执行每个批量规范化。因此，在计算平均值和方差时，会收集所有空间位置的值，然后在给定通道内应用相同的均值和方差，以便在每个空间位置对值进行规范化。</p><h4 id="5-2-3-预测过程中的批量规范化"><a href="#5-2-3-预测过程中的批量规范化" class="headerlink" title="5.2.3. 预测过程中的批量规范化"></a>5.2.3. 预测过程中的批量规范化</h4><p>批量规范化在训练模式和预测模式下的行为通常不同。首先，将训练好的模型用于预测时，我们不再需要样本均值中的噪声以及在微批次上估计每个小批次产生的样本方差了。其次，例如，我们可能需要使用模型对逐个样本进行预测。一种常用的方法是通过移动平均估算整个训练数据集的样本均值和方差，并在预测时使用它们得到确定的输出。可见，和暂退法一样，批量规范化层在训练模式和预测模式下的计算结果也是不一样的。</p><h3 id="5-3-从零实现"><a href="#5-3-从零实现" class="headerlink" title="5.3. 从零实现"></a>5.3. 从零实现</h3><p>从头开始实现一个具有张量的批量规范化层：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_norm</span>(<span class="params">X, gamma, beta, moving_mean, moving_var, eps, momentum</span>):</span></span><br><span class="line">    <span class="comment"># 通过is_grad_enabled来判断当前模式是训练模式还是预测模式</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> torch.is_grad_enabled():</span><br><span class="line">        <span class="comment"># 预测模式下，直接使用传入的移动平均所得的均值和方差</span></span><br><span class="line">        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 训练模式，判断是全连接还是卷积</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(X.shape) <span class="keyword">in</span> (<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(X.shape) == <span class="number">2</span>:</span><br><span class="line">            <span class="comment"># 使用全连接层的情况，计算特征维上的均值和方差</span></span><br><span class="line">            mean = X.mean(dim=<span class="number">0</span>)</span><br><span class="line">            var = ((X - mean) ** <span class="number">2</span>).mean(dim=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。</span></span><br><span class="line">            <span class="comment"># 这里需要保持X的形状以便后面可以做广播运算</span></span><br><span class="line">            mean = X.mean(dim=(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line">            var = ((X - mean) ** <span class="number">2</span>).mean(dim=(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 训练模式下，用当前的均值和方差做标准化</span></span><br><span class="line">        X_hat = (X - mean) / torch.sqrt(var + eps)</span><br><span class="line">        <span class="comment"># 更新移动平均的均值和方差</span></span><br><span class="line">        moving_mean = momentum * moving_mean + (<span class="number">1.0</span> - momentum) * mean</span><br><span class="line">        moving_var = momentum * moving_var + (<span class="number">1.0</span> - momentum) * var</span><br><span class="line">    <span class="comment"># Y为该批量规范化结果</span></span><br><span class="line">    Y = gamma * X_hat + beta  <span class="comment"># 缩放和移位</span></span><br><span class="line">    <span class="keyword">return</span> Y, moving_mean.data, moving_var.data</span><br></pre></td></tr></table></figure></p><p>现在创建一个正确的BatchNorm层。这个层将保持适当的参数：拉伸<code>gamma</code>和偏移<code>beta</code>,这两个参数将在训练过程中更新。此外该层将保存均值和方差的移动平均值，以便在模型预测期间随后使用。</p><p>撇开算法细节，注意这里实现层的基础设计模式。通常情况下会用一个单独的函数定义其数学原理，比如说<code>batch_norm</code>。然后将此功能集成到一个自定义层中，其代码主要处理数据移动到训练设备（如GPU）、分配和初始化任何必需的变量、跟踪移动平均线（此处为均值和方差）等问题。简单起见这里没有采用自动推断输入形状，因此我们需要指定整个特征的数量（num_dims）。但在调用深度学习框架内置的批量规范化API时该迎刃而解。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BatchNorm</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="comment"># num_features：完全连接层的输出数量或卷积层的输出通道数。</span></span><br><span class="line">    <span class="comment"># num_dims：2表示完全连接层，4表示卷积层</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_features, num_dims</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">if</span> num_dims == <span class="number">2</span>:</span><br><span class="line">            shape = (<span class="number">1</span>, num_features)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            shape = (<span class="number">1</span>, num_features, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 参与求梯度和迭代的拉伸和偏移参数，分别初始化成1和0</span></span><br><span class="line">        self.gamma = nn.Parameter(torch.ones(shape))</span><br><span class="line">        self.beta = nn.Parameter(torch.zeros(shape))</span><br><span class="line">        <span class="comment"># 非模型参数的变量初始化为0和1</span></span><br><span class="line">        self.moving_mean = torch.zeros(shape)</span><br><span class="line">        self.moving_var = torch.ones(shape)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        <span class="comment"># 如果X不在内存上，将moving_mean和moving_var</span></span><br><span class="line">        <span class="comment"># 复制到X所在显存上</span></span><br><span class="line">        <span class="keyword">if</span> self.moving_mean.device != X.device:</span><br><span class="line">            self.moving_mean = self.moving_mean.to(X.device)</span><br><span class="line">            self.moving_var = self.moving_var.to(X.device)</span><br><span class="line">        <span class="comment"># 保存更新过的moving_mean和moving_var</span></span><br><span class="line">        Y, self.moving_mean, self.moving_var = batch_norm(</span><br><span class="line">            X, self.gamma, self.beta, self.moving_mean,</span><br><span class="line">            self.moving_var, eps=<span class="number">1e-5</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">        <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure><h3 id="5-4-使用批量规范化层的-LeNet"><a href="#5-4-使用批量规范化层的-LeNet" class="headerlink" title="5.4. 使用批量规范化层的 LeNet"></a>5.4. 使用批量规范化层的 LeNet</h3><p>下面将<em>批量规范化</em>层应用于LeNet模型。批量规范化是在卷积层或全连接层之后、相应的激活函数之前应用的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>), BatchNorm(<span class="number">6</span>, num_dims=<span class="number">4</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>), BatchNorm(<span class="number">16</span>, num_dims=<span class="number">4</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>), nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">16</span>*<span class="number">4</span>*<span class="number">4</span>, <span class="number">120</span>), BatchNorm(<span class="number">120</span>, num_dims=<span class="number">2</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>), BatchNorm(<span class="number">84</span>, num_dims=<span class="number">2</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">84</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure><p>下面将在Fashion-MNIST数据集上训练网络。 这个代码与本书第一次训练LeNet时几乎相同，主要区别在于学习率大得多（大了0.1？）。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lr, num_epochs, batch_size = <span class="number">1.0</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size)</span><br><span class="line">train_ch6(net, train_iter, test_iter, num_epochs, lr, torch.device(<span class="string">&#x27;something&#x27;</span>))</span><br></pre></td></tr></table></figure></p><p>看看从第一个批量规范化层中学到的拉伸参数gamma和偏移参数beta:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">net[<span class="number">1</span>].gamma.reshape((-<span class="number">1</span>,)), net[<span class="number">1</span>].beta.reshape((-<span class="number">1</span>,))</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">(tensor([<span class="number">0.3362</span>, <span class="number">4.0349</span>, <span class="number">0.4496</span>, <span class="number">3.7056</span>, <span class="number">3.7774</span>, <span class="number">2.6762</span>], device=<span class="string">&#x27;cuda:0&#x27;</span>,</span><br><span class="line">        grad_fn=&lt;ReshapeAliasBackward0&gt;),</span><br><span class="line"> tensor([-<span class="number">0.5739</span>,  <span class="number">4.1376</span>,  <span class="number">0.5126</span>,  <span class="number">0.3060</span>, -<span class="number">2.5187</span>,  <span class="number">0.3683</span>], device=<span class="string">&#x27;cuda:0&#x27;</span>,</span><br><span class="line">        grad_fn=&lt;ReshapeAliasBackward0&gt;))</span><br></pre></td></tr></table></figure></p><h3 id="5-5-框架实现"><a href="#5-5-框架实现" class="headerlink" title="5.5. 框架实现"></a>5.5. 框架实现</h3><p>直接使用深度学习框架中定义的BatchNorm。 该代码看起来几乎与从零实现的代码相同。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    <span class="comment"># nn.BatchNorm2d自动推断输入形状</span></span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>), nn.BatchNorm2d(<span class="number">6</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>), nn.BatchNorm2d(<span class="number">16</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>), nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">256</span>, <span class="number">120</span>), nn.BatchNorm1d(<span class="number">120</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>), nn.BatchNorm1d(<span class="number">84</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">84</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure></p><p>使用相同超参数来训练模型。 注意通常高级API变体运行速度快得多，因为它的代码已编译为C++或CUDA，而我们的自定义代码由Python实现。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_ch6(net, train_iter, test_iter, num_epochs, lr, torch.device(<span class="string">&#x27;something&#x27;</span>))</span><br></pre></td></tr></table></figure></p><h3 id="5-6-争议，无法被解释的方法"><a href="#5-6-争议，无法被解释的方法" class="headerlink" title="5.6. 争议，无法被解释的方法"></a>5.6. 争议，无法被解释的方法</h3><p>直观地说，批量规范化被认为可以使优化更加平滑。然而我们必须区分直觉和对我们观察到的现象的真实解释。我们甚至不知道简单的神经网络（多层感知机和传统的卷积神经网络）为什么如此有效。即使在暂退法和权重衰减的情况下，它们仍然非常灵活，因此无法通过常规的学习理论泛化保证来解释它们是否能够泛化到看不见的数据。</p><p>在提出批量规范化的论文中，作者除了介绍了其应用，还解释了其原理：通过减少<em>内部协变量偏移</em>（internal covariate shift）。据推测，作者所说的“内部协变量转移”类似于上述的直觉（即对现象的猜测），即变量值的分布在训练过程中会发生变化。然而，这种解释有两个问题：<br>1、这种偏移与严格定义的<em>协变量偏移</em>（covariate shift）非常不同，所以这个名字用词不当。<br>2、这种解释只提供了一种不明确的直觉，但留下了一个有待后续挖掘的问题：为什么这项技术如此有效？</p><blockquote><p>本书旨在传达实践者用来发展深层神经网络的直觉。然而，重要的是将这些指导性直觉与既定的科学事实区分开来。最终，当你掌握了这些方法，并开始撰写自己的研究论文时，你会希望清楚地区分技术和直觉。</p></blockquote><p>随着批量规范化的普及，“内部协变量偏移”的解释反复出现在技术文献的辩论，特别是关于“如何展示机器学习研究”的更广泛的讨论中。Ali Rahimi在接受2017年NeurIPS大会的“接受时间考验奖”（Test of Time Award）时发表了一篇令人难忘的演讲。他将“内部协变量转移”作为焦点，将现代深度学习的实践比作炼金术。他对该示例进行了详细回顾，概述了机器学习中令人不安的趋势。此外，一些作者对批量规范化的成功提出了另一种解释：在某些方面，批量规范化的表现出与原始论文中声称的行为是相反的。</p><p>然而，与机器学习文献中成千上万类似模糊的说法相比，内部协变量偏移没有更值得批评。很可能，它作为这些辩论的焦点而产生共鸣，要归功于目标受众对它的广泛认可。<br>批量规范化已经被证明是一种不可或缺的方法。它适用于几乎所有图像分类器，并在学术界获得了数万引用。</p><h2 id="6-残差网络（ResNet）"><a href="#6-残差网络（ResNet）" class="headerlink" title="6. 残差网络（ResNet）"></a>6. 残差网络（ResNet）</h2><p>随着网络的设计越来越深，深刻理解“新添加的层如何提升神经网络的性能”变得至关重要。更重要的是设计网络的能力，在这种网络中，添加层会使网络更具表现力，为了取得质的突破，需要一些数学基础知识</p><h3 id="6-1-函数类"><a href="#6-1-函数类" class="headerlink" title="6.1. 函数类"></a>6.1. 函数类</h3><p>首先，假设有一类特定的神经网络架构$\mathcal{F}$，它包括学习速率和其他超参数设置。对于所有$f \in \mathcal{F}$，存在一些参数集（例如权重和偏置），这些参数可以通过在合适的数据集上进行训练而获得。现在假设 $f^{perfect}$ 是我们真正想要找到的函数，如果是$f^{perfect} \in \mathcal{F}$，那么可以轻而易举的训练得到它，但通常不会那么幸运，我们将尝试找到一个函数$f^{perfect}_\mathcal{F}$，这是在$\mathcal{F}$中的最佳选择。例如，给定一个具有$\mathbf{X}$特性和$\mathbf{y}$标签的数据集，我们可以尝试通过解决以下优化问题来找到它：</p><script type="math/tex; mode=display">f^{perfect}_\mathcal{F} := \mathop{\mathrm{argmin}}_f L(\mathbf{X}, \mathbf{y}, f) \text{ subject to } f \in \mathcal{F}.</script><p>为了更近似真正$f^{perfect}$的函数，我们只能尽量设计一个更强大的架构 $\mathcal{F}’$，并预计$f^{perfect}_{\mathcal{F}’}$比$f^{perfect}_{\mathcal{F}}$“更近似”。然而，如果$\mathcal{F} \not\subseteq \mathcal{F}’$，则无法保证新的体系“更近似”。事实上，$f^{perfect}_{\mathcal{F}’}$可能更糟：如下图所示，对于非嵌套函数（non-nested function）类，较复杂的函数类并不总是向“真”函数$f^{perfect}$靠拢（复杂度由$\mathcal{F}_1$向$\mathcal{F}_6$递增）。在图的左边，虽然$\mathcal{F}_3$比$\mathcal{F}_1$更接近$f^{perfect}$，但$\mathcal{F}_6$却离的更远了。而对于图右侧的嵌套函数（nested function）类$\mathcal{F}_1 \subseteq \ldots \subseteq \mathcal{F}_6$，则可以避免上述问题。</p><p><img src="/assets/post_img/article54/functionclasses.svg" alt="对于非嵌套函数类，较复杂（由较大区域表示）的函数类不能保证更接近“真”函数（$f^{perfect}$）。这种现象在嵌套函数类中不会发生"></p><p>因此，只有当较复杂的函数类包含较小的函数类时，才能确保提高它们的性能。对于深度神经网络，如果能将新添加的层训练成<em>恒等映射</em>（identity function）$f(\mathbf{x}) = \mathbf{x}$，新模型和原模型将同样有效。同时，由于新模型可能得出更优的解来拟合训练数据集，因此添加层似乎更容易降低训练误差。</p><p>针对这一问题，何恺明等人提出了<em>残差网络</em>（ResNet）。它在2015年的ImageNet图像识别挑战赛夺魁，并深刻影响了后来的深度神经网络的设计。残差网络的核心思想是：每个附加层都应该更容易地包含原始函数作为其元素之一。于是，残差块（residual blocks）便诞生了，这个设计对如何建立深层神经网络产生了深远的影响。凭借它，ResNet赢得了2015年ImageNet大规模视觉识别挑战赛。</p><h3 id="6-2-残差块"><a href="#6-2-残差块" class="headerlink" title="6.2. 残差块"></a>6.2. 残差块</h3><p>现在聚焦于神经网络局部：如下图所示，假设原始输入为x，而希望学出的理想映射为为$f(\mathbf{x})$（作为图中上方激活函数的输入）。左图虚线框中的部分需要直接拟合出该映射$f(\mathbf{x})$，而右图虚线框中的部分则需要拟合出残差映射$f(\mathbf{x}) - \mathbf{x}$。残差映射在现实中往往更容易优化。以恒等映射作为希望学出的理想映射$f(\mathbf{x})$，我们只需将右图虚线框内上方的加权运算（如仿射）的权重和偏置参数设成0<font color=#FF4500 size=4 face='黑体'>这样结果不就是零了？然后再加x？没看懂</font>，那么$f(\mathbf{x})$即为恒等映射。实际中，当理想映射$f(\mathbf{x})$极接近于恒等映射时，残差映射也易于捕捉恒等映射的细微波动。下图右图是ResNet的基础架构—<em>残差块</em>（residual block）。在残差块中，输入可通过跨层数据线路更快地向前传播。</p><p><img src="/assets/post_img/article54/residual-block.svg" alt="一个正常块（左）和一个残差块（右）"></p><p>ResNet沿用了VGG完整的$3×3$卷积层设计。残差块里首先有2个有相同输出通道数的$3×3$卷积层。每个卷积层后接一个批量规范化层和ReLU激活函数。然后通过跨层数据通路，跳过这2个卷积运算，将输入直接加在最后的ReLU激活函数前。这样的设计要求2个卷积层的输出与输入形状一样，从而使它们可以相加。如果想改变通道数，就需要引入一个额外的$1×1$卷积层来将输入变换成需要的形状后再做相加运算。</p><p>残差块的实现如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Residual</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_channels, num_channels,</span></span></span><br><span class="line"><span class="params"><span class="function">                 use_1x1conv=<span class="literal">False</span>, strides=<span class="number">1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(input_channels, num_channels,</span><br><span class="line">                               kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=strides)</span><br><span class="line">        self.conv2 = nn.Conv2d(num_channels, num_channels,</span><br><span class="line">                               kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> use_1x1conv:</span><br><span class="line">            self.conv3 = nn.Conv2d(input_channels, num_channels,</span><br><span class="line">                                   kernel_size=<span class="number">1</span>, stride=strides)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.conv3 = <span class="literal">None</span></span><br><span class="line">        self.bn1 = nn.BatchNorm2d(num_channels)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(num_channels)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        Y = F.relu(self.bn1(self.conv1(X)))</span><br><span class="line">        Y = self.bn2(self.conv2(Y))</span><br><span class="line">        <span class="keyword">if</span> self.conv3:</span><br><span class="line">            X = self.conv3(X)</span><br><span class="line">        Y += X</span><br><span class="line">        <span class="keyword">return</span> F.relu(Y)</span><br></pre></td></tr></table></figure></p><p>如下图所示，此代码生成两种类型的网络：一种是当 <code>use_1x1conv=False</code>时，应用ReLU非线性函数之前，将输入添加到输出。另一种是当<code>use_1x1conv=True</code>时，添加通过$1×1$卷积调整通道和分辨率。</p><p><img src="/assets/post_img/article54/resnet-block.svg" alt="包含及不包含$1 \times 1$卷积层的残差块"></p><p>下面查看输入和输出形状一致的情况：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">blk = Residual(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">X = torch.rand(<span class="number">4</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">6</span>)</span><br><span class="line">Y = blk(X)</span><br><span class="line">Y.shape</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">6</span>])</span><br></pre></td></tr></table></figure></p><p>也可以在增加输出通道数的同时，减半输出的高和宽。这里设置步长为2，原本的$6×6$ 在padding = 1后实际为$8×8$，在此基础上用$3×3$卷积核进行卷积，会得到$3×3$的大小，故减半了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">blk = Residual(<span class="number">3</span>,<span class="number">6</span>, use_1x1conv=<span class="literal">True</span>, strides=<span class="number">2</span>)</span><br><span class="line">blk(X).shape</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure><h3 id="6-3-ResNet模型"><a href="#6-3-ResNet模型" class="headerlink" title="6.3. ResNet模型"></a>6.3. ResNet模型</h3><p>ResNet的前两层跟之前介绍的GoogLeNet中的一样：在输出通道数为64、步幅为2的$7×7$卷积层后，接步幅为2的$3×3$的最大池化层。不同之处在于ResNet每个卷积层后增加了批量规范化层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">b1 = nn.Sequential(nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">                   nn.BatchNorm2d(<span class="number">64</span>), nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>GoogLeNet在后面接了4个由Inception块组成的模块。ResNet则使用4个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块。第一个模块的通道数同输入通道数一致。由于之前已经使用了步幅为2的最大池化层，所以无须减小高和宽。之后的每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半。</p><p>下面来实现这个模块。注意对第一个模块做了特别处理。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet_block</span>(<span class="params">input_channels, num_channels, num_residuals,</span></span></span><br><span class="line"><span class="params"><span class="function">                 first_block=<span class="literal">False</span></span>):</span></span><br><span class="line">    blk = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_residuals):</span><br><span class="line">        <span class="comment"># 不是第一个块的话需要变更输出通道数</span></span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> first_block:</span><br><span class="line">            blk.append(Residual(input_channels, num_channels,</span><br><span class="line">                                use_1x1conv=<span class="literal">True</span>, strides=<span class="number">2</span>))</span><br><span class="line">        <span class="comment"># 第一个块则不用                                </span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            blk.append(Residual(num_channels, num_channels))</span><br><span class="line">    <span class="keyword">return</span> blk</span><br></pre></td></tr></table></figure><p>接着在ResNet加入所有残差块，这里每个模块使用2个残差块：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">b2 = nn.Sequential(*resnet_block(<span class="number">64</span>, <span class="number">64</span>, <span class="number">2</span>, first_block=<span class="literal">True</span>))</span><br><span class="line">b3 = nn.Sequential(*resnet_block(<span class="number">64</span>, <span class="number">128</span>, <span class="number">2</span>))</span><br><span class="line">b4 = nn.Sequential(*resnet_block(<span class="number">128</span>, <span class="number">256</span>, <span class="number">2</span>))</span><br><span class="line">b5 = nn.Sequential(*resnet_block(<span class="number">256</span>, <span class="number">512</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure><p>最后，与GoogLeNet一样，在ResNet中加入全局平均池化层，以及全连接层输出。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(b1, b2, b3, b4, b5,</span><br><span class="line">                    nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>)),</span><br><span class="line">                    nn.Flatten(), nn.Linear(<span class="number">512</span>, <span class="number">10</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>每个模块有4个卷积层（不包括恒等映射的1×1卷积层）。加上第一个$7×7$卷积层和最后一个全连接层，共有18层。因此，这种模型通常被称为<strong>ResNet-18</strong>。通过配置不同的通道数和模块里的残差块数可以得到不同的ResNet模型，例如更深的含152层的ResNet-152。虽然ResNet的主体架构跟GoogLeNet类似，但ResNet架构更简单，修改也更方便。这些因素都导致了ResNet迅速被广泛使用。</p><p><img src="/assets/post_img/article54/resnet18.svg" alt="ResNet-18 架构"></p><p>观察一下ResNet中不同模块的输入形状是如何变化的：分辨率降低，通道数量增加，直到全局平均池化层聚集所有特征。 这些变化同样适用之前讲述的架构。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__,<span class="string">&#x27;output shape:\t&#x27;</span>, X.shape)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">64</span>, <span class="number">56</span>, <span class="number">56</span>])</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">64</span>, <span class="number">56</span>, <span class="number">56</span>])</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">128</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">14</span>, <span class="number">14</span>])</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">512</span>, <span class="number">7</span>, <span class="number">7</span>])</span><br><span class="line">AdaptiveAvgPool2d output shape:      torch.Size([<span class="number">1</span>, <span class="number">512</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">Flatten output shape:        torch.Size([<span class="number">1</span>, <span class="number">512</span>])</span><br><span class="line">Linear output shape:         torch.Size([<span class="number">1</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure><h3 id="6-4-训练模型"><a href="#6-4-训练模型" class="headerlink" title="6.4. 训练模型"></a>6.4. 训练模型</h3><p>在Fashion-MNIST数据集上训练ResNet：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lr, num_epochs, batch_size = <span class="number">0.05</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=<span class="number">96</span>)</span><br><span class="line">train_ch6(net, train_iter, test_iter, num_epochs, lr, torch.device(<span class="string">&#x27;mps&#x27;</span>))</span><br></pre></td></tr></table></figure></p><h2 id="7-稠密连接网络（DenseNet）"><a href="#7-稠密连接网络（DenseNet）" class="headerlink" title="7. 稠密连接网络（DenseNet）"></a>7. 稠密连接网络（DenseNet）</h2><p>ResNet极大地改变了如何参数化深层网络中函数的观点。 稠密连接网络（DenseNet）在某种程度上是ResNet的逻辑扩展。</p><h3 id="7-1-从ResNet到DenseNet"><a href="#7-1-从ResNet到DenseNet" class="headerlink" title="7.1. 从ResNet到DenseNet"></a>7.1. 从ResNet到DenseNet</h3><p>回想一下任意函数的泰勒展开式（Taylor expansion），它把这个函数分解成越来越高阶的项。在$x$接近0时，</p><script type="math/tex; mode=display">f(x) = f(0) + f'(0) x + \frac{f''(0)}{2!}  x^2 + \frac{f'''(0)}{3!}  x^3 + \ldots</script><p>同样，ResNet将函数展开为</p><script type="math/tex; mode=display">f(\mathbf{x}) = \mathbf{x} + g(\mathbf{x})</script><p>也就是说，ResNet将$f$分解为两部分：一个简单的线性项和一个复杂的非线性项。<br>那么再向前拓展一步，那么如果我们想将$f$拓展成超过两部分的信息呢？一种方案便是DenseNet。</p><p><img src="/assets/post_img/article54/densenet-block.svg" alt="ResNet（左）与 DenseNet（右）在跨层连接上的主要区别：使用相加和使用连结"></p><p>如上图所示，ResNet和DenseNet的关键区别在于，DenseNet输出是<em>连接</em>（用图中的$[,]$表示）而不是如ResNet的简单相加。因此在应用越来越复杂的函数序列后，我们执行从$\mathbf{x}$到其展开式的映射：</p><script type="math/tex; mode=display">\mathbf{x} \to \left[\mathbf{x},f_1(\mathbf{x}),f_2([\mathbf{x}, f_1(\mathbf{x})]), f_3([\mathbf{x}, f_1(\mathbf{x}), f_2([\mathbf{x}, f_1(\mathbf{x})])]), \ldots\right].</script><p>最后，将这些展开式结合到多层感知机中，再次减少特征的数量。实现起来非常简单：我们不需要添加术语，而是将它们连接起来。DenseNet这个名字由变量之间的“稠密连接”而得来，最后一层与之前的所有层紧密相连。稠密连接如下图所示。</p><p><img src="/assets/post_img/article54/densenet.svg" alt="稠密连接"></p><p>稠密网络主要由2部分构成：<em>稠密块</em>（dense block）和<em>过渡层</em>（transition layer）。前者定义如何连接输入和输出，而后者则控制通道数量，使其不会太复杂。</p><h3 id="7-2-稠密块体"><a href="#7-2-稠密块体" class="headerlink" title="7.2. 稠密块体"></a>7.2. 稠密块体</h3><p>DenseNet使用了ResNet改良版的“批量规范化、激活和卷积”架构。首先实现一下这个架构。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_block</span>(<span class="params">input_channels, num_channels</span>):</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.BatchNorm2d(input_channels), nn.ReLU(),</span><br><span class="line">        nn.Conv2d(input_channels, num_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>一个<em>稠密块</em>由多个卷积块组成，每个卷积块使用相同数量的输出通道。在前向传播中，将每个卷积块的输入和输出在通道维上连结。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DenseBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_convs, input_channels, num_channels</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(DenseBlock, self).__init__()</span><br><span class="line">        layer = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_convs):</span><br><span class="line">            layer.append(conv_block(</span><br><span class="line">                num_channels * i + input_channels, num_channels))</span><br><span class="line">        self.net = nn.Sequential(*layer)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        <span class="keyword">for</span> blk <span class="keyword">in</span> self.net:</span><br><span class="line">            Y = blk(X)</span><br><span class="line">            <span class="comment"># 连接通道维度上每个块的输入和输出</span></span><br><span class="line">            X = torch.cat((X, Y), dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><p>下例中，定义一个有2个卷积块、输出通道数为10的<code>DenseBlock</code>。使用通道数为3的输入时，我们会得到通道数为$3+2×10=23$的输出。卷积块的通道数控制了输出通道数相对于输入通道数的增长，因此也被称为<em>增长率</em>（growth rate）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">blk = DenseBlock(<span class="number">2</span>, <span class="number">3</span>, <span class="number">10</span>)</span><br><span class="line">X = torch.randn(<span class="number">4</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line">Y = blk(X)</span><br><span class="line">Y.shape</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">23</span>, <span class="number">8</span>, <span class="number">8</span>])</span><br></pre></td></tr></table></figure><h3 id="7-3-过渡层"><a href="#7-3-过渡层" class="headerlink" title="7.3. 过渡层"></a>7.3. 过渡层</h3><p>由于每个稠密块都会带来通道数的增加，使用过多则会过于复杂化模型。而过渡层可以用来控制模型复杂度。它通过$1×1$卷积层来减小通道数，并使用步幅为2的平均池化层减半高和宽，从而进一步降低模型复杂度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transition_block</span>(<span class="params">input_channels, num_channels</span>):</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.BatchNorm2d(input_channels), nn.ReLU(),</span><br><span class="line">        nn.Conv2d(input_channels, num_channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">        nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>))</span><br></pre></td></tr></table></figure><p>对上一个例子中稠密块的输出使用通道数为10的过渡层。 此时输出的通道数减为10，高和宽均减半。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">blk = transition_block(<span class="number">23</span>, <span class="number">10</span>)</span><br><span class="line">blk(Y).shape</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">10</span>, <span class="number">4</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure><h3 id="7-4-DenseNet模型"><a href="#7-4-DenseNet模型" class="headerlink" title="7.4. DenseNet模型"></a>7.4. DenseNet模型</h3><p>构造DenseNet模型。DenseNet首先使用同ResNet一样的单卷积层和最大池化层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">b1 = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">    nn.BatchNorm2d(<span class="number">64</span>), nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>接下来，类似于ResNet使用的4个残差块，DenseNet使用的是4个稠密块。与ResNet类似，可以设置每个稠密块使用多少个卷积层。这里设成4，从而与之前提到的ResNet-18保持一致。稠密块里的卷积层通道数（即增长率）设为32，所以每个稠密块将增加128个通道。<br>在每个模块之间，ResNet通过步幅为2的残差块减小高和宽，DenseNet则使用过渡层来减半高和宽，并减半通道数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># num_channels为当前的通道数</span></span><br><span class="line">num_channels, growth_rate = <span class="number">64</span>, <span class="number">32</span></span><br><span class="line">num_convs_in_dense_blocks = [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]</span><br><span class="line">blks = []</span><br><span class="line"><span class="keyword">for</span> i, num_convs <span class="keyword">in</span> <span class="built_in">enumerate</span>(num_convs_in_dense_blocks):</span><br><span class="line">    blks.append(DenseBlock(num_convs, num_channels, growth_rate))</span><br><span class="line">    <span class="comment"># 上一个稠密块的输出通道数</span></span><br><span class="line">    num_channels += num_convs * growth_rate</span><br><span class="line">    <span class="comment"># 在稠密块之间添加一个转换层，使通道数量减半</span></span><br><span class="line">    <span class="keyword">if</span> i != <span class="built_in">len</span>(num_convs_in_dense_blocks) - <span class="number">1</span>:</span><br><span class="line">        blks.append(transition_block(num_channels, num_channels // <span class="number">2</span>))</span><br><span class="line">        num_channels = num_channels // <span class="number">2</span></span><br></pre></td></tr></table></figure><p>与ResNet类似，最后接上全局池化层和全连接层来输出结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    b1, *blks,</span><br><span class="line">    nn.BatchNorm2d(num_channels), nn.ReLU(),</span><br><span class="line">    nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    nn.Linear(num_channels, <span class="number">10</span>))</span><br></pre></td></tr></table></figure><h3 id="7-5-训练模型"><a href="#7-5-训练模型" class="headerlink" title="7.5. 训练模型"></a>7.5. 训练模型</h3><p>由于这里使用了比较深的网络，此处将输入高和宽从224降到96来简化计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lr, num_epochs, batch_size = <span class="number">0.1</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=<span class="number">96</span>)</span><br><span class="line">train_ch6(net, train_iter, test_iter, num_epochs, lr, torch.device(<span class="string">&#x27;mps&#x27;</span>))</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;p&gt;介绍现代的卷积神经网络架构，本章中的每一个模型都曾一度占据主导地位，其中许多模型都是ImageNet竞赛的优胜者。ImageNet竞赛自2010年以来，一直是计算机视觉中监督学习进展的指向标。&lt;/p&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://silencezheng.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>卷积神经网络--《动手学深度学习》笔记0x07</title>
    <link href="http://silencezheng.top/2022/07/29/article53/"/>
    <id>http://silencezheng.top/2022/07/29/article53/</id>
    <published>2022-07-29T09:33:54.000Z</published>
    <updated>2022-07-29T09:46:01.517Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><p>本章介绍的卷积神经网络（convolutional neural network，CNN）是一类强大的、为处理图像数据而设计的神经网络。 基于卷积神经网络架构的模型在计算机视觉领域中已经占主导地位，当今几乎所有的图像识别、目标检测或语义分割相关的学术竞赛和商业应用都以这种方法为基础。</p><p>对应实践：<a href="https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x07.ipynb">https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x07.ipynb</a><br><span id="more"></span><br>在之前的学习中，我们将图像数据（二维像素网格，对于黑白图像来说每个像素只是1个数值，而彩色图像则有多个）展平为一维向量后送入MLP中，但这种方式忽略了每个图像的空间结构信息。最优的方式是利用先验知识，即利用相近像素之间的相互关联性，从图像数据中学习得到有效的模型。</p><p>现代卷积神经网络的设计得益于生物学、群论和一系列的补充实验。 卷积神经网络需要的参数少于全连接架构的网络，而且卷积也很容易用GPU并行计算。 卷积神经网络除了能够高效地采样从而获得精确的模型，还能够高效地计算。 即使在通常使用循环神经网络的一维序列结构任务上（例如音频、文本和时间序列分析），卷积神经网络也越来越受欢迎。 通过对卷积神经网络一些巧妙的调整，也使它们在图结构数据和推荐系统中发挥作用。</p><p>本章的主要内容：</p><ul><li>构成所有卷积网络主干的基本元素<ul><li>卷积层本身</li><li>填充（padding）和步幅（stride）的基本细节</li><li>用于在相邻区域池化信息的池化层（pooling）</li><li>在每一层中多通道（channel）的使用</li><li>有关现代卷积网络架构的仔细讨论</li></ul></li><li>一个完整的、可运行的LeNet模型：这是第一个成功应用的卷积神经网络，比现代深度学习兴起时间还要早<h3 id="0-1-结论"><a href="#0-1-结论" class="headerlink" title="0.1. 结论"></a>0.1. 结论</h3></li><li>图像的平移不变性使我们以相同的方式处理局部图像，而不在乎它的位置。</li><li>局部性意味着计算相应的隐藏表示只需一小部分局部图像像素。</li><li>在图像处理中，卷积层通常比全连接层需要更少的参数，但依旧获得高效用的模型。</li><li>卷积神经网络（CNN）是一类特殊的神经网络，它可以包含多个卷积层。</li><li>多个输入和输出通道使模型在每个空间位置可以获取图像的多方面特征。</li><li>二维卷积层的核心计算是二维互相关运算。最简单的形式是，对二维输入数据和卷积核执行互相关操作，然后添加一个偏置。</li><li>可以设计一个卷积核来检测图像的边缘。</li><li>可以从数据中学习卷积核的参数。</li><li>学习卷积核时，无论用严格卷积运算或互相关运算，卷积层的输出不会受太大影响。</li><li>当需要检测输入特征中更广区域时，可以构建一个更深的卷积网络。</li><li>填充可以增加输出的高度和宽度。这常用来使输出与输入具有相同的高和宽。</li><li>步幅可以减小输出的高和宽，例如输出的高和宽仅为输入的高和宽的$1/n$（$n$是一个大于的整数）。</li><li>填充和步幅可用于有效地调整数据的维度。</li><li>多输入多输出通道可以用来扩展卷积层的模型。</li><li>当以每像素为基础应用时，$1 \times 1$卷积层相当于全连接层。</li><li>$1 \times 1$卷积层通常用于调整网络层的通道数量和控制模型复杂性。</li><li>对于给定输入元素，最大池化层会输出该窗口内的最大值，平均池化层会输出该窗口内的平均值。</li><li>池化层的主要优点之一是减轻卷积层对位置的过度敏感。可以指定池化层的填充和步幅。</li><li>使用最大池化层以及大于1的步幅，可减少空间维度（如高度和宽度）。</li><li>池化层的输出通道数与输入通道数相同。</li><li>卷积神经网络（CNN）是一类使用卷积层的网络。</li><li>CNN中组合使用卷积层、非线性激活函数和汇聚层。</li><li>为了构造高性能的卷积神经网络，通常对卷积层进行排列，逐渐降低其表示的空间分辨率，同时增加通道数。</li><li>在传统的卷积神经网络中，卷积块编码得到的表征在输出之前需由一个或多个全连接层进行处理。</li><li>LeNet是最早发布的卷积神经网络之一。</li></ul><h2 id="1-从全连接层到卷积"><a href="#1-从全连接层到卷积" class="headerlink" title="1. 从全连接层到卷积"></a>1. 从全连接层到卷积</h2><p>多层感知机十分适合处理表格数据，其中行对应样本，列对应特征。 对于表格数据，我们寻找的模式可能涉及特征之间的交互，但是我们不能预先假设任何与特征交互相关的先验结构。 此时，多层感知机可能是最好的选择，然而对于高维感知数据，这种缺少结构的网络可能会变得不实用。</p><p>例如，在之前猫狗分类的例子中：假设我们有一个足够充分的照片数据集，数据集中是拥有标注的照片，每张照片具有百万级像素，这意味着网络的每次输入都有一百万个维度。 即使将隐藏层维度降低到1000，这个全连接层也将有$10^6 \times 10^3 = 10^9$个参数。这难以训练且需要大量样本进行拟合。</p><p>如今人类和机器都能很好地区分猫和狗，是因为图像中本就拥有丰富的结构，而这些结构可以被人类和机器学习模型使用。 卷积神经网络（convolutional neural networks，CNN）是机器学习利用自然图像中一些已知结构的创造性方法。</p><h3 id="1-1-不变性"><a href="#1-1-不变性" class="headerlink" title="1.1. 不变性"></a>1.1. 不变性</h3><p>假设想从一张图片中找到某个物体。 合理的假设是：无论哪种方法找到这个物体，都应该和物体的位置无关。 理想情况下的系统应该能够利用常识：猪通常不在天上飞，飞机通常不在水里游泳。 但是，如果一只猪出现在图片顶部，系统还是应该认出它。<br>在沃尔多游戏中包含了许多充斥着活动的混乱场景，而沃尔多通常潜伏在一些不太可能的位置，读者的目标就是找出他。 沃尔多的样子并不取决于他潜藏的地方，因此我们可以使用一个“沃尔多检测器”扫描图像。 该检测器将图像分割成多个区域，并为每个区域包含沃尔多的可能性打分。 卷积神经网络正是将空间不变性（spatial invariance）的这一概念系统化，从而基于这个模型使用较少的参数来学习有用的表示。<br><img src="/assets/post_img/article53/where-wally-walker-books.jpeg" alt="w"><br>总结一下适合于计算机视觉的神经网络架构原则：</p><ol><li>平移不变性（translation invariance）：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。</li><li>局部性（locality）：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。</li></ol><h3 id="1-2-多层感知机的限制"><a href="#1-2-多层感知机的限制" class="headerlink" title="1.2. 多层感知机的限制"></a>1.2. 多层感知机的限制</h3><p>首先, 多层感知机的输入是二维图像 $\mathbf{X}$, 其隐藏表示 $\mathbf{H}$ 在数学上是一个矩阵, 在代码中表示为二维张量。其中 $\mathbf{X}$ 和 $\mathbf{H}$ 具有相同的形状。即输入和隐藏表示都拥有空间结构。<br>用$[\mathbf{X}]_{i, j}$ 和 $[\mathbf{H}]_{i, j}$ 分别表示输入图像和隐藏表示中位置 $(i, j)$ 处的像素。为了使每个隐藏神经元都能接收到每个输入像素的信息, 我们将参数从权重矩阵替换为四阶权重张量$\mathbf{W}$。假设 $\mathbf{U}$ 包含偏置参数, 可以将全连接层形式化地表示为</p><script type="math/tex; mode=display">\begin{aligned}{[\mathbf{H}]_{i, j} } &=[\mathbf{U}]_{i, j}+\sum_{k} \sum_{l}[\mathbf{W}]_{i, j, k, l}[\mathbf{X}]_{k, l} \\&=[\mathbf{U}]_{i, j}+\sum_{a} \sum_{b}[\mathbf{V}]_{i, j, a, b}[\mathbf{X}]_{i+a, j+b} .\end{aligned}</script><p>其中, 从W到V的转换只是形式上的转换, 因为在这两个四阶张量的元素之间存在一一对应的关系。只需重新索引下标 $(k, l)$, 使 $k=i+a 、 l=j+b$, 由此可得 $[\mathrm{V}]_{i, j, a, b}=[\mathrm{W}]_{i, j, i+a, j+b}$ 。索引 $a$ 和 $b$ 通过在正偏移和负偏移之间移动覆盖了整个图像。对于隐藏表示中任意给定位置 $(i, j)$ 处的像素值 $[\mathbf{H}]_{i, j}$, 可以通过在 $x$ 中以 $(i, j)$ 为中心对像素进行加权求和得到, 加权使用的权重为 $[\mathrm{V}]_{i, j, a, b}$ 。</p><h4 id="1-2-1-平移不变性"><a href="#1-2-1-平移不变性" class="headerlink" title="1.2.1. 平移不变性"></a>1.2.1. 平移不变性</h4><p>引用上述的第一个原则：平移不变性。这意味着检测对象在输入 $\mathbf{X}$ 中的平移, 应该仅导致隐藏表示 $\mathbf{H}$ 中的平移。也就是$\mathrm{V}$ 和 $\mathbf{U}$ 实际上不依赖于 $(i, j)$ 的值, 即 $[\mathbf{V}]_{i, j, a, b}=[\mathbf{V}]_{a, b}$ 。并且 $\mathbf{U}$ 是一个常数, 比如 $u$ 。 故可以简化 $\mathbf{H}$ 定义为:</p><script type="math/tex; mode=display">[\mathbf{H}]_{i, j}=u+\sum_{a} \sum_{b}[\mathbf{V}]_{a, b}[\mathbf{X}]_{i+a, j+b}</script><p>这就是<strong>卷积 (convolution)</strong>。我们是在使用系数 $[\mathbf{V}]_{a, b}$ 对位置 $(i, j)$ 附近的像素 $(i+a, j+b)$ 进行加权得到 $[\mathbf{H}]_{i, j}$ 。 $[\mathbf{V}]_{a, b}$ 的系数比 $[\mathbf{V}]_{i, j, a, b}$ 少很多, 因为前者不再依赖于图像中的位置，这是显著的进步!</p><h4 id="1-2-2-局部性"><a href="#1-2-2-局部性" class="headerlink" title="1.2.2. 局部性"></a>1.2.2. 局部性</h4><p>引用上述的第二个原则：局部性。为了收集用来训练参数 $[\mathbf{H}]_{i, j}$ 的相关信息, 我们不应偏离到距 $(i, j)$ 很远的地方。这意味着在 $|a|&gt;\Delta$ 或 $|b|&gt;\Delta$ 的范围之外, 我们可以设置 $[\mathbf{V}]_{a, b}=0$ 。因此, 我们可以将 $[\mathbf{H}]_{i, j}$ 重写为</p><script type="math/tex; mode=display">[\mathbf{H}]_{i, j}=u+\sum_{a=-\Delta}^{\Delta} \sum_{b=-\Delta}^{\Delta}[\mathbf{V}]_{a, b}[\mathbf{X}]_{i+a, j+b} .</script><p>上式是一个卷积层 (convolutional layer) ，而卷积神经网络是包含卷积层的一类特殊的神经网络。在深度学习研究社区中, $\mathbf{V}$ 被称为<em>卷积核 (convolution kernel)</em>或者<em>滤波器（filter）</em>，亦或简单地称之为该卷积层的<em>权重</em>, 通常该权重是可学习的参数。</p><p>当图像处理的局部区域很小时, 卷积神经网络与多层感知机的训练差异可能是巨大的: 以前, 多层感知机可能需要数十亿个参数来表示网络中的一层, 而现在卷积神经网络通常只需要几百个参数, 而且不需要改变输入或隐藏表示的维数。<br>参数大幅减少的代价是, 现在的特征是平移不变的, 并且当确定每个隐藏活性值时, 每一层只包含局部的信息。以上所有的权重学习都将依赖于归纳偏置。当这种偏置与现实相符时, 我们就能得到样本有效的模型, 并且这些模型能很好地泛化到末知数据中。不符时, 如图像不满足平移不变时, 模型可能难以拟合训练数据。</p><h3 id="1-3-卷积"><a href="#1-3-卷积" class="headerlink" title="1.3. 卷积"></a>1.3. 卷积</h3><p>为什么上面的操作被称为卷积？<br>在数学中, 两个函数（比如 $\left.f, g: \mathbb{R}^{d} \rightarrow \mathbb{R}\right)$ 之间的 “卷积”被定义为</p><script type="math/tex; mode=display">(f * g)(\mathbf{x})=\int f(\mathbf{z}) g(\mathbf{x}-\mathbf{z}) d \mathbf{z}</script><p>卷积是当把一个函数”翻转”并移位 $\mathbf{x}$ 时, 测量 $f$ 和 $g$ 之间的重叠。当为离散对象时, 积分就变成求和。例如：对于由 索引为 $\mathbb{Z}$ 的、平方可和的、无限维向量集合中抽取的向量，我们得到以下定义：</p><script type="math/tex; mode=display">(f * g)(i)=\sum_{a} f(a) g(i-a)</script><p>对于二维张量, 则为 $f$ 的索引 $(a, b)$ 和 $g$ 的索引 $(i-a, j-b)$ 上的对应加和：</p><script type="math/tex; mode=display">(f * g)(i, j)=\sum_{a} \sum_{b} f(a, b) g(i-a, j-b)</script><p>这看起来类似于1.2.2中的卷积层公式, 只有一个主要区别：这里不是使用 $(i+a, j+b)$, 而是使用差值。但这种区别是表面的, 因为我们总是可以对应两式之间的符号。我们在中的原始定义更正确地描述了<em>互相关 (cross-correlation)</em>。</p><h3 id="1-4-沃尔多游戏回顾"><a href="#1-4-沃尔多游戏回顾" class="headerlink" title="1.4. 沃尔多游戏回顾"></a>1.4. 沃尔多游戏回顾</h3><p>回到“沃尔多在哪里”游戏，卷积层根据滤波器$V$选取给定大小的窗口，并加权处理图片，如下图所示。我们的目标是学习一个模型，以便探测出在“沃尔多”最可能出现的地方。<br><img src="/assets/post_img/article53/waldo-mask.jpeg" alt="waldo"></p><h4 id="1-4-1-通道"><a href="#1-4-1-通道" class="headerlink" title="1.4.1. 通道"></a>1.4.1. 通道</h4><p>这种方法有一个问题: 忽略了图像一般包含三个通道/三种原色（红色、绿色和蓝色）。实际上图像不是二维张量, 而是一个由高度、宽度和颜色组成的三维张量, 比如包含 $1024 \times 1024 \times 3$ 个像素。前两个轴与像素的空间位置有关, 而第三个轴可以看作是每个像素的多维表示。因此, 我们将X索引为 $[\mathrm{X}]_{i, j, k}$ 。由此卷积相应地调整为 $[\mathbf{V}]_{a, b, c}$，而不是 $[\mathbf{V}]_{a, b}$ 。<br>由于输入图像是三维的, 隐藏表示$\mathbf{H}$也最好采用三维张量。也就是对于每一个空间位置, 采用一组隐藏表示而不是单个。这样一组隐藏表示可以想象成一些互相堆叠的二维网格。<br>把隐藏表示想象为一系列具有二维张量的<em>通道（channel）</em>,也被称为特征映射（feature maps），因为每个通道都向后续层提供一组空间化的学习特征。直观上你可以想象在靠近输入的底层, 一些通道专门识别边缘, 而一些通道专门识别纹理。<br>为了支持输入$\mathbf{X}$和隐藏表示 $\mathrm{H}$ 中的多个通道, 可以在V中添加第四个坐标, 即 $[\mathrm{V}]_{a, b, c, d}$ 综上所述:</p><script type="math/tex; mode=display">[\mathrm{H}]_{i, j, d}=\sum_{a=-\Delta}^{\Delta} \sum_{b=-\Delta}^{\Delta} \sum_{c}[\mathbf{V}]_{a, b, c, d}[\mathbf{X}]_{i+a, j+b, c}</script><p>其中隐藏表示$\mathbf{H}$中的索引 $d$ 表示输出通道, 而随后的输出将继续以三维张量 $\mathrm{H}$ 作为输入进入下一个卷积层。所以上式可以定义具有多个通道的卷积层,其中V是该卷积层的权重。</p><p>然而仍有许多问题需要解决。例如，图像中是否到处都有存在沃尔多的可能? 如何有效地计算输出层? 如何选择适当的激活函数? 为了训练有效的网络，如何做出合理的网络设计选择? 后续节会讨论这些。</p><h2 id="2-图像卷积"><a href="#2-图像卷积" class="headerlink" title="2. 图像卷积"></a>2. 图像卷积</h2><p>以图像为例，探索卷积神经网络的实际应用。</p><h3 id="2-1-互相关运算"><a href="#2-1-互相关运算" class="headerlink" title="2.1. 互相关运算"></a>2.1. 互相关运算</h3><p>严格来说，卷积层是个错误的叫法，因为它所表达的运算其实是<strong>互相关运算（cross-correlation）</strong>，而不是卷积运算。 在卷积层中，输入张量和核张量通过互相关运算产生输出张量。</p><p>首先暂时忽略通道（第三维）这一情况, 看看如何处理二维图像数据和隐藏表示。下图中, 输入是高度为3 、宽度为3的二维张量（形状为 $3 \times 3$ ）。卷积核的高度和宽度都是 2 , 而卷积核窗口（或卷积窗口，即输入处的窗口）的形状由内核的高度和宽度决定 $($ 即 $2 \times 2 ）$。<br><img src="/assets/post_img/article53/correlation.svg" alt="co"><br>图中的阴影部分是第一个输出元素，以及用于计算输出的输入张量元素和核张量元素。</p><p>在二维互相关运算中, 卷积窗口从输入张量的左上角开始, 从左到右、从上到下滑动。当卷积窗口滑动到新一个位置时, 包含在该窗口中的部分张量与卷积核张量进行按元素相乘, 得到的张量再求和得到一个单一的标量值, 由此我们得出了这一位置的 输出张量值。在如上例子中, 输出张量的四个元素由二维互相关运算得到, 这个输出高度为 2 、宽度为 2 , 如下所示：</p><script type="math/tex; mode=display">\begin{aligned}&0 \times 0+1 \times 1+3 \times 2+4 \times 3=19 \\&1 \times 0+2 \times 1+4 \times 2+5 \times 3=25 \\&3 \times 0+4 \times 1+6 \times 2+7 \times 3=37 \\&4 \times 0+5 \times 1+7 \times 2+8 \times 3=43\end{aligned}</script><p>输出大小略小于输入大小是因为卷积核的宽度和高度大于1, 并且<font color=#FF4500 size=4 face='黑体'>卷积核只与图像中每个大小完全适合的位置进行互相关运算</font>。所以，输出大小等于输入大小 $n_{h} \times n_{w}$ 减去卷积核大小 $k_{h} \times k_{w}$, 即：</p><script type="math/tex; mode=display">\left(n_{h}-k_{h}+1\right) \times\left(n_{w}-k_{w}+1\right)</script><p>这是因为需要足够的空间在图像上“移动”卷积核。稍后将看到如何通过在图像边界周围填充零来保证有足够的空间移动卷积核, 从而保持输出大小不变。接下来, 我们在 $\operatorname{corr} 2 \mathrm{~d}$ 函数中实现如上过程, 该函数接受输入张量X和卷积核张量K, 并返回输出张量Y。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">corr2d</span>(<span class="params">X, K</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算二维互相关运算&quot;&quot;&quot;</span></span><br><span class="line">    h, w = K.shape</span><br><span class="line">    <span class="comment"># 确定输出的形状</span></span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>] - h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - w + <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="comment"># 依次计算互相关运算</span></span><br><span class="line">            Y[i, j] = (X[i:i + h, j:j + w] * K).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure><br>验证上述二维互相关运算的输出：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">X = torch.tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]])</span><br><span class="line">K = torch.tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>], [<span class="number">2.0</span>, <span class="number">3.0</span>]])</span><br><span class="line">corr2d(X, K)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[<span class="number">19.</span>, <span class="number">25.</span>],</span><br><span class="line">        [<span class="number">37.</span>, <span class="number">43.</span>]])</span><br></pre></td></tr></table></figure></p><h3 id="2-2-卷积层"><a href="#2-2-卷积层" class="headerlink" title="2.2. 卷积层"></a>2.2. 卷积层</h3><p>卷积层对输入和卷积核权重进行互相关运算，并在添加标量偏置之后产生输出。 所以，卷积层中的两个被训练的参数是卷积核权重和标量偏置。 基于卷积层训练模型时，同样随机初始化卷积核权重。</p><p>基于上面定义的corr2d函数实现二维卷积层。在<strong>init</strong>构造函数中，将weight和bias声明为两个模型参数。前向传播函数调用corr2d函数并添加偏置。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Conv2D</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, kernel_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.rand(kernel_size))</span><br><span class="line">        self.bias = nn.Parameter(torch.zeros(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> corr2d(x, self.weight) + self.bias</span><br></pre></td></tr></table></figure><br>高度和宽度分别为 $h$ 和 $w$ 的卷积核可以被称为 $h \times w$ 卷积或 $h \times w$ 卷积核。 我们也将带有 $h \times w$ 卷积核的卷积层称为 $h \times w$ 卷积层。</p><h3 id="2-3-图像中目标的边缘检测"><a href="#2-3-图像中目标的边缘检测" class="headerlink" title="2.3. 图像中目标的边缘检测"></a>2.3. 图像中目标的边缘检测</h3><p>卷积层的一个简单应用：通过找到像素变化的位置，来检测图像中不同颜色的边缘。<br>首先构造一个$6 \times 8$像素的黑白图像。中间四列为黑色，其余像素为白色。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">X = torch.ones((<span class="number">6</span>, <span class="number">8</span>))</span><br><span class="line">X[:, <span class="number">2</span>:<span class="number">6</span>] = <span class="number">0</span></span><br><span class="line">X</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br></pre></td></tr></table></figure><br>接下来，构造一个高度为1、宽度为2的卷积核K。当进行互相关运算时，如果水平相邻的两元素相同，则输出为零，否则输出为非零。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">K = torch.tensor([[<span class="number">1.0</span>, -<span class="number">1.0</span>]])</span><br></pre></td></tr></table></figure><br>对参数X（输入）和K（卷积核）执行互相关运算。 如下所示，输出Y中的1代表从白色到黑色的边缘，-1代表从黑色到白色的边缘，其他情况的输出为0。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Y = corr2d(X, K)</span><br><span class="line">Y</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, -<span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, -<span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, -<span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, -<span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, -<span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, -<span class="number">1.</span>,  <span class="number">0.</span>]])</span><br></pre></td></tr></table></figure><br>现在将输入的二维图像转置，再进行如上的互相关运算。 其输出如下，之前检测到的垂直边缘消失了。 这是合理的，卷积核K只可以检测垂直边缘，无法检测水平边缘。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">corr2d(X.t(), K)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]])</span><br></pre></td></tr></table></figure></p><h3 id="2-4-学习卷积核"><a href="#2-4-学习卷积核" class="headerlink" title="2.4. 学习卷积核"></a>2.4. 学习卷积核</h3><p>如果只需寻找黑白边缘，那么以上[1, -1]的边缘检测器足以。然而当有了更复杂数值的卷积核，或者连续的卷积层时，我们不可能手动设计滤波器。<br>我们需要学习由X生成Y的卷积核，即根据输入和输出，让程序学习应该使用什么样的卷积核。</p><p>现在看看是否可以通过仅查看“输入-输出”对来学习由X生成Y的卷积核。<br>先构造一个卷积层，并将其卷积核初始化为随机张量。接下来，在每次迭代中比较Y与卷积层输出的平方误差，然后计算梯度来更新卷积核。简单起见在此使用内置的二维卷积层，并忽略偏置。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构造一个二维卷积层，它具有1个输出通道和形状为（1，2）的卷积核</span></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>,<span class="number">1</span>, kernel_size=(<span class="number">1</span>, <span class="number">2</span>), bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这个二维卷积层使用四维输入和输出格式（批量大小、通道、高度、宽度），</span></span><br><span class="line"><span class="comment"># 其中批量大小和通道数都为1</span></span><br><span class="line">X = X.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">8</span>))</span><br><span class="line">Y = Y.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">7</span>))</span><br><span class="line">lr = <span class="number">3e-2</span>  <span class="comment"># 学习率</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    Y_hat = conv2d(X)</span><br><span class="line">    <span class="comment"># 平方损失</span></span><br><span class="line">    l = (Y_hat - Y) ** <span class="number">2</span></span><br><span class="line">    conv2d.zero_grad()</span><br><span class="line">    <span class="comment"># 求和、反向传播</span></span><br><span class="line">    l.<span class="built_in">sum</span>().backward()</span><br><span class="line">    <span class="comment"># 迭代卷积核</span></span><br><span class="line">    conv2d.weight.data[:] -= lr * conv2d.weight.grad</span><br><span class="line">    <span class="keyword">if</span> (i + <span class="number">1</span>) % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;i+<span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;l.<span class="built_in">sum</span>():<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">epoch <span class="number">2</span>, loss <span class="number">1.618</span></span><br><span class="line">epoch <span class="number">4</span>, loss <span class="number">0.298</span></span><br><span class="line">epoch <span class="number">6</span>, loss <span class="number">0.061</span></span><br><span class="line">epoch <span class="number">8</span>, loss <span class="number">0.015</span></span><br><span class="line">epoch <span class="number">10</span>, loss <span class="number">0.004</span></span><br></pre></td></tr></table></figure><br>10次迭代之后，误差已经降到足够低。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">conv2d.weight.data.reshape((<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[ <span class="number">0.9879</span>, -<span class="number">0.9993</span>]])</span><br></pre></td></tr></table></figure><br>结果非常接近之前的卷积核K。</p><h3 id="2-5-互相关和卷积"><a href="#2-5-互相关和卷积" class="headerlink" title="2.5. 互相关和卷积"></a>2.5. 互相关和卷积</h3><p>基于互相关运算和卷积运算之间的对应关系。 为了得到正式的卷积运算输出，我们需要执行中定义的严格卷积运算，而不是互相关运算。 但好在它们差别不大，只需水平和垂直翻转二维卷积核张量，然后对输入张量执行互相关运算。</p><p>由于卷积核是从数据中学习到的，因此无论这些层执行严格的卷积运算还是互相关运算，卷积层的输出都不会受到影响。假设卷积层执行互相关运算并学习之前例子中的卷积核，该卷积核在这里由矩阵$\mathbf{K}$表示。 假设其他条件不变，当这个层执行严格的卷积时，学习的卷积核$\mathbf{K}’$在水平和垂直翻转之后将与$\mathbf{K}$相同。 也就是说，当卷积层对例子中的输入和$\mathbf{K}’$执行严格卷积运算时，将得到与互相关运算相同的输出。</p><p>在深度学习文献中，将继续把“互相关运算”称为卷积运算，尽管它们略有不同。 对于<em>卷积核张量上的权重</em>，我们称其为<strong>元素</strong>。</p><h3 id="2-6-特征映射和感受域"><a href="#2-6-特征映射和感受域" class="headerlink" title="2.6. 特征映射和感受域"></a>2.6. 特征映射和感受域</h3><p>输出的卷积层有时被称为<em>特征映射 (feature map)</em> ，因为它可以被视为一个输入映射到下一层的空间维度的转换器。在卷积神经网络中, 对于某一层的任意元素 $x$, 其感受域 (receptive field) 是指在前向传播期间可能影响 $x$ 计算的所有元素（来自所有先前层）。</p><p>感受野可能大于输入的实际大小。用2.1的图为例来解释：给定 $2 \times 2$ 卷积核, 阴影输出元素值19的感受域是输入阴影部分的四个元素。假设之前输出为 $\mathbf{Y}$, 其大小为 $2 \times 2$, 现在我们在其后附加一个卷积层, 该卷积层以 $\mathbf{Y}$ 为输入, 输出单个元素 $z$ 。此时$z$ 的感受域包括 $\mathbf{Y}$ 的所有四个元素, 以及最初所有九个输入元素。<br>重点来啦，根据这一特质，当一个特征图中的任意元素需要检测更广区域的输入特征时，可以构建一个更深的网络。</p><p>PS：实在受不了感受野这翻译，浅动一个字改成感受域吧。</p><h2 id="3-填充和步幅"><a href="#3-填充和步幅" class="headerlink" title="3. 填充和步幅"></a>3. 填充和步幅</h2><p>现在已经知道假设输入形状为 $n_{h} \times n_{w}$, 卷积核形状为 $k_{h} \times k_{w}$, 那么输出形状将是 $\left(n_{h}-k_{h}+1\right) \times\left(n_{w}-k_{w}+1\right)$ 。 即卷积的输出形状取决于输入形状和卷积核的形状。</p><p>本节将介绍填充（padding）和步幅（stride）。假设以下情景：有时在应用了连续的卷积之后, 最终得到的输出远小于输入大小。这是由于卷积核的宽度和高度通常大于 1 所导致的。比如, 一个 $240 \times 240$ 像素的图像, 经过 10 层 $5 \times 5$ 的卷积后, 将减少到 $200 \times 200$ 像素。这会导致原始图像边界上的有用信息被丢弃，<strong>填充</strong>是解决此问题最有效的方法。有时可能希望大幅降低图像的宽度和高度。例如原始的输入分辨率十分冗余，<strong>步幅</strong>可以在这类情况下提供帮助。</p><h3 id="3-1-填充（padding）"><a href="#3-1-填充（padding）" class="headerlink" title="3.1. 填充（padding）"></a>3.1. 填充（padding）</h3><p>在应用多层卷积时，常常丢失边缘像素。 由于我们通常使用小卷积核，因此对于任何单个卷积，可能只会丢失几个像素。 但随着应用许多连续卷积层，累积丢失的像素数就多了。 解决这个问题的简单方法为填充（padding）：在输入图像的边界填充元素（通常填充元素是）。<br>以上节中的例子来说，将$3 \times 3$输入填充到$5 \times 5$，那么它的输出就增加为$4 \times 4$。如下图，阴影部分是第一个输出元素以及用于输出计算的输入和核张量元素：<br><img src="/assets/post_img/article53/conv-pad.svg" alt="padding"><br>通常, 如果添加 $p_{h}$ 行填充（大约一半在顶部, 一半在底部）和 $p_{w}$ 列填充（左侧大约一半, 右侧一半）, 则输出形状将为</p><script type="math/tex; mode=display">\left(n_{h}-k_{h}+p_{h}+1\right) \times\left(n_{w}-k_{w}+p_{w}+1\right)</script><p>即输出的高度和宽度将分别增加 $p_{h}$ 和 $p_{w}$ 。</p><p>在许多情况下, 我们需要设置 $p_{h}=k_{h}-1$ 和 $p_{w}=k_{w}-1$, 使输入和输出具有相同的高度和宽度。这样可以在构建网络时更容易地预测每个图层的输出形状。假设 $k_{h}$ 是奇数, 将在高度的两侧填充 $p_{h} / 2$ 行。如果 $k_{h}$ 是偶数, 则可以在输入顶部填充 $\left\lceil p_{h} / 2\right\rceil$ 行, 在底部填充 $\left\lfloor p_{h} / 2\right\rfloor$ 行。然后对宽度两侧按同样的道理填充。</p><p>卷积神经网络中卷积核的高度和宽度通常为奇数, 例如 $1、3、5、7$。选择奇数的好处是在保持空间维度的同时，可以在顶部和底部填充相同数量的行, 在左侧和右侧填充相同数量的列。</p><p>使用奇数的核大小和填充大小也提供了书写上的便利。对于任何二维张量X, 当满足：1. 卷积核的大小是奇数；2. 所有边的填充行数和列数相同; 3. 输出与输入具有相同高度和宽度 则可以得出：输出 $Y[i, j]$ 是通过以输入 $[i, j]$ 为中心, 与卷积核进行互相关计算得到的。</p><p>下例中, 创建一个高度和宽度为3的二维卷积层, 并在所有侧边填充1个像素。给定高度和宽度为8的输入, 则输出的高度和宽度也是8。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了方便起见，定义了一个计算卷积层的函数。</span></span><br><span class="line"><span class="comment"># 此函数初始化卷积层权重，并对输入和输出提高和缩减相应的维数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">comp_conv2d</span>(<span class="params">conv2d, X</span>):</span></span><br><span class="line">    <span class="comment"># 这里的（1，1）表示批量大小和通道数都是1</span></span><br><span class="line">    X = X.reshape((<span class="number">1</span>, <span class="number">1</span>) + X.shape)</span><br><span class="line">    Y = conv2d(X)</span><br><span class="line">    <span class="comment"># 省略前两个维度：批量大小和通道</span></span><br><span class="line">    <span class="keyword">return</span> Y.reshape(Y.shape[<span class="number">2</span>:])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 请注意，这里每边都填充了1行或1列，因此总共添加了2行或2列</span></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">X = torch.rand(size=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">torch.Size([<span class="number">8</span>, <span class="number">8</span>])</span><br></pre></td></tr></table></figure><br>当卷积核的高度和宽度不同时，则可以填充不同的高度和宽度，使输出和输入具有相同的高度和宽度。在下例中，使用高度为5，宽度为3的卷积核，高度和宽度两边的填充分别为2和1。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=(<span class="number">5</span>, <span class="number">3</span>), padding=(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">torch.Size([<span class="number">8</span>, <span class="number">8</span>])</span><br></pre></td></tr></table></figure></p><h3 id="3-2-步幅（stride）"><a href="#3-2-步幅（stride）" class="headerlink" title="3.2. 步幅（stride）"></a>3.2. 步幅（stride）</h3><p>在计算互相关时，卷积窗口从输入张量的左上角开始，向下、向右滑动。 在前面的例子中默认每次滑动一个元素。 但有时候为了高效计算或是缩减采样次数，卷积窗口可以跳过中间位置，每次滑动多个元素。</p><p>通常将每次滑动元素的数量称为步幅（stride）。 下图是垂直步幅为3，水平步幅为2的二维互相关运算。 着色部分是输出元素以及用于输出计算的输入和内核张量元素：$0\times0+0\times1+1\times2+2\times3=8$、$0\times0+6\times1+0\times2+0\times3=6$。<br><img src="/assets/post_img/article53/conv-stride.svg" alt="stride"><br>可以看到，为了计算输出中第一列的第二个元素和第一行的第二个元素，卷积窗口分别向下滑动三行和向右滑动两列。但是，当卷积窗口继续向右滑动两列时，没有输出，因为输入元素无法填充窗口。</p><p>通常, 当垂直步幅为 $s_{h}$ 、水平步幅为 $s_{w}$ 时, 输出形状为</p><script type="math/tex; mode=display">\left\lfloor\left(n_{h}-k_{h}+p_{h}+s_{h}\right) / s_{h}\right\rfloor \times\left\lfloor\left(n_{w}-k_{w}+p_{w}+s_{w}\right) / s_{w}\right\rfloor .</script><p>如果设置了 $p_{h}=k_{h}-1$ 和 $p_{w}=k_{w}-1$, 则输出形状将简化为 $\left\lfloor\left(n_{h}+s_{h}-1\right) / s_{h}\right\rfloor \times\left\lfloor\left(n_{w}+s_{w}-1\right) / s_{w}\right\rfloor$ 。如果在此基础上，输入的高度和宽度可以被垂直和水平步幅整除, 则输出形状将为 $\left(n_{h} / s_{h}\right) \times\left(n_{w} / s_{w}\right)$ 。</p><p>下面将高度和宽度的步幅设置为 2 , 从而将输入的高度和宽度减半。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure><br>另一个复杂点的例子，只填充列：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=(<span class="number">3</span>, <span class="number">5</span>), padding=(<span class="number">0</span>, <span class="number">1</span>), stride=(<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure><br>为了简洁起见, 当输入高度和宽度两侧的填充数量分别为 $p_{h}$ 和 $p_{w}$ 时, 我们称之为填充 $\left(p_{h}, p_{w}\right)$ 。当 $p_{h}=p_{w}=p$ 时, 填充是 $p_{\text {。 }}$<br>默认情况下, 填充为 0 , 步幅为 1 。<br><em>在实践中很少使用不一致的步幅或填充, 通常有 $p_{h}=p_{w}$ 和 $s_{h}=s_{w}$ 。</em></p><h2 id="4-多输入多输出通道"><a href="#4-多输入多输出通道" class="headerlink" title="4. 多输入多输出通道"></a>4. 多输入多输出通道</h2><p>目前为止展示的单个输入和单个输出通道的简单例子，使得我们可以将输入、卷积核和输出看作二维张量。</p><p>当添加通道时，输入和隐藏的表示都变成了三维张量。例如，每个RGB输入图像具有$3\times h\times w$的形状。我们将这个大小为$3$的轴称为通道（channel）维度。</p><h3 id="4-1-多输入通道"><a href="#4-1-多输入通道" class="headerlink" title="4.1. 多输入通道"></a>4.1. 多输入通道</h3><p>当输入包含多个通道时，需要构造一个与输入数据具有相同输入通道数的卷积核，以便与输入数据进行互相关运算。假设输入的通道数为$c_i$，那么卷积核的输入通道数也需要为$c_i$。</p><p>如果卷积核的窗口形状是$k_h\times k_w$，当$c_i=1$时，可以把卷积核看作形状为$k_h\times k_w$的二维张量。<br>当$c_i&gt;1$时，卷积核的每个输入通道将包含形状为$k_h\times k_w$的张量。将这些张量$c_i$连结在一起可以得到形状为$c_i\times k_h\times k_w$的卷积核。</p><p>输入和卷积核都有$c_i$个通道，所以可以对每个通道输入的二维张量和卷积核的二维张量进行互相关运算，再对通道求和（将$c_i$的结果相加）得到二维张量。这是多通道输入和多输入通道卷积核之间进行二维互相关运算的结果。</p><p>下图演示了一个具有两个输入通道的二维互相关运算的示例。阴影部分是第一个输出元素以及用于计算这个输出的输入和核张量元素：</p><script type="math/tex; mode=display">(1\times1+2\times2+4\times3+5\times4)+(0\times0+1\times1+3\times2+4\times3)=56</script><p><img src="/assets/post_img/article53/conv-multi-in.svg" alt="multi-channel"><br>实现一下多输入通道互相关运算，所做的就是对每个通道执行互相关操作，然后将结果相加。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">corr2d_multi_in</span>(<span class="params">X, K</span>):</span></span><br><span class="line">    <span class="comment"># 先遍历“X”和“K”的第0个维度（通道维度），再把它们加在一起</span></span><br><span class="line">    <span class="comment"># 前面定义的corr2d函数</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(corr2d(x, k) <span class="keyword">for</span> x, k <span class="keyword">in</span> <span class="built_in">zip</span>(X, K))</span><br></pre></td></tr></table></figure><br>构造上图中的例子，验证互相关运算的输出：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">X = torch.tensor([[[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]],</span><br><span class="line">               [[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], [<span class="number">7.0</span>, <span class="number">8.0</span>, <span class="number">9.0</span>]]])</span><br><span class="line">K = torch.tensor([[[<span class="number">0.0</span>, <span class="number">1.0</span>], [<span class="number">2.0</span>, <span class="number">3.0</span>]], [[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>]]])</span><br><span class="line"></span><br><span class="line">corr2d_multi_in(X, K)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[ <span class="number">56.</span>,  <span class="number">72.</span>],</span><br><span class="line">        [<span class="number">104.</span>, <span class="number">120.</span>]])</span><br></pre></td></tr></table></figure></p><h3 id="4-2-多输出通道"><a href="#4-2-多输出通道" class="headerlink" title="4.2. 多输出通道"></a>4.2. 多输出通道</h3><p>目前为止还只有一个输出通道，但每一层有多个输出通道是至关重要的。在最流行的神经网络架构中，随着神经网络层数的加深，经常会增加输出通道的维数，通过减少空间分辨率以获得更大的通道深度。直观地说，可以将每个通道看作是对不同特征的响应，但他们不是互相独立，而是一个不可分割的整体。因为每个通道不是独立学习的，而是为了共同使用而优化的，多输出通道并不仅是学习多个单通道的检测器。</p><p>用$c_i$和$c_o$分别表示输入和输出通道的数目，并让$k_h$和$k_w$为卷积核的高度和宽度。为了获得多个通道的输出，可以为每个输出通道创建一个形状为$c_i\times k_h\times k_w$的卷积核张量，这样卷积核的形状是$c_o\times c_i\times k_h\times k_w$。在互相关运算中，每个输出通道先获取所有输入通道，再以对应该输出通道的卷积核计算出结果。</p><p>下面实现一个计算多个通道的输出的互相关函数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">corr2d_multi_in_out</span>(<span class="params">X, K</span>):</span></span><br><span class="line">    <span class="comment"># 迭代“K”的第0个维度，每次都对输入“X”执行互相关运算。</span></span><br><span class="line">    <span class="comment"># 最后将所有结果都叠加在一起，stack</span></span><br><span class="line">    <span class="keyword">return</span> torch.stack([corr2d_multi_in(X, k) <span class="keyword">for</span> k <span class="keyword">in</span> K], <span class="number">0</span>)</span><br></pre></td></tr></table></figure><br>通过将核张量K与K+1（K中每个元素加1）和K+2连接起来，构造了一个具有3个输出通道的卷积核：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">K = torch.stack((K, K + <span class="number">1</span>, K + <span class="number">2</span>), <span class="number">0</span>)</span><br><span class="line">K.shape</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure><br>此时，卷积核K的形状为$3\times2\times2\times2$，可以想作是3片2通道的2维张量。然后对输入张量X与卷积核张量K执行互相关运算。现在的输出包含3个通道，第一个通道的结果与先前输入张量X和多输入单输出通道的结果一致：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">corr2d_multi_in_out(X, K)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[[ <span class="number">56.</span>,  <span class="number">72.</span>],</span><br><span class="line">         [<span class="number">104.</span>, <span class="number">120.</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">76.</span>, <span class="number">100.</span>],</span><br><span class="line">         [<span class="number">148.</span>, <span class="number">172.</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">96.</span>, <span class="number">128.</span>],</span><br><span class="line">         [<span class="number">192.</span>, <span class="number">224.</span>]]])</span><br></pre></td></tr></table></figure></p><h3 id="4-3-1-times1-卷积层"><a href="#4-3-1-times1-卷积层" class="headerlink" title="4.3. $1\times1$卷积层"></a>4.3. $1\times1$卷积层</h3><p>$1 \times 1$卷积，即$k_h = k_w = 1$，看起来似乎没有多大意义。<br>毕竟，卷积的本质是有效提取相邻像素间的相关特征，而$1 \times 1$卷积并没有这种作用。<br>尽管如此，$1 \times 1$卷积核仍然十分流行，经常包含在复杂深层网络的设计中。下面详细地解读一下它的实际作用。</p><p>因为使用了最小窗口，$1\times 1$卷积失去了卷积层的特有能力——在高度和宽度维度上，识别相邻元素间相互作用的能力。<br>其实，$1\times 1$卷积的唯一计算发生在通道上。</p><p>下图展示了使用$1\times 1$卷积核与$3$个输入通道和$2$个输出通道的互相关计算。<br>这里输入和输出具有相同的高度和宽度，输出中的每个元素都是从输入图像中同一位置的元素的线性组合。对于图像来说，实际上是对每个像素点，在不同的通道上进行线性组合（信息整合），且保留了图片的原有平面结构，完成升维或降维的功能。</p><p>可以将$1 \times 1$卷积层看作是在每个像素位置应用的全连接层，以$c_i$个输入值转换为$c_o$个输出值。也就是说$1 \times 1$卷积核的一个作用是调整通道数，类比多层感知机中的全连接层，调整输入和输出的大小。<br>这仍然是一个卷积层，所以跨像素的权重是一致的。同时，$1\times 1$卷积层需要的权重维度为$c_o\times c_i$，再额外加上一个偏置。<br><img src="/assets/post_img/article53/conv-1x1.svg" alt="1*1"><br>使用全连接层实现$1 \times 1$卷积，注意需要对输入和输出的数据形状进行调整：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># X是输入，K是卷积核</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">corr2d_multi_in_out_1x1</span>(<span class="params">X, K</span>):</span></span><br><span class="line">    c_i, h, w = X.shape</span><br><span class="line">    c_o = K.shape[<span class="number">0</span>]</span><br><span class="line">    X = X.reshape((c_i, h * w))</span><br><span class="line">    K = K.reshape((c_o, c_i))</span><br><span class="line">    <span class="comment"># 全连接层中的矩阵乘法</span></span><br><span class="line">    Y = torch.matmul(K, X)</span><br><span class="line">    <span class="keyword">return</span> Y.reshape((c_o, h, w))</span><br></pre></td></tr></table></figure><br>当执行$1 \times 1$卷积运算时，上述函数相当于先前实现的互相关函数corr2d_multi_in_out：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 注意这里的 X 和 K 的形状，在reshape后的矩阵乘法应用了广播机制</span></span><br><span class="line"><span class="comment"># 3行9列的X 和 2行3列的K 进行矩阵乘法，将K扩展为9行3列进行运算</span></span><br><span class="line">X = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">K = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">Y1 = corr2d_multi_in_out_1x1(X, K)</span><br><span class="line">Y2 = corr2d_multi_in_out(X, K)</span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">float</span>(torch.<span class="built_in">abs</span>(Y1 - Y2).<span class="built_in">sum</span>()) &lt; <span class="number">1e-6</span></span><br></pre></td></tr></table></figure><br>综上，$1 \times 1$卷积层通常用于调整网络层的通道数量和控制模型复杂性。</p><h2 id="5-池化层"><a href="#5-池化层" class="headerlink" title="5. 池化层"></a>5. 池化层</h2><p>当处理图像时，通常希望逐渐降低隐藏表示的空间分辨率、聚集信息，这样随着我们在神经网络中层叠的上升，每个神经元对其敏感的感受域（输入）就越大。</p><p>而机器学习任务通常会跟全局图像的问题有关（例如，“图像是否包含一只猫呢？”），所以最后一层的神经元应该对整个输入的全局敏感。通过逐渐聚合信息，生成越来越粗糙的映射，最终实现学习全局表示的目标，同时将卷积图层的所有优势保留在中间层。</p><p>此外，当检测较底层的特征时（例如边缘检测），通常希望这些特征保持某种程度上的平移不变性。例如，如果拍摄黑白之间轮廓清晰的图像X，并将整个图像向右移动一个像素，即Z[i, j] = X[i, j + 1]，则新图像Z的输出可能大不相同。而在现实中，随着拍摄角度的移动，任何物体不可能出现在同一位置上。即使用三脚架拍摄一个静止的物体，由于快门的移动而引起的相机振动，可能会使所有物体左右移动一个像素。</p><p>本节介绍池化（pooling）层，它具有双重目的：降低卷积层对位置的敏感性，同时降低对空间降采样表示的敏感性。</p><h3 id="5-1-最大池化层和平均池化层"><a href="#5-1-最大池化层和平均池化层" class="headerlink" title="5.1. 最大池化层和平均池化层"></a>5.1. 最大池化层和平均池化层</h3><p>与卷积层类似，池化层运算符由一个固定形状的窗口组成，该窗口根据其步幅大小在输入的所有区域上滑动，为固定形状窗口（有时称为池化窗口）遍历的每个位置计算一个输出。 但不同于卷积层中的输入与卷积核之间的互相关计算，池化层不包含参数。 相反，池运算是确定性的，我们通常计算池化窗口中所有元素的最大值或平均值。这些操作分别称为最大池化层（maximum pooling）和平均池化层（average pooling）。</p><p>在这两种情况下，与互相关运算符一样，池化窗口从输入张量的左上角开始，从左往右、从上往下的在输入张量内滑动。在池化窗口到达的每个位置，它计算该窗口中输入子张量的最大值或平均值。计算最大值或平均值是取决于使用了最大池化层还是平均池化层。<br><img src="/assets/post_img/article53/pooling.svg" alt="pooling"><br>着色部分是第一个输出元素，以及用于计算这个输出的输入元素:$\max(0, 1, 3, 4)=4$</p><p>池化窗口形状为$p \times q$的池化层称为$p \times q$池化层，池化操作称为$p \times q$池化。</p><p>回到开头提到的对象边缘检测示例，现在我们将使用卷积层的输出作为$2\times 2$最大池化的输入。<br>设置卷积层输入为<code>X</code>，池化层输出为<code>Y</code>。<br>无论是移动<code>X[i, j]</code>到<code>X[i, j + 1]</code>，或移动<code>X[i, j]</code>到<code>X[i + 1, j]</code>，池化层始终输出<code>Y[i, j] = 1</code>。<br>也就是说，使用$2\times 2$最大池化层，即使在高度或宽度上移动一个元素，卷积层仍然可以识别到模式。</p><p>在下面的代码中的<code>pool2d</code>函数，实现了池化层的正向传播。这类似于之前实现的<code>corr2d</code>函数。<br>但这里我们没有卷积核，输出为输入中每个区域的最大值或平均值。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pool2d</span>(<span class="params">X, pool_size, mode=<span class="string">&#x27;max&#x27;</span></span>):</span></span><br><span class="line">    p_h, p_w = pool_size</span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>] - p_h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - p_w + <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">if</span> mode == <span class="string">&#x27;max&#x27;</span>:</span><br><span class="line">                Y[i, j] = X[i: i + p_h, j: j + p_w].<span class="built_in">max</span>()</span><br><span class="line">            <span class="keyword">elif</span> mode == <span class="string">&#x27;avg&#x27;</span>:</span><br><span class="line">                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure><br>验证二维最大池化层的输出：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X = torch.tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]])</span><br><span class="line">pool2d(X, (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[<span class="number">4.</span>, <span class="number">5.</span>],</span><br><span class="line">        [<span class="number">7.</span>, <span class="number">8.</span>]])</span><br></pre></td></tr></table></figure><br>验证平均池化层：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pool2d(X, (<span class="number">2</span>, <span class="number">2</span>), <span class="string">&#x27;avg&#x27;</span>)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[<span class="number">2.</span>, <span class="number">3.</span>],</span><br><span class="line">        [<span class="number">5.</span>, <span class="number">6.</span>]])</span><br></pre></td></tr></table></figure></p><h3 id="5-2-填充和步幅"><a href="#5-2-填充和步幅" class="headerlink" title="5.2. 填充和步幅"></a>5.2. 填充和步幅</h3><p>与卷积层一样，池化层也可以通过填充和步幅以获得所需的输出形状。 下面用深度学习框架中内置的二维最大池化层，来演示池化层中填充和步幅的使用。 首先构造了一个输入张量X，它有四个维度，其中样本数和通道数都是1。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">X = torch.arange(<span class="number">16</span>, dtype=torch.float32).reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">X</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[[[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>],</span><br><span class="line">          [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>],</span><br><span class="line">          [ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>],</span><br><span class="line">          [<span class="number">12.</span>, <span class="number">13.</span>, <span class="number">14.</span>, <span class="number">15.</span>]]]])</span><br></pre></td></tr></table></figure><br>默认情况下，深度学习框架中的步幅与池化窗口的大小相同。 因此，如果我们使用形状为(3, 3)的池化窗口，那么默认情况下，我们得到的步幅形状为(3, 3)。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pool2d = nn.MaxPool2d(<span class="number">3</span>)</span><br><span class="line">pool2d(X)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[[[<span class="number">10.</span>]]]])</span><br></pre></td></tr></table></figure><br>可以设定一个任意大小的矩形池化窗口，并分别设定填充和步幅的高度和宽度。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pool2d = nn.MaxPool2d((<span class="number">2</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">3</span>), padding=(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">pool2d(X)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[[[ <span class="number">5.</span>,  <span class="number">7.</span>],</span><br><span class="line">          [<span class="number">13.</span>, <span class="number">15.</span>]]]])</span><br></pre></td></tr></table></figure></p><h3 id="5-3-多个通道"><a href="#5-3-多个通道" class="headerlink" title="5.3. 多个通道"></a>5.3. 多个通道</h3><p>在处理多通道输入数据时，<em>池化层在每个输入通道上单独运算，而不是像卷积层一样在通道上对输入进行汇总</em>。 这意味着池化层的输出通道数与输入通道数相同。 下面在通道维度上连结张量X和X + 1，以构建具有2个通道的输入。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">X = torch.cat((X, X + <span class="number">1</span>), <span class="number">1</span>)</span><br><span class="line">X</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[[[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>],</span><br><span class="line">          [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>],</span><br><span class="line">          [ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>],</span><br><span class="line">          [<span class="number">12.</span>, <span class="number">13.</span>, <span class="number">14.</span>, <span class="number">15.</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">4.</span>],</span><br><span class="line">          [ <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>,  <span class="number">8.</span>],</span><br><span class="line">          [ <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>, <span class="number">12.</span>],</span><br><span class="line">          [<span class="number">13.</span>, <span class="number">14.</span>, <span class="number">15.</span>, <span class="number">16.</span>]]]])</span><br></pre></td></tr></table></figure><br>池化后输出通道的数量仍然是2：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">pool2d = nn.MaxPool2d(<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">pool2d(X)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[[[ <span class="number">5.</span>,  <span class="number">7.</span>],</span><br><span class="line">          [<span class="number">13.</span>, <span class="number">15.</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">6.</span>,  <span class="number">8.</span>],</span><br><span class="line">          [<span class="number">14.</span>, <span class="number">16.</span>]]]])</span><br></pre></td></tr></table></figure></p><h2 id="6-卷积神经网络（LeNet）"><a href="#6-卷积神经网络（LeNet）" class="headerlink" title="6. 卷积神经网络（LeNet）"></a>6. 卷积神经网络（LeNet）</h2><p>现在，对于Fashion-MNIST数据集中的服装图片，我们已经掌握了卷积层的处理方法，可以在图像中保留空间结构（不需要像MLP中一样展平）。 同时，用卷积层代替全连接层的另一个好处是：模型更简洁、所需的参数更少。</p><p>本节将介绍LeNet，它是最早发布的卷积神经网络之一，因其在计算机视觉任务中的高效性能而受到广泛关注。 这个模型是由AT&amp;T贝尔实验室的研究员Yann LeCun在1989年提出的（并以其命名），目的是识别图像中的手写数字。 当时，Yann LeCun发表了第一篇通过反向传播成功训练卷积神经网络的研究，这项工作代表了十多年来神经网络研究开发的成果。</p><p>LeNet在当时取得了与支持向量机（support vector machines, SVM）性能相媲美的成果，成为监督学习的主流方法。 LeNet被广泛用于自动取款机（ATM）机中，帮助识别处理支票的数字。 时至今日，一些自动取款机仍在运行Yann LeCun和他的同事Leon Bottou在上世纪90年代写的代码！</p><h3 id="6-1-LeNet"><a href="#6-1-LeNet" class="headerlink" title="6.1. LeNet"></a>6.1. LeNet</h3><p>总体来看，LeNet（LeNet-5）由两个部分组成：</p><ul><li>卷积编码器：由两个卷积层组成;</li><li>全连接层密集块：由三个全连接层组成。</li></ul><p>该架构如下所示（LeNet中的数据流。输入是手写数字，输出为10种可能结果的概率）：<br><img src="/assets/post_img/article53/lenet.svg" alt="lenet"><br>每个卷积块中的基本单元是一个卷积层、一个sigmoid激活函数和平均池化层。虽然ReLU和最大池化层更有效，但它们在20世纪90年代还没有出现。</p><p>每个卷积层使用$5\times 5$卷积核和一个sigmoid激活函数。这些层将输入映射到多个二维特征输出，通常同时增加通道的数量。第一卷积层有6个输出通道，而第二个卷积层有16个输出通道。每个$2\times2$池操作（步幅2）通过空间下采样将维数减少4倍。卷积的输出形状由批量大小、通道数、高度、宽度决定。</p><p>为了将卷积块的输出传递给稠密块，必须在小批量中展平每个样本。也就是将这个四维输入转换成全连接层所期望的二维输入。这里的二维表示的第一个维度索引小批量中的样本，第二个维度给出每个样本的平面向量表示。LeNet的稠密块有三个全连接层，分别有120、84和10个输出。因为我们在执行分类任务，所以输出层的10维对应于最后输出结果的数量。</p><p>用深度学习框架实现此类模型非常简单，只需要实例化一个<code>Sequential</code>块并将需要的层连接在一起。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    <span class="comment"># 通道数 1 -&gt; 6</span></span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    <span class="comment"># 通道数 6 -&gt; 16</span></span><br><span class="line">    nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">84</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure><br>这对原始模型做了一点小改动，去掉了最后一层的高斯激活。除此之外，这个网络与最初的LeNet-5一致。</p><p>下面将一个大小为$28 \times 28$的单通道（黑白）图像通过LeNet。通过在每一层打印输出的形状可以检查模型，以确保其操作与我们期望的一致。<br><img src="/assets/post_img/article53/lenet-vert.svg" alt="vert"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>), dtype=torch.float32)</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__,<span class="string">&#x27;output shape: \t&#x27;</span>,X.shape)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">Conv2d output shape:         torch.Size([<span class="number">1</span>, <span class="number">6</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line">Sigmoid output shape:        torch.Size([<span class="number">1</span>, <span class="number">6</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line">AvgPool2d output shape:      torch.Size([<span class="number">1</span>, <span class="number">6</span>, <span class="number">14</span>, <span class="number">14</span>])</span><br><span class="line">Conv2d output shape:         torch.Size([<span class="number">1</span>, <span class="number">16</span>, <span class="number">10</span>, <span class="number">10</span>])</span><br><span class="line">Sigmoid output shape:        torch.Size([<span class="number">1</span>, <span class="number">16</span>, <span class="number">10</span>, <span class="number">10</span>])</span><br><span class="line">AvgPool2d output shape:      torch.Size([<span class="number">1</span>, <span class="number">16</span>, <span class="number">5</span>, <span class="number">5</span>])</span><br><span class="line">Flatten output shape:        torch.Size([<span class="number">1</span>, <span class="number">400</span>])</span><br><span class="line">Linear output shape:         torch.Size([<span class="number">1</span>, <span class="number">120</span>])</span><br><span class="line">Sigmoid output shape:        torch.Size([<span class="number">1</span>, <span class="number">120</span>])</span><br><span class="line">Linear output shape:         torch.Size([<span class="number">1</span>, <span class="number">84</span>])</span><br><span class="line">Sigmoid output shape:        torch.Size([<span class="number">1</span>, <span class="number">84</span>])</span><br><span class="line">Linear output shape:         torch.Size([<span class="number">1</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure><p>注意，在整个卷积块中，与上一层相比，每一层特征的高度和宽度都减小了。 第一个卷积层使用2个像素的填充，来补偿$5 \times 5$卷积核导致的特征减少。 相反，第二个卷积层没有填充，因此高度和宽度都减少了4个像素。 随着层叠的上升，通道的数量从输入时的1个，增加到第一个卷积层之后的6个，再到第二个卷积层之后的16个。 同时，每个池化层的高度和宽度都减半。最后，每个全连接层减少维数，最终输出一个维数与结果分类数相匹配的输出。</p><h3 id="6-2-模型训练"><a href="#6-2-模型训练" class="headerlink" title="6.2. 模型训练"></a>6.2. 模型训练</h3><p>现在已经实现了LeNet，看看它在Fashion-MNIST数据集上的表现：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line"><span class="comment"># 省略了load_data_fashion_mnist的实现</span></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size=batch_size)</span><br></pre></td></tr></table></figure><br>虽然卷积神经网络的参数较少，但与深度的多层感知机相比，它们的计算成本仍然很高，因为每个参数都参与更多的乘法。 可以使用GPU加快训练。<br>由于完整的数据集位于内存中，因此在模型使用GPU计算数据集之前需要将其复制到显存中。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_accuracy_gpu</span>(<span class="params">net, data_iter, device=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用GPU计算模型在数据集上的精度&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module):</span><br><span class="line">        net.<span class="built_in">eval</span>()  <span class="comment"># 设置为评估模式</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> device:</span><br><span class="line">            device = <span class="built_in">next</span>(<span class="built_in">iter</span>(net.parameters())).device</span><br><span class="line">    <span class="comment"># 正确预测的数量，总预测的数量</span></span><br><span class="line">    <span class="comment"># 省略了Accumulator的实现</span></span><br><span class="line">    metric = Accumulator(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="comment"># 复制入显存</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(X, <span class="built_in">list</span>):</span><br><span class="line">                <span class="comment"># BERT微调所需的（之后将介绍）</span></span><br><span class="line">                X = [x.to(device) <span class="keyword">for</span> x <span class="keyword">in</span> X]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                X = X.to(device)</span><br><span class="line">            y = y.to(device)</span><br><span class="line">            <span class="comment"># 省略了accuracy的实现</span></span><br><span class="line">            metric.add(accuracy(net(X), y), y.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br></pre></td></tr></table></figure><br>训练方面，在进行正向和反向传播之前，我们需要将每一小批量数据移动到指定的设备（例如GPU）上。<br>以下训练函数假定从高级API创建的模型作为输入，并进行相应的优化，使用Xavier随机初始化模型参数。 与全连接层一样，使用交叉熵损失函数和小批量随机梯度下降。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_ch6</span>(<span class="params">net, train_iter, test_iter, num_epochs, lr, device</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;用GPU训练模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_weights</span>(<span class="params">m</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear <span class="keyword">or</span> <span class="built_in">type</span>(m) == nn.Conv2d:</span><br><span class="line">            nn.init.xavier_uniform_(m.weight)</span><br><span class="line">    net.apply(init_weights)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;training on&#x27;</span>, device)</span><br><span class="line">    net.to(device)</span><br><span class="line">    optimizer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">    <span class="comment"># 交叉熵</span></span><br><span class="line">    loss = nn.CrossEntropyLoss()</span><br><span class="line">    <span class="comment"># 省略了Animator实现</span></span><br><span class="line">    animator = Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs],</span><br><span class="line">                            legend=[<span class="string">&#x27;train loss&#x27;</span>, <span class="string">&#x27;train acc&#x27;</span>, <span class="string">&#x27;test acc&#x27;</span>])</span><br><span class="line">    <span class="comment"># 省略了Timer实现</span></span><br><span class="line">    timer, num_batches = Timer(), <span class="built_in">len</span>(train_iter)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="comment"># 训练损失之和，训练准确率之和，样本数</span></span><br><span class="line">        metric = Accumulator(<span class="number">3</span>)</span><br><span class="line">        net.train()</span><br><span class="line">        <span class="keyword">for</span> i, (X, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_iter):</span><br><span class="line">            timer.start()</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            X, y = X.to(device), y.to(device)</span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            l = loss(y_hat, y)</span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                metric.add(l * X.shape[<span class="number">0</span>], accuracy(y_hat, y), X.shape[<span class="number">0</span>])</span><br><span class="line">            timer.stop()</span><br><span class="line">            train_l = metric[<span class="number">0</span>] / metric[<span class="number">2</span>]</span><br><span class="line">            train_acc = metric[<span class="number">1</span>] / metric[<span class="number">2</span>]</span><br><span class="line">            <span class="keyword">if</span> (i + <span class="number">1</span>) % (num_batches // <span class="number">5</span>) == <span class="number">0</span> <span class="keyword">or</span> i == num_batches - <span class="number">1</span>:</span><br><span class="line">                animator.add(epoch + (i + <span class="number">1</span>) / num_batches,</span><br><span class="line">                             (train_l, train_acc, <span class="literal">None</span>))</span><br><span class="line">        test_acc = evaluate_accuracy_gpu(net, test_iter)</span><br><span class="line">        animator.add(epoch + <span class="number">1</span>, (<span class="literal">None</span>, <span class="literal">None</span>, test_acc))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;loss <span class="subst">&#123;train_l:<span class="number">.3</span>f&#125;</span>, train acc <span class="subst">&#123;train_acc:<span class="number">.3</span>f&#125;</span>, &#x27;</span></span><br><span class="line">          <span class="string">f&#x27;test acc <span class="subst">&#123;test_acc:<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;metric[<span class="number">2</span>] * num_epochs / timer.<span class="built_in">sum</span>():<span class="number">.1</span>f&#125;</span> examples/sec &#x27;</span></span><br><span class="line">          <span class="string">f&#x27;on <span class="subst">&#123;<span class="built_in">str</span>(device)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><br>训练和评估LeNet-5模型（M1芯片GPU）：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lr, num_epochs = <span class="number">0.9</span>, <span class="number">10</span></span><br><span class="line">train_ch6(net, train_iter, test_iter, num_epochs, lr, torch.device(<span class="string">&#x27;mps&#x27;</span>))</span><br></pre></td></tr></table></figure><br>关于这一部分的完整代码在<em>实践</em>中可以找到。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;p&gt;本章介绍的卷积神经网络（convolutional neural network，CNN）是一类强大的、为处理图像数据而设计的神经网络。 基于卷积神经网络架构的模型在计算机视觉领域中已经占主导地位，当今几乎所有的图像识别、目标检测或语义分割相关的学术竞赛和商业应用都以这种方法为基础。&lt;/p&gt;
&lt;p&gt;对应实践：&lt;a href=&quot;https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x07.ipynb&quot;&gt;https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x07.ipynb&lt;/a&gt;&lt;br&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://silencezheng.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Python机器学习库简介</title>
    <link href="http://silencezheng.top/2022/07/28/article52/"/>
    <id>http://silencezheng.top/2022/07/28/article52/</id>
    <published>2022-07-28T14:09:25.000Z</published>
    <updated>2022-07-28T14:15:47.372Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>学深度学习也有一段时间了，想着同时也需要看一看机器学习的算法，对机器学习的基础有一个全面些的了解，过程中发现对个别机器学习库的了解不多，写个博客简单总结一下。</p><p>主要介绍的库有：</p><ul><li>pandas</li><li>numpy</li><li>scipy</li><li>sklearn<span id="more"></span><h2 id="NumPy"><a href="#NumPy" class="headerlink" title="NumPy"></a>NumPy</h2>NumPy是使用Python进行科学计算的基础软件包。以纯数学的矩阵计算为基础。<br>核心功能包括：</li><li>功能强大的N维数组对象。</li><li>精密广播功能函数。</li><li>集成 C/C+和Fortran 代码的工具。</li><li>强大的线性代数、傅立叶变换和随机数功能。</li></ul><p>NumPy 最重要的一个特点是其 N 维数组对象 ndarray，它是一系列同类型数据的集合，以 0 下标为开始进行集合中元素的索引。ndarray 对象是用于存放同类型元素的多维数组。ndarray 中的每个元素在内存中都有相同存储大小的区域。</p><p>ndarray对象的内容可以通过索引或切片来访问和修改，与 Python 中 list 的切片操作一样。ndarray 数组可以基于 0 - n 的下标进行索引，切片对象可以通过内置的 slice 函数，并设置 start, stop 及 step 参数进行，从原数组中切割出一个新数组。</p><h2 id="Pandas"><a href="#Pandas" class="headerlink" title="Pandas"></a>Pandas</h2><p>Pandas是一个强大的分析结构化数据的工具集；它的使用基础是Numpy（提供高性能的矩阵运算）；用于数据挖掘和数据分析，同时也提供数据清洗功能。</p><p>Series是一种类似于一维数组的对象，由一组数据(各种NumPy数据类型)以及一组与之相关的数据标签(即索引)组成。仅由一组数据也可产生简单的Series对象。<br><img src="/assets/post_img/article52/Series.jpeg" alt="Series"><br>DataFrame是Pandas中核心的一个表格型的数据结构，包含有一组有序的列，每列可以是不同的值类型(数值、字符串、布尔型等)，DataFrame即有行索引也有列索引，可以被看做是由Series组成的字典。</p><p>Pandas 适用于处理以下类型的数据：</p><ul><li>与 SQL 或 Excel 表类似的，含异构列的表格数据;</li><li>有序和无序（非固定频率）的时间序列数据;</li><li>带行列标签的矩阵数据，包括同构或异构型数据;</li><li>任意其它形式的观测、统计数据集, 数据转入 Pandas 数据结构时不必事先标记。</li></ul><p>利用Pandas做数据清洗也是一个非常常见的应用，例如空值、重复数据和错误数据的清洗。</p><h2 id="SciPy"><a href="#SciPy" class="headerlink" title="SciPy"></a>SciPy</h2><p>SciPy 是基于 Numpy 的科学计算库，用于数学、科学、工程学等领域，很多有一些高阶抽象和物理模型需要使用 SciPy。</p><p>NumPy 能够做一些基础的分析或变换，比如转置/逆矩阵/均值方差的计算等; SciPy则可以提供高阶的分析，比如拟合/回归/参数估计等。</p><p>SciPy被组织成覆盖不同科学计算领域的子包，如下：</p><div class="table-container"><table><thead><tr><th style="text-align:center">子包</th><th style="text-align:center">应用</th></tr></thead><tbody><tr><td style="text-align:center">scipy.cluster</td><td style="text-align:center">矢量量化/Kmeans</td></tr><tr><td style="text-align:center">scipy.constants</td><td style="text-align:center">物理和数学常数</td></tr><tr><td style="text-align:center">scipy.fftpack</td><td style="text-align:center">傅里叶变换</td></tr><tr><td style="text-align:center">scipy.integrate</td><td style="text-align:center">集成例程</td></tr><tr><td style="text-align:center">scipy.interpolate</td><td style="text-align:center">插值</td></tr><tr><td style="text-align:center">scipy.io</td><td style="text-align:center">数据输入和输出</td></tr><tr><td style="text-align:center">scipy.linalg</td><td style="text-align:center">线性代数例程</td></tr><tr><td style="text-align:center">scipy.ndimage</td><td style="text-align:center">n维图像包</td></tr><tr><td style="text-align:center">scipy.odr</td><td style="text-align:center">正交距离回归</td></tr><tr><td style="text-align:center">scipy.optimize</td><td style="text-align:center">优化</td></tr><tr><td style="text-align:center">scipy.signal</td><td style="text-align:center">信号处理</td></tr><tr><td style="text-align:center">scipy.sparse</td><td style="text-align:center">稀疏矩阵</td></tr><tr><td style="text-align:center">scipy.spatial</td><td style="text-align:center">空间数据结构和算法</td></tr><tr><td style="text-align:center">scipy.special</td><td style="text-align:center">任何特殊的数学函数</td></tr><tr><td style="text-align:center">scipy.stats</td><td style="text-align:center">统计</td></tr></tbody></table></div><h2 id="Sklearn"><a href="#Sklearn" class="headerlink" title="Sklearn"></a>Sklearn</h2><p>全称Scikit-learn，Scikit-learn是一个开源的机器学习库，它支持有监督和无监督的学习。它还提供了用于模型拟合，数据预处理，模型选择和评估以及许多其他实用程序的各种工具。</p><p>支持机器学习的六大任务模块：分类（Classification）、回归(Regression)、聚类（Clustering）、降维、模型选择和预处理。</p><p>分类：识别某个对象属于哪个类别，常用的算法有：SVM（支持向量机）、nearest neighbors（最近邻）、random forest（随机森林），常见的应用有：垃圾邮件识别、图像识别。</p><p>回归：预测与对象相关联的连续值属性，常见的算法有：SVR（支持向量机）、 ridge regression（岭回归）、Lasso，常见的应用有：药物反应，预测股价。</p><p>聚类：将相似对象自动分组，常用的算法有：k-Means、 spectral clustering、mean-shift，常见的应用有：客户细分，分组实验结果。</p><p>降维：减少要考虑的随机变量的数量，常见的算法有：PCA（主成分分析）、feature selection（特征选择）、non-negative matrix factorization（非负矩阵分解），常见的应用有：可视化，提高效率。</p><p>模型选择：比较，验证，选择参数和模型，常用的模块有：grid search（网格搜索）、cross validation（交叉验证）、 metrics（度量）。它的目标是通过参数调整提高精度。</p><p>预处理：特征提取和归一化，常用的模块有：preprocessing，feature extraction，常见的应用有：把输入数据（如文本）转换为机器学习算法可用的数据。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;学深度学习也有一段时间了，想着同时也需要看一看机器学习的算法，对机器学习的基础有一个全面些的了解，过程中发现对个别机器学习库的了解不多，写个博客简单总结一下。&lt;/p&gt;
&lt;p&gt;主要介绍的库有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;pandas&lt;/li&gt;
&lt;li&gt;numpy&lt;/li&gt;
&lt;li&gt;scipy&lt;/li&gt;
&lt;li&gt;sklearn</summary>
    
    
    
    
    <category term="Python" scheme="http://silencezheng.top/tags/Python/"/>
    
    <category term="机器学习" scheme="http://silencezheng.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
