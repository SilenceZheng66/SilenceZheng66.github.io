<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>SilenceZheng66-BLOG</title>
  
  <subtitle>log</subtitle>
  <link href="http://silencezheng.top/atom.xml" rel="self"/>
  
  <link href="http://silencezheng.top/"/>
  <updated>2023-01-05T14:48:54.641Z</updated>
  <id>http://silencezheng.top/</id>
  
  <author>
    <name>SilenceZheng66</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>初识Docker</title>
    <link href="http://silencezheng.top/2023/01/05/article89/"/>
    <id>http://silencezheng.top/2023/01/05/article89/</id>
    <published>2023-01-05T14:46:46.000Z</published>
    <updated>2023-01-05T14:48:54.641Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言："><a href="#前言：" class="headerlink" title="前言："></a>前言：</h2><p>初识Docker，本文涵盖内容：</p><ul><li>什么是Docker？</li><li>如何安装Docker？</li><li>Docker的基本使用</li></ul><p>ps：多数内容来自《第一本Docker书》，但其中部分内容已经过于老旧了，由我（SilenceZheng66）进行了重新编写。<br>pps：本来就想写一个简单的便捷教程，没想到越来越长…<br><span id="more"></span></p><h2 id="什么是Docker？"><a href="#什么是Docker？" class="headerlink" title="什么是Docker？"></a>什么是Docker？</h2><p>要了解什么是Docker，需要先了解什么是<strong>容器技术</strong>。</p><h3 id="什么是容器技术？"><a href="#什么是容器技术？" class="headerlink" title="什么是容器技术？"></a>什么是容器技术？</h3><p>相信本篇的读者一定接触过虚拟机，或者说<strong>管理程序虚拟化</strong>（hypervisor virtualization，HV）。HV通过中间层将一台或多台独立的机器虚拟运行于物理硬件之上，它解决的主要技术问题是，大多数物理硬件一次只能运行一个操作系统。这种限制往往导致资源浪费，因为单一的操作系统很少能充分利用硬件的能力。</p><p><img src="/assets/post_img/article89/img-hypervisor.png" alt="hv"></p><p>而<strong>容器</strong>则是直接运行在操作系统内核之上的用户空间，因此，容器虚拟化也被称为“操作系统级虚拟化”，容器技术可以让多个独立的用户空间运行在同一台宿主机上。</p><p>由于“客居”于操作系统，容器只能只能运行于底层宿主机相同或相似的操作系统。尽管有诸多局限性，容器还是被广泛部署于各种场合，例如超大规模的多租户服务部署、轻量级沙盒以及对安全要求不太高的隔离环境中等等（chroot jail是一种容器）。</p><p>容器经常被认为是精益技术，因为它需要的开销有限。和传统虚拟化与半虚拟化（paravirtualization）相比，容器运行不需要模拟层（emulation layer）和管理层（hypervisor layer），而是<strong>使用操作系统的系统调用接口</strong>。这降低了运行单个容器所需的开销，也使得宿主机中可运行更多的容器。</p><h3 id="Docker的诞生"><a href="#Docker的诞生" class="headerlink" title="Docker的诞生"></a>Docker的诞生</h3><p>Docker的出现是为了解决容器技术的复杂性：容器本身就比较复杂，不易安装、管理和自动化。</p><p>Docker是一个能够把开发的应用程序自动部署到容器的开源引擎。Docker的特别之处在于，它在虚拟化的容器执行环境中增加了一个应用程序部署引擎。该引擎的目标就是提供一个轻量、快速的环境，能够运行开发者的程序，并方便高效的将程序从开发者的笔记本部署到测试环境，然后再部署到生产环境。</p><p>使用Docker，开发人员只需要关心容器中运行的应用，而运维人员只需要关心如何管理容器。Docker设计的目的有许多：</p><ul><li>加强开发人员写代码的开发环境与应用程序要部署的生产环境的一致性，从而降低那种“开发时一切都正常，肯定是运维的问题”的风险。</li><li>缩短代码从开发、测试到部署、上线的周期，使应用具备可移植性，易于构建并易于协作。</li></ul><p>Docker还鼓励面向服务的架构和微服务架构，推荐单个容器只运行一个应用程序或者进程，这样就形成了一个分布式的应用程序模型。在这种模型下，应用程序或服务都可以表示为一系列内部互联的容器，从而使分布式部署应用程序，扩展或调试应用程序都变得非常简单，同时也提高了程序的内省性。当然，这种模式不是必须的。</p><h3 id="Docker的核心组件"><a href="#Docker的核心组件" class="headerlink" title="Docker的核心组件"></a>Docker的核心组件</h3><p>Docker的核心组件主要有四个：</p><ul><li>Docker客户端和服务器（Docker引擎）</li><li>Docker镜像（Image）</li><li>镜像库（Registry）</li><li>Docker容器（Container）</li></ul><h4 id="Docker客户端和服务器"><a href="#Docker客户端和服务器" class="headerlink" title="Docker客户端和服务器"></a>Docker客户端和服务器</h4><p>Docker是一个客户端/服务器(C/S)架构的程序。Docker客户端只需向Docker服务器或守护进程发出请求，服务器或守护进程将完成所有工作并返回结果。Docker守护进程有时也称为Docker引擎。</p><p><img src="/assets/post_img/article89/arch.jpeg" alt="arch"></p><p>Docker提供了一个命令行工具docker以及一整套RESTful API来与守护进程交互。用户可以在同一台宿主机上运行Docker守护进程和客户端，也可以从本地的Docker客户端连接到运行在另一台宿主机上的远程Docker守护进程。</p><h4 id="Docker镜像（Image）"><a href="#Docker镜像（Image）" class="headerlink" title="Docker镜像（Image）"></a>Docker镜像（Image）</h4><p>镜像是构建Docker世界的基石。用户基于镜像来运行自己的容器。镜像也是Docker生命周期中的“<strong>构建</strong>”部分。镜像是基于<strong>联合(Union)文件系统</strong>的一种层式的结构，由一系列指令一步一步构建出来。例如:</p><ul><li>添加一个文件;</li><li>执行一个命令;</li><li>打开一个端口。</li></ul><p>可以把镜像当作容器的“源代码”，也可以把镜像理解为可执行文件，容器则是执行后运行起来的进程。镜像体积很小，非常“便携”，易于分享、存储和更新。</p><p>以上是关于镜像的最简单和模糊的解释，但是在这里我想更进一步，说明一下到底什么是镜像，这有助于我们之后使用镜像。</p><p>Docker镜像是<strong>由文件系统叠加而成</strong>。最底端是一个引导文件系统，即bootfs，这很像典型的 Linux/Unix的引导文件系统。Docker用户几乎永远不会和引导文件系统有什么交互。实际上，当一个容器启动后，它将会被移到内存中，而引导文件系统则会被卸载(unmount)，以留出更多的内存供initrd磁盘镜像使用。</p><p>到目前为止，Docker看起来还很像一个典型的Linux虚拟化栈。实际上，Docker镜像的第二层是root文件系统rootfs，它位于引导文件系统之上。rootfs可以是<strong>一种或多种</strong>操作系统(如Debian或者Ubuntu文件系统)。</p><p>在传统的Linux引导过程中，root文件系统会最先以只读的方式加载，当引导结束并完成了完整性检查之后，它才会被切换为读写模式。但是在Docker里，root文件系统永远只能是只读状态，并且Docker利用联合加载(union mount)技术又会在root文件系统层上加载更多的只读文件系统。<strong>联合加载指的是一次同时加载多个文件系统，但是在外面看起来只能看到一个文件系统</strong>。联合加载会将各层文件系统叠加到一起，这样最终的文件系统会包含所有底层的文件和目录。</p><p>Docker将这样的文件系统称为镜像。<strong>一个镜像可以放到另一个镜像的顶部（这会构成一种层叠结构）</strong>。位于下面的镜像称为<strong>父镜像</strong>(parent image)，可以依次类推，直到镜像栈的最底部，最底部的镜像称为<strong>基础镜像</strong>(base image)。最后，<strong>当从一个镜像启动容器时，Docker会在该镜像的最顶层加载一个读写文件系统</strong>。我们想在Docker中运行的程序就是在这个读写层中执行的。</p><p><img src="/assets/post_img/article89/image.jpeg" alt="images"></p><p>当Docker第一次启动一个容器时，初始的读写层是空的。当文件系统发生变化时，这些变化都会应用到这一层上。比如，如果想修改一个文件，这个文件首先会从该读写层下面的只读层复制到该读写层。该文件的只读版本依然存在，但是已经被读写层中的该文件副本所隐藏。</p><p>通常这种机制被称为<strong>写时复制</strong>(copy on write)，这也是使Docker如此强大的技术之一。每个只读镜像层都是只读的，并且以后永远不会变化。当创建一个新容器时，Docker会构建出一个镜像栈，并在栈的最顶端添加一个读写层。这个读写层再加上其下面的镜像层以及一些配置数据，就构成了一个容器。在上一章我们已经知道，容器是可以修改的，它们都有自己的状态，并且是可以启动和停止的。容器的这种特点加上<strong>镜像分层框架</strong>(image-layering framework)，使我们可以快速构建镜像并运行包含我们自己的应用程序和服务的容器。</p><h4 id="镜像库（Registry）"><a href="#镜像库（Registry）" class="headerlink" title="镜像库（Registry）"></a>镜像库（Registry）</h4><p>Docker用Registry来保存用户构建的镜像，可以分为公共和私有两种。Docker公司运营的公共Registry为Docker Hub。用户可以在Docker Hub注册账号，分享并保存自己的镜像。</p><p>用户也可以在Docker Hub上保存自己的私有镜像。例如，包含源代码或专利信息等需要保密的镜像，或者只在团队或组织内部可见的镜像。</p><p>用户甚至可以架设自己的私有Registry。私有Registry可以受到防火墙的保护，将镜像保存在防火墙后面，以满足一些组织的特殊需求。</p><p>这里我将Registry称为镜像库是不太准确的，事实上镜像仓库存在于Registry中。</p><h4 id="Docker容器（Container）"><a href="#Docker容器（Container）" class="headerlink" title="Docker容器（Container）"></a>Docker容器（Container）</h4><p>Docker可以帮用户构建和部署容器，用户只需要把自己的应用程序或服务打包放进容器即可。我们刚刚提到，容器是基于镜像启动起来的，容器中可以运行一个或多个进程。我们可以认为，<strong>镜像是Docker生命周期中的构建或打包阶段，而容器则是启动或执行阶段</strong>。</p><p>总结起来，Docker容器就是:</p><ul><li>一个镜像格式;</li><li>一系列标准的操作;</li><li>一个执行环境。</li></ul><p>Docker借鉴了标准集装箱的概念。标准集装箱将货物运往世界各地，Docker将这个模型运用到自己的设计哲学中，唯一不同的是：集装箱运输货物，而Docker运输软件。每个容器都包含一个软件镜像，也就是容器的“货物”，而且与真正的货物一样，</p><p>容器里的软件镜像可以进行一些操作。例如，镜像可以被创建、启动、关闭、重启以及销毁。和集装箱一样，Docker在执行上述操作时，并不关心容器中到底塞进了什么，它不管里面是Web服务器，还是数据库，或者是应用程序服务器什么的。所有容器都按照相同的方式将内容“装载”进去。</p><h3 id="Docker能做什么？"><a href="#Docker能做什么？" class="headerlink" title="Docker能做什么？"></a>Docker能做什么？</h3><p>容器可以为各种测试提供很好的沙盒环境。并且，容器本身就具有“标准性”的特征，非常适合为服务创建构建块。Docker的一些应用场景如下：</p><ul><li>加速本地开发和构建流程，使其更加高效、更加轻量化。本地开发人员可以构建、运行并分享Docker容器。容器可以在开发环境中构建，然后轻松地提交到测试环境中，并最终进入生产环境。 </li><li>能够让独立服务或应用程序在不同的环境中，得到相同的运行结果。这一点在面向服务的架构和重度依赖微型服务的部署中尤其实用。 </li><li>用Docker创建隔离的环境来进行测试。例如，用Jenkins CI这样的持续集成工具启动一个用于测试的容器。 </li><li>Docker可以让开发者先在本机上构建一个复杂的程序或架构来进行测试，而不是一开始就在生产环境部署、测试。 </li><li>构建一个多用户的平台即服务(PaaS)基础设施。 </li><li>为开发、测试提供一个轻量级的独立沙盒环境，或者将独立的沙盒环境用于技术教学，如Unix shell的使用、编程语言教学。 </li><li>提供软件即服务(SaaS)应用程序。</li><li>高性能、超大规模的宿主机部署。</li></ul><h3 id="Docker的技术组件"><a href="#Docker的技术组件" class="headerlink" title="Docker的技术组件"></a>Docker的技术组件</h3><p>Docker可以运行于任何安装了现代Linux内核的x64主机上，现在也支持很多其他平台。Docker的开销比较低，可以用于服务器、台式机或笔记本。它包括以下几个部分。</p><ul><li>一个原生的Linux容器格式，Docker中称为libcontainer。 </li><li>Linxu内核的命名空间(namespace)，用于隔离文件系统、进程和网络。</li><li>文件系统隔离：每个容器都有自己的root文件系统。 </li><li>进程隔离：每个容器都运行在自己的进程环境中。 </li><li>网络隔离：容器间的虚拟网络接口和IP地址都是分开的。 </li><li>资源隔离和分组：使用cgroups(即control group，Linux的内核特性之一)将CPU和内存之类的资源独立分配给每个Docker容器。 </li><li>写时复制：文件系统都是通过写时复制创建的，这就意味着文件系统是分层 的、快速的，而且占用的磁盘空间更小。 </li><li>日志：容器产生的STDOUT、STDERR和STDIN这些IO流都会被收集并记入日志， 用来进行日志分析和故障排错。</li><li>交互式shell：用户可以创建一个伪tty终端，将其连接到STDIN，为容器提供 一个交互式的shell。</li></ul><h2 id="如何安装Docker？"><a href="#如何安装Docker？" class="headerlink" title="如何安装Docker？"></a>如何安装Docker？</h2><p>我会演示如何在Ubuntu中安装Docker，强烈建议用户在首次安装前阅读<a href="https://docs.docker.com/engine/install/">官方文档</a>以了解目前官方支持的系统版本以及软硬件条件。</p><p>我会列举几种不同的安装方式，但不包括如何使用二进制文件安装 Docker。</p><h3 id="检查系统版本"><a href="#检查系统版本" class="headerlink" title="检查系统版本"></a>检查系统版本</h3><p>在使用任何安装方式前，都需要确定系统版本是否匹配，目前对于Ubuntu的系统要求如下：</p><blockquote><p>To install Docker Engine, you need the 64-bit version of one of these Ubuntu versions:</p><ul><li>Ubuntu Kinetic 22.10</li><li>Ubuntu Jammy 22.04 (LTS)</li><li>Ubuntu Focal 20.04 (LTS)</li><li>Ubuntu Bionic 18.04 (LTS)</li></ul></blockquote><p>在Ubuntu终端键入<code>cat /etc/issue</code>，可以看到我的版本号为<code>Ubuntu 18.04.6 LTS</code>，符合要求。</p><h3 id="卸载旧版本Docker"><a href="#卸载旧版本Docker" class="headerlink" title="卸载旧版本Docker"></a>卸载旧版本Docker</h3><p>确定系统符合要求后，还有一步前置工作就是删除掉可能已安装的旧版本Docker，它们的名字可能是<code>docker, docker.io, or docker-engine</code>。</p><p>删除旧版本Docker：<code>sudo apt-get remove docker docker-engine docker.io containerd runc</code></p><p>清除容器、镜像和Volumes：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo rm -rf /var/lib/docker</span><br><span class="line">sudo rm -rf /var/lib/containerd</span><br></pre></td></tr></table></figure></p><p>这里简单介绍一下Volume，Volume就是目录或文件，它可以绕过默认的联合文件系统，以正常的文件或目录的形式存在于宿主机。利用Volume可以将容器以及容器产生的新数据分离开来，这样使用<code>docker rm 容器</code>删除容器时，不会影响相关数据。</p><h3 id="从apt存储库安装Docker"><a href="#从apt存储库安装Docker" class="headerlink" title="从apt存储库安装Docker"></a>从apt存储库安装Docker</h3><p>该方法通过apt安装Docker，在首次安装时，需要先设置Docker apt repository。</p><p>1、更新apt包索引并安装包以允许apt通过 HTTPS 使用存储库：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line"></span><br><span class="line">sudo apt-get install \</span><br><span class="line">    ca-certificates \</span><br><span class="line">    curl \</span><br><span class="line">    gnupg \</span><br><span class="line">    lsb-release</span><br></pre></td></tr></table></figure></p><p>2、添加 Docker 的官方 GPG 密钥：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir -p /etc/apt/keyrings</span><br><span class="line"></span><br><span class="line">curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg</span><br></pre></td></tr></table></figure></p><p>3、设置存储库：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> \</span><br><span class="line"><span class="string">&quot;deb [arch=<span class="subst">$(dpkg --print-architecture)</span> signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \</span></span><br><span class="line"><span class="string"><span class="subst">$(lsb_release -cs)</span> stable&quot;</span> | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null</span><br></pre></td></tr></table></figure></p><p>4、安装Docker引擎（latest）：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line"></span><br><span class="line">sudo apt-get install docker-ce docker-ce-cli containerd.io docker-compose-plugin</span><br></pre></td></tr></table></figure></p><p>5、通过运行镜像验证 Docker Engine 安装是否成功 ：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run hello-world</span><br></pre></td></tr></table></figure></p><h3 id="从软件包安装"><a href="#从软件包安装" class="headerlink" title="从软件包安装"></a>从软件包安装</h3><p>该方法通过下载deb文件并手动安装。注意使用该方法安装，每次升级 Docker Engine 时都需要下载一个新文件。</p><p>1、到 <a href="https://download.docker.com/linux/ubuntu/dists/">https://download.docker.com/linux/ubuntu/dists/</a> 选择对应系统版本。</p><p>2、转到<code>pool/stable/</code>并选择适用的架构（amd64、 armhf、arm64或s390x）。</p><p>3、下载下列软件包：<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">containerd.io_<span class="symbol">&lt;version&gt;</span>_<span class="symbol">&lt;arch&gt;</span>.<span class="keyword">deb</span></span><br><span class="line">docker-ce_<span class="symbol">&lt;version&gt;</span>_<span class="symbol">&lt;arch&gt;</span>.<span class="keyword">deb</span></span><br><span class="line">docker-<span class="keyword">ce</span>-cli_<span class="symbol">&lt;version&gt;</span>_<span class="symbol">&lt;arch&gt;</span>.<span class="keyword">deb</span></span><br><span class="line">docker-compose-plugin_<span class="symbol">&lt;version&gt;</span>_<span class="symbol">&lt;arch&gt;</span>.<span class="keyword">deb</span></span><br></pre></td></tr></table></figure></p><p>4、安装软件包：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo dpkg -i ./containerd.io_&lt;version&gt;_&lt;arch&gt;.deb \</span><br><span class="line">  ./docker-ce_&lt;version&gt;_&lt;arch&gt;.deb \</span><br><span class="line">  ./docker-ce-cli_&lt;version&gt;_&lt;arch&gt;.deb \</span><br><span class="line">  ./docker-compose-plugin_&lt;version&gt;_&lt;arch&gt;.deb</span><br></pre></td></tr></table></figure></p><p>5、通过运行镜像验证 Docker Engine 安装是否成功 ：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run hello-world</span><br></pre></td></tr></table></figure></p><h3 id="从快捷脚本安装"><a href="#从快捷脚本安装" class="headerlink" title="从快捷脚本安装"></a>从快捷脚本安装</h3><p>Docker 在 <a href="https://get.docker.com/">https://get.docker.com/</a> 上提供了一个方便的脚本，用于以非交互方式将 Docker 安装到开发环境中。官方不建议将快捷脚本用于生产环境，但它对于创建适合用户需求的供应脚本很有用。</p><p>1、可以使用DRY_RUN=1选项运行脚本以了解脚本在调用时将运行哪些步骤：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl -fsSL https://get.docker.com -o get-docker.sh</span><br><span class="line">DRY_RUN=1 sudo sh ./get-docker.sh</span><br></pre></td></tr></table></figure></p><p>2、安装：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl -fsSL https://get.docker.com -o get-docker.sh</span><br><span class="line">sudo sh get-docker.sh</span><br></pre></td></tr></table></figure></p><h3 id="安装之后"><a href="#安装之后" class="headerlink" title="安装之后"></a>安装之后</h3><p>在进行完上述的安装方式后，我们只能通过<code>sudo docker</code>来执行docker命令，你可能想要通过非 root 用户身份管理 Docker。</p><p>此外，在 Debian 和 Ubuntu 上，Docker 服务默认在启动时启动。对于其他Linux发行版，可以通过使用 systemd 来管理。</p><p>同时，Docker 提供日志记录驱动程序，用于收集和查看主机上运行的所有容器的日志数据。默认日志记录驱动程序<code>json-file</code>将日志数据写入主机文件系统上的 JSON 格式文件。随着时间的推移，这些日志文件的大小会不断扩大，从而可能导致磁盘资源耗尽。 我们可以通过配置日志轮换或其他方式解决。</p><p>关于以上信息的详细内容可以参见<a href="https://docs.docker.com/engine/install/linux-postinstall/">官方教程</a>。</p><h2 id="Docker的基本使用"><a href="#Docker的基本使用" class="headerlink" title="Docker的基本使用"></a>Docker的基本使用</h2><p>现在我们可以学习Docker的使用方式了，我将列举一些常见操作。</p><ul><li>查看Docker功能是否正常</li><li>创建并使用容器</li><li>管理容器的基本操作</li><li>使用Docker镜像和仓库</li><li>用Dockerfile构建镜像</li><li>共享和发布镜像</li></ul><h3 id="查看Docker信息"><a href="#查看Docker信息" class="headerlink" title="查看Docker信息"></a>查看Docker信息</h3><p>Docker是基于客户端-服务器构架的。它有一个docker程序，既能作为客户端，也可以作为服务器端。作为客户端时，docker程序向Docker守护进程发送请求(如请求返回守护进程自身的信息)，然后再对返回的请求结果进行处理。</p><p>下面的例子中我们调用了docker可执行程序的info命令。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">docker info</span><br><span class="line"></span><br><span class="line">Client:</span><br><span class="line"> Context:    default</span><br><span class="line"> Debug Mode: <span class="literal">false</span></span><br><span class="line"> Plugins:</span><br><span class="line">  app: Docker App (Docker Inc., v0.9.1-beta3)</span><br><span class="line">  buildx: Docker Buildx (Docker Inc., v0.9.1-docker)</span><br><span class="line">  compose: Docker Compose (Docker Inc., v2.14.1)</span><br><span class="line">  scan: Docker Scan (Docker Inc., v0.23.0)</span><br><span class="line"></span><br><span class="line">Server:</span><br><span class="line"> Containers: 1</span><br><span class="line">  Running: 0</span><br><span class="line">  Paused: 0</span><br><span class="line">  Stopped: 1</span><br><span class="line"> Images: 1</span><br><span class="line"> Server Version: 20.10.22</span><br><span class="line"> Storage Driver: overlay2</span><br><span class="line">  Backing Filesystem: extfs</span><br><span class="line">  Supports d_type: <span class="literal">true</span></span><br><span class="line">  Native Overlay Diff: <span class="literal">true</span></span><br><span class="line">  userxattr: <span class="literal">false</span></span><br><span class="line"> Logging Driver: json-file</span><br><span class="line"> Cgroup Driver: cgroupfs</span><br><span class="line"> Cgroup Version: 1</span><br><span class="line"> Plugins:</span><br><span class="line">  Volume: <span class="built_in">local</span></span><br><span class="line">  Network: bridge host ipvlan macvlan null overlay</span><br><span class="line">  Log: awslogs fluentd gcplogs gelf journald json-file <span class="built_in">local</span> logentries splunk syslog</span><br><span class="line"> Swarm: inactive</span><br><span class="line"> Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc</span><br><span class="line"> Default Runtime: runc</span><br><span class="line"> Init Binary: docker-init</span><br><span class="line"> containerd version: 9ba4b25036dde94bb71def331423aa32</span><br><span class="line"> runc version: v1.1.4-0-g5fd4</span><br><span class="line"> init version: de40a</span><br><span class="line"> Security Options:</span><br><span class="line">  apparmor</span><br><span class="line">  seccomp</span><br><span class="line">   Profile: default</span><br><span class="line"> Kernel Version: 4.15.0-169-generic</span><br><span class="line"> Operating System: Ubuntu 18.04.6 LTS</span><br><span class="line"> OSType: linux</span><br><span class="line"> Architecture: x86_64</span><br><span class="line"> CPUs: 1</span><br><span class="line"> Total Memory: 1.946GiB</span><br><span class="line"> Name: name</span><br><span class="line"> ID: id</span><br><span class="line"> Docker Root Dir: /var/lib/docker</span><br><span class="line"> Debug Mode: <span class="literal">false</span></span><br><span class="line"> Registry: https://index.docker.io/v1/</span><br><span class="line"> Labels:</span><br><span class="line"> Experimental: <span class="literal">false</span></span><br><span class="line"> Insecure Registries:</span><br><span class="line">  127.0.0.0/8</span><br><span class="line"> Live Restore Enabled: <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">WARNING: No swap <span class="built_in">limit</span> support</span><br></pre></td></tr></table></figure><p>该命令会返回所有容器和镜像的数量、Docker使用的执行驱动和存储驱动(execution and storage driver)，以及Docker的基本配置。</p><h3 id="创建并使用容器"><a href="#创建并使用容器" class="headerlink" title="创建并使用容器"></a>创建并使用容器</h3><p>现在，我会尝试启动一个Docker容器。我们可以使用<code>docker run</code>命令来创建并运行容器。</p><p><code>docker create</code>命令仅用来创建容器而不会运行它，这让我们可以在自己的容器工作流中对其进行细粒度的控制。</p><h4 id="运行一个Ubuntu容器"><a href="#运行一个Ubuntu容器" class="headerlink" title="运行一个Ubuntu容器"></a>运行一个Ubuntu容器</h4><p>首先，我们运行<code>docker run -it ubuntu /bin/bash</code>命令，<code>-i</code>参数表示保证容器中STDIN是开启的，即有持久的标准输入，这是交互式shell的“半边天”。<code>-t</code>参数则为要创建的容器分配一个伪tty终端，这样新建的容器才能提供一个交互式shell。若要在命令行下创建一个我们能与之进行交互的容器，而不是一个运行后台服务的容器，则这两个参数已经是最基本的参数了。</p><p>接下来我们告诉Docker基于什么镜像来创建容器，<code>ubuntu</code>镜像是一个常备镜像，也可以称为“基础”(base)镜像，它由Docker公司提供，保存在Docker Hub Registry上。用户可以拿基础镜像为基础，在选择的操作系统上构建自己的镜像。</p><p>最后，我们告诉Docker在新容器中要运行什么命令，在本例中我们在容器中运行<code>/bin/bash</code>命令启动了一个Bash shell。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -it ubuntu /bin/bash</span><br><span class="line">Unable to find image <span class="string">&#x27;ubuntu:latest&#x27;</span> locally</span><br><span class="line">latest: Pulling from library/ubuntu</span><br><span class="line">6e3729cf69e0: Pull complete </span><br><span class="line">Digest: sha256:27cb6e6ccef575a4698b66f5de06c7ecd61589132d5a91d098f7f3f9285415a9</span><br><span class="line">Status: Downloaded newer image <span class="keyword">for</span> ubuntu:latest</span><br><span class="line">root@a3fefa7e42c4:/<span class="comment"># </span></span><br></pre></td></tr></table></figure><p>根据终端输出，我们可以了解Docker对于该命令的工作流程：首先Docker会检查本地是否存在ubuntu镜像，如果本地还没有该镜像的话，那么Docker就会连接官方维护的Docker Hub Registry，查看Docker Hub中是否有该镜像。Docker一旦找到该镜像，就会下载该镜像并将其保存到本地宿主机中。</p><p>随后，Docker在文件系统内部用这个镜像创建了一个新容器。该容器拥有自己的网络、IP地址，以及一个用来和宿主机进行通信的桥接网络接口。</p><p>最后，当容器创建完毕之后，Docker就会执行容器中的<code>/bin/bash</code>命令，这时就可以看到我们处于容器内的shell了。</p><h4 id="使用容器"><a href="#使用容器" class="headerlink" title="使用容器"></a>使用容器</h4><p>现在我们已经进入到容器的交互式shell中，可以看到容器的ID为<code>a3fefa7e42c4</code>，这是一个完整的Ubuntu系统，我们可以对其为所欲为了～</p><p>那么我们先来尝试对容器进行一些操作：</p><p>1、查看<code>/etc/hosts</code>文件<br><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">root<span class="variable">@a3fefa7e42c4</span><span class="symbol">:/</span><span class="comment"># cat /etc/hosts</span></span><br><span class="line"><span class="number">127.0</span>.0.<span class="number">1</span>localhost</span><br><span class="line">::<span class="number">1</span>localhost ip6-localhost ip6-loopback</span><br><span class="line">fe00::0ip6-localnet</span><br><span class="line">ff00::0ip6-mcastprefix</span><br><span class="line">ff02::<span class="number">1</span>ip6-allnodes</span><br><span class="line">ff02::<span class="number">2</span>ip6-allrouters</span><br><span class="line"><span class="number">172.17</span>.0.<span class="number">2</span>a3fefa7e42c4</span><br><span class="line">root<span class="variable">@a3fefa7e42c4</span><span class="symbol">:/</span><span class="comment"># </span></span><br></pre></td></tr></table></figure></p><p>可以看到Docker已经在hosts文件中为该容器的IP添加了一条主机配置项<code>172.17.0.2    a3fefa7e42c4</code>。</p><p>2、查看容器中运行的进程<br><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">root</span>@a<span class="number">3</span>fefa<span class="number">7</span>e<span class="number">42</span>c<span class="number">4</span>:/# ps -aux</span><br><span class="line"><span class="attribute">USER</span>       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND</span><br><span class="line"><span class="attribute">root</span>         <span class="number">1</span>  <span class="number">0</span>.<span class="number">0</span>  <span class="number">0</span>.<span class="number">1</span>   <span class="number">4624</span>  <span class="number">3700</span> pts/<span class="number">0</span>    Ss+  <span class="number">11</span>:<span class="number">30</span>   <span class="number">0</span>:<span class="number">00</span> /bin/bash</span><br><span class="line"><span class="attribute">root</span>         <span class="number">9</span>  <span class="number">0</span>.<span class="number">0</span>  <span class="number">0</span>.<span class="number">1</span>   <span class="number">4620</span>  <span class="number">3728</span> pts/<span class="number">1</span>    Ss   <span class="number">11</span>:<span class="number">46</span>   <span class="number">0</span>:<span class="number">00</span> /bin/bash</span><br><span class="line"><span class="attribute">root</span>        <span class="number">18</span>  <span class="number">0</span>.<span class="number">0</span>  <span class="number">0</span>.<span class="number">0</span>   <span class="number">7056</span>  <span class="number">1608</span> pts/<span class="number">1</span>    R+   <span class="number">11</span>:<span class="number">50</span>   <span class="number">0</span>:<span class="number">00</span> ps -aux</span><br></pre></td></tr></table></figure><br>这里我意外退出了一次连接，于是当我再次进入容器后（使用exec命令），产生了两个<code>/bin/bash</code>进程。</p><p>3、安装一个软件包<br><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">root</span>@a<span class="number">3</span>fefa<span class="number">7</span>e<span class="number">42</span>c<span class="number">4</span>:/# apt-get update &amp;&amp; apt-get install vim</span><br></pre></td></tr></table></figure></p><p>没什么区别，这只是一个Ubuntu系统。</p><p>4、退出<br>当我们完成工作，输入<code>exit</code>即可退出容器的交互shell，返回宿主机的终端了。</p><p>此时，这个容器已经停止运行了！这是因为只有在我们创建容器时指定的<code>/bin/bash</code>命令处于运行状态时，容器才会相应的处于运行状态，否则容器也会相应停止。</p><h3 id="管理容器的基本操作"><a href="#管理容器的基本操作" class="headerlink" title="管理容器的基本操作"></a>管理容器的基本操作</h3><h4 id="查看容器列表"><a href="#查看容器列表" class="headerlink" title="查看容器列表"></a>查看容器列表</h4><p>我们可以使用<code>docker ps -a</code>命令<strong>查看当前系统中的容器列表</strong>，<code>-a</code>参数表示列出所有容器，否则<code>docker ps</code>只会列出正在运行的容器。</p><figure class="highlight llvm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ docker ps -a</span><br><span class="line">CONTAINER ID   IMAGE         COMMAND       CREATED          STATUS                       PORTS     NAMES</span><br><span class="line">a<span class="number">3</span>fefa<span class="number">7e42</span><span class="keyword">c</span><span class="number">4</span>   ubuntu        <span class="string">&quot;/bin/bash&quot;</span>   <span class="number">30</span> minutes ago   Exited (<span class="number">137</span>) <span class="number">2</span> minutes ago             exciting_thompson</span><br><span class="line"><span class="keyword">c</span><span class="number">1</span>fb<span class="number">392401</span><span class="keyword">c</span><span class="number">6</span>   hello-world   <span class="string">&quot;/hello&quot;</span>      <span class="number">3</span> hours ago      Exited (<span class="number">0</span>) <span class="number">3</span> hours ago                 friendly_heyrovsky</span><br></pre></td></tr></table></figure><p>注意我们可以通过三种方式唯一指代容器：<strong>短UUID、长UUID和名称</strong>。</p><h4 id="容器命名"><a href="#容器命名" class="headerlink" title="容器命名"></a>容器命名</h4><p>Docker会为我们创建的每一个容器自动生成一个随机的名称。例如，上面我们刚刚 创建的容器就被命名为exciting_thompson。如果想为容器指定一个名称，而不是使用自动生成的名称，则可以用—name标志来实现。</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">run</span> <span class="comment">--name NAME -it ubuntu /bin/bash</span></span><br></pre></td></tr></table></figure><p>在很多Docker命令中都可以使用名称来替代容器ID，设定特定的名称有利于我们区分和管理容器。容器的命名必须是唯一的。如果试图创建两个名称相同的容器，则命令将会失败。 如果要使用的容器名称已经存在，可以先用<code>docker rm</code>命令删除已有的同名容器后，再来创建新的容器。</p><p>我们还有<code>docker rename old_name new_name</code>命令用于重命名容器。</p><h4 id="容器的启动和停止"><a href="#容器的启动和停止" class="headerlink" title="容器的启动和停止"></a>容器的启动和停止</h4><p>我们可以通过<code>docker start</code>命令来启动容器，当然它支持容器名称和ID。</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">docker</span> start a<span class="number">3</span>fefa<span class="number">7</span>e<span class="number">42</span>c<span class="number">4</span></span><br></pre></td></tr></table></figure><p><code>start</code>命令还有许多玩法，例如加入<code>-i</code>参数或<code>--interactive</code>参数连接回容器的交互式shell，或者一次启动多个容器。</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">docker</span> start -i a<span class="number">3</span>fefa<span class="number">7</span>e<span class="number">42</span>c<span class="number">4</span></span><br></pre></td></tr></table></figure><p>此外，我们还有<code>docker restart</code>命令用于重启容器。</p><p>关于容器的停止，我们使用<code>docker stop</code>命令。</p><h4 id="连接到容器交互"><a href="#连接到容器交互" class="headerlink" title="连接到容器交互"></a>连接到容器交互</h4><p>当然，即便我们在启动容器忘记添加<code>-i</code>参数，我们依然可以通过<code>docker attach</code>命令连接回容器的回话上。</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">docker</span> attach a<span class="number">3</span>fefa<span class="number">7</span>e<span class="number">42</span>c<span class="number">4</span></span><br></pre></td></tr></table></figure><p>另外，我们还有<code>docker exec</code>命令可以用于新建一个对容器的交互式shell。从广义上来说，该命令事实上是用于在正在运行的容器中运行新命令。</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">docker</span> exec -it a<span class="number">3</span>fefa<span class="number">7</span>e<span class="number">42</span>c<span class="number">4</span> /bin/bash</span><br></pre></td></tr></table></figure><p>与<code>attach</code>命令不同的是，以上命令会在容器中新建一个交互式shell进程，当该shell通过<code>exit</code>退出时，如果容器中仍有在运行的<code>/bin/bash</code>进程，容器就不会停止。</p><h4 id="创建守护式容器"><a href="#创建守护式容器" class="headerlink" title="创建守护式容器"></a>创建守护式容器</h4><p>此前我们接触的容器称为交互式容器（interactive container），我们也可以创建长期运行的守护式容器（daemonized container），它们非常适合运行应用程序和服务。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --name daemon_dave -d ubuntu /bin/sh -c <span class="string">&quot;while  true; do echo hello world; sleep 1; done&quot;</span></span><br></pre></td></tr></table></figure><p>上面的<code>docker run</code>命令使用了<code>-d</code>参数，因此Docker会将容器放到后台运行。我们还在容器要运行的命令里使用了一个while循环，该循环会一直打印hello world，直到容器或其进程停止运行。</p><p>现在我们已经有了一个守护式容器，那我们该如何查看容器内部正在进行的活动呢？可以用<code>docker logs</code>命令来获取容器的日志。</p><blockquote><p>此命令仅适用于使用 json-file 或 journald 日志记录驱动程序启动的容器。</p></blockquote><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">docker logs daemon_dave</span></span><br></pre></td></tr></table></figure><p>同样，该命令也有许多参数选项，例如<code>-t</code>显示时间戳。</p><p>Docker支持多种日志驱动，例如包括Syslog，可以通过在启动守护进程时指定<code>--log-driver</code>选项设置驱动。</p><h4 id="查看容器内进程"><a href="#查看容器内进程" class="headerlink" title="查看容器内进程"></a>查看容器内进程</h4><p>除了容器的日志，还可以查看容器内部运行的进程，使用<code>docker top</code>命令。</p><figure class="highlight coq"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">top</span> daemon_dave</span><br></pre></td></tr></table></figure><h4 id="查看容器统计信息"><a href="#查看容器统计信息" class="headerlink" title="查看容器统计信息"></a>查看容器统计信息</h4><p>除了docker top命令，还可以使用<code>docker stats</code>命令，它用来显示一个或多个容器的统计信息。可以直接键入该命令，或后跟若干指定容器。</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">docker</span> stats </span><br><span class="line"></span><br><span class="line"><span class="attribute">CONTAINER</span> ID   NAME          CPU %     MEM USAGE / LIMIT     MEM %     NET I/O     BLOCK I/O   PIDS</span><br><span class="line"><span class="attribute">11e563078bf8</span>   daemon_dave   <span class="number">0</span>.<span class="number">06</span>%     <span class="number">684</span>KiB / <span class="number">1</span>.<span class="number">946</span>GiB     <span class="number">0</span>.<span class="number">03</span>%     <span class="number">836</span>B / <span class="number">0</span>B   <span class="number">0</span>B / <span class="number">0</span>B     <span class="number">2</span></span><br><span class="line"><span class="attribute">a3fefa7e42c4</span>   ubun          <span class="number">0</span>.<span class="number">00</span>%     <span class="number">1</span>.<span class="number">051</span>MiB / <span class="number">1</span>.<span class="number">946</span>GiB   <span class="number">0</span>.<span class="number">05</span>%     <span class="number">746</span>B / <span class="number">0</span>B   <span class="number">0</span>B / <span class="number">0</span>B     <span class="number">1</span></span><br></pre></td></tr></table></figure><p>我们能看到一个守护式容器的列表，以及它们的CPU、内存、网络I/O及存储I/O的性能和指标。这对快速监控一台主机上的一组容器非常有用。</p><h4 id="在容器内部运行进程"><a href="#在容器内部运行进程" class="headerlink" title="在容器内部运行进程"></a>在容器内部运行进程</h4><p>前面已经介绍了利用<code>docker exec</code>打开一个新的交互式shell，这实质上就是在容器内部运行了一个新的交互式shell进程。我们可以通过该命令在容器内部启动额外的新进程，包括<strong>后台任务</strong>和<strong>交互式任务</strong>。</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker exec -d daemon_dave touch <span class="regexp">/etc/</span>new_config</span><br><span class="line"></span><br><span class="line">docker exec -it daemon_dave <span class="regexp">/bin/</span>bash</span><br></pre></td></tr></table></figure><p>上面两条命令分别创建了后台任务和交互式任务。第一条命令会在daemon_dave容器内创建了一个名为<code>/etc/new_config</code>的空文件。通过docker exec后台命令，可以在正在运行的容器中进行维护、监控及管理任务。</p><p>第二条命令则会创建一个新会话，我们也可以通过该shell在容器中进行操作。</p><h4 id="自动重启容器"><a href="#自动重启容器" class="headerlink" title="自动重启容器"></a>自动重启容器</h4><p>如果由于某种错误而导致容器停止运行，还可以通过<code>--restart</code>标志，让Docker自动重新启动该容器。<code>--restart</code>标志会检查容器的退出代码，并据此来决定是否要重启容器。Docker默认不会重启容器。下面是一个在<code>docker run</code>命令中使用该参数的例子。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --restart=always --name daemon_dave -d ubuntu /bin/sh -c <span class="string">&quot;while  true; do echo hello world; sleep 1; done&quot;</span></span><br></pre></td></tr></table></figure><p><code>--restart</code>标志被设置为<code>always</code>。无论容器的退出代码是什么，Docker 都会自动重启该容器。除了<code>always</code>，还可以将这个标志设为<code>on-failure</code>，这样，只有当容器的退出代码为非0值的时候，才会自动重启。另外，<code>on-failure</code>还接受一个可选的重启次数参数，如<code>--restart=on-failure:5</code>表示尝试重启5次。</p><h4 id="获取容器的详细信息"><a href="#获取容器的详细信息" class="headerlink" title="获取容器的详细信息"></a>获取容器的详细信息</h4><p>可以使用<code>docker inspect</code>来获得更多的容器信息，该命令会对容器进行详细的检查，然后返回其配置信息，包括名称、 命令、网络配置以及很多有用的数据。</p><p>结合<code>-f</code>或<code>--format</code>参数，可以选定查看结果，如：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker inspect --format=<span class="string">&#x27;&#123;&#123; .State.Running &#125;&#125;&#x27;</span> daemon_dave</span><br></pre></td></tr></table></figure></p><p>上面的命令会返回容器的运行状态。我们还可以同时指定多个容器以返回每个容器的检查结果。</p><h4 id="删除容器"><a href="#删除容器" class="headerlink" title="删除容器"></a>删除容器</h4><p>当容器不再使用时，可以使用<code>docker rm</code>命令删除它们。</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">rm</span> ubun</span><br><span class="line"></span><br><span class="line">docker <span class="built_in">rm</span> <span class="operator">-f</span> daemon_dave</span><br><span class="line"></span><br><span class="line">docker <span class="built_in">rm</span> `docker <span class="built_in">ps</span> <span class="literal">-a</span> <span class="literal">-q</span>`</span><br></pre></td></tr></table></figure><p>我们可以通过<code>-f</code>参数强制删除容器，无论容器是否在运行。还可以通过<code>docker ps -a -q</code>返回所有容器id，并用<code>rm</code>全部删除。</p><h3 id="使用Docker镜像和仓库"><a href="#使用Docker镜像和仓库" class="headerlink" title="使用Docker镜像和仓库"></a>使用Docker镜像和仓库</h3><p>这一部分主要介绍容器的构建基石：Docker镜像。之前我们了解了如何使用容器，现在我们来了解一下如何使用镜像。</p><h4 id="列出本地镜像"><a href="#列出本地镜像" class="headerlink" title="列出本地镜像"></a>列出本地镜像</h4><p>我们可以使用<code>docker images</code>命令列出Docker主机上可用的镜像。</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ docker images</span><br><span class="line">REPOSITORY   <span class="keyword">TAG</span>       <span class="title">IMAGE</span> ID       CREATED        SIZE</span><br><span class="line">ubuntu        latest    f36d9597bf44   <span class="number">3</span> months ago   <span class="number">111M</span>B</span><br></pre></td></tr></table></figure><p>可以看到，我们已经获得了一个镜像列表，现在里面只有一个镜像，来源于一个名为ubuntu的仓库。这个镜像的来历是先前我们执行docker run命令时，同时进行了镜像下载。</p><p>本地镜像都保存在Docker宿主机的<code>/var/lib/docker</code>目录下。每个镜像都保存在Docker所采用的存储驱动目录下面，如<code>aufs</code>或者<code>devicemapper</code>。也可以在<code>/var/lib/docker/containers</code>目录下面看到所有的容器。</p><p>镜像从仓库下载下来。<strong>镜像保存在仓库中，而仓库存在于Registry中</strong>。默认的Registry是由Docker公司运营的公共Registry服务，即Docker Hub。</p><blockquote><p>Docker Registry的代码是开源的，所以我们也可以运行自己的私有Registry。同时，Docker公司也提供了一个商业版的Docker Hub，即Docker Trusted Registry，这是一个可以运行在公司防火墙内部的产品，之前被称为Docker Enterprise Hub。</p></blockquote><p>你可能会注意到在一开始我们下载Docker引擎时，它的名字为<code>docker-ce</code>，这表示它是社区版（Community Edition），Docker同时还提供付费的企业版。</p><p>在Registry（无论是Docker Hub还是私人Registry）中，镜像被保存在仓库中。可以将镜像仓库想象为类似Git仓库的东西。它包括<strong>镜像、层以及关于镜像的元数据</strong> (metadata)。</p><h4 id="深入Registry与镜像仓库"><a href="#深入Registry与镜像仓库" class="headerlink" title="深入Registry与镜像仓库"></a>深入Registry与镜像仓库</h4><p>事实上，不仅Registry中可以包含多个仓库，每个镜像仓库都可以存放很多镜像(比如，ubuntu仓库包含了 Ubuntu 23.04、22.04、18.04、16.04…的镜像)。</p><p>让我们来分析<code>docker images</code>命令的输出：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">REPOSITORY</span>   TAG       IMAGE ID       CREATED        SIZE</span><br><span class="line"><span class="attribute">ubuntu</span>        latest    f<span class="number">36</span>d<span class="number">9597</span>bf<span class="number">44</span>   <span class="number">3</span> months ago   <span class="number">111</span>MB</span><br></pre></td></tr></table></figure><p>我们可以看到对于下载的ubuntu镜像，输出不仅指明了镜像的仓库为<code>ubuntu</code>，同时还提供了一个标签。这就是镜像仓库能够存放许多镜像的秘诀—<strong>标签机制</strong>。</p><p>为了区分同一个仓库中的不同镜像，Docker提供了一种称为标签（tag）的功能。每个镜像在列出来时都带有一个标签，如18.04、22.04、quantal或者precise 等。每个标签对组成特定镜像的一些<strong>镜像层</strong>进行标记(比如，标签18.04就是对所有 Ubuntu 18.04镜像的层的标记)。</p><p>我们可以通过在仓库名后加上一个冒号和标签名的方式来指定该仓库中的某一镜像，如<code>ubuntu:16.04</code>。</p><blockquote><p>我们虽然称其为Ubuntu操作系统，但实际上它并不是一个完整的操作系统。它只是一个裁剪版，只包含最低限度的支持系统运行的组件。</p></blockquote><p><strong>一个镜像可以有多个标签</strong>。这使我们可以方便地对镜像进行打标签并且很容易查找镜像。在构建容器时指定仓库的标签也是一个很好的习惯。这样便可以准确地指定容器 源于哪里。不同标签的镜像会有不同，比如Ubutnu 18.04和22.04就不一样，指定镜像的标签会让我们确切知道自己使用的是ubuntu:22.04，这样我们就能准确知道自己在干什么。</p><p>除此之外，Registry中的仓库名称也有讲究，Docker Hub中有两种类型的仓库：<strong>用户仓库</strong>(user repository)和<strong>顶层仓库</strong> (top-level repository)。用户仓库的镜像都是由Docker用户创建的，而顶层仓库则是由Docker内部的人来管理的。</p><p>用户仓库的命名由用户名和仓库名两部分组成，如<code>jamtur01/puppet</code>。而顶层仓库就如<code>ubuntu</code>一般，不存在<code>/</code>分隔。</p><h4 id="查找镜像"><a href="#查找镜像" class="headerlink" title="查找镜像"></a>查找镜像</h4><p>Docker Hub中通常包含很多镜像仓库，那么我们如何了解其中目前有哪些镜像仓库呢？</p><p>如果你可以使用浏览器，访问<code>https://hub.docker.com</code>无疑是最方便的。如果想要查找<code>ubuntu</code>相关的仓库，我们只需要访问<code>https://hub.docker.com/search?q=ubuntu</code>。</p><p>或者，我们还可以使用<code>docker search</code>命令查找仓库，但这种方式无法查看仓库中有何标签，也就是说无法了解镜像仓库中具体包含哪些镜像。这可以通过镜像仓库的RESTful API查询，这里不展开说明。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ docker search --<span class="built_in">limit</span> 5 ubuntu</span><br><span class="line">NAME                 DESCRIPTION                                     STARS     OFFICIAL   AUTOMATED</span><br><span class="line">ubuntu               Ubuntu is a Debian-based Linux operating sys…   15420     [OK]       </span><br><span class="line">websphere-liberty    WebSphere Liberty multi-architecture images …   291       [OK]       </span><br><span class="line">neurodebian          NeuroDebian provides neuroscience research s…   97        [OK]       </span><br><span class="line">open-liberty         Open Liberty multi-architecture images based…   56        [OK]       </span><br><span class="line">ubuntu-debootstrap   DEPRECATED; use <span class="string">&quot;ubuntu&quot;</span> instead                50        [OK] </span><br></pre></td></tr></table></figure><p>上面的命令在Docker Hub上查找了所有带有<code>ubuntu</code>的镜像，并返回前五条信息。我们可以看到信息包括如下内容：</p><ul><li>仓库名;</li><li>镜像描述;</li><li>用户评价(Stars)—反应出一个镜像的受欢迎程度; </li><li>是否官方(Official)—由上游开发者管理的镜像(如fedora镜像由Fedora 团队管理);</li><li>自动构建(Automated)—表示这个镜像是由Docker Hub的自动构建 (Automated Build)流程创建的。</li></ul><p>另外，现在除了<code>Docker Official Image</code>外，还提供了两种较为可靠的信任等级<code>Verified Publisher</code>和<code>Sponsored OSS</code>。可以在官网查看。</p><blockquote><p>用户贡献的镜像都是由Docker社区用户提供的，这些镜像并没有经过Docker公司的确认和验证，在使用这些镜像时需要自己承担相应的风险。</p></blockquote><h4 id="拉取镜像"><a href="#拉取镜像" class="headerlink" title="拉取镜像"></a>拉取镜像</h4><p>用<code>docker run</code>命令从镜像启动一个容器时，如果该镜像不在本地，Docker会先从Docker Hub下载该镜像。如果没有指定具体的镜像标签，那么Docker会自动下载<code>latest</code>标签的镜像。</p><p>如果想单独下载镜像，则需要使用<code>docker pull</code>命令。下面我们来拉取一个<code>ubuntu:16.04</code>的镜像。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ docker pull ubuntu:16.04      </span><br><span class="line">16.04: Pulling from library/ubuntu</span><br><span class="line">828b35a09f0b: Pull complete </span><br><span class="line">66927c6d1d3d: Pull complete</span><br><span class="line">000560be9165: Pull complete </span><br><span class="line">6225a0253717: Pull complete </span><br><span class="line">Digest: sha256:1f1a2d56de1d604801a9671f301190704c25d604a416f59e03c04f5c6ffee0d6</span><br><span class="line">Status: Downloaded newer image <span class="keyword">for</span> ubuntu:16.04</span><br><span class="line">docker.io/library/ubuntu:16.04</span><br></pre></td></tr></table></figure><p>现在，我们可以通过<code>docker images</code>查看下载好的镜像：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ docker images                </span><br><span class="line">REPOSITORY   TAG       IMAGE ID       CREATED        SIZE</span><br><span class="line">ubuntu       16.04     d125c6a1fe22   2 months ago   119MB</span><br></pre></td></tr></table></figure><h3 id="用Dockerfile构建镜像"><a href="#用Dockerfile构建镜像" class="headerlink" title="用Dockerfile构建镜像"></a>用Dockerfile构建镜像</h3><p>前面我们已经看到了如何拉取已经构建好的带有定制内容的Docker镜像，那么如何修改自己的镜像，并且更新和管理这些镜像呢?构建Docker镜像有以下两种方法。</p><ul><li>使用<code>docker commit</code>命令。</li><li>使用<code>docker build</code>命令和<strong>Dockerfile</strong>文件。</li></ul><p>现在我们并不推荐使用<code>docker commit</code>命令，而应该使用更灵活、更强大的Dockerfile来构建Docker镜像。因此这里将重点介绍Docker所推荐的镜像构建方法:编写Dockerfile之后使用<code>docker build</code>命令。</p><blockquote><p>可以想象<code>docker commit</code>与<code>git commit</code>十分类似，它仅提交创建容器的镜像与容器当前状态间的差异部分，因此十分轻量。</p><p>该命令同样可以指定更多信息选项，包括提交信息、作者信息、设定标签等。最简单的使用只需要在退出容器后执行<code>docker commit 容器ID 目标仓库名</code>，例如<code>docker commit id silencezheng/ubuntu:webserver</code>。</p></blockquote><p><strong>Dockerfile</strong>使用基本的基于 <strong>DSL(Domain Specific Language)</strong> 语法的指令来构建一个Docker镜像，推荐使用Dockerfile是因为通过它来构建镜像更具备可重复性、透明性以及幂等性。</p><p>一旦有了Dockerfile，我们就可以使用<code>docker build</code>命令基于该Dockerfile中的指令构建一个新的镜像。</p><h4 id="创建Dockerfile"><a href="#创建Dockerfile" class="headerlink" title="创建Dockerfile"></a>创建Dockerfile</h4><p>首先我们将尝试创建一个包含简单Web服务器的Docker镜像。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir static_web &amp;&amp; <span class="built_in">cd</span> static_web &amp;&amp; touch Dockerfile</span><br></pre></td></tr></table></figure><p>我们创建了一个名为static_web的目录用来保存Dockerfile，这个目录就是我们的<strong>构建环境</strong>(build environment)，Docker则称此环境为上下文(context)或者<strong>构建上下文</strong>(build context)。Docker会在构建镜像时将<strong>构建上下文</strong>和<strong>该上下文中的文件和目录</strong>上传到Docker守护进程。这样Docker守护进程就能直接访问用户想在镜像中存储的任何代码、文件或者其他数据。</p><p>下面我们编写Dockerfile的内容，使其可以被构建为一个Web服务器镜像。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Version: 0.0.1 </span></span><br><span class="line">FROM ubuntu:16.04 </span><br><span class="line">MAINTAINER Silence Zheng <span class="string">&quot;silencezheng66@126.com&quot;</span>  </span><br><span class="line">RUN apt-get update &amp;&amp; apt-get install -y nginx  </span><br><span class="line">RUN <span class="built_in">echo</span> <span class="string">&#x27;Hi, I am in your container&#x27;</span> \ </span><br><span class="line">   &gt;/usr/share/nginx/html/index.html </span><br><span class="line">EXPOSE 80</span><br></pre></td></tr></table></figure><p>该Dockerfile由一系列指令和参数组成。每条指令，如FROM，都必须为<strong>大写字母</strong>，且后面要跟随一个参数：<code>FROM ubuntu:16.04</code>。Dockerfile中的指令会按顺序<strong>从上到下执行</strong>，所以应该根据需要合理安排指令的顺序。</p><p><strong>每条指令都会创建一个新的镜像层并对镜像进行提交</strong>。Docker大体上按照如下流程执行Dockerfile中的指令。</p><ul><li>Docker从基础镜像运行一个容器。 </li><li>执行一条指令，对容器做出修改。</li><li>执行类似<code>docker commit</code>的操作，提交一个新的镜像层。 </li><li>Docker再基于刚提交的镜像运行一个新容器。</li><li>执行Dockerfile中的下一条指令，直到所有指令都执行完毕。</li></ul><p>基于此，即便用户的Dockerfile由于某些原因(如某条指令失败了)没有正常结束，用户也将得到了一个可以使用的镜像。这对调试非常有帮助：可以基于该镜像运行一个具备交互功能的容器，使用最后创建的镜像对为什么用户的指<br>令会失败进行调试。</p><p>每个Dockerfile的第一条指令必须是<code>FROM</code>。<code>FROM</code>指令指定一个已经存在的镜像，后续指令都将基于该镜像进行，这个镜像被称为<strong>基础镜像</strong>(base iamge)。</p><p>在前面的Dockerfile示例里，我们指定了<code>ubuntu:16.04</code>作为新镜像的基础镜像。基于这个Dockerfile构建的新镜像将以Ubuntu 16.04操作系统为基础。在运行一个容器时，必须要指明是基于哪个基础镜像在进行构建。</p><p>接着指定了<code>MAINTAINER</code>指令，这条指令会告诉Docker该镜像的作者是谁，以及作者的电子邮件地址。这有助于标识镜像的所有者和联系方式。</p><p>在这些指令之后，我们指定了两条<code>RUN</code>指令。<code>RUN</code>指令会在当前镜像中运行指定的命令。在这个例子里，我们通过<code>RUN</code>指令更新了已经安装的APT仓库，安装了nginx包，之后创建了<code>/usr/share/nginx/html/index.html</code>文件，该文件有一些简单的示例文本。像前面说的那样，每条<code>RUN</code>指令都会创建一个新的镜像层，如果该指令执行成功，就会将此镜像层提交，之后继续执行Dockerfile中的下一条指令。</p><p>默认情况下，<code>RUN</code>指令会在shell里使用命令包装器<code>/bin/sh -c</code>来执行。如果是在一个不支持shell的平台上运行或者不希望在shell中运行(比如避免shell字符串篡改)，也可以使用exec格式的<code>RUN</code>指令，如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RUN [<span class="string">&quot;apt-get&quot;</span>, <span class="string">&quot;install&quot;</span>, <span class="string">&quot;-y&quot;</span>, <span class="string">&quot;nginx&quot;</span>]</span><br></pre></td></tr></table></figure><p>在这种方式中，我们使用一个数组来指定要运行的命令和传递给该命令的每个参数。</p><p>接着我们设置了<code>EXPOSE</code>指令，这条指令告诉Docker该容器内的应用程序将会使用容器的指定端口。这并不意味着可以自动访问任意容器运行中服务的端口(这里是80)。</p><p>出于安全的原因，Docker并不会自动打开该端口，而是需要用户在使用<code>docker run</code>运行容器时来指定需要打开哪些端口。一会儿我们将会看到如何从这一镜像创建一个新容器。</p><p>可以指定多个<code>EXPOSE</code>指令来向外部公开多个端口。</p><blockquote><p>Docker也使用<code>EXPOSE</code>指令来帮助将多个容器链接。用户可以在运行时以<code>docker run</code>命令通过<code>--expose</code>选项来指定对外部公开的端口。</p></blockquote><h4 id="基于Dockerfile构建新镜像"><a href="#基于Dockerfile构建新镜像" class="headerlink" title="基于Dockerfile构建新镜像"></a>基于Dockerfile构建新镜像</h4><p>执行<code>docker build</code>命令时，Dockerfile中的所有指令都会被执行并且提交，并且在该命令成功结束后返回一个新镜像。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ docker build -t=<span class="string">&quot;silencezheng/static_web&quot;</span> .</span><br><span class="line">[+] Building 98.8s (7/7) FINISHED                                                                                                    </span><br><span class="line"> =&gt; [internal] load build definition from Dockerfile                                                                            0.0s</span><br><span class="line"> =&gt; =&gt; transferring dockerfile: 271B                                                                                            0.0s</span><br><span class="line"> =&gt; [internal] load .dockerignore                                                                                               0.0s</span><br><span class="line"> =&gt; =&gt; transferring context: 2B                                                                                                 0.0s</span><br><span class="line"> =&gt; [internal] load metadata <span class="keyword">for</span> docker.io/library/ubuntu:16.04                                                                 0.0s</span><br><span class="line"> =&gt; [1/3] FROM docker.io/library/ubuntu:16.04                                                                                   0.0s</span><br><span class="line"> =&gt; [2/3] RUN apt-get update &amp;&amp; apt-get install -y nginx                                                                       98.3s</span><br><span class="line"> =&gt; [3/3] RUN <span class="built_in">echo</span> <span class="string">&#x27;Hi, I am in your container&#x27;</span>    &gt;/usr/share/nginx/html/index.html                                            0.2s </span><br><span class="line"> =&gt; exporting to image                                                                                                          0.2s </span><br><span class="line"> =&gt; =&gt; exporting layers                                                                                                         0.2s </span><br><span class="line"> =&gt; =&gt; writing image sha256:63710bc4ee3c8f71b0741f31d583872b474cf8f70fa4194555e237a30c06f9c9                                    0.0s </span><br><span class="line"> =&gt; =&gt; naming to docker.io/silencezheng/static_web  </span><br></pre></td></tr></table></figure><p>我们通过指定<code>-t</code>选项为新镜像设置了仓库和名称，本例中仓库为<code>silencezheng</code>，镜像名为<code>static_web</code>。强烈建议各位为自己的镜像设置合适的名字以方便追踪和管理。也可以在构建镜像的过程中为镜像设置一个标签，其使用方法为<code>-t=&quot;silencezheng/static_web:v1&quot;</code>。</p><blockquote><p>如果没有制定任何标签，Docker将会自动为镜像设置一个latest标签。</p></blockquote><p>上面命令中最后的<code>.</code>告诉Docker到本地目录中去找Dockerfile文件。也可以指定一个Git仓库的源地址来指定Dockerfile的位置，如用<code>git@github.com:jamtur01/docker-static_web</code>替换它。Docker会假设该Git仓库根目录下存在Dockerfile。</p><p>在构建过程中，构建上下文已经上传到了Docker守护进程。如果在<strong>构建上下文的根目录</strong>下存在以<code>.dockerignore</code>命名的文件的话，那么该文件内容会被按行进行分割，每一行都是一条文件过滤匹配模式。这非常像<code>.gitignore</code>文件，该文件用来设置哪些文件不会被当作构建上下文的一部分，因此可以防止它们被上传到Docker守护进程中去。该文件中模式的匹配规则采用了Go语言中的<code>filepath</code>（因为Docker是用GO编写的）。</p><p>现在，如果我们执行<code>docker images</code>，就可以看到我们创建好的镜像。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">REPOSITORY                TAG       IMAGE ID       CREATED         SIZE</span><br><span class="line">silencezheng/static_web   latest    63710bc4ee3c   7 minutes ago   200MB</span><br></pre></td></tr></table></figure><p>虽然镜像相较于虚拟机已经缩小了几倍甚至数十倍，但对于256G硬盘的电脑（是的，MacBook）来说依然过于庞大，使用Dockerfile能够有效减少容量压力😆。</p><h4 id="Dockerfile和构建缓存"><a href="#Dockerfile和构建缓存" class="headerlink" title="Dockerfile和构建缓存"></a>Dockerfile和构建缓存</h4><p>由于每一步的构建过程都会将结果提交为镜像，所以Docker的构建镜像过程就显得非常聪明。它会将之前的镜像层看作缓存。比如，当我们构建的镜像在第4步出现问题时，我们不需要在第1步到第3步之间进行任何修改，因此Docker会<strong>将之前构建时创建的镜像当做缓存</strong>并作为新的开始点。实际上，当再次进行构建时，Docker会直接从第4步开始。当之前的构建步骤没有变化时，这会节省大量的时间。如果真的在第1步到第3步之间做了什么修改，Docker则会从第一条发生了变化的指令开始。</p><p>然而，有些时候需要确保构建过程不会使用缓存。比如，如果已经缓存了前面的第3步，即<code>apt-get update</code>，那么Docker将不会再次刷新APT包的缓存。这时用户可能需要取得每个包的最新版本。要想略过缓存功能，可以使用<code>docker build</code>的<code>--no-cache</code>。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker build --no-cache -t=<span class="string">&quot;silencezheng/static_web&quot;</span> .</span><br></pre></td></tr></table></figure><h4 id="基于构建缓存的Docker模版"><a href="#基于构建缓存的Docker模版" class="headerlink" title="基于构建缓存的Docker模版"></a>基于构建缓存的Docker模版</h4><p>构建缓存带来的一个好处就是，我们可以实现简单的Dockerfile模板(比如在Dockerfile文件顶部增加包仓库或者更新包，从而尽可能确保缓存命中)。James Turnbull一般都会在他的Dockerfile文件顶部使用相同的指令集模板，比如对Ubuntu，使用下方的模版。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">FROM ubuntu:14.04 </span><br><span class="line">MAINTAINER James Turnbull <span class="string">&quot;james@example.com&quot;</span>  </span><br><span class="line">ENV REFRESHED_AT 2014-07-01 </span><br><span class="line">RUN apt-get -qq update</span><br></pre></td></tr></table></figure><p>在这个例子里，他通过<code>ENV</code>指令来设置了一个名为<code>REFRESHED_AT</code>的环境变量，这个环境变量用来表明该镜像模板最后的更新时间。最后，使用了<code>RUN</code>指令来运行<code>apt-get -qq update</code>命令。该指令运行时将会刷新APT包的缓存，用来确保我们能将要安装的每个软件包都更新到最新版本。</p><p>有了这个模板，如果想刷新一个构建，只需修改<code>ENV</code>指令中的日期。这使Docker在命中<code>ENV</code>指令时开始重置这个缓存，并运行后续指令而无须依赖该缓存。也就是说，<code>RUN apt-get update</code>这条指令将会被再次执行，包缓存也将会被刷新为最新内容。可以扩展此模板，适配到不同的平台或者添加额外的需求。</p><h4 id="查看新镜像"><a href="#查看新镜像" class="headerlink" title="查看新镜像"></a>查看新镜像</h4><p>我们当然可以使用<code>docker images</code>查看全部镜像的简略信息。</p><p>此外，如果想要深入了解镜像是如何被构建出来的，可以使用<code>docker history</code>命令：</p><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ docker history silencezheng/static_web</span><br><span class="line">IMAGE          CREATED          CREATED <span class="keyword">BY</span>                                      SIZE      COMMENT</span><br><span class="line">63710bc4ee3c   26 minutes ago   EXPOSE map[80/tcp:&#123;&#125;]                           0B        buildkit.dockerfile.v0</span><br><span class="line">&lt;missing&gt;      26 minutes ago   <span class="keyword">RUN</span> /bin/<span class="keyword">sh</span> -c echo &#x27;Hi, I am <span class="keyword">in</span> your contai…   27B       buildkit.dockerfile.v0</span><br><span class="line">&lt;missing&gt;      26 minutes ago   <span class="keyword">RUN</span> /bin/<span class="keyword">sh</span> -c apt-get <span class="keyword">update</span> &amp;&amp; apt-get <span class="keyword">ins</span>…   80.5MB    buildkit.dockerfile.v0</span><br><span class="line">&lt;missing&gt;      26 minutes ago   MAINTAINER Silence Zheng &quot;silencezheng66@126…   0B        buildkit.dockerfile.v0</span><br><span class="line">&lt;missing&gt;      2 months ago     /bin/<span class="keyword">sh</span> -c #(nop)  CMD [<span class="string">&quot;/bin/bash&quot;</span>]            0B        </span><br><span class="line">&lt;missing&gt;      2 months ago     /bin/<span class="keyword">sh</span> -c <span class="keyword">mkdir</span> -p /<span class="keyword">run</span>/systemd &amp;&amp; echo &#x27;<span class="keyword">do</span>…   7B        </span><br><span class="line">&lt;missing&gt;      2 months ago     /bin/<span class="keyword">sh</span> -c <span class="keyword">rm</span> -rf /<span class="keyword">var</span>/lib/apt/lists<span class="comment">/*          0B        </span></span><br><span class="line"><span class="comment">&lt;missing&gt;      2 months ago     /bin/sh -c set -xe   &amp;&amp; echo &#x27;#!/bin/sh&#x27; &gt; /…   745B      </span></span><br><span class="line"><span class="comment">&lt;missing&gt;      2 months ago     /bin/sh -c #(nop) ADD file:3c6dc937cb7b4c81b…   119MB </span></span><br></pre></td></tr></table></figure><p>从输出中可以看到指定镜像的每一层，以及创建这些层的Dockerfile指令。</p><h4 id="从新镜像启动容器"><a href="#从新镜像启动容器" class="headerlink" title="从新镜像启动容器"></a>从新镜像启动容器</h4><p>现在来检查一下<code>silencezheng/static_web</code>能否正常工作吧！</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d -p 80 --name static_web silencezheng/static_web nginx -g <span class="string">&quot;daemon off;&quot;</span> </span><br></pre></td></tr></table></figure><p>上面的命令基于刚才构建的镜像的名字，启动了一个名为<code>static_web</code>的新容器。我们同时指定了<code>-d</code>选项，告诉Docker以分离(detached)的方式在后台运行。这种方式非常适合运行类似Nginx守护进程这样的需要长时间运行的进程。我们也指定了需要在容器中运行的命令：<code>nginx -g &quot;daemon off;&quot;</code>。这将以前台运行的方式启动Nginx，来作为我们的Web服务器。</p><p>我们这里也使用了一个新的<code>-p</code>标志，该标志用来控制Docker在运行时应该公开哪些网络端口给外部(宿主机)。运行一个容器时，Docker可以通过两种方法来在宿主机上分配端口。</p><ul><li>Docker可以在宿主机上随机选择一个位于<code>32768~61000</code>的一个比较大的端口号来映射到容器中的80端口上。</li><li>可以在Docker宿主机中指定一个具体的端口号来映射到容器中的80端口上。</li></ul><p><code>docker run</code>命令将在Docker宿主机上随机打开一个端口，这个端口会连接到容器中的80端口上。下面使用<code>docker ps</code>命令来看一下容器的端口分配情况。<code>-l</code>选项用于显示最新创建的容器。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ docker ps -l                                                                         </span><br><span class="line">CONTAINER ID   IMAGE                     COMMAND                  CREATED         STATUS         PORTS                   NAMES</span><br><span class="line">4c53957ff093   silencezheng/static_web   <span class="string">&quot;nginx -g &#x27;daemon of…&quot;</span>   4 seconds ago   Up 4 seconds   0.0.0.0:63007-&gt;80/tcp   static_web</span><br></pre></td></tr></table></figure><p>可以看到，容器中的80端口被映射到了宿主机的63007上。我们也可以通过<code>docker port</code>来查看容器的端口映射情况。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ docker port static_web</span><br><span class="line">80/tcp -&gt; 0.0.0.0:63007</span><br><span class="line"></span><br><span class="line">$ docker port static_web 80</span><br><span class="line">0.0.0.0:63007</span><br></pre></td></tr></table></figure><p><code>-p</code>选项还为我们在将容器端口向宿主机公开时提供了一定的灵活性。比如，可以指定将容器中的端口映射到Docker宿主机的某一特定端口上，例如<code>-p &lt;宿主机端口&gt;:&lt;容器端口&gt;</code>。</p><p>我们也可以将端口绑定限制在特定的网络接口(即IP地址)上，例如使用<code>-p 127.0.0.1:80:80</code>，将容器内的80端口绑定到本地宿主机的127.0.0.1这个IP的80端口上。或者使用<code>-p 127.0.0.1::80</code>将容器内的80端口绑定到本地宿主机的127.0.0.1这个IP的随机端口上。</p><p>此外，也可以通过在端口绑定时使用<code>/udp</code>后缀来指定UDP端口。</p><p>Docker还提供了一个更简单的方式，即<code>-P</code>参数，该参数可以用来对外公开在Dockerfile中通过<code>EXPOSE</code>指令公开的所有端口。采用<code>-P</code>参数会将通过<code>EXPOSE</code>指令公开的所有端口绑定到宿主机的随机端口上。</p><p>现在，我们的Web服务器已经启动了，可以通过访问宿主机localhost访问它：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">$ curl localhost:63007                </span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">&lt;title&gt;Welcome to nginx!&lt;/title&gt;</span><br><span class="line">&lt;style&gt;</span><br><span class="line">    body &#123;</span><br><span class="line">        width: 35em;</span><br><span class="line">        margin: 0 auto;</span><br><span class="line">        font-family: Tahoma, Verdana, Arial, sans-serif;</span><br><span class="line">    &#125;</span><br><span class="line">&lt;/style&gt;</span><br><span class="line">&lt;/head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;</span><br><span class="line">&lt;p&gt;If you see this page, the nginx web server is successfully installed and</span><br><span class="line">working. Further configuration is required.&lt;/p&gt;</span><br><span class="line"></span><br><span class="line">&lt;p&gt;For online documentation and support please refer to</span><br><span class="line">&lt;a href=<span class="string">&quot;http://nginx.org/&quot;</span>&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;</span><br><span class="line">Commercial support is available at</span><br><span class="line">&lt;a href=<span class="string">&quot;http://nginx.com/&quot;</span>&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;</span><br><span class="line"></span><br><span class="line">&lt;p&gt;&lt;em&gt;Thank you <span class="keyword">for</span> using nginx.&lt;/em&gt;&lt;/p&gt;</span><br><span class="line">&lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure><p>可以看到Nginx已经在正常运行了，但是我们配置的初始页并没有生效。</p><h4 id="修改镜像"><a href="#修改镜像" class="headerlink" title="修改镜像"></a>修改镜像</h4><p>Nginx初始页不生效的原因是我们没有在配置文件中设置server块，于是访问时nginx返回默认起始页，该起始页位于<code>var/www/html/</code>下。</p><blockquote><p>Nginx安装完成后的目录情况：</p><p>/usr/sbin/nginx  #主程序<br>/etc/nginx   #存放配置文件<br>/usr/share/nginx  #存放静态文件<br>/var/log/nginx  #存放日志</p></blockquote><p>于是我们可以在配置文件中增加如下内容：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">server&#123;</span><br><span class="line">    listen 80;</span><br><span class="line">    server_name localhost;</span><br><span class="line">    location / &#123;</span><br><span class="line">        root /usr/share/nginx/html;</span><br><span class="line">        index index.html;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>调试成功后，修改Dockerfile为如下内容：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Version: 0.0.1 </span></span><br><span class="line">FROM ubuntu:16.04 </span><br><span class="line">MAINTAINER Silence Zheng <span class="string">&quot;silencezheng66@126.com&quot;</span>  </span><br><span class="line">RUN apt-get update &amp;&amp; apt-get install -y nginx  </span><br><span class="line">RUN <span class="built_in">echo</span> <span class="string">&#x27;Hi, I am in your container&#x27;</span> \ </span><br><span class="line">   &gt;/usr/share/nginx/html/index.html</span><br><span class="line">RUN sed -i <span class="string">&#x27;28a server&#123;\n\</span></span><br><span class="line"><span class="string">    listen 80;\n\</span></span><br><span class="line"><span class="string">    server_name localhost;\n\</span></span><br><span class="line"><span class="string">    location / &#123;\n\</span></span><br><span class="line"><span class="string">        root /usr/share/nginx/html;\n\</span></span><br><span class="line"><span class="string">        index index.html;\n\</span></span><br><span class="line"><span class="string">    &#125;\n\</span></span><br><span class="line"><span class="string">        &#125;&#x27;</span> /etc/nginx/nginx.conf</span><br><span class="line">EXPOSE 80</span><br></pre></td></tr></table></figure><p>然后我们重新构建镜像，并打上<code>fixed</code>标签：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker build -t=<span class="string">&quot;silencezheng/static_web:fixed&quot;</span> .</span><br><span class="line">[+] Building 0.3s (8/8) FINISHED </span><br></pre></td></tr></table></figure><p>由于存在缓存，本次构建只花费了0.3秒。下面我们基于这个版本的镜像启动容器：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d -P --name static_web_fixed silencezheng/static_web:fixed nginx -g <span class="string">&quot;daemon off;&quot;</span> </span><br></pre></td></tr></table></figure><p>这次我们通过<code>-P</code>参数对外开放端口，启动了新的容器<code>static_web_fixed</code>。测试：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ docker ps   </span><br><span class="line">CONTAINER ID   IMAGE                           COMMAND                  CREATED          STATUS          PORTS                   NAMES</span><br><span class="line">5bd1d50deb5d   silencezheng/static_web:fixed   <span class="string">&quot;nginx -g &#x27;daemon of…&quot;</span>   13 seconds ago   Up 13 seconds   0.0.0.0:55000-&gt;80/tcp   static_web_fixed</span><br><span class="line">4c53957ff093   silencezheng/static_web         <span class="string">&quot;nginx -g &#x27;daemon of…&quot;</span>   2 hours ago      Up 8 minutes    0.0.0.0:55398-&gt;80/tcp   static_web</span><br><span class="line">$ curl localhost:55000</span><br><span class="line">Hi, I am <span class="keyword">in</span> your container</span><br></pre></td></tr></table></figure><p>测试成功！我们修复了Dockerfile，并基于它构建了正确的镜像。现在我们得到了一个简单的基于Docker的Web服务器。</p><h3 id="共享和发布镜像"><a href="#共享和发布镜像" class="headerlink" title="共享和发布镜像"></a>共享和发布镜像</h3><p>在构建了我们自己的镜像后，我们总是希望能够共享和发布镜像，这样能够方便传递我们的工作成果。下面我将演示如何将镜像推送到Docker Hub上。</p><h4 id="创建Docker-Hub账号"><a href="#创建Docker-Hub账号" class="headerlink" title="创建Docker Hub账号"></a>创建Docker Hub账号</h4><p>到 <a href="https://hub.docker.com">https://hub.docker.com</a> 注册账号并激活。</p><h4 id="登录到Docker-Hub"><a href="#登录到Docker-Hub" class="headerlink" title="登录到Docker Hub"></a>登录到Docker Hub</h4><p>我们可以使用<code>docker login</code>命令登录到Docker Hub。这条命令将会完成登录到Docker Hub的工作，并将认证信息保存起来以供后面使用。</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ docker <span class="keyword">login</span></span><br><span class="line"><span class="keyword">Login</span> <span class="keyword">with</span> your Docker ID <span class="keyword">to</span> push <span class="keyword">and</span> pull images <span class="keyword">from</span> Docker Hub. <span class="keyword">If</span> you don<span class="string">&#x27;t have a Docker ID, head over to https://hub.docker.com to create one.</span></span><br><span class="line"><span class="string">Username: silencezheng66</span></span><br><span class="line"><span class="string">Password: </span></span><br><span class="line"><span class="string">Login Succeeded</span></span><br></pre></td></tr></table></figure><blockquote><p> 用户的个人认证信息将会保存到<code>$HOME/.docker/config.json</code>。</p></blockquote><p>当不需要保持登录状态时，可以使用<code>docker logout</code>命令从一个Registry服务器退出。</p><h4 id="将镜像推送到Docker-Hub"><a href="#将镜像推送到Docker-Hub" class="headerlink" title="将镜像推送到Docker Hub"></a>将镜像推送到Docker Hub</h4><p>镜像构建完毕之后，我们也可以将它上传到Docker Hub上面去，这样其他人就能使用这个镜像了。比如，我们可以在组织内共享这个镜像，或者完全公开这个镜像。</p><blockquote><p>Docker Hub也提供了对私有仓库的支持，这是一个需要付费的功能，用户可以将镜像存储到私有仓库中，这样只有用户或者任何与用户共享这个私有仓库的人才能访问该镜像。</p></blockquote><p>我们可以通过<code>docker push</code>命令将镜像推送到Docker Hub。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ docker push silencezheng/static_web</span><br><span class="line">Using default tag: latest</span><br><span class="line">The push refers to repository [docker.io/silencezheng/static_web]</span><br><span class="line">8fd82ef660ab: Preparing </span><br><span class="line">..</span><br><span class="line">d6021e78cec6: Waiting </span><br><span class="line">denied: requested access to the resource is denied</span><br></pre></td></tr></table></figure><p>推送没有成功，这是因为我的Docker Hub账户名为<code>silencezheng66</code>，而本地的镜像名为<code>silencezheng/static_web</code>，虽然格式合法但用户名与实际不符。于是我需要修改镜像名称，使其与我的Docker Hub账户名一致。</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="keyword">tag</span> <span class="title">silencezheng</span>/static_web:fixed silencezheng66/static_web</span><br></pre></td></tr></table></figure><p>这里我们使用了<code>docker tag SOURCE_IMAGE[:TAG] TARGET_IMAGE[:TAG]</code>创建一个引用 SOURCE_IMAGE 的标签 TARGET_IMAGE。</p><p>接着我们重新推送镜像：</p><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ docker <span class="keyword">push</span> silencezheng66/static_web </span><br><span class="line">Using <span class="keyword">default</span> tag: latest</span><br><span class="line">The <span class="keyword">push</span> refers to repository [docker.io<span class="regexp">/silencezheng66/</span>static_web]</span><br><span class="line">ba77f540ca67: Pushed </span><br><span class="line">..</span><br><span class="line">d6021e78cec6: Pushed </span><br><span class="line">latest: digest: sha256:f8eca79b2d629d5407067c3acd6a2e961b2771909b642079ed1fd1d16aca3584 <span class="keyword">size</span>: <span class="number">1776</span></span><br></pre></td></tr></table></figure><p>我们可以通过Docker Hub网页查看我们上传的镜像。</p><h4 id="删除镜像"><a href="#删除镜像" class="headerlink" title="删除镜像"></a>删除镜像</h4><p>我们可以使用<code>docker rmi</code>命令删除镜像，该命令也支持指定一个镜像名列表来删除多个。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker rmi silencezheng/static_web silencezheng66/static_web</span><br></pre></td></tr></table></figure><p>当存在基于镜像构建的容器时，将不能删除镜像，我们可以通过添加<code>-f</code>参数强制删除镜像，但这不会删除对应的容器。</p><p>需要注意的是，当镜像同属一个仓库，但拥有不同标签时（一个镜像仓库中的多个镜像），需要通过指定标签删除，否则只会删除<code>latest</code>镜像。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ docker rmi silencezheng/static_web:fixed                    </span><br><span class="line">Untagged: silencezheng/static_web:fixed</span><br><span class="line">Deleted: sha256:32b6943ed92ef2654856cc7d3606597d16e0b03f11c80ab6e2e7f8ee390df8db</span><br></pre></td></tr></table></figure><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]<a href="https://www.nutanix.cn/info/hypervisor">https://www.nutanix.cn/info/hypervisor</a><br>[2]James Turnbull等. 第一本Docker书. 人民邮电出版社, 2015.<br>[3]<a href="https://zhuanlan.zhihu.com/p/35493900">https://zhuanlan.zhihu.com/p/35493900</a><br>[4]<a href="https://blog.csdn.net/oopxiajun2011/article/details/105029232">https://blog.csdn.net/oopxiajun2011/article/details/105029232</a></p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于 <a href="https://silencezheng.top">silencezheng.top</a>，转载请注明出处。</p><h2 id="写文时的一点思考"><a href="#写文时的一点思考" class="headerlink" title="写文时的一点思考"></a>写文时的一点思考</h2><p>在撰写本文时，我对写博客可能产生的知识产权问题也有了一些思考，本文的大部分内容实际上都摘自《第一本Docker书》，这是不是一种变相的“盗版书”散发呢？</p><p>从法律的角度讲，似乎这是的，虽然我没有以盈利为目的，但据知乎“平老虎（贾纪谦，青岛市北区人，身份证尾号0018）”的说法，这侵犯了著作权人的网络传播权和著作权。</p><p>得知这样的结果也确实有些无奈吧。Docker本身具有开源属性，我也仅是书写了简单的基础知识，相信原作者不至于会找我麻烦（当然也是因为我没流量，如果全网几百万粉丝，可能出版社闻着味就过来了，人怕出名猪怕壮）。</p><p>我的内容目前也只是学习笔记的性质，当我忘记的时候方便回看，当有人问我的时候，直接甩给他一篇我整理好的文章去读，十分方便。</p><p>做一项小而美的工作，也多是一件美事。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言：&quot;&gt;&lt;a href=&quot;#前言：&quot; class=&quot;headerlink&quot; title=&quot;前言：&quot;&gt;&lt;/a&gt;前言：&lt;/h2&gt;&lt;p&gt;初识Docker，本文涵盖内容：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;什么是Docker？&lt;/li&gt;
&lt;li&gt;如何安装Docker？&lt;/li&gt;
&lt;li&gt;Docker的基本使用&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ps：多数内容来自《第一本Docker书》，但其中部分内容已经过于老旧了，由我（SilenceZheng66）进行了重新编写。&lt;br&gt;pps：本来就想写一个简单的便捷教程，没想到越来越长…&lt;br&gt;</summary>
    
    
    
    
    <category term="Docker" scheme="http://silencezheng.top/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>滑动窗口</title>
    <link href="http://silencezheng.top/2023/01/03/article88/"/>
    <id>http://silencezheng.top/2023/01/03/article88/</id>
    <published>2023-01-03T14:05:40.000Z</published>
    <updated>2023-01-03T14:07:08.226Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>滑动窗口算法的基本思想和一些实践。</p><span id="more"></span><h2 id="滑动窗口算法（Sliding-Window-Algorithm）"><a href="#滑动窗口算法（Sliding-Window-Algorithm）" class="headerlink" title="滑动窗口算法（Sliding Window Algorithm）"></a>滑动窗口算法（Sliding Window Algorithm）</h2><blockquote><p>Sliding window algorithm is used to perform required operation on specific window size of given large buffer or array.<br>滑动窗口算法是在给定特定窗口大小的数组或字符串上执行要求的操作。</p><p>This technique shows how a nested for loop in few problems can be converted to single for loop and hence reducing the time complexity.<br>该技术可以将一部分问题中的嵌套循环转变为一个单循环，因此它可以减少时间复杂度。</p></blockquote><p><strong>滑动</strong>：表示窗口是移动的。</p><p><strong>窗口</strong>：窗口的大小并不一定是固定的，可以不断扩容直到满足一定的条件；也可以不断缩小，直到找到一个满足条件的最小窗口；当然也可以是固定大小。</p><p><img src="/assets/post_img/article88/slide%20window.gif" alt="sw"></p><p>滑动窗口算法主要应用在<strong>数组类结构</strong>上（包括字符串），其思想可以用来解决一些 <em>查找满足一定条件的连续区间的性质（如长度等）的问题</em>。<strong>由于区间连续，因此当区间发生变化时，可以通过已有的计算结果对搜索空间进行剪枝</strong>，这样便减少了重复计算，降低了时间复杂度。</p><p><strong>总之，滑动窗口法的核心思想就是利用已有计算结果减少重复计算，关键就是如何利用，所谓的滑动窗口只是其外在表现。</strong></p><p>学习过计算机网络的读者应该对滑动窗口的模式并不陌生，而其他初次听说该思想的同学可能会一头雾水，下面我们通过一个简单的例子解释它。</p><h2 id="例一：长度最小的子数组"><a href="#例一：长度最小的子数组" class="headerlink" title="例一：长度最小的子数组"></a>例一：<a href="https://leetcode.cn/problems/minimum-size-subarray-sum/">长度最小的子数组</a></h2><blockquote><p>给定一个含有 n 个正整数的数组和一个正整数 target 。</p><p>找出该数组中满足其和 ≥ target 的长度最小的 连续子数组 [numsl, numsl+1, …, numsr-1, numsr] ，并返回其长度。如果不存在符合条件的子数组，返回 0 。</p></blockquote><h3 id="暴力法"><a href="#暴力法" class="headerlink" title="暴力法"></a>暴力法</h3><p>暴力法：双循环，每次依据一个开始位置寻找长度最小的符合条件子数组。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: SilenceZheng66</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">minSubArrayLen</span><span class="params">(<span class="keyword">int</span> target, <span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> subLen = nums.length+<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;nums.length;i++)&#123;</span><br><span class="line">            <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">int</span> tmpSubLen = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">int</span> j = i;</span><br><span class="line">            <span class="keyword">while</span>(j&lt;nums.length&amp;&amp;sum&lt;target)&#123;</span><br><span class="line">                sum += nums[j++];</span><br><span class="line">                tmpSubLen++;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(sum&gt;=target&amp;&amp;tmpSubLen&lt;subLen) subLen = tmpSubLen;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(subLen&lt;=nums.length) <span class="keyword">return</span> subLen;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">return</span> <span class="number">0</span>; </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最终超时，原因在于重复计算过多。时间复杂度$O(n^2)$。</p><h3 id="滑动窗口"><a href="#滑动窗口" class="headerlink" title="滑动窗口"></a>滑动窗口</h3><p>滑动窗口：通过左右指针划定窗口位置，指针由左向右滑动模拟窗口滑动，初始时左右指针重合指向索引0，而后右指针移动至第一个符合条件的位置（窗口内子数组符合条件），记录长度，此后左指针逐位移动，每次移动若窗口符合条件则再记录，直至窗口不符合条件，执行此左右移动过程直至右指针推出数组。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: SilenceZheng66</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">minSubArrayLen</span><span class="params">(<span class="keyword">int</span> target, <span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 滑动窗口</span></span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>, left = <span class="number">0</span>, right = <span class="number">0</span>, minSubLen = nums.length + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(right&lt;nums.length)&#123;</span><br><span class="line">            <span class="comment">// 寻找符合条件的最小窗口</span></span><br><span class="line">            <span class="keyword">while</span>(sum&lt;target&amp;&amp;right&lt;nums.length) sum+=nums[right++];</span><br><span class="line">            <span class="comment">// 缩小窗口直至不符合条件</span></span><br><span class="line">            <span class="keyword">while</span>(sum&gt;=target)&#123;</span><br><span class="line">                <span class="comment">// 记录最小窗口大小</span></span><br><span class="line">                <span class="keyword">if</span>((right-left)&lt;minSubLen&amp;&amp;sum&gt;=target) minSubLen = right-left;</span><br><span class="line">                sum-=nums[left++];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(minSubLen&lt;=nums.length) <span class="keyword">return</span> minSubLen;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">return</span> <span class="number">0</span>; </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>该算法的最终时间复杂度为$O(n)$，关于这个时间复杂度，可以想像数组中的每个元素至多被操作两次（右指针划过、左指针划过），因此操作数为 $2n$，即$O(n)$。被减少的运算就是各符合条件的窗口的交集。</p><p>该例中，窗口的大小是不固定的，事实上窗口的固定与否并不重要（固定窗口用单指针+长度即可），<strong>关键在于如何构造窗口并利用已有计算结果</strong>。现在我们已经了解了如何使用滑动窗口算法，下面进行更多实践来加深理解。</p><h2 id="例二：尽可能使字符串相等"><a href="#例二：尽可能使字符串相等" class="headerlink" title="例二：尽可能使字符串相等"></a>例二：<a href="https://leetcode.cn/problems/get-equal-substrings-within-budget/">尽可能使字符串相等</a></h2><blockquote><p>给你两个长度相同的字符串，s 和 t。</p><p>将 s 中的第 i 个字符变到 t 中的第 i 个字符需要 |s[i] - t[i]| 的开销（开销可能为 0），也就是两个字符的 ASCII 码值的差的绝对值。</p><p>用于变更字符串的最大预算是 maxCost。在转化字符串时，总开销应当小于等于该预算，这也意味着字符串的转化可能是不完全的。</p><p>如果你可以将 s 的子字符串转化为它在 t 中对应的子字符串，则返回可以转化的最大长度。</p><p>如果 s 中没有子字符串可以转化成 t 中对应的子字符串，则返回 0。</p></blockquote><p>采用滑动窗口解决，主要思路还是通过左右指针划定窗口位置，但要注意例二和例一的显著区别：<strong>例二求的是符合条件的最大长度，而例一求的是最小长度</strong>。在求解时，我们的滑动窗口通常都是由小至大进行扩张，求符合条件的最小子数组时，我们可以认为最初符合条件的窗口为符合条件的最小长度；而在求最大子数组时，只有当窗口内子数组初次不符合条件时，我们才能知道前一窗口为当前符合条件的最大窗口。</p><p>下面给出两种解法：</p><h3 id="滑动窗口一"><a href="#滑动窗口一" class="headerlink" title="滑动窗口一"></a>滑动窗口一</h3><p>我写的滑动窗口法的思路是每次右指针都移动到初次不符合条件的位置，然后以右指针位置减去一表示真实符合条件的右指针位置。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: SilenceZheng66</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">equalSubstring</span><span class="params">(String s, String t, <span class="keyword">int</span> maxCost)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> left = <span class="number">0</span>, right = <span class="number">0</span>, maxLen = <span class="number">0</span>, sum = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(right&lt;s.length())&#123;</span><br><span class="line">            <span class="comment">// 移动右至符合条件</span></span><br><span class="line">            <span class="keyword">while</span>(right&lt;s.length()&amp;&amp;sum&lt;=maxCost) sum+=Math.abs(s.charAt(right)-t.charAt(right++));</span><br><span class="line">            <span class="comment">// 判断，由于求最大长所以不用每次移动左都判断</span></span><br><span class="line">            <span class="comment">// 在右指针推出的情况下窗口内数组可能不满足条件，故判断并修正</span></span><br><span class="line">            <span class="keyword">if</span>(right&gt;=s.length()&amp;&amp;sum&lt;=maxCost) maxLen = Math.max(maxLen, right-left);</span><br><span class="line">            <span class="keyword">else</span> maxLen = Math.max(maxLen, right-left-<span class="number">1</span>);</span><br><span class="line">            <span class="comment">// 移动左至不符合条件</span></span><br><span class="line">            <span class="comment">// 注意sum==maxCost时不能移动，否则可能会错过最优解</span></span><br><span class="line">            <span class="keyword">while</span>(sum&gt;maxCost) sum-=Math.abs(s.charAt(left)-t.charAt(left++));</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> maxLen;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这种写法看似思路简单，每一轮循环都想一次将右指针移到合适的位置，其实需要考虑的边界条件较多，如右指针推出但窗口不符合条件。 另外，对于本题而言，有一共性问题需要注意，即左指针在<code>sum==maxCost</code>时不能继续移动，因为有可能出现窗口右侧<code>s[i]==t[i]</code>的情况。</p><h3 id="滑动窗口二（思想来自-1-）"><a href="#滑动窗口二（思想来自-1-）" class="headerlink" title="滑动窗口二（思想来自[1]）"></a>滑动窗口二（思想来自[1]）</h3><p>该写法的思想是每次右指针右移都附加判断和指针左移，这样一来就可以避免“右指针移动到初次不符合条件的位置”带来的问题。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: SilenceZheng66</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">equalSubstring</span><span class="params">(String s, String t, <span class="keyword">int</span> maxCost)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> left = <span class="number">0</span>, right = <span class="number">0</span>, maxLen = <span class="number">0</span>, sum = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(right&lt;s.length())&#123;</span><br><span class="line">            <span class="comment">// 移动右</span></span><br><span class="line">            sum+=Math.abs(s.charAt(right)-t.charAt(right++));</span><br><span class="line">            <span class="comment">// 移动左</span></span><br><span class="line">            <span class="keyword">while</span>(sum&gt;maxCost) sum-=Math.abs(s.charAt(left)-t.charAt(left++));</span><br><span class="line">            <span class="comment">// 判断</span></span><br><span class="line">            maxLen = Math.max(maxLen, right-left);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> maxLen;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看出，两种方法的窗口构造也不尽相同，笔者的方法是“真窗口存在于假窗口之中”。</p><h2 id="例三：滑动窗口最大值"><a href="#例三：滑动窗口最大值" class="headerlink" title="例三：滑动窗口最大值"></a>例三：<a href="https://leetcode.cn/problems/sliding-window-maximum/">滑动窗口最大值</a></h2><blockquote><p>给你一个整数数组 nums，有一个大小为 k 的滑动窗口从数组的最左侧移动到数组的最右侧。你只可以看到在滑动窗口内的 k 个数字。滑动窗口每次只向右移动一位</p><p>返回 滑动窗口中的最大值 。</p></blockquote><p>这是一道固定窗口大小的题目，给出两种解法，分别是稍加记忆的暴力法和滑动窗口法，后者的速度是前者的40倍。本题其实更能体现出滑动窗口与暴力法的区别，即算法对于单一元素指针划过的次数。</p><h3 id="稍加记忆的暴力法"><a href="#稍加记忆的暴力法" class="headerlink" title="稍加记忆的暴力法"></a>稍加记忆的暴力法</h3><p>选择使用左指针加偏移构造窗口，则暴力的求窗口内元素最值的方式为遍历窗口。窗口在移动的过程中，若最值仍位于窗口内，则只需比较新进入的元素和原始最值；若原始最值被排出，则需要重新选举最值。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: SilenceZheng66</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span>[] maxSlidingWindow(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> k) &#123;</span><br><span class="line">        <span class="keyword">int</span>[] ans = <span class="keyword">new</span> <span class="keyword">int</span>[nums.length-(k-<span class="number">1</span>)];</span><br><span class="line">        <span class="comment">// offset = k-1</span></span><br><span class="line">        <span class="keyword">int</span> left = <span class="number">0</span>, max = Integer.MIN_VALUE, maxIndex = -<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(left&lt;ans.length)&#123;</span><br><span class="line">            <span class="keyword">int</span> right = left + (k-<span class="number">1</span>);</span><br><span class="line">            <span class="keyword">if</span>(maxIndex&gt;=left&amp;&amp;maxIndex&lt;=right)&#123;</span><br><span class="line">                <span class="keyword">if</span>(nums[right]&gt;=max)&#123;</span><br><span class="line">                    max = nums[right];</span><br><span class="line">                    maxIndex = right;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">int</span> tmpMax = Integer.MIN_VALUE; </span><br><span class="line">                <span class="keyword">int</span> tmpMaxIndex = maxIndex;</span><br><span class="line">                <span class="keyword">for</span>(<span class="keyword">int</span> i = left; i &lt;= right; i++)&#123;</span><br><span class="line">                    <span class="comment">// 取等于有利于减少最大值索引被排出的情况</span></span><br><span class="line">                    <span class="keyword">if</span>(nums[i]&gt;=tmpMax)&#123;</span><br><span class="line">                        tmpMax = nums[i];</span><br><span class="line">                        tmpMaxIndex = i;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                max = tmpMax;</span><br><span class="line">                maxIndex = tmpMaxIndex;</span><br><span class="line">            &#125;</span><br><span class="line">            ans[left++] = max;</span><br><span class="line">        &#125; </span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>该方法在最优情况（原始最值永远不被排出，just fantasy）下可以取得不错的时间效率，但最坏情况下等同于暴力法，故属于效率较差的算法。</p><h3 id="滑动窗口（思想来自-1-）"><a href="#滑动窗口（思想来自-1-）" class="headerlink" title="滑动窗口（思想来自[1]）"></a>滑动窗口（思想来自[1]）</h3><p>上一个方法没有利用好窗口内的计算结果，下面介绍一个更好的思路，该方法采用右指针构造窗口，核心思路在于维持一个队首最大的双端队列（这里用LinkedList代替）以保存最大值，可以设想数组逐步进入该队列，当数组中第一个窗口进入队列后，就开始对队首元素进行检验，若已经滑出窗口则出队。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: SilenceZheng66</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span>[] maxSlidingWindow(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> k) &#123;</span><br><span class="line">        <span class="keyword">int</span> right = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span>[] res = <span class="keyword">new</span> <span class="keyword">int</span>[nums.length - (k-<span class="number">1</span>)];</span><br><span class="line">        <span class="keyword">int</span> index = <span class="number">0</span>;</span><br><span class="line">        LinkedList&lt;Integer&gt; list = <span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (right &lt; nums.length) &#123;</span><br><span class="line">            <span class="comment">// 维持list的首位为窗口中最大元素</span></span><br><span class="line">            <span class="keyword">while</span> (!list.isEmpty() &amp;&amp; nums[right] &gt; list.peekLast()) &#123;</span><br><span class="line">                list.removeLast();</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 不断添加</span></span><br><span class="line">            list.addLast(nums[right++]);</span><br><span class="line">            <span class="comment">// 构造窗口完成，这时候需要根据条件做一些操作</span></span><br><span class="line">            <span class="keyword">if</span> (right &gt;= k)&#123;</span><br><span class="line">                res[index++]=list.peekFirst();</span><br><span class="line">                <span class="comment">// 若list首位已位于窗口外</span></span><br><span class="line">                <span class="keyword">if</span>(list.peekFirst() == nums[right-k]) &#123;</span><br><span class="line">                    list.removeFirst();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由此可见，滑动窗口算法的核心思想在于如何表示窗口以及利用已有信息，有时窗口可能是抽象的。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]<a href="https://www.cnblogs.com/huansky/p/13488234.html">https://www.cnblogs.com/huansky/p/13488234.html</a><br>[2]<a href="https://programmercarl.com/0209.%E9%95%BF%E5%BA%A6%E6%9C%80%E5%B0%8F%E7%9A%84%E5%AD%90%E6%95%B0%E7%BB%84.html">https://programmercarl.com/0209.%E9%95%BF%E5%BA%A6%E6%9C%80%E5%B0%8F%E7%9A%84%E5%AD%90%E6%95%B0%E7%BB%84.html</a></p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于 <a href="https://silencezheng.top">silencezheng.top</a>，转载请注明出处。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;滑动窗口算法的基本思想和一些实践。&lt;/p&gt;</summary>
    
    
    
    
    <category term="数据结构与算法" scheme="http://silencezheng.top/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>实用主义的Git入门教程</title>
    <link href="http://silencezheng.top/2022/12/22/article87/"/>
    <id>http://silencezheng.top/2022/12/22/article87/</id>
    <published>2022-12-22T14:47:27.000Z</published>
    <updated>2022-12-23T02:23:20.797Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>实用主义的Git入门教程，快速了解常用的Git命令。</p><span id="more"></span><h2 id="1-git-clone"><a href="#1-git-clone" class="headerlink" title="1. git clone"></a>1. git clone</h2><p>从Git服务器拉取代码：<code>git clone https://github.com/SilenceZheng66/SilenceZheng66.github.io.git</code></p><p>代码会下载到当前工作目录中，名称为<code>SilenceZheng66.github.io</code>。</p><h2 id="2-git-init"><a href="#2-git-init" class="headerlink" title="2. git init"></a>2. git init</h2><p>当然，如果想要在本地新建一个git仓库，只需要在对应工作目录中输入<code>git init</code>命令。</p><p>首次创建仓库需要commit后才能创建主分支master。</p><h2 id="3-git-config"><a href="#3-git-config" class="headerlink" title="3. git config"></a>3. git config</h2><p>该命令用于获取并设置存储库或全局选项。这些变量可以控制Git的外观和操作的各个方面。</p><p>全局配置开发者用户名：<code>git config --global user.name SilenceZheng66</code></p><p>对某一项目配置开发者邮箱（需进入项目目录）：<code>git config user.email silencezheng66@126.com</code></p><p>查看当前用户全局配置：<code>git config --global --list</code></p><p>查看当前仓库配置：<code>git config --local --list</code></p><h2 id="4-git-branch"><a href="#4-git-branch" class="headerlink" title="4. git branch"></a>4. git branch</h2><p>创建、重命名、查看、删除项目分支，通过 Git 做项目开发时，一般都是在开发分支中进行，开发完成后合并分支到主干。</p><p><code>git branch daily/0.0.0</code>，创建一个名为<code>daily/0.0.0</code>的日常开发分支，分支名只要不包括特殊字符即可。</p><p><code>git branch -m daily/0.0.0 daily/0.0.1</code>，如果觉得之前的分支名不合适，可以为新建的分支重命名，如重命名为<code>daily/0.0.1</code>。</p><p>查看当前项目分支列表：<code>git branch</code>，按q退出。</p><p>删除分支：<code>git branch -d daily/0.0.1</code></p><h2 id="5-git-checkout"><a href="#5-git-checkout" class="headerlink" title="5. git checkout"></a>5. git checkout</h2><p>该命令用于切换分支。</p><p>如：<code>git checkout daily/0.0.1</code></p><p>切换到历史版本：<code>git checkout &lt;commit SHA&gt;</code></p><h2 id="6-git-status"><a href="#6-git-status" class="headerlink" title="6. git status"></a>6. git status</h2><p>该命令用于查看文件变动状态。</p><p>比如可以在项目中新建一个文件<code>hh.txt</code>后，键入该命令查看状态，得到：<code>Untracked files:...</code>。</p><h2 id="7-git-add"><a href="#7-git-add" class="headerlink" title="7. git add"></a>7. git add</h2><p>该命令用于添加文件变动到暂存区。</p><p><code>git add hh.txt</code>，查看状态得到：<code>Changes to be committed:...</code></p><p>将所有修改添加到暂存区：<code>git add .</code>。</p><h2 id="8-git-commit"><a href="#8-git-commit" class="headerlink" title="8. git commit"></a>8. git commit</h2><p>该命令用于提交文件变动到版本库。</p><p><code>git commit -m &#39;提交原因&#39;</code>，通过<code>-m</code>参数可直接在命令行里输入提交描述文本。</p><h2 id="9-git-push"><a href="#9-git-push" class="headerlink" title="9. git push"></a>9. git push</h2><p>该命令用于将本地的代码改动推送到服务器。</p><p><code>git push origin daily/0.0.1</code>，<code>origin</code>指代的是当前的git服务器地址。</p><h2 id="10-git-pull"><a href="#10-git-pull" class="headerlink" title="10. git pull"></a>10. git pull</h2><p>该命令用于将服务器上的最新代码拉取到本地。</p><p><code>git pull origin daily/0.0.1</code>，将服务器上<code>daily/0.0.1</code>分支的代码拉取到本地。</p><p>如果线上代码做了变动（其他项目成员进行了编辑），而你本地的代码也有变动（你做了改动），拉取的代码就有可能会跟你本地的改动<em>冲突</em>，一般情况下 Git 会自动处理这种冲突合并，但如果改动的是同一行，那就需要手动来合并代码，编辑文件，保存最新的改动，再通过<code>git add .</code>和<code>git commit -m &#39;xxx&#39;</code>来提交合并。</p><h2 id="11-git-log"><a href="#11-git-log" class="headerlink" title="11. git log"></a>11. git log</h2><p>该命令用于查看版本提交记录，我们可以查看整个项目的版本提交记录，它里面包含了散列码、提交人、日期、提交原因等信息。</p><p>提交记录可能会非常多，按 J 键往下翻，按 K 键往上翻，按 Q 键退出查看</p><p>查看分支树：<code>git log --oneline --graph --decorate --all</code></p><h2 id="12-git-tag"><a href="#12-git-tag" class="headerlink" title="12. git tag"></a>12. git tag</h2><p>该命令用于为项目标记里程碑。</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">git</span> tag publish/<span class="number">0</span>.<span class="number">0</span>.<span class="number">1</span></span><br><span class="line"><span class="attribute">git</span> push origin publish/<span class="number">0</span>.<span class="number">0</span>.<span class="number">1</span></span><br></pre></td></tr></table></figure><p>当我们完成某个功能需求准备发布上线时，应该将此次完整的项目代码做个标记，并将这个标记好的版本发布到线上，这里表示以<code>publish/0.0.1</code>为标记名并发布。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] gafish的Git教程，找不到原文链接了，可以在 GitHub 搜到作者。</p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于 <a href="https://silencezheng.top">silencezheng.top</a>，转载请注明出处。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;实用主义的Git入门教程，快速了解常用的Git命令。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Git" scheme="http://silencezheng.top/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>用Tkinter构建简易图形用户界面</title>
    <link href="http://silencezheng.top/2022/12/21/article86/"/>
    <id>http://silencezheng.top/2022/12/21/article86/</id>
    <published>2022-12-21T12:11:45.000Z</published>
    <updated>2022-12-21T12:12:52.362Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>介绍如何使用Tkinter构建一个简易的图形用户界面。</p><p>在使用Python方便我们的日常工作时，我们可能希望将脚本、程序打包起来，方便我们在多端使用或供他人使用。对于那些不了解如何使用命令行程序的用户而言，设计一个可用的图形用户界面就十分重要。</p><span id="more"></span><h2 id="Tkinter"><a href="#Tkinter" class="headerlink" title="Tkinter"></a>Tkinter</h2><p>如果你还不知道什么是Tkinter：</p><blockquote><p>Tkinter（即 tk interface） 是 Python 标准 GUI 库，简称 “Tk”；从本质上来说，它是对 TCL/TK 工具包的一种 Python 接口封装。Tkinter 是 Python 自带的标准库，因此无须另行安装，它支持跨平台运行，不仅可以在 Windows 平台上运行，还支持在 Linux 和 Mac 平台上运行。</p></blockquote><p>你可能会注意到 tkinter 下的 ttk 组件库，它们似乎拥有着与tkinter相似的组件：</p><blockquote><p>tkinter中的窗口小部件高度易于配置。您几乎可以完全控制它们的外观：边框宽度，字体，图像，颜色等。<br>ttk小部件使用样式定义外观，因此，如果要使用非标准按钮，则需要花费更多的工作。 ttk小部件的文档也很少。理解底层的主题和布局引擎(小部件本身中的布局，而不是pack，grid和place)是一个挑战。</p></blockquote><p>总的来说，如果想使GUI程序更加美观，更现代，ttk组件库能够帮到你，但本篇作为 silencezheng.top 的第一篇tkinter相关文章，将不会谈论这一组件库。</p><p>写本篇文章的目的在于讲述如何快速的为Python程序制作一个“单页GUI”，不会涉及过多的技术细节，仅要求读者对于GUI程序的基本组件和设计逻辑有一个基本的了解。</p><h2 id="设计界面布局"><a href="#设计界面布局" class="headerlink" title="设计界面布局"></a>设计界面布局</h2><p>有许多可视化布局工具可供使用，这里我推荐一个在线布局工具：<a href="https://www.pytk.net/tkinter-helper/">https://www.pytk.net/tkinter-helper/</a></p><p>当然，如果不想使用这个工具（或者以后它需要付费了），只要有一个合理的代码框架也可以很容易的对界面布局进行设计，下面我提供一个左右布局（包含两个Frame）的代码框架。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># author: SilenceZheng66</span></span><br><span class="line"><span class="keyword">from</span> tkinter <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WinGUI</span>(<span class="params">Tk</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.__win()</span><br><span class="line">        self.tk_frame_left = Frame_left(self)</span><br><span class="line">        self.tk_frame_right = Frame_right(self)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__win</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.title(<span class="string">&quot;Tkinter Demo&quot;</span>)</span><br><span class="line">        <span class="comment"># 设置窗口大小、居中</span></span><br><span class="line">        width = <span class="number">810</span></span><br><span class="line">        height = <span class="number">600</span></span><br><span class="line">        screenwidth = self.winfo_screenwidth()</span><br><span class="line">        screenheight = self.winfo_screenheight()</span><br><span class="line">        geometry = <span class="string">&#x27;%dx%d+%d+%d&#x27;</span> % (width, height, (screenwidth - width) / <span class="number">2</span>, (screenheight - height) / <span class="number">2</span>)</span><br><span class="line">        self.geometry(geometry)</span><br><span class="line">        self.resizable(width=<span class="literal">False</span>, height=<span class="literal">False</span>)</span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Frame_left</span>(<span class="params">Frame</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,parent</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(parent)</span><br><span class="line">        self.__frame()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__frame</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.place(x=<span class="number">0</span>, y=<span class="number">0</span>, width=<span class="number">400</span>, height=<span class="number">599</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Frame_right</span>(<span class="params">Frame</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,parent</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(parent)</span><br><span class="line">        self.__frame()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__frame</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.place(x=<span class="number">410</span>, y=<span class="number">0</span>, width=<span class="number">400</span>, height=<span class="number">599</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Win</span>(<span class="params">WinGUI</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.__event_bind()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__event_bind</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">        </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    win = Win()</span><br><span class="line">    win.mainloop()</span><br></pre></td></tr></table></figure><p>效果如下：</p><p><img src="/assets/post_img/article86/step1.png" alt="s1"></p><h2 id="设计一个简易的文件编辑器"><a href="#设计一个简易的文件编辑器" class="headerlink" title="设计一个简易的文件编辑器"></a>设计一个简易的文件编辑器</h2><p>做事情需要有目标，下面我们的目标就是以方才提供的布局为蓝本，设计一个<em>文件编辑器</em>。</p><p>左侧Frame用来提供输入和功能按钮，右侧Frame用来显示内容。</p><h3 id="输入和功能区域"><a href="#输入和功能区域" class="headerlink" title="输入和功能区域"></a>输入和功能区域</h3><p>我们希望可以提供两种打开文本文件的方式，输入路径或通过文件选择器进行选择。 这使用到四个组件：Label、Entry、Button和Filedailog，但我们先关注前三个。</p><p>另外，我们希望提供 <em>编辑</em> 和 <em>保存</em> 两个功能按钮，这方便我们对文本文件进行编辑操作。 </p><p>我习惯直接上代码，在代码中注释思路，这里我们仅关注左侧Frame部分。当然这一部分没什么可说的，相信读者看一眼就能明白。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># author: SilenceZheng66</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Frame_left</span>(<span class="params">Frame</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, parent</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(parent)</span><br><span class="line">        self.__frame()</span><br><span class="line">        self.tk_input_open_file = self.__tk_input_open_file()</span><br><span class="line">        self.tk_label_open_file = self.__tk_label_open_file()</span><br><span class="line">        self.tk_button_open_file = self.__tk_button_open_file()</span><br><span class="line">        self.tk_button_edit_file = self.__tk_button_edit_file()</span><br><span class="line">        self.tk_button_save_file = self.__tk_button_save_file()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__frame</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.place(x=<span class="number">0</span>, y=<span class="number">0</span>, width=<span class="number">400</span>, height=<span class="number">599</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__tk_input_open_file</span>(<span class="params">self</span>):</span></span><br><span class="line">        ipt = Entry(self)</span><br><span class="line">        ipt.place(x=<span class="number">100</span>, y=<span class="number">60</span>, width=<span class="number">140</span>, height=<span class="number">24</span>)</span><br><span class="line">        <span class="keyword">return</span> ipt</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__tk_label_open_file</span>(<span class="params">self</span>):</span></span><br><span class="line">        label = Label(self, text=<span class="string">&quot;打开文件&quot;</span>, anchor=<span class="string">&quot;center&quot;</span>)</span><br><span class="line">        label.place(x=<span class="number">10</span>, y=<span class="number">60</span>, width=<span class="number">69</span>, height=<span class="number">24</span>)</span><br><span class="line">        <span class="keyword">return</span> label</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__tk_button_open_file</span>(<span class="params">self</span>):</span></span><br><span class="line">        btn = Button(self, text=<span class="string">&quot;打开&quot;</span>)</span><br><span class="line">        btn.place(x=<span class="number">260</span>, y=<span class="number">60</span>, width=<span class="number">80</span>, height=<span class="number">24</span>)</span><br><span class="line">        <span class="keyword">return</span> btn</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__tk_button_edit_file</span>(<span class="params">self</span>):</span></span><br><span class="line">        btn = Button(self, text=<span class="string">&quot;编辑&quot;</span>)</span><br><span class="line">        btn.place(x=<span class="number">100</span>, y=<span class="number">100</span>, width=<span class="number">80</span>, height=<span class="number">24</span>)</span><br><span class="line">        <span class="keyword">return</span> btn</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__tk_button_save_file</span>(<span class="params">self</span>):</span></span><br><span class="line">        btn = Button(self, text=<span class="string">&quot;保存&quot;</span>)</span><br><span class="line">        btn.place(x=<span class="number">260</span>, y=<span class="number">100</span>, width=<span class="number">80</span>, height=<span class="number">24</span>)</span><br><span class="line">        <span class="keyword">return</span> btn</span><br></pre></td></tr></table></figure><p>效果如下：</p><p><img src="/assets/post_img/article86/step2.png" alt="s2"></p><h3 id="内容显示区域"><a href="#内容显示区域" class="headerlink" title="内容显示区域"></a>内容显示区域</h3><p>我们使用带滚动条的文本组件ScrolledText来实现这部分内容：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># author: SilenceZheng66</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Frame_right</span>(<span class="params">Frame</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, parent</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(parent)</span><br><span class="line">        self.__frame()</span><br><span class="line">        self.tk_text_workplace = self.__tk_text_workplace()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__frame</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.place(x=<span class="number">410</span>, y=<span class="number">0</span>, width=<span class="number">400</span>, height=<span class="number">599</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__tk_text_workplace</span>(<span class="params">self</span>):</span></span><br><span class="line">        sct = ScrolledText(self)</span><br><span class="line">        <span class="comment"># 注意这里的起始坐标是相对于Frame的，故应设置为（0，0）</span></span><br><span class="line">        sct.place(x=<span class="number">0</span>, y=<span class="number">0</span>, width=<span class="number">400</span>, height=<span class="number">599</span>)</span><br><span class="line">        <span class="keyword">return</span> sct</span><br></pre></td></tr></table></figure><p>效果如下：<br><img src="/assets/post_img/article86/step3.png" alt="s3"></p><h3 id="绑定事件"><a href="#绑定事件" class="headerlink" title="绑定事件"></a>绑定事件</h3><p>到这里，界面的主体框架就已经实现了，最后一步就是将按钮和事件进行绑定，并添加上一定的逻辑。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># author: SilenceZheng66</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Win</span>(<span class="params">WinGUI</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.__event_bind()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__event_bind</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 绑定按钮事件</span></span><br><span class="line">        self.tk_frame_left.tk_button_open_file.bind(<span class="string">&quot;&lt;Button-1&gt;&quot;</span>, self.__do_open_file)</span><br><span class="line">        self.tk_frame_left.tk_button_edit_file.bind(<span class="string">&quot;&lt;Button-1&gt;&quot;</span>, self.__do_edit_file)</span><br><span class="line">        self.tk_frame_left.tk_button_save_file.bind(<span class="string">&quot;&lt;Button-1&gt;&quot;</span>, self.__do_save_file)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__do_edit_file</span>(<span class="params">self, event</span>):</span></span><br><span class="line">        <span class="comment"># 切换文本编辑区的可用状态</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获取编辑区状态</span></span><br><span class="line">        state = self.tk_frame_right.tk_text_workplace.cget(<span class="string">&#x27;state&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> state == <span class="string">&#x27;disabled&#x27;</span>:</span><br><span class="line">            self.tk_frame_right.tk_text_workplace.config(state=<span class="string">&#x27;normal&#x27;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.tk_frame_right.tk_text_workplace.config(state=<span class="string">&#x27;disabled&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__do_save_file</span>(<span class="params">self, event</span>):</span></span><br><span class="line">        <span class="comment"># 保存文件</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__do_open_file</span>(<span class="params">self, event</span>):</span></span><br><span class="line">        <span class="comment"># 获取文件路径</span></span><br><span class="line">        <span class="keyword">if</span> self.tk_frame_left.tk_input_open_file.get() != <span class="string">&quot;&quot;</span>:</span><br><span class="line">            file_path = self.tk_frame_left.tk_input_open_file.get()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            file_path = tkinter.filedialog.askopenfilename()</span><br><span class="line">        <span class="comment"># 判断路径有效</span></span><br><span class="line">        <span class="comment"># 打开文件</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><h3 id="界面美化"><a href="#界面美化" class="headerlink" title="界面美化"></a>界面美化</h3><p>简易界面不代表完全没有美化，适当的颜色变化可以显著提高界面的可用性，方便用户操作。下面是部分代码示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># author: SilenceZheng66</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__tk_input_open_file</span>(<span class="params">self</span>):</span></span><br><span class="line">    ipt = Entry(self)</span><br><span class="line">    ipt.place(x=<span class="number">100</span>, y=<span class="number">60</span>, width=<span class="number">140</span>, height=<span class="number">24</span>)</span><br><span class="line">    <span class="comment"># 选中文本时的前景、背景色</span></span><br><span class="line">    ipt.config(selectforeground=<span class="string">&#x27;green&#x27;</span>, selectbackground=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> ipt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__tk_button_open_file</span>(<span class="params">self</span>):</span></span><br><span class="line">    btn = Button(self, text=<span class="string">&quot;打开&quot;</span>)</span><br><span class="line">    btn.place(x=<span class="number">260</span>, y=<span class="number">60</span>, width=<span class="number">80</span>, height=<span class="number">24</span>)</span><br><span class="line">    <span class="comment"># 鼠标点击按钮时，按钮的文本颜色</span></span><br><span class="line">    btn.config(activeforeground=<span class="string">&#x27;yellow&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> btn</span><br></pre></td></tr></table></figure><p>效果如下：</p><p><img src="/assets/post_img/article86/step4.gif" alt="s4"></p><p>这里没有过多调整组件背景颜色是由于MacOS下很多组件的背景调色会失效，比如Button。有一个库叫tkmacosx可以解决。</p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于 <a href="https://silencezheng.top">silencezheng.top</a>，转载请注明出处。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;介绍如何使用Tkinter构建一个简易的图形用户界面。&lt;/p&gt;
&lt;p&gt;在使用Python方便我们的日常工作时，我们可能希望将脚本、程序打包起来，方便我们在多端使用或供他人使用。对于那些不了解如何使用命令行程序的用户而言，设计一个可用的图形用户界面就十分重要。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Python" scheme="http://silencezheng.top/tags/Python/"/>
    
    <category term="Tkinter" scheme="http://silencezheng.top/tags/Tkinter/"/>
    
  </entry>
  
  <entry>
    <title>VMware Fusion13使用体验</title>
    <link href="http://silencezheng.top/2022/12/19/article85/"/>
    <id>http://silencezheng.top/2022/12/19/article85/</id>
    <published>2022-12-19T15:03:16.000Z</published>
    <updated>2022-12-19T15:09:10.865Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>2022年11月17日，VMware Fusion13.0发布，build号20802013，官方宣布Fusion 现在支持在 Apple Silicon Mac 上运行 Arm 虚拟机，刚刚得知消息的我决定来体验一下。</p><p>其实早在今年的4月份时，我便发布了一篇名为《M1芯片VMfusion安装win11教程》的博客，当时的VMF版本为12，整体用下来还是有不少痛点和BUG，希望VMF13能够解决！<br><span id="more"></span></p><h2 id="VMF12痛点"><a href="#VMF12痛点" class="headerlink" title="VMF12痛点"></a>VMF12痛点</h2><p>今年上旬，我使用的Fusion12专业版build为19431034，整体用下来主要有以下痛点：</p><ol><li>Win11安装复杂，需要绕TPM。</li><li>Win11使用定时闪退。</li><li>Linux系统无法安装，很奇怪，Ubantu无法安装含GUI版本，即便后期手动加装GUI也不行。</li></ol><h2 id="VMF13使用体验"><a href="#VMF13使用体验" class="headerlink" title="VMF13使用体验"></a>VMF13使用体验</h2><p>关于13版本的发行说明，请见：<a href="https://docs.vmware.com/cn/VMware-Fusion/13.0/rn/vmware-fusion-130-release-notes/index.html">https://docs.vmware.com/cn/VMware-Fusion/13.0/rn/vmware-fusion-130-release-notes/index.html</a></p><p>下载：<a href="https://www.vmware.com/cn/products/fusion/fusion-evaluation.html">https://www.vmware.com/cn/products/fusion/fusion-evaluation.html</a></p><h3 id="1-安装"><a href="#1-安装" class="headerlink" title="1. 安装"></a>1. 安装</h3><p>安装直接一路点即可，许可证部分可以自己申请，也可以从网上找，总之能安装上即可。</p><h3 id="2-打开先前的虚拟机"><a href="#2-打开先前的虚拟机" class="headerlink" title="2. 打开先前的虚拟机"></a>2. 打开先前的虚拟机</h3><p>在打开之前建立的Win11虚拟机时需要先升级虚拟机，升级后不再向下兼容。</p><p>正常打开后效果如下，VMware Tools安装选项亮起。</p><p><img src="/assets/post_img/article85/vmtools.png" alt="vmt"></p><h3 id="3-建立Ubuntu桌面版虚拟机"><a href="#3-建立Ubuntu桌面版虚拟机" class="headerlink" title="3. 建立Ubuntu桌面版虚拟机"></a>3. 建立Ubuntu桌面版虚拟机</h3><p>这里我选择的镜像是ARM版的focal20.04.4，截止目前（2022.12.19），官方说还不能安装Ubuntu22.04：</p><blockquote><p>使用 Fusion 在 Apple Silicon Mac 上安装 RHEL 9、Ubuntu 22.04 和 Fedora 36 客户机失败：<br>如果尝试在虚拟机上安装 Rhel 9、Ubuntu 22.04 或 Fedora 36，安装将显示黑屏，而非预期内容，并且进程不会继续。</p></blockquote><p>一路点然后直接安装即可，效果如下：</p><p><img src="/assets/post_img/article85/ubuntu.png" alt="ub"></p><p>但安装VMware Tools会报错，表示不支持该系统。</p><h3 id="4-建立Windows11桌面版虚拟机"><a href="#4-建立Windows11桌面版虚拟机" class="headerlink" title="4. 建立Windows11桌面版虚拟机"></a>4. 建立Windows11桌面版虚拟机</h3><p>为了验证究竟是否能够在Windows11虚拟机上安装VMware Tools，我决定换一个更新版本的镜像重新建立虚拟机进行测试。</p><p>下面记录安装流程：</p><ol><li>将Win11专业版镜像拖入VMF13中，连续点击默认设置。</li><li>遇到设置TPM密码，可以随机生成，说是会存放在APPLE keys中，我后来也没找到。</li><li>第一次运行时一定要快速按任意键，否则需要重启。</li><li>进入操作系统安装后一路点到联网，发现无法连接，先唤出命令行。</li><li>输入<code>OOBE\BYPASSSNRO</code>，跳过网络设置。</li><li>然后重新进入到网络设置步骤，就可以在不用网络的情况下进行受限设置了。</li><li>进入系统后，需要把网络配置回来，直接按步骤安装VMware Tools即可。</li></ol><p>全部配置好后如下：</p><p><img src="/assets/post_img/article85/win11.png" alt="win11"></p><p>配置好VMware Tools的机器可以联网、更改分辨率和缩放比例，但仍不能实现主客机间拖拽传送文件，这是一个缺憾（官方文档证实）。但是这一点可以通过其他手段解决，比如samba，配合finder使用效果也是不错的。</p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>关于VMF12的痛点，总的来说还是在很大程度上进行了解决，希望后续版本有更好的表现！</p><p>首发于 <a href="https://silencezheng.top">silencezheng.top</a>，转载请注明出处。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;2022年11月17日，VMware Fusion13.0发布，build号20802013，官方宣布Fusion 现在支持在 Apple Silicon Mac 上运行 Arm 虚拟机，刚刚得知消息的我决定来体验一下。&lt;/p&gt;
&lt;p&gt;其实早在今年的4月份时，我便发布了一篇名为《M1芯片VMfusion安装win11教程》的博客，当时的VMF版本为12，整体用下来还是有不少痛点和BUG，希望VMF13能够解决！&lt;br&gt;</summary>
    
    
    
    
    <category term="MacOS" scheme="http://silencezheng.top/tags/MacOS/"/>
    
  </entry>
  
  <entry>
    <title>数值溢出问题</title>
    <link href="http://silencezheng.top/2022/12/16/article84/"/>
    <id>http://silencezheng.top/2022/12/16/article84/</id>
    <published>2022-12-16T13:02:44.000Z</published>
    <updated>2022-12-16T13:03:36.222Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>数值溢出，不得不考虑的问题…</p><span id="more"></span><h2 id="什么是溢出"><a href="#什么是溢出" class="headerlink" title="什么是溢出"></a>什么是溢出</h2><p>以Java语言为例，整数类型的上下溢出如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: SilenceZheng66</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Overflow</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> posMax = <span class="number">0b1111111111111111111111111111111</span>;</span><br><span class="line">        <span class="keyword">int</span> negMin = -<span class="number">0b1111111111111111111111111111111</span>;</span><br><span class="line"></span><br><span class="line">        System.out.println(posMax); <span class="comment">// 2147483647</span></span><br><span class="line">        System.out.println(posMax+<span class="number">1</span>); <span class="comment">// -2147483648</span></span><br><span class="line">        System.out.println(posMax+<span class="number">2</span>); <span class="comment">// -2147483647</span></span><br><span class="line"></span><br><span class="line">        System.out.println(negMin); <span class="comment">// -2147483647</span></span><br><span class="line">        System.out.println(negMin-<span class="number">1</span>); <span class="comment">// -2147483648</span></span><br><span class="line">        System.out.println(negMin-<span class="number">2</span>); <span class="comment">// 2147483647</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>一旦出现溢出，程序的运行结果就变得不可控，逻辑上成立的算法实际上无法得到想要的结果，需要考虑数据类型的上下界限。</p><h2 id="如何防止数值溢出"><a href="#如何防止数值溢出" class="headerlink" title="如何防止数值溢出"></a>如何防止数值溢出</h2><p>以二分查找为例，查找在1到n内的某整数，若输入n为int型，则可能产生上溢出：<code>mid = (start + end)/2;</code>。</p><p>此处start的最小值为1，end的最大值为2147483647，在<code>start+end</code>值大于2147483647时就会发生上溢出。</p><p>一种聪明的解决办法是将求中位数的操作改变为：<code>mid = start + (end - start)/2;</code>，此时由于end不会超过int的上界，故不会发生溢出。</p><p>另一种解决办法是将求和操作放到long型中计算，然后转回int型，相较于上一种内存消耗增加：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: SilenceZheng66</span></span><br><span class="line"><span class="keyword">long</span> midL = ((<span class="keyword">long</span>)start + (<span class="keyword">long</span>)end)/<span class="number">2</span>;</span><br><span class="line">mid = (<span class="keyword">int</span>) midL;</span><br></pre></td></tr></table></figure><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于 <a href="https://silencezheng.top">silencezheng.top</a>，转载请注明出处。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;数值溢出，不得不考虑的问题…&lt;/p&gt;</summary>
    
    
    
    
    <category term="数据结构与算法" scheme="http://silencezheng.top/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>常见字符编码介绍</title>
    <link href="http://silencezheng.top/2022/11/29/article83/"/>
    <id>http://silencezheng.top/2022/11/29/article83/</id>
    <published>2022-11-29T06:36:09.000Z</published>
    <updated>2022-11-29T06:37:37.311Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>平时写代码、写博客总少不了与字符编码打交道，应该整理一下来做系统的了解。当然，我认为这种“目录”性质的博客不应写的过于深入，只要通过简短的文字介绍基本信息即可。<br><span id="more"></span></p><h2 id="字符编码（Character-encoding）"><a href="#字符编码（Character-encoding）" class="headerlink" title="字符编码（Character encoding）"></a>字符编码（Character encoding）</h2><p>字符编码是把<strong>字符集</strong>中的字符<strong>编码</strong>为指定集合中某一对象，以便文本在计算机中存储和通过通信网络的传递。</p><p>对于软件开发中的场景来说，字符编码就是构建一个字符到数字的一一对应的映射关系。然而，在实际的应用中我们还需要解决<strong>字符间分隔问题</strong>。</p><blockquote><p>在显示器上看见的文字、图片等信息在电脑里面其实并不是我们看见的样子，即使你知道所有信息都存储在硬盘里，把它拆开也看不见里面有任何东西，只有些盘片。假设，你用显微镜把盘片放大，会看见盘片表面凹凸不平，凸起的地方被磁化，凹的地方是没有被磁化；凸起的地方代表数字1，凹的地方代表数字0。硬盘只能用0和1来表示所有文字、图片等信息。</p></blockquote><p>如上面引文中提到的，计算机只能对二进制数字进行读写，当它遇到<code>00100001 00010001</code>，它该如何知道这是一个双字节编码字符，又或是两个单字节编码的字符呢？通常解决方案要么就是规定好每个字长度（例如所有文字都是2 bytes，不够的前面用0补齐），要么就是在用0和1表示的时候，不仅需要表示出数字编码，还要暗示给计算机接下来多少个连续byte构成一个字。</p><p>不同的字符编码（有时也称为字符集）的区别主要在于两点：<strong>可以表示的字符范围</strong> 和 <strong>编码方式</strong>。几种常见的中文编码之间兼容性如下图所示，兼容是指映射间的包含关系：</p><p><img src="/assets/post_img/article83/compatibility.webp" alt=""></p><h2 id="ASCII"><a href="#ASCII" class="headerlink" title="ASCII"></a>ASCII</h2><p><strong>ASCII 字符集</strong>和 <strong>ASCII 码</strong>（ ASCII 是 American Standard Code for Information Interchange 的缩写）可能是我们最先接触到的英文字符集及其编码，它同时也被国际标准化组织（ International Organization for Standardization, <strong>ISO</strong> ）批准为国际标准。</p><p><img src="/assets/post_img/article83/ascii-1-1.png" alt="ascii"></p><p>基本的 ASCII 字符集共有 128 个字符，其中有 96 个可打印字符（可打印和可显示仍有一个字符的区别），包括常用的字母、数字、标点符号等，另外还有 32 个控制字符。标准 ASCII 码使用 7 个二进位对字符进行编码，对应的 ISO 标准为 ISO646 标准。</p><p>由于标准 ASCII 字符集字符数目有限，在实际应用中往往无法满足要求。为此，国际标准化组织又制定了 ISO2022 标准，它规定了在保持与 ISO646 兼容的前提下将 ASCII 字符集扩充为 8 位代码的统一方法。 ISO 陆续制定了一批适用于不同地区的扩充 ASCII 字符集，每种扩充 ASCII 字符集分别可以扩充 128 个字符，这些扩充字符的编码均为高位为 1 的 8 位代码（即十进制数 128~255 ），称为扩展 ASCII 码。</p><p>字母和数字的 ASCII 码的记忆是非常简单的。我们只要记住了一个字母或数字的 ASCII 码（例如记住 A 为 65 ， 0 的 ASCII 码为 48 ），知道相应的大小写字母之间差 32 ，就可以推算出其余字母、数字的 ASCII 码。</p><p>ASCII编码几乎被世界上所有编码所兼容（UTF-16和UTF-32是个例外），因此如果一个文本文档里面的内容全都由ASCII里面的字母或符号构成，那么不管你如何展示该文档的内容，都不可能出现乱码的情况。</p><h2 id="ANSI"><a href="#ANSI" class="headerlink" title="ANSI"></a>ANSI</h2><p>准确说，并不存在哪种具体的编码方式叫做ANSI，它只是一个Windows操作系统上的别称而已。在中文简体Windows操作系统上，ANSI就是GBK；在泰语操作系统上，ANSI就是TIS-620（一种泰语编码）；在韩语操作系统上，ANSI就是EUC-KR（一种韩语编码）。</p><p>为了扩充ASCII编码，以用于显示本国的语言，不同的国家和地区制定了不同的标准，由此产生了 GB2312, BIG5, JIS 等各自的编码标准。这些使用 <strong>2 个字节</strong>来代表一个字符的各种汉字延伸编码方式，称为 <strong>ANSI 编码</strong>，又称为”MBCS（Muilti-Bytes Character Set，多字节字符集）”。 <strong>不同 ANSI 编码之间互不兼容</strong>，当信息在国际间交流时，无法将属于两种语言的文字，存储在同一段 ANSI 编码的文本中。一个很大的缺点是，同一个编码值，在不同的编码体系里代表着不同的字。这样就容易造成混乱。于是催生了Unicode。</p><p>其中每个语言下的ANSI编码，都有一套一对一的编码转换器，Unicode变成所有编码转换的中间介质。所有的编码都有一个转换器可以转换到Unicode，而Unicode也可以转换到其他所有的编码。</p><h2 id="GBK、GB2312"><a href="#GBK、GB2312" class="headerlink" title="GBK、GB2312"></a>GBK、GB2312</h2><p>GB即国标，为了满足国内在计算机中使用汉字的需要，中国国家标准总局发布了一系列的汉字字符集国家标准编码，统称为GB码，或国标码。其中最有影响的是于1980年发布的《信息交换用汉字编码字符集 基本集》，标准号为GB 2312-1980,因其使用非常普遍，也常被通称为国标码。GB2312编码通行于我国内地；新加坡等地也采用此编码。几乎所有的中文系统和国际化的软件都支持GB2312。</p><p>GBK和GB2312都是双字节编码。</p><p>GB2312是一个简体中文字符集，由6763个常用汉字和682个全角的非汉字字符组成。其中汉字根据使用的频率分为两级。一级汉字3755个，二级汉字3008个。</p><p>GB2312的出现，基本满足了汉字的计算机处理需要，但对于人名、古汉语等方面出现的罕用字，GB2312不能处理，这导致了后来GBK的出现。经过GBK编码后，可以表示的汉字达到了20902个，另有984个汉语标点符号、部首等。</p><p>当GBK仍然无法满足使用的需要时，就产生了<strong>GB18030</strong>，这时2bytes已经不能满足使用的需要（2bytes最多只有65536种组合，然而为了和ASCII兼容，最高位不能为0就已经直接淘汰了一半的组合，只剩下3万多种组合无法满足全部汉字要求），因此GB18030多出来的汉字使用4bytes编码。</p><h2 id="Unicode"><a href="#Unicode" class="headerlink" title="Unicode"></a>Unicode</h2><p>Unicode是为了解决传统的字符编码方案的局限而产生的，它为每种语言中的每个字符设定了统一并且唯一的二进制编码，以满足跨语言、跨平台进行文本转换、处理的要求。Universal Multiple-Octet Coded Character Set，简称为<strong>UCS</strong>。UCS-2，即2字节编码字符集，UCS-4则是4字节编码字符集。</p><p>但是，Unicode仅仅是一本很厚的字典，规定了符合对应的二进制代码，至于这个二进制代码如何存储则没有任何规定。也就是说，其中一个字符可能只对应7位二进制数，而另一个字符则对应27位二进制数。如果按照统一用4字节存储字符编码的话，无疑会造成极大的资源浪费（对磁盘、对网络都是）。为了解决这一问题，产生了许多Unicode的实现方式，如utf-8、utf-16等等。</p><h2 id="UTF-8（8-bit-Unicode-Transformation-Format）"><a href="#UTF-8（8-bit-Unicode-Transformation-Format）" class="headerlink" title="UTF-8（8-bit Unicode Transformation Format）"></a>UTF-8（8-bit Unicode Transformation Format）</h2><p>UTF-8解决字符间分隔的方式是数<strong>二进制中最高位连续1的个数</strong>来决定这个字是几字节编码。0开头的属于单字节，和ASCII码重合，做到了兼容。</p><p><img src="/assets/post_img/article83/utf-8.png" alt="uft-8"></p><p>从这种表示方式也可以很显然地看出来，UTF-8 和 GBK 没有任何关系，除了都兼容ASCII以外。这也是文件乱码的主要原因之一。</p><p>UTF-8中，中文占 3 个字节，其他数字、英文、符号占一个字节。但 emoji 符号占 4 个字节，一些较复杂的文字、繁体字也是 4 个字节。</p><p>我们可能会注意到MySQL中有两套UTF-8的实现，分别是<code>utf8</code>和<code>utf8mb4</code>，它们的区别如下：</p><ul><li><code>utf8</code>：只支持1至3个字节。</li><li><code>utf8mb4</code>：完整实现，最多支持4个字节表示字符。</li></ul><p>因此，如果需要存储emoji类型的数据或者一些比较复杂的文字、繁体字到MySQL的话，数据库的编码一定要指定为<code>utf8mb4</code>。</p><h2 id="关于更多Unicode编码方式的内容"><a href="#关于更多Unicode编码方式的内容" class="headerlink" title="关于更多Unicode编码方式的内容"></a>关于更多Unicode编码方式的内容</h2><p>其实我认为常用的字符编码除了utf-8，还有utf-16。汉字的Unicode范围在<code>[0x4E00, 0x9FA5]</code>，这里是码点的范围，也就是<code>U+4E00 到 U+9FA5</code>。这个范围是CJK Unified Ideographs。</p><p>更多内容可以参考[7]、[8]。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1]<a href="https://baike.baidu.com/item/%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81/8446880">https://baike.baidu.com/item/%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81/8446880</a><br>[2]<a href="https://www.cnblogs.com/zhanghengscnc/p/7664120.html">https://www.cnblogs.com/zhanghengscnc/p/7664120.html</a><br>[3]<a href="https://zhuanlan.zhihu.com/p/46216008">https://zhuanlan.zhihu.com/p/46216008</a><br>[4]<a href="https://baike.baidu.com/item/%E7%BB%9F%E4%B8%80%E7%A0%81/2985798">https://baike.baidu.com/item/%E7%BB%9F%E4%B8%80%E7%A0%81/2985798</a><br>[5]<a href="https://www.cnblogs.com/crazylqy/p/10184291.html">https://www.cnblogs.com/crazylqy/p/10184291.html</a><br>[6]<a href="https://javaguide.cn/database/character-set.html">https://javaguide.cn/database/character-set.html</a><br>[7]<a href="https://cloud.tencent.com/developer/article/1341908">https://cloud.tencent.com/developer/article/1341908</a><br>[8]<a href="https://www.cnblogs.com/benbenalin/p/6921553.html">https://www.cnblogs.com/benbenalin/p/6921553.html</a></p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于 <a href="https://silencezheng.top">silencezheng.top</a>，转载请注明出处。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;平时写代码、写博客总少不了与字符编码打交道，应该整理一下来做系统的了解。当然，我认为这种“目录”性质的博客不应写的过于深入，只要通过简短的文字介绍基本信息即可。&lt;br&gt;</summary>
    
    
    
    
    <category term="字符编码" scheme="http://silencezheng.top/tags/%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81/"/>
    
  </entry>
  
  <entry>
    <title>Linux常用命令--深度学习向</title>
    <link href="http://silencezheng.top/2022/11/28/article82/"/>
    <id>http://silencezheng.top/2022/11/28/article82/</id>
    <published>2022-11-28T11:44:22.000Z</published>
    <updated>2022-11-28T11:45:41.308Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>总结一下Linux服务器使用的常见操作，深度学习向。<br><span id="more"></span></p><h2 id="SSH密码登录"><a href="#SSH密码登录" class="headerlink" title="SSH密码登录"></a>SSH密码登录</h2><p><code>ssh account@ip</code><br><code>ssh -p port account@ip</code></p><h2 id="查看进程"><a href="#查看进程" class="headerlink" title="查看进程"></a>查看进程</h2><p><code>ps aux</code>，又称BSD方式，其中a表示显示所有用户的进程(show processes for all users)；u表示显示用户(display the process’s user/owner)；x表示显示无控制终端的进程(also show processes not attached to a terminal)。</p><p><code>ps -ef</code>，又称System V方式，e效果与a相同，f表示用ASCII字符显示树状结构，表达程序间的相互关系(ASCII art forest)。</p><p>查看用户abc运行的进程：<code>ps -u abc</code></p><p>显示System V格式下java进程：<code>ps -ef|grep java</code></p><p>BSD在grep java下获取title：<code>ps aux|head -1;ps aux|grep java</code></p><p>System V在grep java下获取title：<code>ps -ef|head -1;ps -ef|grep java</code></p><p>top工具：<code>top</code></p><h2 id="查看目前登入系统的用户信息"><a href="#查看目前登入系统的用户信息" class="headerlink" title="查看目前登入系统的用户信息"></a>查看目前登入系统的用户信息</h2><p><code>w</code></p><h2 id="查看网卡信息"><a href="#查看网卡信息" class="headerlink" title="查看网卡信息"></a>查看网卡信息</h2><p><code>ifconfig</code><br><code>ip addr show</code></p><h2 id="查看CPU"><a href="#查看CPU" class="headerlink" title="查看CPU"></a>查看CPU</h2><p>查看主频信息：<code>ls cpu</code></p><p>查看详细信息：<code>cat /proc/cpuinfo</code></p><h2 id="查看内存信息"><a href="#查看内存信息" class="headerlink" title="查看内存信息"></a>查看内存信息</h2><p>以MB为单位：<code>free -m</code></p><p>以GB为单位：<code>free -g</code></p><h2 id="查看系统版本"><a href="#查看系统版本" class="headerlink" title="查看系统版本"></a>查看系统版本</h2><p>查看系统版本：<code>cat /etc/issue</code></p><h2 id="查看硬盘空间大小"><a href="#查看硬盘空间大小" class="headerlink" title="查看硬盘空间大小"></a>查看硬盘空间大小</h2><p>查看文件系统磁盘使用情况统计：<code>df -h</code></p><p>查看指定目录所属磁盘情况：<code>df -h 目录名</code></p><h2 id="Nvidia显卡配置"><a href="#Nvidia显卡配置" class="headerlink" title="Nvidia显卡配置"></a>Nvidia显卡配置</h2><p>查看显卡驱动版本、CUDA版本和显卡信息：<code>nvidia-smi</code></p><p>每隔半秒刷新一次GPU信息：<code>watch -n 0.5 nvidia-smi</code></p><p>查看CUDA Runtime版本：<code>nvcc -V</code></p><p>TensorFlow查看显卡是否可用：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(tf.config.list_physical_devices(<span class="string">&#x27;GPU&#x27;</span>))</span><br></pre></td></tr></table></figure></p><p>Pytorch查看显卡是否可用：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.cuda.is_available())</span><br><span class="line"></span><br><span class="line">ngpu = <span class="number">1</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> (torch.cuda.is_available() <span class="keyword">and</span> ngpu &gt; <span class="number">0</span>) <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(device)</span><br><span class="line"><span class="built_in">print</span>(torch.cuda.get_device_name(<span class="number">0</span>))</span><br></pre></td></tr></table></figure></p><p>显卡驱动下载：<a href="https://www.nvidia.com/download/index.aspx?lang=en-us">https://www.nvidia.com/download/index.aspx?lang=en-us</a></p><h2 id="Conda"><a href="#Conda" class="headerlink" title="Conda"></a>Conda</h2><p>查看环境信息：<code>conda info -envs</code></p><p>创建环境：<code>conda create --name ENVNAME python=3.x pkg1 pkg2 ...</code></p><p>删除环境：<code>conda env remove -n ENVNAME</code></p><p>查看通道：<code>conda config --show</code></p><p>添加通道（中科大镜像）：<code>conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/free/</code></p><p>删除通道：<code>conda config --remove channels ...</code></p><h2 id="GNU-Screen"><a href="#GNU-Screen" class="headerlink" title="GNU Screen"></a>GNU Screen</h2><p>安装screen：<code>apt/yum install screen</code></p><p>版本查看：<code>screen -v</code></p><p>查看已创建的screen终端：<code>screenv -ls</code>，同名终端要用PID区分，输出格式为<code>PID.Name</code>；screen有两种状态<code>Attached</code>（活跃）和<code>Detached</code>（挂起）。</p><p>创建一个叫Hello的虚拟终端（可创建同名）：<code>screen -S Hello</code></p><p>创建（回到）一个叫Hello的虚拟终端（不可创建同名，如果存在则直接进入该终端）：<code>screen -R Hello</code></p><p>清除虚拟终端法一：<code>进入对应终端，exit</code></p><p>清除虚拟终端法二：<code>screen -R [PID/Name] -X quit</code></p><p>后台运行虚拟终端：<code>在终端中按下ctrl+a 再按下d</code></p><p>更多绑定键信息：<code>在终端中按下ctrl+a 再输入?</code></p><p>进入活跃的虚拟终端前需要先挂起：<code>screen -d [PID/Name]</code>，否则<code>-R</code>回到活跃终端反而会创建新终端。</p><p>总之，创建和回到终端前先<code>screenv -ls</code>确认一下是最好的。</p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于 <a href="https://silencezheng.top">silencezheng.top</a>，转载请注明出处。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;总结一下Linux服务器使用的常见操作，深度学习向。&lt;br&gt;</summary>
    
    
    
    
    <category term="Linux" scheme="http://silencezheng.top/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Java位运算</title>
    <link href="http://silencezheng.top/2022/11/24/article81/"/>
    <id>http://silencezheng.top/2022/11/24/article81/</id>
    <published>2022-11-24T02:41:37.000Z</published>
    <updated>2022-11-24T02:42:45.103Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>从原补反到Java位运算，学习记录。<br><span id="more"></span></p><h2 id="机器数"><a href="#机器数" class="headerlink" title="机器数"></a>机器数</h2><p>一个数在计算机的存储形式是二进制数，我们称这些二进制数为机器数，机器数是有符号，在计算机中用机器数的最高位存放符号位，0表示正数，1表示负数。</p><p>因为带有符号位，所以机器数的形式值不等于其真值，以机器数<code>1000 0111</code>为例，其真正表示的值为-7，而形式值为135。将带符号的机器数的真正表示的值称为机器数的真值。</p><p><strong>无符号数</strong>是指整个机器字长的全部二进制位均表示数值位，相当于数的绝对值。还是以<code>1000 0111</code>为例，无符号数就是指其形式值135。Java中不存在无符号整数类型。</p><h2 id="原码、反码、补码"><a href="#原码、反码、补码" class="headerlink" title="原码、反码、补码"></a>原码、反码、补码</h2><p>简单起见，假设使用8位二进制数（一个字节）存储整数，实际上在Java中为四个字节，即32位。</p><p><strong>原码</strong>的表示与机器数真值表示的一样，即用第一位表示符号，其余位表示数值。</p><p><strong>反码</strong>的表示中，正数的反码是其原码本身，负数的反码是在其原码的基础上，符号位不变，其余各位取反。</p><p><strong>补码</strong>的表示中，正数的补码是其原码本身，负数的补码是在其原码的基础上，符号位不变，其余各位取反后加1。</p><p>以十进制正数1和-1为例，其各码如下：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">1:</span></span><br><span class="line"><span class="string">原码=反码=补码=0000</span> <span class="number">0001</span></span><br><span class="line"></span><br><span class="line"><span class="number">-1</span><span class="string">:</span></span><br><span class="line"><span class="string">原码=1000</span> <span class="number">0001</span></span><br><span class="line"><span class="string">反码=1111</span> <span class="number">1110</span></span><br><span class="line"><span class="string">补码=1111</span> <span class="number">1111</span></span><br></pre></td></tr></table></figure></p><p>计算机实际只存储补码，因为<strong>只有补码的计算是准确的</strong>，这一点在此不作证明了。同时原码转换为补码的过程，也可以理解为数据存储到计算机内存中的过程，如下图：</p><p><img src="/assets/post_img/article81/yfb.png" alt="yfb"></p><p>八位原码和八位反码能够表示的区间只有<code>[-127, 127]</code>，而八位补码可以表示的范围为<code>[-128, 127]</code>，因为十进制-127和-1的相加运算用补码表示算得的结果为<code>1000 0000</code>，即用原本的“-0”来表示-128。所以计算机中一个字节的取值范围是<code>[-128,127]</code>。</p><p>需要注意的是，在计算机运算过后，对于负数结果需要转换为原码才能得到其真值。例如补码<code>0b1000 0001</code>，先转换为反码<code>0b1000 0000</code>，再按位取反（符号位不变）得到原码<code>0b1111 1111</code>，可知其真值为-127。</p><h2 id="Java中的位运算符"><a href="#Java中的位运算符" class="headerlink" title="Java中的位运算符"></a>Java中的位运算符</h2><p>Java 定义的位运算（bitwise operators）直接对整数类型的位进行操作，这些整数类型包括 long（64位），int（32位），short（16位），char（16位） 和 byte（8位）。</p><p>位运算符主要用来对操作数二进制的位进行运算。按位运算表示<strong>按每个二进制位（bit）进行计算</strong>，其操作数和运算结果都是整型值。</p><p>Java 语言中的位运算符分为<strong>位逻辑运算符</strong>和<strong>位移运算符</strong>两类，下面详细介绍每类包含的运算符。</p><h3 id="位逻辑运算符"><a href="#位逻辑运算符" class="headerlink" title="位逻辑运算符"></a>位逻辑运算符</h3><p>位逻辑运算符包含 4 个：<code>&amp;（AND）</code>、<code>|（OR）</code>、<code>~（NOT）</code>和 <code>^（XOR）</code>。除了 <code>~</code> 为单目运算符外，其余都为双目运算符。</p><p>位与运算符为<code>&amp;</code>，其运算规则是：参与运算的数字，低位对齐，高位不足的补零，如果对应的二进制位同时为 1，那么计算结果才为 1，否则为 0。因此，任何数与 0 进行按位与运算，其结果都为 0。</p><p>位或运算符为<code>|</code>，其运算规则是：参与运算的数字，低位对齐，高位不足的补零。如果对应的二进制位只要有一个为 1，那么结果就为 1；如果对应的二进制位都为 0，结果才为 0。</p><p>位异或运算符为<code>^</code>，其运算规则是：参与运算的数字，低位对齐，高位不足的补零，如果对应的二进制位相同（同时为 0 或同时为 1）时，结果为 0；如果对应的二进制位不相同，结果则为 1。即<strong>相同为0，不同为1</strong>。</p><p>位取反运算符为<code>~</code>，其运算规则是：只对一个操作数进行运算，将操作数二进制中的 1 改为 0，0 改为 1。</p><h3 id="位移运算符"><a href="#位移运算符" class="headerlink" title="位移运算符"></a>位移运算符</h3><p>位移运算符用来将操作数向某个方向（向左或者右）移动指定的二进制位数，它们都属于双目运算符。</p><p><strong>左移位运算符</strong>为<code>&lt;&lt;</code>，其运算规则是：按二进制形式把所有的数字向左移动对应的位数，高位移出（舍弃），低位的空位补零。</p><p><strong>有符号右位移运算符</strong>为<code>&gt;&gt;</code>，其运算规则是：按二进制形式把所有的数字向右移动对应的位数，低位移出（舍弃），若操作数为正数则高位补0，操作数为负数则高位补1。也称为<strong>算数右移</strong>。</p><p><strong>无符号右位移运算符</strong>为<code>&gt;&gt;&gt;</code>，其运算规则是：按二进制形式把所有的数字向右移动对应的位数，低位移出（舍弃），高位补0。也称为<strong>逻辑右移</strong>。</p><p>特别需要注意的是，在对char、byte、short类型的数进行移位操作前，<strong>编译器都会自动地将数值转化为int类型，然后才进行移位操作（这也导致位移表达式返回值为整型）</strong>。由于int型变量只占4字节，当右移位数超过32bit时，移位运算没有任何意义。所以，在Java语言中，为了保证移动位数地有效性，以使移动的位数不超过32bit，采取了取余的操作，即<code>a&gt;&gt;n</code>等价于<code>a&gt;&gt;(n%32)</code>。这一性质对于左右移位都有效。</p><p>另外，左移$n$位表示原来的值乘$2^n$，经常用来代替乘法操作，由于CPU直接支持位运算，因此位运算比乘法运算效率高。</p><h3 id="复合位赋值运算符"><a href="#复合位赋值运算符" class="headerlink" title="复合位赋值运算符"></a>复合位赋值运算符</h3><p>所有的二进制位运算符都有一种<strong>将赋值与位运算组合在一起</strong>的简写形式。复合位赋值运算符由赋值运算符与位逻辑运算符或位移运算符组合而成。</p><p>包括<code>&amp;=</code>、<code>｜=</code>、<code>^=</code>、<code>&lt;&lt;=</code>、<code>&gt;&gt;&gt;=</code>和<code>&gt;&gt;=</code>。</p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>关于位运算呢，虽然看起来容易掌握，但是实际使用中可能还是会遇到一些问题，有时是由于真值与补码混淆造成的，我写了一个小demo来解释一下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: SilenceZheng66</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">bitwiseOperation</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">byte</span> a = -<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">byte</span> b = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        System.out.println(~a); <span class="comment">// 0</span></span><br><span class="line">        System.out.println(~b); <span class="comment">// -2</span></span><br><span class="line">        System.out.println(a&amp;b); <span class="comment">// 1</span></span><br><span class="line">        System.out.println(a|b); <span class="comment">// -1</span></span><br><span class="line">        System.out.println(b^a); <span class="comment">// -2</span></span><br><span class="line"></span><br><span class="line">        b^=a;</span><br><span class="line">        System.out.println(b); <span class="comment">// -2</span></span><br><span class="line">        System.out.println(b&gt;&gt;<span class="number">1</span>); <span class="comment">// -1</span></span><br><span class="line">        System.out.println(b&lt;&lt;<span class="number">1</span>); <span class="comment">// -4</span></span><br><span class="line"></span><br><span class="line">        System.out.println(Integer.toBinaryString(a&gt;&gt;&gt;<span class="number">2</span>));<span class="comment">// 00111111111111111111111111111111</span></span><br><span class="line">        System.out.println(Integer.toBinaryString(a&gt;&gt;<span class="number">2</span>)); <span class="comment">// 11111111111111111111111111111111</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>首先明确“真值=原码“，所以可以得到$a$的原码为<code>1000 0001</code>，$b$的原码为<code>0000 0001</code>。但在内存中，$a$被存储为<code>1111 1111</code>，这是由<code>1000 0001 -&gt; 1111 1110 -&gt; 1111 1111</code>得到的，也就是原码转换为补码的过程。$b$由于是正数，仍然被存储为原码形式。</p><p>那么<code>~a</code>为何是$0$呢？可以列出计算过程：<code>1111 1111(a) -&gt; 0000 0000(~a)</code>。由于<code>0000 0000</code>的原码与补码相同，故其真值为$0$。下面再计算一个<code>b^a</code>吧，过程如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">先计算异或：0000</span> <span class="number">0001</span> <span class="string">^</span> <span class="number">1111 </span><span class="number">1111</span> <span class="string">=</span> <span class="number">1111 </span><span class="number">1110</span></span><br><span class="line"><span class="string">补码转化为真值：1111</span> <span class="number">1110</span> <span class="string">-&gt;</span> <span class="number">1111 </span><span class="number">1101</span> <span class="string">-&gt;</span> <span class="number">1000 </span><span class="number">0010</span></span><br><span class="line"><span class="string">得到结果为-2</span></span><br></pre></td></tr></table></figure><h2 id="例题一：Leetcode190-颠倒二进制位"><a href="#例题一：Leetcode190-颠倒二进制位" class="headerlink" title="例题一：Leetcode190. 颠倒二进制位"></a>例题一：Leetcode190. 颠倒二进制位</h2><p>问题描述：颠倒给定的 32 位无符号整数的二进制位。</p><p>思路一（我的愚蠢解法）：获取读入十进制数的二进制字符串，存入字符数组，反转后再转为int输出。</p><p>实现：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: SilenceZheng66</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="comment">// you need treat n as an unsigned value</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">reverseBits</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">        String input = Integer.toBinaryString(n);</span><br><span class="line">        <span class="keyword">char</span>[] input_arr = input.toCharArray();</span><br><span class="line">        <span class="keyword">char</span>[] ans = <span class="keyword">new</span> <span class="keyword">char</span>[<span class="number">32</span>];</span><br><span class="line">        <span class="keyword">int</span> j = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = input_arr.length-<span class="number">1</span>; i&gt;=<span class="number">0</span>; i--)&#123;</span><br><span class="line">            ans[j] = input_arr[i];</span><br><span class="line">            j++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// toBinaryString中高位的0不输出，需要补充。</span></span><br><span class="line">        <span class="keyword">while</span>(j&lt;<span class="number">32</span>)&#123;</span><br><span class="line">            ans[j] = <span class="string">&#x27;0&#x27;</span>;</span><br><span class="line">            j++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> Integer.parseUnsignedInt(String.valueOf(ans), <span class="number">2</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>思路二（官解一）：将 $n$ 视作一个长为 32 的二进制串，从低位往高位枚举 $n$ 的每一位，将其倒序添加到翻转结果中。代码实现中，每枚举一位就将 $n$ 右移一位，这样当前 $n$ 的最低位就是我们要枚举的比特位。当 $n$ 为 0 时即可结束循环。</p><p>实现：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">reverseBits</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> rev = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">32</span> &amp;&amp; n != <span class="number">0</span>; ++i) &#123;</span><br><span class="line">            <span class="comment">// n&amp;1 只保留低位</span></span><br><span class="line">            <span class="comment">// &lt;&lt; (31 - i) 移到反转后的位置</span></span><br><span class="line">            <span class="comment">// rev = rev ｜ (n &amp; 1) &lt;&lt; (31 - i) 存入翻转结果 rev</span></span><br><span class="line">            rev |= (n &amp; <span class="number">1</span>) &lt;&lt; (<span class="number">31</span> - i);</span><br><span class="line">            <span class="comment">// n = n &gt;&gt;&gt; 1 逻辑右移，高位补0</span></span><br><span class="line">            n &gt;&gt;&gt;= <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> rev;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>思路三（JDK用法）：Integer包装类提供了<code>reverse</code>方法，速度很快。</p><p>实现：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">reverseBits</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> Integer.reverse(n);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1]<a href="http://c.biancheng.net/view/784.html">http://c.biancheng.net/view/784.html</a><br>[2]<a href="https://blog.csdn.net/xwu_09/article/details/78285785">https://blog.csdn.net/xwu_09/article/details/78285785</a><br>[3]<a href="https://www.cnblogs.com/linjiaxin/p/14870850.html">https://www.cnblogs.com/linjiaxin/p/14870850.html</a><br>[4]<a href="https://zhuanlan.zhihu.com/p/371184302">https://zhuanlan.zhihu.com/p/371184302</a><br>[5]<a href="https://blog.csdn.net/MaybeForever/article/details/89109596">https://blog.csdn.net/MaybeForever/article/details/89109596</a></p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于 <a href="https://silencezheng.top">silencezheng.top</a>，转载请注明出处。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;从原补反到Java位运算，学习记录。&lt;br&gt;</summary>
    
    
    
    
    <category term="Java" scheme="http://silencezheng.top/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>parseInt() 和 parseUnsignedInt()</title>
    <link href="http://silencezheng.top/2022/11/23/article80/"/>
    <id>http://silencezheng.top/2022/11/23/article80/</id>
    <published>2022-11-23T15:38:54.000Z</published>
    <updated>2022-11-25T03:16:30.922Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>22.11.25：事后发现是我蠢了，Java的意思是表示一个数直接用正负号表示符号，用原码表示数值，哪有人用补码转int的…</p><p>在做题的过程中，发现了一个神奇的“bug”，Integer.valueOf()对于32位二进制数字符串转化成整型爆出了<code>java.lang.NumberFormatException.forInputString</code>错误，往下研究了一下，发现问题出在了parseInt()上。<br><span id="more"></span></p><h2 id="Integer-valueOf"><a href="#Integer-valueOf" class="headerlink" title="Integer.valueOf()"></a>Integer.valueOf()</h2><p>该函数返回的是整型包装类，其内部调用了<code>Integer.parseInt(String s, int radix)</code>。于是问题聚焦在这个函数上。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Integer <span class="title">valueOf</span><span class="params">(String s, <span class="keyword">int</span> radix)</span> <span class="keyword">throws</span> NumberFormatException </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> Integer.valueOf(parseInt(s,radix));</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h2 id="parseInt-和parseUnsignedInt"><a href="#parseInt-和parseUnsignedInt" class="headerlink" title="parseInt()和parseUnsignedInt()"></a>parseInt()和parseUnsignedInt()</h2><p>还原一下问题场景：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: SilenceZheng66</span></span><br><span class="line">String a = <span class="string">&quot;11111111111111111111111111111101&quot;</span>;</span><br><span class="line"><span class="comment">// 可行，返回-3</span></span><br><span class="line">System.out.println(Integer.parseUnsignedInt(b, <span class="number">2</span>)); </span><br><span class="line"><span class="comment">// 报错</span></span><br><span class="line">System.out.println(Integer.parseInt(b, <span class="number">2</span>));</span><br></pre></td></tr></table></figure><p>这就有点迷惑了，明明输入的二进制字符串是32位的（在Integer范围内），为什么会说我格式错误呢？另外，这个<code>parseUnsignedInt()</code>又是干什么用的呢？带着这样的问题，我们来阅读二者的源码。首先看<code>parseUnsignedInt()</code>：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Parses the string argument as an unsigned integer in the radix</span></span><br><span class="line"><span class="comment">     * specified by the second argument.  An unsigned integer maps the</span></span><br><span class="line"><span class="comment">     * values usually associated with negative numbers to positive</span></span><br><span class="line"><span class="comment">     * numbers larger than MAX_VALUE.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">parseUnsignedInt</span><span class="params">(String s, <span class="keyword">int</span> radix)</span></span></span><br><span class="line"><span class="function">                <span class="keyword">throws</span> NumberFormatException </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (s == <span class="keyword">null</span>)  &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> NumberFormatException(<span class="string">&quot;null&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> len = s.length();</span><br><span class="line">        <span class="keyword">if</span> (len &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">char</span> firstChar = s.charAt(<span class="number">0</span>);</span><br><span class="line">            <span class="keyword">if</span> (firstChar == <span class="string">&#x27;-&#x27;</span>) &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span></span><br><span class="line">                    NumberFormatException(String.format(<span class="string">&quot;Illegal leading minus sign &quot;</span> +</span><br><span class="line">                                                       <span class="string">&quot;on unsigned string %s.&quot;</span>, s));</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">if</span> (len &lt;= <span class="number">5</span> || <span class="comment">// Integer.MAX_VALUE in Character.MAX_RADIX is 6 digits</span></span><br><span class="line">                    (radix == <span class="number">10</span> &amp;&amp; len &lt;= <span class="number">9</span>) ) &#123; <span class="comment">// Integer.MAX_VALUE in base 10 is 10 digits</span></span><br><span class="line">                    <span class="keyword">return</span> parseInt(s, radix);</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="keyword">long</span> ell = Long.parseLong(s, radix);</span><br><span class="line">                    <span class="keyword">if</span> ((ell &amp; <span class="number">0xffff_ffff_0000_0000L</span>) == <span class="number">0</span>) &#123;</span><br><span class="line">                        <span class="keyword">return</span> (<span class="keyword">int</span>) ell;</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        <span class="keyword">throw</span> <span class="keyword">new</span></span><br><span class="line">                            NumberFormatException(String.format(<span class="string">&quot;String value %s exceeds &quot;</span> +</span><br><span class="line">                                                                <span class="string">&quot;range of unsigned int.&quot;</span>, s));</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">throw</span> NumberFormatException.forInputString(s);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>Java1.8中，<code>Character.MIN_RADIX = 2, Character.MAX_RADIX = 36</code>，也就是说最大支持到36进制。<code>parseUnsignedInt</code>方法首先要求<code>s</code>不能带有负号，对于上述问题场景，该方法首先将输入转化为长整型，然后再转回整型输出。由于Java中不存在无符号整型，在整型中，32位二进制数字的第一位必须为符号位，所以返回结果为带符号数-3。再阐述一下，我输入的无符号数 <code>11111111111111111111111111111101 = 4294967293</code> 超出了Java整型的大小限制（2147483647），但没有超过长度限制（32位），于是应该是可以正常表示的。</p><p>总之<code>parseUnsignedInt</code>方法是用于获取无符号数的，它尚且可以根据输入来获得整型，为什么<code>parseInt()</code>不行呢？我们再来看一下源码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">parseInt</span><span class="params">(String s, <span class="keyword">int</span> radix)</span></span></span><br><span class="line"><span class="function">                <span class="keyword">throws</span> NumberFormatException</span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * WARNING: This method may be invoked early during VM initialization</span></span><br><span class="line"><span class="comment">         * before IntegerCache is initialized. Care must be taken to not use</span></span><br><span class="line"><span class="comment">         * the valueOf method.</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (s == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> NumberFormatException(<span class="string">&quot;null&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (radix &lt; Character.MIN_RADIX) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> NumberFormatException(<span class="string">&quot;radix &quot;</span> + radix +</span><br><span class="line">                                            <span class="string">&quot; less than Character.MIN_RADIX&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (radix &gt; Character.MAX_RADIX) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> NumberFormatException(<span class="string">&quot;radix &quot;</span> + radix +</span><br><span class="line">                                            <span class="string">&quot; greater than Character.MAX_RADIX&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> result = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">boolean</span> negative = <span class="keyword">false</span>;</span><br><span class="line">        <span class="keyword">int</span> i = <span class="number">0</span>, len = s.length();</span><br><span class="line">        <span class="keyword">int</span> limit = -Integer.MAX_VALUE;</span><br><span class="line">        <span class="keyword">int</span> multmin;</span><br><span class="line">        <span class="keyword">int</span> digit;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (len &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">char</span> firstChar = s.charAt(<span class="number">0</span>);</span><br><span class="line">            <span class="keyword">if</span> (firstChar &lt; <span class="string">&#x27;0&#x27;</span>) &#123; <span class="comment">// Possible leading &quot;+&quot; or &quot;-&quot;</span></span><br><span class="line">                <span class="keyword">if</span> (firstChar == <span class="string">&#x27;-&#x27;</span>) &#123;</span><br><span class="line">                    negative = <span class="keyword">true</span>;</span><br><span class="line">                    limit = Integer.MIN_VALUE;</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (firstChar != <span class="string">&#x27;+&#x27;</span>)</span><br><span class="line">                    <span class="keyword">throw</span> NumberFormatException.forInputString(s);</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (len == <span class="number">1</span>) <span class="comment">// Cannot have lone &quot;+&quot; or &quot;-&quot;</span></span><br><span class="line">                    <span class="keyword">throw</span> NumberFormatException.forInputString(s);</span><br><span class="line">                i++;</span><br><span class="line">            &#125;</span><br><span class="line">            multmin = limit / radix;</span><br><span class="line">            <span class="keyword">while</span> (i &lt; len) &#123;</span><br><span class="line">                <span class="comment">// Accumulating negatively avoids surprises near MAX_VALUE</span></span><br><span class="line">                digit = Character.digit(s.charAt(i++),radix);</span><br><span class="line">                <span class="keyword">if</span> (digit &lt; <span class="number">0</span>) &#123;</span><br><span class="line">                    <span class="keyword">throw</span> NumberFormatException.forInputString(s);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">if</span> (result &lt; multmin) &#123;</span><br><span class="line">                    <span class="keyword">throw</span> NumberFormatException.forInputString(s);</span><br><span class="line">                &#125;</span><br><span class="line">                result *= radix;</span><br><span class="line">                <span class="keyword">if</span> (result &lt; limit + digit) &#123;</span><br><span class="line">                    <span class="keyword">throw</span> NumberFormatException.forInputString(s);</span><br><span class="line">                &#125;</span><br><span class="line">                result -= digit;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">throw</span> NumberFormatException.forInputString(s);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> negative ? result : -result;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>我们可以发现，<code>parseInt</code>方法首先判断第一个字符是不是正负号，是负号则说明值是负的，否则，值就是正的。这个逻辑在非二进制环境下没有问题，因为非二进制表示的int变量，都会前置负号来表示负数。然而，在二进制数中，并没有所谓的“正负号”概念，数值的正负由符号位表示。所以<code>parseInt()</code>在转换<code>&quot;11111111111111111111111111111101&quot;</code>时将符号位也当做实际的值计算进去了，导致了数值溢出报错。</p><p>一种解决办法是将首位数字改为正负号，可以运行成功，但丧失了原本的意义，为什么这么说呢？请看下例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: SilenceZheng66</span></span><br><span class="line">String a = <span class="string">&quot;11111111111111111111111111111101&quot;</span>;</span><br><span class="line">String b = <span class="string">&quot;-1111111111111111111111111111101&quot;</span>;</span><br><span class="line">System.out.println(Integer.parseUnsignedInt(a, <span class="number">2</span>)); <span class="comment">// -3</span></span><br><span class="line">System.out.println(Integer.valueOf(b, <span class="number">2</span>)); <span class="comment">// -2147483645</span></span><br></pre></td></tr></table></figure><p>b的输出竟然是-2147483645，并不是想要表达的真实含义（-3），这是因为<code>parseInt()</code>首先将<code>&quot;-1111111111111111111111111111101&quot;</code> 拆分为 <code>&quot;-&quot;和&quot;01111111111111111111111111111101&quot;</code>，其中后者表示的真值为2147483645，然后再把负号放进来组合成了输出，这显然与补码<code>&quot;11111111111111111111111111111101&quot;</code>的真值不一致。</p><p>综上，在处理32位补码时，需要采用<code>parseUnsignedInt()</code>。其实感觉这个<code>parseInt()</code>可以优化一下，对32位二进制数用首位数字判断符号就好了。</p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于 <a href="https://silencezheng.top">silencezheng.top</a>，转载请注明出处。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;22.11.25：事后发现是我蠢了，Java的意思是表示一个数直接用正负号表示符号，用原码表示数值，哪有人用补码转int的…&lt;/p&gt;
&lt;p&gt;在做题的过程中，发现了一个神奇的“bug”，Integer.valueOf()对于32位二进制数字符串转化成整型爆出了&lt;code&gt;java.lang.NumberFormatException.forInputString&lt;/code&gt;错误，往下研究了一下，发现问题出在了parseInt()上。&lt;br&gt;</summary>
    
    
    
    
    <category term="Java" scheme="http://silencezheng.top/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>Java中的==和equals</title>
    <link href="http://silencezheng.top/2022/11/20/article79/"/>
    <id>http://silencezheng.top/2022/11/20/article79/</id>
    <published>2022-11-20T06:10:47.000Z</published>
    <updated>2022-11-20T06:12:25.015Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>从Java数据类型出发，聊聊Java中的==和equals。<br><span id="more"></span></p><h2 id="Java数据类型"><a href="#Java数据类型" class="headerlink" title="Java数据类型"></a>Java数据类型</h2><p>Java 语言支持的数据类型分为两种：<strong>基本数据类型</strong>（Primitive Type）和<strong>引用数据类型</strong>（Reference Type）。</p><p><img src="/assets/post_img/article79/datatype.jpeg" alt="datatype"></p><p>引用数据类型建立在基本数据类型的基础上，包括数组、类和接口。引用数据类型是由用户自定义，用来限制其他数据的类型。<strong>空类型null也是一种引用类型，不能转换成基本类型，因此不要把一个 null 值赋给基本数据类型的变量。</strong></p><p>对于基本数据类型，Java 为每种基本数据类型分别设计了对应的类，称之为<strong>包装类</strong>（Wrapper Classes），除了Integer和Character以外，包装类名都和基本数据类型相同，只是首字母大写。关于数据类型的更多细节，就不在这里赘述了。</p><p><img src="/assets/post_img/article79/pd.jpeg" alt="pd"></p><h2 id="和-equals"><a href="#和-equals" class="headerlink" title="== 和 equals"></a>== 和 equals</h2><p>首先明确一点，<strong>equals方法不能作用于基本数据类型变量</strong>。关于这句话其实可以展开说明一下，首先基本类型是不能作为方法的主体的，这是一定的，但基本类型能不能作为equals中的参数呢？有时也是可以的，因为Java的“自动装箱”机制，基本类型在传入时可以被封装为包装类，事实上参与函数的是包装类。</p><p>在比较基本数据类型时，<code>==</code>比较的是值。</p><p>在比较引用类型时，<code>==</code>比较的是对象的内存地址。如果equals方法没有经过重写，则与<code>==</code>相同，比较地址；如果equals方法经过重写，对于Java提供的类来说，则是比较对象存储的内容是否相同，也就是比较“值”。</p><p>Java提供的绝大多数类（不是全部）都重写了equals方法，以上讲述的区别与联系主要是关于Java内置类和基本数据类型。</p><h2 id="equals-和-hashCode"><a href="#equals-和-hashCode" class="headerlink" title="equals() 和 hashCode()"></a>equals() 和 hashCode()</h2><p>对于我们自己写代码，新建一个类而言，要么就不重写equals方法，此时等同于<code>==</code>；要么就同时重写equals和hashCode方法，按照我们定义的规则来比较对象是否相等。</p><blockquote><p>Java中对equals()的规范</p><ol><li>对称性：如果x.equals(y)返回是”true”，那么y.equals(x)也应该返回是”true”。</li><li>自反性：x.equals(x)必须返回是”true”。</li><li>传递性：如果x.equals(y)返回是”true”，而且y.equals(z)返回是”true”，那么x.equals(z)也应该返回是”true”。</li><li>一致性：如果x.equals(y)返回是”true”，只要x和y内容一直不变，不管重复x.equals(y)多少次，返回都是”true”。</li><li>非空性，x.equals(null)，永远返回是”false”；假设z是和x不同类型的对象，则x.equals(z)永远返回是”false”。</li></ol></blockquote><p>hashCode()的作用是用来获取哈希码，用于确定对象在哈希表中的位置，与equals()一样，所有的类都有hashCode方法。hashCode()只有在创建某个类的哈希表时才有用，需要根据方法返回值确认对象在哈希表中的位置。如果一个对象一定不会在散列表中使用，那么是没有必要复写hashCode方法的。但一般情况下我们还是会复写hashCode方法，因为谁能保证这个对象不会出现在HashMap、HashSet、HashTable…中呢？</p><blockquote><p>Object.hashCode()的通用约定</p><ol><li>在应用程序中，只要对象的equals方法的比较操作所用的信息没有修改，那么对于同一个对象的多次调用hashCode()，必须始终返回同一个哈希值。</li><li>如果两个对象通过equals()比较相等，那么它们的哈希值相同。</li><li>如果两个对象通过equals()比较不等，他们的哈希值可能相同也可能不同，取决于hashCode的实现，由此哈希表的性能也会有区别。</li></ol></blockquote><p>考虑一个常见的场景，HashSet是一个不允许有重复元素的集合，该集合会维护一个已存入对象的哈希值表。当插入一个新的对象时，我们首先会想到使用equals()逐个比较来确定是否有重复元素，但这必然造成效率问题。另外一个合理的方式就是对该对象调用hashCode()得到哈希值，然后在哈希值表中进行比对，如果不存在则直接存入；如果存在，则再调用equals()进行比较，相同的话就不再存入，不同的话散列到其他地址。这样一来实际调用equals()的次数就大大降低了。</p><p>因此，如果不重写对象的hashCode()方法，就有可能造成相同对象产生不同哈希值的情况，这就破坏了HashSet的性质。当然，这只是不重写hashCode(),或者说不遵守hashCode()规范的其中一个坏处。</p><p>对于equals() 和 hashCode()的部分，可以用两个问题来加深印象：<br>1、两个对象，如果a.equals(b)==true，那么a和b是否相等？<br>答：相等，但对象地址不一定相等。</p><p>2、两个对象，如果哈希值一样，那么两个对象是否相等？<br>答：不一定相等，判断两个对象是否相等，需要使用equals()。</p><p>最后，记录一个实现高质量equals()的诀窍：</p><blockquote><ol><li>使用<code>==</code>操作符检查“参数是否为这个对象的引用”；</li><li>使用<code>instanceof</code>操作符检查“参数是否为正确的类型”；</li><li>对于类中的关键属性，检查参数传入对象的属性是否与之相匹配；</li><li>编写完equals方法后，问自己它是否满足对称性、传递性、一致性；</li><li>重写equals方法时总是要重写hashCode方法；</li><li>不要将equals方法参数中的Object对象替换为其他的类型，在重写时不要忘掉<code>@Override</code>注解。</li></ol></blockquote><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1]<a href="http://c.biancheng.net/view/5672.html">http://c.biancheng.net/view/5672.html</a><br>[2]<a href="https://blog.csdn.net/qq_44543508/article/details/95449363">https://blog.csdn.net/qq_44543508/article/details/95449363</a><br>[3]<a href="https://www.jianshu.com/p/da7491e5be53">https://www.jianshu.com/p/da7491e5be53</a><br>[4]<a href="https://blog.csdn.net/u013063153/article/details/78808923">https://blog.csdn.net/u013063153/article/details/78808923</a><br>[5]<a href="https://cloud.tencent.com/developer/article/1018529">https://cloud.tencent.com/developer/article/1018529</a></p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于 <a href="https://silencezheng.top">silencezheng.top</a>，转载请注明出处。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;从Java数据类型出发，聊聊Java中的==和equals。&lt;br&gt;</summary>
    
    
    
    
    <category term="Java" scheme="http://silencezheng.top/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>记忆化递归</title>
    <link href="http://silencezheng.top/2022/11/19/article78/"/>
    <id>http://silencezheng.top/2022/11/19/article78/</id>
    <published>2022-11-19T05:59:19.000Z</published>
    <updated>2022-11-19T05:59:52.552Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>一旦写完一个可行的算法，便不想推倒重来，不知道有多少人和我一样有这样的想法。然而写完一个trash的递归算法，计算时间太慢固然也是不行的，这时记忆化递归可能会帮到你。</p><span id="more"></span><h2 id="记忆化递归"><a href="#记忆化递归" class="headerlink" title="记忆化递归"></a>记忆化递归</h2><p><strong>记忆化递归</strong>的核心思想就是将已经算好的值给存起来，等再次需要用到的时候，就直接取而不用计算，这样就大大节省了计算时间。笔者认为，虽然看起来是用空间换时间，但是相比于每次都进行庞大的递归计算来说，用合理的空间存放之前求得的值是明智的。</p><p>思想很简单，实现起来其实也并不复杂，下面举例说明之。</p><h2 id="例一：Leetcode119-杨辉三角-II"><a href="#例一：Leetcode119-杨辉三角-II" class="headerlink" title="例一：Leetcode119. 杨辉三角 II"></a>例一：Leetcode119. 杨辉三角 II</h2><p>简单描述：给定一个非负索引 rowIndex，返回「杨辉三角」的第 rowIndex 行。</p><p>普通递归（超时）：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: SilenceZheng66</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">recusion</span><span class="params">(<span class="keyword">int</span> pos, <span class="keyword">int</span> rowIndex)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(pos&gt;rowIndex) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">if</span>(pos==rowIndex||pos==<span class="number">0</span>) <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">return</span> (recusion(pos-<span class="number">1</span>, rowIndex-<span class="number">1</span>) + recusion(pos, rowIndex-<span class="number">1</span>));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;Integer&gt; <span class="title">getRow</span><span class="params">(<span class="keyword">int</span> rowIndex)</span> </span>&#123;</span><br><span class="line">        ArrayList&lt;Integer&gt; ans = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        ans.add(<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">if</span>(rowIndex==<span class="number">0</span>) <span class="keyword">return</span> ans;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=rowIndex;i++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(i==rowIndex) ans.add(<span class="number">1</span>);</span><br><span class="line">            <span class="keyword">else</span>&#123;</span><br><span class="line">                ans.add(recusion(i, rowIndex));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>利用对称（超时）：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: SilenceZheng66</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">recusion</span><span class="params">(<span class="keyword">int</span> pos, <span class="keyword">int</span> rowIndex)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(pos&gt;rowIndex) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">if</span>(pos==rowIndex||pos==<span class="number">0</span>) <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">return</span> (recusion(pos-<span class="number">1</span>, rowIndex-<span class="number">1</span>) + recusion(pos, rowIndex-<span class="number">1</span>));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;Integer&gt; <span class="title">getRow</span><span class="params">(<span class="keyword">int</span> rowIndex)</span> </span>&#123;</span><br><span class="line">        ArrayList&lt;Integer&gt; ans = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        ans.add(<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">if</span>(rowIndex==<span class="number">0</span>) <span class="keyword">return</span> ans;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> step = rowIndex/<span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=step;i++)&#123;</span><br><span class="line">            ans.add(recusion(i, rowIndex));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(rowIndex%<span class="number">2</span>==<span class="number">0</span>)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i=step-<span class="number">1</span>;i&gt;=<span class="number">0</span>;i--)&#123;</span><br><span class="line">                ans.add(ans.get(i));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i=step;i&gt;=<span class="number">0</span>;i--)&#123;</span><br><span class="line">                ans.add(ans.get(i));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>记忆化递归+利用对称：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: SilenceZheng66</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span>[][] rem = <span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">34</span>][<span class="number">34</span>];</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">recusion</span><span class="params">(<span class="keyword">int</span> pos, <span class="keyword">int</span> rowIndex)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(pos&gt;rowIndex) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">if</span>(pos==rowIndex||pos==<span class="number">0</span>) <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">if</span>(rem[rowIndex][pos]!=<span class="number">0</span>) <span class="keyword">return</span> rem[rowIndex][pos];</span><br><span class="line"></span><br><span class="line">        rem[rowIndex][pos] = (recusion(pos-<span class="number">1</span>, rowIndex-<span class="number">1</span>) + recusion(pos, rowIndex-<span class="number">1</span>));</span><br><span class="line">        <span class="keyword">return</span> rem[rowIndex][pos];</span><br><span class="line">    &#125;</span><br><span class="line">     </span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;Integer&gt; <span class="title">getRow</span><span class="params">(<span class="keyword">int</span> rowIndex)</span> </span>&#123;</span><br><span class="line">        ArrayList&lt;Integer&gt; ans = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        ans.add(<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">if</span>(rowIndex==<span class="number">0</span>) <span class="keyword">return</span> ans;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> step = rowIndex/<span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=step;i++)&#123;</span><br><span class="line">            ans.add(recusion(i, rowIndex));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(rowIndex%<span class="number">2</span>==<span class="number">0</span>)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i=step-<span class="number">1</span>;i&gt;=<span class="number">0</span>;i--)&#123;</span><br><span class="line">                ans.add(ans.get(i));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i=step;i&gt;=<span class="number">0</span>;i--)&#123;</span><br><span class="line">                ans.add(ans.get(i));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于 <a href="https://silencezheng.top">silencezheng.top</a>，转载请注明出处。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;一旦写完一个可行的算法，便不想推倒重来，不知道有多少人和我一样有这样的想法。然而写完一个trash的递归算法，计算时间太慢固然也是不行的，这时记忆化递归可能会帮到你。&lt;/p&gt;</summary>
    
    
    
    
    <category term="数据结构与算法" scheme="http://silencezheng.top/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>MySQL中的Join查询</title>
    <link href="http://silencezheng.top/2022/11/18/article77/"/>
    <id>http://silencezheng.top/2022/11/18/article77/</id>
    <published>2022-11-18T04:50:46.000Z</published>
    <updated>2022-11-18T04:52:50.466Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>业精于勤，荒于嬉。</p><span id="more"></span><h2 id="闲谈"><a href="#闲谈" class="headerlink" title="闲谈"></a>闲谈</h2><p>在整理Join相关的内容时，我提出了几个问题，整理到一个部分记录一下，想看“干货”的读者可以跳过了。</p><p>问题一：<strong>Join和Key有啥关系？</strong><br>Key无非主、外、候选、公共之类的内容，一个row的identifier罢了，无非是对内对外，同时它也是一个field。那么我们在规划表结构、塞数据的时候就有了一个方便的方法，把想表达的一个row的数据用一个key概括，需要获取所有数据时通过多表查询即可。总之，笔者认为，Join和Key可以说没关系，Key在任何时候都发挥着identifier的作用。</p><p>问题二：<strong>MySQL中不用Key也能Join，为什么？</strong><br>其实这个问题本身有点奇怪（我突然想出来的），首先关系代数中连接（Join）也没有要求一定要用键做连接，其次上面也说了这俩没多大关系。但我一搜吧，还真有个<a href="https://blog.csdn.net/lamanchas/article/details/121366276">回答</a>，主要是说外键约束有成本，对高并发情况不合适之类的，一时不知道是我有问题还是理解不到位，知道的大神可以告诉我，感谢。</p><p>问题三：<strong>为什么不能用Where替代Join?</strong><br>关于这个问题，我简单思考了一下，首先就拿纯<code>WHERE</code>、<code>JOIN</code>和<code>LEFT JOIN</code>来说，我写了以下三个查询：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">select</span> *</span><br><span class="line"><span class="attribute">from</span> node_graph<span class="number">1</span> left join edge_graph<span class="number">1</span> <span class="literal">on</span> node_graph<span class="number">1</span>.node_list = edge_graph<span class="number">1</span>.node<span class="number">1</span>_name;</span><br><span class="line"></span><br><span class="line"><span class="attribute">select</span> *</span><br><span class="line"><span class="attribute">from</span> node_graph<span class="number">1</span>, edge_graph<span class="number">1</span> where node_graph<span class="number">1</span>.node_list = edge_graph<span class="number">1</span>.node<span class="number">1</span>_name;</span><br><span class="line"></span><br><span class="line"><span class="attribute">select</span> *</span><br><span class="line"><span class="attribute">from</span> node_graph<span class="number">1</span> join edge_graph<span class="number">1</span> <span class="literal">on</span> node_graph<span class="number">1</span>.node_list = edge_graph<span class="number">1</span>.node<span class="number">1</span>_name;</span><br></pre></td></tr></table></figure><p>其中后两个的效果是一致的，而<code>LEFT JOIN</code>会返回node_graph1的所有结果，即使没有match到，这可能是<code>WHERE</code>做不到的一个地方。更多还是要在实践中发掘。</p><h2 id="Join查询"><a href="#Join查询" class="headerlink" title="Join查询"></a>Join查询</h2><p>SQL中Join用于根据两个或多个表中的列之间的关系，从这些表中查询数据。日常使用中对多表查询有广泛的需求，Join查询自然是必不可少。</p><p>用Join联合表时需要在每个表中选择一个<strong>字段</strong>，并对这些字段的值进行比较，值相同的两条记录将合并为一条。联合表的本质就是将不同表的记录合并起来，形成一张新表。当然，这张新表只是临时的，它仅存在于本次查询期间。</p><p>那么SQL中的Join都有哪些呢？先上一张总览：</p><p><img src="/assets/post_img/article77/SQL-Join.png" alt="overview"></p><p>下面开始逐个说一下。</p><h3 id="1-内连接（Inner-Join"><a href="#1-内连接（Inner-Join" class="headerlink" title="1. 内连接（Inner Join)"></a>1. 内连接（Inner Join)</h3><p>INNER JOIN 是 SQL 中最重要、最常用的表连接形式，只有当连接的两个或者多个表中都存在满足条件的记录时，才返回行。任何一条只存在于某一张表中的数据，都不会返回。</p><h3 id="2-左外连接（Left-Outer-Join"><a href="#2-左外连接（Left-Outer-Join" class="headerlink" title="2. 左外连接（Left Outer Join)"></a>2. 左外连接（Left Outer Join)</h3><p>LEFT OUTER JOIN 以左表为主，即左表中的<strong>所有记录</strong>都会被返回，具体分为以下三种情况：</p><ul><li>如果 TableA 中的某条记录在 TableB 中刚好只有一条记录可以匹配，那么在返回的结果中会生成一个新的行。</li><li>如果 TableA 中的某条记录在 TableB 中有 N 条记录可以匹配，那么在返回结果中也会生成 N 个新的行，这些行所包含的 TableA 的字段值是重复的。</li><li>如果 TableA 中的某条记录在 TableB 中没有匹配的记录，那么在返回结果中仍然会生成一个新的行，只是该行所包含的 TableB 的字段值都是 NULL。<h3 id="3-右外连接（Right-Outer-Join"><a href="#3-右外连接（Right-Outer-Join" class="headerlink" title="3. 右外连接（Right Outer Join)"></a>3. 右外连接（Right Outer Join)</h3>RIGHT OUTER JOIN 以右表为主，即右表中的<strong>所有记录</strong>都会被返回，具体分为以下三种情况：</li><li>如果 TableB 中的某条记录在 TableA 中刚好只有一条记录可以匹配，那么在返回的结果中会生成一个新的行。</li><li>如果 TableB 中的某条记录在 TableA 中有 N 条记录可以匹配，那么在返回的结果中也会生成 N 个新的行，这些行所包含的 TableB 的字段值是重复的。</li><li>如果 TableB 中的某条记录在 TableA 中没有匹配记录，那么在返回结果中仍然会生成一个新的行，只是该行所包含的 TableA 的字段值都是 NULL。<h3 id="4-左外连接-with-exclusion（Left-Outer-Join-with-exclusion"><a href="#4-左外连接-with-exclusion（Left-Outer-Join-with-exclusion" class="headerlink" title="4. 左外连接 with exclusion（Left Outer Join with exclusion)"></a>4. 左外连接 with exclusion（Left Outer Join with exclusion)</h3>在左外连接的基础上，去除TableB可匹配到的部分，只返回B.Key为NULL的记录。<h3 id="5-右外连接-with-exclusion（Right-Outer-Join-with-exclusion"><a href="#5-右外连接-with-exclusion（Right-Outer-Join-with-exclusion" class="headerlink" title="5. 右外连接 with exclusion（Right Outer Join with exclusion)"></a>5. 右外连接 with exclusion（Right Outer Join with exclusion)</h3>在右外连接的基础上，去除TableA可匹配到的部分，只返回A.Key为NULL的记录。<h3 id="6-全外连接（Full-Outer-Join）"><a href="#6-全外连接（Full-Outer-Join）" class="headerlink" title="6. 全外连接（Full Outer Join）"></a>6. 全外连接（Full Outer Join）</h3>FULL OUTER JOIN 先执行 LEFT OUTER JOIN 遍历左表，再执行 RIGHT OUTER JOIN 遍历右表，最后将 RIGHT OUTER JOIN 的结果直接追加到 LEFT OUTER JOIN 后面。注意，FULL OUTER JOIN 会返回重复的行，它们会被保留，不会被删除。<h3 id="7-全外连接-with-exclusion（Full-Outer-Join-with-exclusion）"><a href="#7-全外连接-with-exclusion（Full-Outer-Join-with-exclusion）" class="headerlink" title="7. 全外连接 with exclusion（Full Outer Join with exclusion）"></a>7. 全外连接 with exclusion（Full Outer Join with exclusion）</h3>两表的FULL OUTER JOIN去除重合部分，也就是返回 左外连接 with exclusion 和 右外连接 with exclusion 的 FULL OUTER JOIN 记录。</li></ul><h2 id="MySQL支持的Join方式"><a href="#MySQL支持的Join方式" class="headerlink" title="MySQL支持的Join方式"></a>MySQL支持的Join方式</h2><p>在聊这个之前，先简单了解一下<strong>驱动表和被驱动表</strong>的概念。在LEFT OUTER JOIN时，左表为驱动表，右表为被驱动表；在RIGHT OUTER JOIN时，右表为驱动表，左表为被驱动表。关于驱动表和被驱动表的作用，实际上是与MySQL表关联算法和SQL优化有关的，通常来说，用小表<strong>驱动</strong>大表能够获得更高的效率，这里不详细展开了。</p><p>以MySQL8.0.11为例，MySQL提供的JOIN关键字有：<code>JOIN</code>、<code>INNER JOIN</code>、<code>LEFT JOIN</code>、<code>LEFT OUTER JOIN</code>、<code>RIGHT JOIN</code>、<code>RIGHT OUTER JOIN</code>、<code>CROSS JOIN</code>和<code>STRAIGHT_JOIN</code>。</p><p>其中，<code>JOIN</code>和<code>INNER JOIN</code>为内连接，<code>LEFT JOIN</code>与<code>LEFT OUTER JOIN</code>是等价的，都对应着左外连接（右也是一样的道理）。也就是说，上面提到的七种JOIN方式，MySQL关键字只支持前三种，对于4、5可以结合WHERE来实现，但不提供全外连接关键字。</p><p>这三种（或六个）关键字的通用Join查询结构如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">&lt;</span>row_list<span class="operator">&gt;</span> </span><br><span class="line">  <span class="keyword">FROM</span> <span class="operator">&lt;</span>left_table<span class="operator">&gt;</span> </span><br><span class="line">    <span class="operator">&lt;</span><span class="keyword">inner</span><span class="operator">|</span><span class="keyword">left</span><span class="operator">|</span><span class="keyword">right</span><span class="operator">&gt;</span> <span class="keyword">JOIN</span> <span class="operator">&lt;</span>right_table<span class="operator">&gt;</span> </span><br><span class="line">      <span class="keyword">ON</span> <span class="operator">&lt;</span>join_condition<span class="operator">&gt;</span> </span><br><span class="line">        <span class="keyword">WHERE</span> <span class="operator">&lt;</span>where_condition<span class="operator">&gt;</span></span><br></pre></td></tr></table></figure><p>可以看到这里有两种条件，分别是<strong>join_condition</strong>和<strong>where_condition</strong>，两者执行存在先后顺序。数据库通过JOIN关键字返回记录时会先生成一张临时表，通过临时表返回记录，<strong>join_condition</strong>是在生成临时表时使用的条件，而<strong>where_condition</strong>是在临时表生成后再对其进行过滤的条件。以<code>LEFT JOIN</code>为例，在生成临时表时无论<strong>join_condition</strong>是否为真都会将左表记录加入到临时表中，所以“左表中的<strong>所有记录</strong>都会被返回”。</p><p>那么<code>CROSS JOIN</code>和<code>STRAIGHT_JOIN</code>又是什么呢？</p><p><code>CROSS JOIN</code>子句从连接的表返回行的笛卡儿积，它的通用查询结构如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">&lt;</span>row_list<span class="operator">&gt;</span> </span><br><span class="line">  <span class="keyword">FROM</span> <span class="operator">&lt;</span>left_table<span class="operator">&gt;</span> </span><br><span class="line">    <span class="keyword">CROSS</span> <span class="keyword">JOIN</span> <span class="operator">&lt;</span>right_table<span class="operator">&gt;</span> </span><br><span class="line">        <span class="keyword">ON</span> <span class="operator">&lt;</span>join_condition<span class="operator">&gt;</span></span><br><span class="line">            <span class="keyword">WHERE</span> <span class="operator">&lt;</span>where_condition<span class="operator">&gt;</span></span><br></pre></td></tr></table></figure><p>注意，仅当不添加<strong>join_condition</strong>和<strong>where_condition</strong>的时候，<code>CROSS JOIN</code>才能返回笛卡尔积，如果添加了这些条件，那么工作方式将和<code>JOIN</code>相同。</p><p>至于<code>STRAIGHT_JOIN</code>，其实是提供给用户一种自主决定驱动表与被驱动表关系的方式，它的用法与<code>JOIN</code>相同，只是<code>STRAIGHT_JOIN</code>前面的表一定是驱动表，后面的表一定是被驱动表。而在MySQL中，<code>JOIN</code>会自动选择小表作为驱动表，大表作为被驱动表。用户可以使用<code>STRAIGHT_JOIN</code>来解决MySQL优化器不能解决的部分。</p><p>关于全外连接以及其他各种连接方式在MySQL中的实现，我找到了一张图，是由Steve Stedman制作的，供读者参考。</p><p><img src="/assets/post_img/article77/MySQL-Join.png" alt="MySQLJoinType"></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1]<a href="https://blog.csdn.net/lamanchas/article/details/121366276">https://blog.csdn.net/lamanchas/article/details/121366276</a><br>[2]<a href="https://blog.csdn.net/asd051377305/article/details/115320564">https://blog.csdn.net/asd051377305/article/details/115320564</a><br>[3]<a href="http://c.biancheng.net/sql/join.html">http://c.biancheng.net/sql/join.html</a><br>[4]<a href="https://cloud.tencent.com/developer/article/1167929">https://cloud.tencent.com/developer/article/1167929</a><br>[5]<a href="https://www.jianshu.com/p/76c90b03b7bd">https://www.jianshu.com/p/76c90b03b7bd</a><br>[6]<a href="https://blog.csdn.net/javaanddonet/article/details/109693672">https://blog.csdn.net/javaanddonet/article/details/109693672</a><br>[7]<a href="https://blog.csdn.net/weixin_37692493/article/details/106970429">https://blog.csdn.net/weixin_37692493/article/details/106970429</a></p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于 <a href="https://silencezheng.top">silencezheng.top</a>，转载请注明出处。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;业精于勤，荒于嬉。&lt;/p&gt;</summary>
    
    
    
    
    <category term="MySQL" scheme="http://silencezheng.top/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>二叉树非递归遍历</title>
    <link href="http://silencezheng.top/2022/11/09/article76/"/>
    <id>http://silencezheng.top/2022/11/09/article76/</id>
    <published>2022-11-09T13:41:12.000Z</published>
    <updated>2022-11-11T02:21:08.306Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>不积跬步，无以至千里。</p><span id="more"></span><p>用Java写一下二叉树的非递归遍历，用print表示操作了，主要关注算法。</p><p>树定义如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TreeNode</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> val;</span><br><span class="line">        TreeNode left;</span><br><span class="line">        TreeNode right;</span><br><span class="line">        TreeNode() &#123;&#125;</span><br><span class="line">        TreeNode(<span class="keyword">int</span> val) &#123; <span class="keyword">this</span>.val = val; &#125;</span><br><span class="line">        TreeNode(<span class="keyword">int</span> val, TreeNode left, TreeNode right) &#123;</span><br><span class="line">          <span class="keyword">this</span>.val = val;</span><br><span class="line">          <span class="keyword">this</span>.left = left;</span><br><span class="line">          <span class="keyword">this</span>.right = right;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h2 id="先序遍历（直觉版）"><a href="#先序遍历（直觉版）" class="headerlink" title="先序遍历（直觉版）"></a>先序遍历（直觉版）</h2><p>思路：根左右，从根节点开始先走到最左下节点，然后依次出栈，如果出栈的节点有右子节点则对右子节点再走到最左下，直至栈空。</p><p>步骤：</p><ol><li>对当前节点走到最左下，每次对当前节点操作+入栈。</li><li>元素出栈，若有右子节点则重复1，若无则重复2。</li><li>栈空结束。</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">preorderTraversal</span><span class="params">(TreeNode root)</span></span>&#123;</span><br><span class="line">        Stack&lt;TreeNode&gt; nodeStack = <span class="keyword">new</span> Stack&lt;&gt;();</span><br><span class="line">        <span class="keyword">if</span>(root == <span class="keyword">null</span>)&#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;Null root error!&quot;</span>);</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            TreeNode temp = root;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> (temp!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                <span class="comment">// 操作</span></span><br><span class="line">                System.out.println(temp.val);</span><br><span class="line">                nodeStack.add(temp);</span><br><span class="line">                temp = temp.left;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> (!nodeStack.isEmpty())&#123;</span><br><span class="line">                temp = nodeStack.pop();</span><br><span class="line">                <span class="keyword">if</span> (temp.right!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                    temp = temp.right;</span><br><span class="line">                    <span class="keyword">while</span> (temp!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                        <span class="comment">// 操作</span></span><br><span class="line">                        System.out.println(temp.val);</span><br><span class="line">                        nodeStack.add(temp);</span><br><span class="line">                        temp = temp.left;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>这个版本的基础上可以进行各种优化。</p><h2 id="中序遍历（直觉版）"><a href="#中序遍历（直觉版）" class="headerlink" title="中序遍历（直觉版）"></a>中序遍历（直觉版）</h2><p>思路：左根右，从根节点走到最左下节点，然后依次出栈并操作，如果出栈的节点有右节点则对右节点再走到最左下，直至栈空。</p><p>步骤：</p><ol><li>对当前节点走到最左下，每次对当前节点入栈。</li><li>元素出栈，操作当前节点，若有右子节点则重复1，若无则重复2。</li><li>栈空结束。</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">inorderTraversal</span><span class="params">(TreeNode root)</span></span>&#123;</span><br><span class="line">        Stack&lt;TreeNode&gt; nodeStack = <span class="keyword">new</span> Stack&lt;&gt;();</span><br><span class="line">        <span class="keyword">if</span>(root == <span class="keyword">null</span>)&#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;Null root error!&quot;</span>);</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            TreeNode temp = root;</span><br><span class="line">            <span class="keyword">while</span> (temp!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                nodeStack.add(temp);</span><br><span class="line">                temp = temp.left;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            temp = nodeStack.pop();</span><br><span class="line">            <span class="keyword">while</span> (temp!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                <span class="comment">// 操作</span></span><br><span class="line">                System.out.println(temp.val);</span><br><span class="line">                <span class="keyword">if</span>(temp.right!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                    temp = temp.right;</span><br><span class="line">                    <span class="keyword">while</span> (temp!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                        nodeStack.add(temp);</span><br><span class="line">                        temp = temp.left;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (nodeStack.isEmpty())&#123;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                temp = nodeStack.pop();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>这个版本的基础上可以进行各种优化。</p><h2 id="后序遍历（直觉版）"><a href="#后序遍历（直觉版）" class="headerlink" title="后序遍历（直觉版）"></a>后序遍历（直觉版）</h2><p>思路：左右根，后序不能采用先序和中序的同款算法的主要原因是判断到当前节点存在右子树时，则不能对当前节点进行操作，而需要先对右子树做后序遍历，而即便是保留当前节点，并把右子树遍历完毕后，再对当前节点进行操作，仍然需要对当前节点的右子树是否已被遍历的状态进行判断，判断的依据是上一次操作的节点是否是右子节点。</p><p>步骤：</p><ol><li>对当前节点走到最左下，每次对当前节点入栈。</li><li>元素出栈，判断当前节点，若无右子树则操作并标记当前节点，若有右子树则判断右子树是否被访问过，若未被访问则保留当前节点状态，并依次入栈右子树左支，标记右子节点；若已访问过则操作当前节点，并标记当前节点。重复2。</li><li>栈空结束。</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">postorderTraversal</span><span class="params">(TreeNode root)</span></span>&#123;</span><br><span class="line">        Stack&lt;TreeNode&gt; nodeStack = <span class="keyword">new</span> Stack&lt;&gt;();</span><br><span class="line">        <span class="keyword">if</span>(root == <span class="keyword">null</span>)&#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;Null root error!&quot;</span>);</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            TreeNode temp = root;</span><br><span class="line">            TreeNode mark = <span class="keyword">null</span>;</span><br><span class="line">            <span class="keyword">while</span> (temp!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                nodeStack.add(temp);</span><br><span class="line">                temp = temp.left;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            temp = nodeStack.pop();</span><br><span class="line">            <span class="keyword">while</span> (temp!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                <span class="keyword">if</span>(temp.right==<span class="keyword">null</span>)&#123;</span><br><span class="line">                    <span class="comment">// 操作</span></span><br><span class="line">                    System.out.println(temp.val);</span><br><span class="line">                    <span class="comment">// 标记当前节点</span></span><br><span class="line">                    mark = temp;</span><br><span class="line">                &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                    <span class="comment">// 当前节点含右子树的情况，判断前次处理节点是否是右子节点。</span></span><br><span class="line">                    <span class="keyword">if</span>(temp.right==mark)&#123;</span><br><span class="line">                        <span class="comment">// 操作</span></span><br><span class="line">                        System.out.println(temp.val);</span><br><span class="line">                        <span class="comment">// 标记当前节点</span></span><br><span class="line">                        mark = temp;</span><br><span class="line">                    &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">                        <span class="comment">// 保留状态</span></span><br><span class="line">                        nodeStack.add(temp);</span><br><span class="line">                        <span class="comment">// 入栈右子树的左支，并标记右子节点。</span></span><br><span class="line">                        temp = temp.right;</span><br><span class="line">                        mark = temp;</span><br><span class="line">                        <span class="keyword">while</span> (temp!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                            nodeStack.add(temp);</span><br><span class="line">                            temp = temp.left;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (nodeStack.isEmpty())&#123;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                temp = nodeStack.pop();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>这个版本的基础上可以进行各种优化。</p><h2 id="层序遍历"><a href="#层序遍历" class="headerlink" title="层序遍历"></a>层序遍历</h2><p>思路：按从上到下，从左到右的顺序遍历。用队列实现，先入当前节点，出队再入左、右两子节点，然后每出一个就入队该节点的左、右子节点，直到队空。</p><p>步骤：</p><ol><li>入队当前节点。</li><li>出队一个节点，入队该节点的左、右子节点，重复2。</li><li>队空结束。</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">layerSequenceTraversal</span><span class="params">(TreeNode root)</span></span>&#123;</span><br><span class="line">        LinkedBlockingQueue&lt;TreeNode&gt; que =  <span class="keyword">new</span> LinkedBlockingQueue&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(root == <span class="keyword">null</span>)&#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;Null root error!&quot;</span>);</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            que.offer(root);</span><br><span class="line">            TreeNode temp = <span class="keyword">null</span>;</span><br><span class="line">            <span class="keyword">while</span> (!que.isEmpty())&#123;</span><br><span class="line">                temp = que.poll();</span><br><span class="line">                System.out.println(temp.val);</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (temp.left!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                    que.offer(temp.left);</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (temp.right!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                    que.offer(temp.right);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>先把我个人认为符合直觉的遍历方法写一下，看起来比较复杂但是容易理解，后面再进行优化补充。</p><p>首发于 <a href="https://silencezheng.top">silencezheng.top</a>。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;不积跬步，无以至千里。&lt;/p&gt;</summary>
    
    
    
    
    <category term="数据结构与算法" scheme="http://silencezheng.top/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>深度学习常用术语解释</title>
    <link href="http://silencezheng.top/2022/11/08/article75/"/>
    <id>http://silencezheng.top/2022/11/08/article75/</id>
    <published>2022-11-08T14:00:16.000Z</published>
    <updated>2022-11-08T14:00:52.846Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>深度学习常用术语解释，持续更新～<br><span id="more"></span></p><h2 id="Backbone"><a href="#Backbone" class="headerlink" title="Backbone"></a>Backbone</h2><p>主干网络，或称骨干网络，通常是网络的一部分，大多时候指的是提取特征的网络，其作用就是提取图片中的信息，供后面的网络使用。这些网络经常使用的是ResNet、VGG等，而不是我们自己设计的网络，因为这些网络已经证明了在分类等问题上的特征提取能力是很强的。在用这些网络作为Backbone的时候，都是直接加载官方已经训练好的模型参数，后面接着我们自己的网络。让网络的这两个部分同时进行训练，因为加载的Backbone模型已经具有提取特征的能力了，在我们的训练过程中，会对它进行微调，使得其更适合于我们自己的任务。</p><h2 id="Head"><a href="#Head" class="headerlink" title="Head"></a>Head</h2><p>Head即整个网络的头部，是获取网络输出内容的网络，利用之前（Backbone）提取的特征，做出预测。</p><h2 id="Neck"><a href="#Neck" class="headerlink" title="Neck"></a>Neck</h2><p>是指放在Backbone和Head之间的层，是为了更好的利用Backbone提取的特征。</p><h2 id="Pretext-task"><a href="#Pretext-task" class="headerlink" title="Pretext task"></a>Pretext task</h2><p>用于预训练的任务，可以翻译为前置任务或代理任务。</p><h2 id="Downstream-task"><a href="#Downstream-task" class="headerlink" title="Downstream task"></a>Downstream task</h2><p>下游任务，用于微调的任务。</p><h2 id="Warm-up"><a href="#Warm-up" class="headerlink" title="Warm up"></a>Warm up</h2><p>用一个小的学习率先训练几个epoch，这是因为网络的参数是随机初始化的，一开始就采用较大的学习率容易数值不稳定。</p><h2 id="End-to-End"><a href="#End-to-End" class="headerlink" title="End to End"></a>End to End</h2><p>端到端，给一个输入，获得一个输出，中间的处理过程处于黑箱中，相当于打包成应用了。</p><h2 id="Standardization"><a href="#Standardization" class="headerlink" title="Standardization"></a>Standardization</h2><p>标准化，指将数据按比例缩放，使其落入一个小区间中，缩放后均值为$0$，方差为$1$。在某些比较和评价的指标处理中经常会用到，去除数据的单位限制，将其转化为无量纲的纯数值，便于不同单位或量级的指标能够进行比较和加权。</p><p>例如在数据预处理时为了将所有特征放在一个共同的尺度上，会通过<strong>将特征重新缩放到零均值和单位方差</strong>来标准化数据。这既能方便优化，又能避免惩罚分配给某一特征的系数超过其他特征（一视同仁）。</p><p>在训练过程中，对输入进行规范化可以加速深度网络权重参数的收敛速度。</p><h2 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h2><p>这个词真的需要好好理解一下，其实首先应该想到翻译为“规范化”，它包括归一化、标准化甚至正则化，作为一个统称。比如Batch Normalization其实做的是Standardization的事，所以翻译成批量规范化或者批量标准化。</p><p>其次这个词又可以指归一化，即把数值放缩到$0$到$1$的小区间中。归一化可以消除量纲对最终结果的影响，使不同变量具有可比性。</p><p>关于这个词的解读是要具体问题具体分析了，甚至有时Standardization也被作为统称。</p><h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><p>正则化，一般形式是在整个平均损失函数的最后增加一个正则项（比如L2范数正则化，也有其他形式的正则化，作用不同）。正则项越大表明惩罚力度越大，等于0表示不做惩罚。</p><h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p>[1]<a href="https://zhuanlan.zhihu.com/p/348800083">https://zhuanlan.zhihu.com/p/348800083</a><br>[2]<a href="https://zhuanlan.zhihu.com/p/343692147">https://zhuanlan.zhihu.com/p/343692147</a><br>[3]<a href="https://blog.csdn.net/u014381464/article/details/81101551">https://blog.csdn.net/u014381464/article/details/81101551</a></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;深度学习常用术语解释，持续更新～&lt;br&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://silencezheng.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="机器学习" scheme="http://silencezheng.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>注意力机制--《动手学深度学习》笔记0x0B</title>
    <link href="http://silencezheng.top/2022/11/07/article74/"/>
    <id>http://silencezheng.top/2022/11/07/article74/</id>
    <published>2022-11-07T07:25:27.000Z</published>
    <updated>2022-11-07T07:27:27.834Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><p>灵长类动物的视觉系统接受了大量的感官输入，这些感官输入远远超过了大脑能够完全处理的程度。然而，并非所有刺激的影响都是相等的。意识的聚集和专注使灵长类动物能够在复杂的视觉环境中将注意力引向感兴趣的物体，例如猎物和天敌。只关注一小部分信息的能力对进化更加有意义，使人类得以生存和成功。</p><p>自19世纪以来，科学家们一直致力于研究认知神经科学领域的注意力。本章的很多章节将涉及到一些研究。<br><span id="more"></span><br>本章首先回顾一个经典注意力框架，解释如何在视觉场景中展开注意力。受此框架中的<em>注意力提示</em>（attention cues）的启发，我们将设计能够利用这些注意力提示的模型。1964年的Nadaraya-Waston核回归（kernel regression）正是具有<em>注意力机制</em>（attention mechanism）的机器学习的简单演示。</p><p>然后继续介绍注意力函数，它们在深度学习的注意力模型设计中被广泛使用。具体来说将展示如何使用这些函数来设计<em>Bahdanau注意力</em>。Bahdanau注意力是深度学习中的具有突破性价值的注意力模型，它双向对齐并且可以微分。</p><p>最后将描述仅仅基于注意力机制的<em>Transformer</em>架构，该架构中使用了<em>多头注意力</em>（multi-head attention）和<em>自注意力</em>（self-attention）。自2017年横空出世，Transformer一直都普遍存在于现代的深度学习应用中，例如语言、视觉、语音和强化学习领域。</p><p>这一章目前只做了解，关于NLP的内容没有实验，也没有详细调查。</p><h3 id="0-1-小结"><a href="#0-1-小结" class="headerlink" title="0.1. 小结"></a>0.1. 小结</h3><ul><li>人类的注意力是有限的、有价值和稀缺的资源。</li><li>受试者使用非自主性和自主性提示有选择性地引导注意力。前者基于突出性，后者则依赖于主体的意识。</li><li>注意力机制与全连接层或者池化层的区别源于增加的自主提示。</li><li>由于包含了自主性提示，注意力机制与全连接的层或池化层不同。</li><li>注意力机制通过注意力池化使选择偏向于值（感官输入），其中包含查询（自主性提示）和键（非自主性提示）。键和值是成对的。</li><li>我们可以可视化查询和键之间的注意力权重。</li><li>Nadaraya-Watson核回归是具有注意力机制的机器学习范例。</li><li>Nadaraya-Watson核回归的注意力池化是对训练数据中输出的加权平均。从注意力的角度来看，分配给每个值的注意力权重取决于将值所对应的键和查询作为输入的函数。</li><li>注意力池化可以分为非参数型和带参数型</li><li>注意力池化的输出可以计算为值的加权平均，选择不同的注意力评分函数会带来不同的注意力池化操作。</li><li>当查询和键是不同长度的矢量时，可以使用<em>加性注意力评分函数</em>。当它们的长度相同时，使用<em>缩放的“点－积”注意力评分函数</em>的计算效率更高。</li><li>在预测词元时，如果不是所有输入词元都是相关的，那么具有Bahdanau注意力的循环神经网络编码器-解码器会有选择地统计输入序列的不同部分。这是通过将上下文变量视为加性注意力池化的输出来实现的。</li><li>在循环神经网络编码器-解码器中，Bahdanau注意力将上一时间步的解码器隐状态视为查询，在所有时间步的编码器隐状态同时视为键和值。</li><li>多头注意力融合了来自于多个注意力池化的不同知识，这些知识的不同来源于相同的查询、键和值的不同的子空间表示。</li><li>基于适当的张量操作，可以实现多头注意力的并行计算。</li><li>在自注意力中，查询、键和值都来自同一组输入。</li><li>卷积神经网络和自注意力都拥有并行计算的优势，而且自注意力的最大路径长度最短。但是因为其计算复杂度是关于序列长度的二次方，所以在很长的序列中计算会非常慢。</li><li>为了使用序列的顺序信息，我们可以通过在输入表示中添加位置编码，来注入绝对的或相对的位置信息。</li><li>transformer是编码器－解码器架构的一个实践，尽管在实际情况中编码器或解码器可以单独使用。</li><li>在transformer中，多头自注意力用于表示输入序列和输出序列，不过解码器必须通过掩蔽机制来保留自回归属性。</li><li>transformer中的残差连接和层规范化是训练非常深度模型的重要工具。</li><li>transformer模型中基于位置的前馈网络使用同一个多层感知机，作用是对所有序列位置的表示进行转换。</li></ul><h2 id="1-注意力提示"><a href="#1-注意力提示" class="headerlink" title="1. 注意力提示"></a>1. 注意力提示</h2><p>感谢读者对本书的关注，因为读者的注意力是一种稀缺的资源：此刻读者正在阅读本书（而忽略了其他的书），因此读者的注意力是用机会成本（与金钱类似）来支付的。为了确保读者现在投入的注意力是值得的，作者们尽全力（全部的注意力）创作一本好书。</p><p>自经济学研究稀缺资源分配以来，人们正处在“注意力经济”时代，即人类的注意力被视为可以交换的、有限的、有价值的且稀缺的商品。许多商业模式也被开发出来去利用这一点：在音乐或视频流媒体服务上，人们要么消耗注意力在广告上，要么付钱来隐藏广告；为了在网络游戏世界的成长，人们要么消耗注意力在游戏战斗中，从而帮助吸引新的玩家，要么付钱立即变得强大。总之，注意力不是免费的。</p><p>注意力是稀缺的，而环境中的干扰注意力的信息却并不少。比如人类的视觉神经系统大约每秒收到$10^8$位的信息，这远远超过了大脑能够完全处理的水平。幸运的是，人类的祖先已经从经验（也称为数据）中认识到“并非感官的所有输入都是一样的”。在整个人类历史中，这种只将注意力引向感兴趣的一小部分信息的能力，使人类的大脑能够更明智地分配资源来生存、成长和社交，例如发现天敌、找寻食物和伴侣。</p><h3 id="1-1-生物学中的注意力提示"><a href="#1-1-生物学中的注意力提示" class="headerlink" title="1.1. 生物学中的注意力提示"></a>1.1. 生物学中的注意力提示</h3><p>注意力是如何应用于视觉世界中的呢？这要从当今十分普及的<em>双组件</em>（two-component）的框架开始讲起：这个框架的出现可以追溯到19世纪90年代的威廉·詹姆斯，他被认为是“美国心理学之父” [<code>James.2007</code>]。在这个框架中，受试者基于<em>非自主性提示</em>和<em>自主性提示</em>有选择地引导注意力的焦点。</p><p>非自主性提示是基于环境中物体的突出性和易见性。想象一下，假如我们面前有五个物品：一份报纸、一篇研究论文、一杯咖啡、一本笔记本和一本书，就像下图。所有纸制品都是黑白印刷的，但咖啡杯是红色的。换句话说，这个咖啡杯在这种视觉环境中是突出和显眼的，不由自主地引起人们的注意。所以我们会把视力最敏锐的地方放到咖啡上，如下图所示。</p><p><img src="/assets/post_img/article74/eye-coffee.svg" alt="由于突出性的非自主性提示（红杯子），注意力不自主地指向了咖啡杯"></p><p>喝咖啡后，我们会变得兴奋并想读书，所以转过头，重新聚焦眼睛，然后看看书，就像下图中描述那样。与上图中由于突出性导致的选择不同，此时选择书是受到了认知和意识的控制，因此注意力在基于自主性提示去辅助选择时将更为谨慎。受试者的主观意愿推动，选择的力量也就更强大。</p><p><img src="/assets/post_img/article74/eye-book.svg" alt="依赖于任务的意志提示（想读一本书），注意力被自主引导到书上"></p><h3 id="1-2-查询、键和值"><a href="#1-2-查询、键和值" class="headerlink" title="1.2. 查询、键和值"></a>1.2. 查询、键和值</h3><p>自主性的与非自主性的注意力提示解释了人类注意力的方式，下面来看看如何通过这两种注意力提示，用神经网络来设计注意力机制的框架。</p><p>首先，对于只使用非自主性提示的情况。要想将选择偏向于感官输入，则可以简单地使用参数化的全连接层，甚至是非参数化的最大池化层或平均池化层。个人理解，这就是说，这种情况下不需要在以往的神经网络上做出修改，因为非自主性提示来自客体的差异。</p><p>因此，“是否包含自主性提示”将<strong>注意力机制</strong>与全连接层或池化层区别开来。在注意力机制的背景下，自主性提示被称为<em>查询</em>（query）。给定任何查询，注意力机制通过<em>注意力池化</em>（attention pooling）将选择引导至<em>感官输入</em>（sensory inputs，例如中间特征表示）。在注意力机制中，这些感官输入被称为<em>值</em>（value）。更通俗的解释是，每个值都与一个感官输入的非自主提示配对，这些对应的非自主性提示称为<em>键</em>（key）。如下图所示，可以通过设计注意力池化的方式，使给定的查询（自主性提示）与键（非自主性提示）进行匹配，这将引导得出最匹配的值（感官输入）。</p><p><img src="/assets/post_img/article74/qkv.svg" alt="注意力机制通过注意力池化将*查询*（自主性提示）和*键*（非自主性提示）结合在一起，实现对*值*（感官输入）的选择倾向"></p><p>鉴于上面所提框架在上图中的主导地位，因此这个框架下的模型将成为本章的中心。然而，注意力机制的设计有许多替代方案。例如可以设计一个不可微的注意力模型，该模型可以使用强化学习方法 [<code>Mnih.Heess.Graves.ea.2014</code>]进行训练。</p><h3 id="1-3-注意力的可视化"><a href="#1-3-注意力的可视化" class="headerlink" title="1.3. 注意力的可视化"></a>1.3. 注意力的可视化</h3><p>平均池化层可以被视为输入的加权平均值，其中各输入的权重是一样的。实际上，注意力池化得到的是加权平均的总和值，其中权重是在给定的查询和不同的键之间计算得出的。</p><p>为了可视化注意力权重，需要定义一个<code>show_heatmaps</code>函数。其输入<code>matrices</code>的形状是（要显示的行数，要显示的列数，查询的数目，键的数目）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_heatmaps</span>(<span class="params">matrices, xlabel, ylabel, titles=<span class="literal">None</span>, figsize=(<span class="params"><span class="number">2.5</span>, <span class="number">2.5</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="function">                  cmap=<span class="string">&#x27;Reds&#x27;</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;显示矩阵热图&quot;&quot;&quot;</span></span><br><span class="line">    d2l.use_svg_display()</span><br><span class="line">    num_rows, num_cols = matrices.shape[<span class="number">0</span>], matrices.shape[<span class="number">1</span>]</span><br><span class="line">    fig, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize,</span><br><span class="line">                                 sharex=<span class="literal">True</span>, sharey=<span class="literal">True</span>, squeeze=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">for</span> i, (row_axes, row_matrices) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(axes, matrices)):</span><br><span class="line">        <span class="keyword">for</span> j, (ax, matrix) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(row_axes, row_matrices)):</span><br><span class="line">            pcm = ax.imshow(matrix.detach().numpy(), cmap=cmap)</span><br><span class="line">            <span class="keyword">if</span> i == num_rows - <span class="number">1</span>:</span><br><span class="line">                ax.set_xlabel(xlabel)</span><br><span class="line">            <span class="keyword">if</span> j == <span class="number">0</span>:</span><br><span class="line">                ax.set_ylabel(ylabel)</span><br><span class="line">            <span class="keyword">if</span> titles:</span><br><span class="line">                ax.set_title(titles[j])</span><br><span class="line">    fig.colorbar(pcm, ax=axes, shrink=<span class="number">0.6</span>);</span><br></pre></td></tr></table></figure><p>下面使用一个简单的例子进行演示，本例中，仅当查询和键相同时（即客体特征符合主体意识时），注意力权重为1，否则为0。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">attention_weights = torch.eye(<span class="number">10</span>).reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">show_heatmaps(attention_weights, xlabel=<span class="string">&#x27;Keys&#x27;</span>, ylabel=<span class="string">&#x27;Queries&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_attention-cues_1.svg" alt="输出"></p><p>后面的章节将经常调用<code>show_heatmaps</code>函数来显示注意力权重。</p><h2 id="2-注意力池化：Nadaraya-Watson-核回归"><a href="#2-注意力池化：Nadaraya-Watson-核回归" class="headerlink" title="2. 注意力池化：Nadaraya-Watson 核回归"></a>2. 注意力池化：Nadaraya-Watson 核回归</h2><p>上节介绍了框架下的注意力机制的主要成分：查询（自主提示）和键（非自主提示）之间的交互形成了注意力池化；注意力池化有选择地聚合了值（感官输入）以生成最终的输出。本节将介绍注意力池化的更多细节，以便从宏观上了解注意力机制在实践中的运作方式。1964年提出的Nadaraya-Watson核回归模型是一个简单但完整的例子，可以用于演示具有注意力机制的机器学习。</p><h3 id="2-1-生成数据集"><a href="#2-1-生成数据集" class="headerlink" title="2.1. 生成数据集"></a>2.1. 生成数据集</h3><p>简单起见，考虑这个回归问题：给定的成对的“输入－输出”数据集${(x_1, y_1), \ldots, (x_n, y_n)}$，如何学习$f$来预测任意新输入$x$的输出$\hat{y} = f(x)$？</p><p>根据下面的非线性函数生成一个人工数据集，其中加入的噪声项为$\epsilon$：</p><script type="math/tex; mode=display">y_i = 2\sin(x_i) + x_i^{0.8} + \epsilon,</script><p>其中$\epsilon$服从均值为$0$和标准差为$0.5$的正态分布。在这里生成了$50$个训练样本和$50$个测试样本。为了更好地可视化之后的注意力模式，需要将训练样本进行排序。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">n_train = <span class="number">50</span>  <span class="comment"># 训练样本数</span></span><br><span class="line"><span class="comment"># torch.sort返回排序后的张量和原张量在排序后张量中的对应索引</span></span><br><span class="line">x_train, _ = torch.sort(torch.rand(n_train) * <span class="number">5</span>)   <span class="comment"># 排序后的训练样本</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * torch.sin(x) + x**<span class="number">0.8</span></span><br><span class="line"></span><br><span class="line">y_train = f(x_train) + torch.normal(<span class="number">0.0</span>, <span class="number">0.5</span>, (n_train,))  <span class="comment"># 训练样本的输出</span></span><br><span class="line">x_test = torch.arange(<span class="number">0</span>, <span class="number">5</span>, <span class="number">0.1</span>)  <span class="comment"># 测试样本</span></span><br><span class="line">y_truth = f(x_test)  <span class="comment"># 测试样本的真实输出</span></span><br><span class="line">n_test = <span class="built_in">len</span>(x_test)  <span class="comment"># 测试样本数</span></span><br></pre></td></tr></table></figure><p>下面的函数将绘制所有的训练样本（样本由圆圈表示），不带噪声项的真实数据生成函数$f$（标记为“Truth”），以及学习得到的预测函数（标记为“Pred”）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_kernel_reg</span>(<span class="params">y_hat</span>):</span></span><br><span class="line">    d2l.plot(x_test, [y_truth, y_hat], <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;y&#x27;</span>, legend=[<span class="string">&#x27;Truth&#x27;</span>, <span class="string">&#x27;Pred&#x27;</span>],</span><br><span class="line">             xlim=[<span class="number">0</span>, <span class="number">5</span>], ylim=[-<span class="number">1</span>, <span class="number">5</span>])</span><br><span class="line">    d2l.plt.plot(x_train, y_train, <span class="string">&#x27;o&#x27;</span>, alpha=<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure><h3 id="2-2-平均池化"><a href="#2-2-平均池化" class="headerlink" title="2.2. 平均池化"></a>2.2. 平均池化</h3><p>先使用最简单的估计器来解决回归问题。基于平均池化来计算所有训练样本输出值的平均值：</p><script type="math/tex; mode=display">f(x) = \frac{1}{n}\sum_{i=1}^n y_i,</script><p>如下图所示，这个估计器确实不够聪明。真实函数$f$（“Truth”）和预测函数（“Pred”）相差很大。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里先求了一下训练集标签的平均值，得到一个单值张量（无维度）。</span></span><br><span class="line"><span class="comment"># torch.repeat_interleave(): 将输入张量按照指定维度进行扩展，若未指定维度则会将输入拉张开为1维向量再进行扩展。</span></span><br><span class="line"><span class="comment"># 这里没有指定dim，故先将单值张量转为1维张量，然后在该维度上复制成n_test个元素。</span></span><br><span class="line">y_hat = torch.repeat_interleave(y_train.mean(), n_test)</span><br><span class="line">plot_kernel_reg(y_hat)</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_nadaraya-waston_1.svg" alt="输出"></p><h3 id="2-3-非参数注意力池化"><a href="#2-3-非参数注意力池化" class="headerlink" title="2.3. 非参数注意力池化"></a>2.3. 非参数注意力池化</h3><p>显然，平均池化忽略了输入$x_i$。于是Nadaraya[<code>Nadaraya.1964</code>]和Watson[<code>Watson.1964</code>]提出了一个更好的想法，根据输入的位置对输出$y_i$进行加权：</p><script type="math/tex; mode=display">f(x) = \sum_{i=1}^n \frac{K(x - x_i)}{\sum_{j=1}^n K(x - x_j)} y_i,</script><p>其中$K$是<em>核</em>（kernel）。上式所描述的估计器被称为<em>Nadaraya-Watson核回归</em>（Nadaraya-Watson kernel regression）。这里不会深入讨论核函数的细节，但受此启发，我们可以从<a href="#12-查询键和值">第一节图中</a>的注意力机制框架的角度重写上式，成为一个更加通用的<em>注意力池化</em>（attention pooling）公式：</p><script type="math/tex; mode=display">f(x) = \sum_{i=1}^n \alpha(x, x_i) y_i,</script><p>其中$x$是查询，$(x_i, y_i)$是键值对。比较两个公式，注意力池化是$y_i$的加权平均。将查询$x$和键$x_i$之间的关系建模为<em>注意力权重</em>（attention weight）$\alpha(x, x_i)$，如上式所示，这个权重将被分配给每一个对应值$y_i$。对于任何查询，模型在所有键值对注意力权重都是一个有效的概率分布：它们是非负的，并且总和为1。</p><p>为了更好地理解注意力池化，考虑一个<em>高斯核</em>（Gaussian kernel），其定义为：</p><script type="math/tex; mode=display">K(u) = \frac{1}{\sqrt{2\pi}} \exp(-\frac{u^2}{2}).</script><p>将高斯核代入<em>Nadaraya-Watson核回归公式</em> 和 <em>注意力池化公式</em> 可以得到：</p><script type="math/tex; mode=display">\begin{aligned} f(x) &=\sum_{i=1}^n \alpha(x, x_i) y_i\\ &= \sum_{i=1}^n \frac{\exp\left(-\frac{1}{2}(x - x_i)^2\right)}{\sum_{j=1}^n \exp\left(-\frac{1}{2}(x - x_j)^2\right)} y_i \\&= \sum_{i=1}^n \mathrm{softmax}\left(-\frac{1}{2}(x - x_i)^2\right) y_i. \end{aligned}</script><p>在上式中，如果一个键$x_i$越是接近给定的查询$x$，那么分配给这个键对应值$y_i$的注意力权重就会越大，也就“获得了更多的注意力”。</p><p>这里穿插解释一下参数模型和非参数模型。</p><p>参数模型<br>: 在统计学中，参数模型通常假设总体服从某个分布，这个分布可以由一些参数确定，如正态分布由均值和标准差确定，在此基础上构建的模型称为参数模型。</p><p>非参数模型<br>: 非参数模型对于总体的分布不做任何假设或者说是数据分布假设自由，只知道其分布是存在的，所以就无法得到其分布的相关参数，只能通过非参数统计的方法进行推断。</p><p>总之，参数模型和非参数模型中的“参数”并不是模型中的参数，而是数据分布的参数。</p><p>Nadaraya-Watson核回归是一个非参数模型。因此，上式是<em>非参数的注意力池化</em>（nonparametric attention pooling）模型。接下来将基于这个非参数的注意力池化模型来绘制预测结果。从绘制的结果会发现新的模型预测线是平滑的，并且比平均池化的预测更接近真实。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># X_repeat的形状:(n_test,n_train),</span></span><br><span class="line"><span class="comment"># 每一行都包含着相同的测试输入（例如：同样的查询）</span></span><br><span class="line">X_repeat = x_test.repeat_interleave(n_train).reshape((-<span class="number">1</span>, n_train))</span><br><span class="line"><span class="comment"># x_train包含着键。attention_weights的形状：(n_test,n_train),</span></span><br><span class="line"><span class="comment"># 每一行都包含着要在给定的每个查询的值（y_train）之间分配的注意力权重</span></span><br><span class="line">attention_weights = nn.functional.softmax(-(X_repeat - x_train)**<span class="number">2</span> / <span class="number">2</span>, dim=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># y_hat的每个元素都是值的加权平均值，其中的权重是注意力权重</span></span><br><span class="line">y_hat = torch.matmul(attention_weights, y_train)</span><br><span class="line">plot_kernel_reg(y_hat)</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_nadaraya-waston_2.svg" alt="输出"></p><p>现在来观察注意力的权重。 这里测试数据的输入相当于查询，而训练数据的输入相当于键。 因为两个输入都是经过排序的，因此由观察可知“查询-键”对越接近，注意力池化的注意力权重就越高。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(attention_weights.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>),</span><br><span class="line">                  xlabel=<span class="string">&#x27;Sorted training inputs&#x27;</span>,</span><br><span class="line">                  ylabel=<span class="string">&#x27;Sorted testing inputs&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_nadaraya-waston_3.svg" alt="输出"></p><h3 id="2-4-带参数注意力池化"><a href="#2-4-带参数注意力池化" class="headerlink" title="2.4. 带参数注意力池化"></a>2.4. 带参数注意力池化</h3><p>非参数的Nadaraya-Watson核回归具有<em>一致性</em>（consistency）的优点：如果有足够的数据，此模型会收敛到最优结果。尽管如此，我们还是可以轻松地将可学习的参数集成到注意力池化中。</p><p>例如，与上一节略有不同，在下面的查询$x$和键$x_i$之间的距离乘以可学习参数$w$：</p><script type="math/tex; mode=display">\begin{aligned}f(x) &= \sum_{i=1}^n \alpha(x, x_i) y_i \\&= \sum_{i=1}^n \frac{\exp\left(-\frac{1}{2}((x - x_i)w)^2\right)}{\sum_{j=1}^n \exp\left(-\frac{1}{2}((x - x_j)w)^2\right)} y_i \\&= \sum_{i=1}^n \mathrm{softmax}\left(-\frac{1}{2}((x - x_i)w)^2\right) y_i \end{aligned}</script><p>本节的余下部分将通过训练这个模型来学习注意力池化的参数。</p><h4 id="2-4-1-批量矩阵乘法"><a href="#2-4-1-批量矩阵乘法" class="headerlink" title="2.4.1. 批量矩阵乘法"></a>2.4.1. 批量矩阵乘法</h4><p>为了更有效地计算小批量数据的注意力，可以利用深度学习开发框架中提供的批量矩阵乘法。</p><p>假设第一个小批量数据包含$n$个矩阵$\mathbf{X}_1,\ldots, \mathbf{X}_n$，第二个小批量包含$n$个矩阵$\mathbf{Y}_1, \ldots, \mathbf{Y}_n$，形状为$a\times b$，形状为$b\times c$。它们的批量矩阵乘法得到$n$个矩阵$\mathbf{X}_1\mathbf{Y}_1, \ldots, \mathbf{X}_n\mathbf{Y}_n$，形状为$a\times c$。因此，假定两个张量的形状分别是$(n,a,b)$和$(n,b,c)$，它们的批量矩阵乘法输出的形状为$(n,a,c)$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X = torch.ones((<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>))</span><br><span class="line">Y = torch.ones((<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>))</span><br><span class="line">torch.bmm(X, Y).shape</span><br></pre></td></tr></table></figure><p>在注意力机制的背景中，我们可以使用小批量矩阵乘法来计算小批量数据中的加权平均值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">weights = torch.ones((<span class="number">2</span>, <span class="number">10</span>)) * <span class="number">0.1</span></span><br><span class="line">values = torch.arange(<span class="number">20.0</span>).reshape((<span class="number">2</span>, <span class="number">10</span>))</span><br><span class="line"><span class="comment"># 权重在第一维升维，值在第二维升维，这里unsqueeze(-1) = unsqueeze(2)</span></span><br><span class="line">torch.bmm(weights.unsqueeze(<span class="number">1</span>), values.unsqueeze(-<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[[ <span class="number">4.5000</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">14.5000</span>]]])</span><br></pre></td></tr></table></figure><h4 id="2-4-2-定义模型"><a href="#2-4-2-定义模型" class="headerlink" title="2.4.2. 定义模型"></a>2.4.2. 定义模型</h4><p>基于上述的带参数的注意力池化，使用小批量矩阵乘法，定义Nadaraya-Watson核回归的带参数版本为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NWKernelRegression</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line">        self.w = nn.Parameter(torch.rand((<span class="number">1</span>,), requires_grad=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, queries, keys, values</span>):</span></span><br><span class="line">        <span class="comment"># queries和attention_weights的形状为(查询个数，“键－值”对个数)</span></span><br><span class="line">        queries = queries.repeat_interleave(keys.shape[<span class="number">1</span>]).reshape((-<span class="number">1</span>, keys.shape[<span class="number">1</span>]))</span><br><span class="line">        self.attention_weights = nn.functional.softmax(</span><br><span class="line">            -((queries - keys) * self.w)**<span class="number">2</span> / <span class="number">2</span>, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># values的形状为(查询个数，“键－值”对个数)</span></span><br><span class="line">        <span class="keyword">return</span> torch.bmm(self.attention_weights.unsqueeze(<span class="number">1</span>),</span><br><span class="line">                         values.unsqueeze(-<span class="number">1</span>)).reshape(-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h4 id="2-4-3-训练"><a href="#2-4-3-训练" class="headerlink" title="2.4.3. 训练"></a>2.4.3. 训练</h4><p>接下来，将训练数据集变换为键和值用于训练注意力模型。 在带参数的注意力池化模型中， 任何一个训练样本的输入都会和除自己以外的所有训练样本的“键－值”对进行计算， 从而得到其对应的预测输出。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># X_tile的形状:(n_train，n_train)，每一行都包含着相同的训练输入</span></span><br><span class="line">X_tile = x_train.repeat((n_train, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># Y_tile的形状:(n_train，n_train)，每一行都包含着相同的训练输出</span></span><br><span class="line">Y_tile = y_train.repeat((n_train, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># eye函数为了生成对角线全1，其余部分全0的二维数组。然后转化为对角线False的矩阵，从X_tile中去除了对角线元素后reshape为新的二维数组。</span></span><br><span class="line"><span class="comment"># keys的形状:(&#x27;n_train&#x27;，&#x27;n_train&#x27;-1)</span></span><br><span class="line">keys = X_tile[(<span class="number">1</span> - torch.eye(n_train)).<span class="built_in">type</span>(torch.<span class="built_in">bool</span>)].reshape((n_train, -<span class="number">1</span>))</span><br><span class="line"><span class="comment"># values的形状:(&#x27;n_train&#x27;，&#x27;n_train&#x27;-1)</span></span><br><span class="line">values = Y_tile[(<span class="number">1</span> - torch.eye(n_train)).<span class="built_in">type</span>(torch.<span class="built_in">bool</span>)].reshape((n_train, -<span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>训练带参数的注意力池化模型时，使用平方损失函数和随机梯度下降。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">net = NWKernelRegression()</span><br><span class="line">loss = nn.MSELoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.5</span>)</span><br><span class="line">animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>, xlim=[<span class="number">1</span>, <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    trainer.zero_grad()</span><br><span class="line">    l = loss(net(x_train, keys, values), y_train)</span><br><span class="line">    l.<span class="built_in">sum</span>().backward()</span><br><span class="line">    trainer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;<span class="built_in">float</span>(l.<span class="built_in">sum</span>()):<span class="number">.6</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    animator.add(epoch + <span class="number">1</span>, <span class="built_in">float</span>(l.<span class="built_in">sum</span>()))</span><br></pre></td></tr></table></figure><p>如下所示，训练完带参数的注意力池化模型后，我们发现： 在尝试拟合带噪声的训练数据时， 预测结果绘制的线不如之前非参数模型的平滑。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># keys的形状:(n_test，n_train)，每一行包含着相同的训练输入（例如，相同的键）</span></span><br><span class="line">keys = x_train.repeat((n_test, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># value的形状:(n_test，n_train)</span></span><br><span class="line">values = y_train.repeat((n_test, <span class="number">1</span>))</span><br><span class="line">y_hat = net(x_test, keys, values).unsqueeze(<span class="number">1</span>).detach()</span><br><span class="line">plot_kernel_reg(y_hat)</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_nadaraya-waston_4.svg" alt="输出"></p><p>为什么新的模型更不平滑了呢？ 来看一下输出结果的绘制图： 与非参数的注意力池化模型相比，带参数的模型加入可学习的参数后， 曲线在注意力权重较大的区域变得更不平滑。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(net.attention_weights.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>),</span><br><span class="line">                  xlabel=<span class="string">&#x27;Sorted training inputs&#x27;</span>,</span><br><span class="line">                  ylabel=<span class="string">&#x27;Sorted testing inputs&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_nadaraya-waston_5.svg" alt="5"></p><h2 id="3-注意力评分函数"><a href="#3-注意力评分函数" class="headerlink" title="3. 注意力评分函数"></a>3. 注意力评分函数</h2><p>上一节中，我们使用高斯核来对查询和键之间的关系建模。可以将其中的高斯核指数部分视为<strong>注意力评分函数</strong>（attention scoring function）， 简称<em>评分函数</em>（scoring function），然后把这个函数的输出结果输入到softmax函数中进行运算。 通过上述步骤，我们将得到与键对应的值的概率分布（即注意力权重）。 最后，注意力池化的输出就是基于这些注意力权重的值的加权和。</p><p>从宏观来看，我们可以使用上述算法来实现<a href="#12-查询键和值">1.2</a>中的注意力机制框架。下图说明了如何将注意力池化的输出计算成为值的加权和，其中 $a$ 表示注意力评分函数。 由于注意力权重是概率分布， 因此加权和其本质上是加权平均值。</p><p><img src="/assets/post_img/article74/attention-output.svg" alt="attention output"></p><p>用数学语言描述，假设有一个查询 $\mathbf{q} \in \mathbb{R}^q$ 和 $m$ 个“键－值”对 $(\mathbf{k}_1, \mathbf{v}_1), \ldots, (\mathbf{k}_m, \mathbf{v}_m)$，其中$\mathbf{k}_i \in \mathbb{R}^k$，$\mathbf{v}_i \in \mathbb{R}^v$。注意力池化函数 $f$ 就被表示成值的加权和：</p><script type="math/tex; mode=display">f(\mathbf{q}, (\mathbf{k}_1, \mathbf{v}_1), \ldots, (\mathbf{k}_m, \mathbf{v}_m)) = \sum_{i=1}^m \alpha(\mathbf{q}, \mathbf{k}_i) \mathbf{v}_i \in \mathbb{R}^v</script><p>其中查询$\mathbf{q}$和键$\mathbf{k}_i$的注意力权重（标量）是通过注意力评分函数$a$将两个向量映射成标量，再经过softmax运算得到的：</p><script type="math/tex; mode=display">\alpha(\mathbf{q}, \mathbf{k}_i) = \mathrm{softmax}(a(\mathbf{q}, \mathbf{k}_i)) = \frac{\exp(a(\mathbf{q}, \mathbf{k}_i))}{\sum_{j=1}^m \exp(a(\mathbf{q}, \mathbf{k}_j))} \in \mathbb{R}</script><p>正如上图所示，选择不同的注意力评分函数 $a$ 会导致不同的注意力池化操作。本节将介绍两个流行的评分函数，稍后将用他们来实现更复杂的注意力机制。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><h3 id="3-1-掩蔽softmax操作"><a href="#3-1-掩蔽softmax操作" class="headerlink" title="3.1. 掩蔽softmax操作"></a>3.1. 掩蔽softmax操作</h3><p>正如上面提到的，softmax操作用于输出一个概率分布作为注意力权重。在某些情况下，并非所有的值都应该被纳入到注意力池化中。例如，为了在“机器翻译与数据集”一节中高效处理小批量数据集，某些文本序列被填充了没有意义的特殊词元。为了仅将有意义的词元作为值来获取注意力池化，可以指定一个有效序列长度（即词元的个数），以便在计算softmax时过滤掉超出指定范围的位置。下面的<code>masked_softmax</code>函数实现了这样的<em>掩蔽softmax操作</em>（masked softmax operation），其中任何超出有效长度的位置都被掩蔽并置为0。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">masked_softmax</span>(<span class="params">X, valid_lens</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;通过在最后一个轴上掩蔽元素来执行softmax操作&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># X:3D张量，valid_lens:1D或2D张量</span></span><br><span class="line">    <span class="keyword">if</span> valid_lens <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> nn.functional.softmax(X, dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        shape = X.shape</span><br><span class="line">        <span class="keyword">if</span> valid_lens.dim() == <span class="number">1</span>:</span><br><span class="line">            valid_lens = torch.repeat_interleave(valid_lens, shape[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            valid_lens = valid_lens.reshape(-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 最后一轴上被掩蔽的元素使用一个非常大的负值替换，从而其softmax输出为0</span></span><br><span class="line">        X = d2l.sequence_mask(X.reshape(-<span class="number">1</span>, shape[-<span class="number">1</span>]), valid_lens,</span><br><span class="line">                              value=-<span class="number">1e6</span>)</span><br><span class="line">        <span class="keyword">return</span> nn.functional.softmax(X.reshape(shape), dim=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>为了演示此函数是如何工作的，考虑由两个$2 \times 4$矩阵表示的样本，这两个样本的有效长度分别为$2$和$3$。经过掩蔽softmax操作，超出有效长度的值都被掩蔽为0。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">masked_softmax(torch.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>), torch.tensor([<span class="number">2</span>, <span class="number">3</span>]))</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[[<span class="number">0.5423</span>, <span class="number">0.4577</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>],</span><br><span class="line">         [<span class="number">0.6133</span>, <span class="number">0.3867</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">0.3324</span>, <span class="number">0.2348</span>, <span class="number">0.4329</span>, <span class="number">0.0000</span>],</span><br><span class="line">         [<span class="number">0.2444</span>, <span class="number">0.3943</span>, <span class="number">0.3613</span>, <span class="number">0.0000</span>]]])</span><br></pre></td></tr></table></figure><p>也可以使用二维张量，为矩阵样本中的每一行指定有效长度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">masked_softmax(torch.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>), torch.tensor([[<span class="number">1</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">4</span>]]))</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[[<span class="number">1.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>],</span><br><span class="line">         [<span class="number">0.4142</span>, <span class="number">0.3582</span>, <span class="number">0.2275</span>, <span class="number">0.0000</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">0.5565</span>, <span class="number">0.4435</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>],</span><br><span class="line">         [<span class="number">0.3305</span>, <span class="number">0.2070</span>, <span class="number">0.2827</span>, <span class="number">0.1798</span>]]])</span><br></pre></td></tr></table></figure><h3 id="3-2-加性注意力"><a href="#3-2-加性注意力" class="headerlink" title="3.2. 加性注意力"></a>3.2. 加性注意力</h3><p>一般来说，当查询和键是<strong>不同长度</strong>的矢量时，可以使用加性注意力作为评分函数。给定查询$\mathbf{q} \in \mathbb{R}^q$和键$\mathbf{k} \in \mathbb{R}^k$，<em>加性注意力</em>（additive attention）的评分函数为</p><script type="math/tex; mode=display">a(\mathbf q, \mathbf k) = \mathbf w_v^\top \text{tanh}(\mathbf W_q\mathbf q + \mathbf W_k \mathbf k) \in \mathbb{R}</script><p>其中可学习的参数是$\mathbf W_q\in\mathbb R^{h\times q}$、$\mathbf W_k\in\mathbb R^{h\times k}$和$\mathbf w_v\in\mathbb R^{h}$。如上式所示，将查询和键连结起来后输入到一个多层感知机（MLP）中，感知机包含一个隐藏层，其隐藏单元数是一个超参数$h$。通过使用$\tanh$作为激活函数，并且禁用偏置项。</p><p>下面来实现加性注意力。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AdditiveAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;加性注意力&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, key_size, query_size, num_hiddens, dropout, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(AdditiveAttention, self).__init__(**kwargs)</span><br><span class="line">        self.W_k = nn.Linear(key_size, num_hiddens, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_q = nn.Linear(query_size, num_hiddens, bias=<span class="literal">False</span>)</span><br><span class="line">        self.w_v = nn.Linear(num_hiddens, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, queries, keys, values, valid_lens</span>):</span></span><br><span class="line">        queries, keys = self.W_q(queries), self.W_k(keys)</span><br><span class="line">        <span class="comment"># 在维度扩展后，</span></span><br><span class="line">        <span class="comment"># queries的形状：(batch_size，查询的个数，1，num_hidden)</span></span><br><span class="line">        <span class="comment"># key的形状：(batch_size，1，“键－值”对的个数，num_hiddens)</span></span><br><span class="line">        <span class="comment"># 使用广播方式进行求和</span></span><br><span class="line">        features = queries.unsqueeze(<span class="number">2</span>) + keys.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        features = torch.tanh(features)</span><br><span class="line">        <span class="comment"># self.w_v仅有一个输出，因此从形状中移除最后那个维度。</span></span><br><span class="line">        <span class="comment"># scores的形状：(batch_size，查询的个数，“键-值”对的个数)</span></span><br><span class="line">        scores = self.w_v(features).squeeze(-<span class="number">1</span>)</span><br><span class="line">        self.attention_weights = masked_softmax(scores, valid_lens)</span><br><span class="line">        <span class="comment"># values的形状：(batch_size，“键－值”对的个数，值的维度)</span></span><br><span class="line">        <span class="keyword">return</span> torch.bmm(self.dropout(self.attention_weights), values)</span><br></pre></td></tr></table></figure><p>用一个小例子来演示上面的<code>AdditiveAttention</code>类，其中查询、键和值的形状为（批量大小，步数或词元序列长度，特征大小），实际输出为$(2,1,20)$、$(2,10,2)$和$(2,10,4)$。注意力池化输出的形状为（批量大小，查询的步数，值的维度）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">queries, keys = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">1</span>, <span class="number">20</span>)), torch.ones((<span class="number">2</span>, <span class="number">10</span>, <span class="number">2</span>))</span><br><span class="line"><span class="comment"># values的小批量，两个值矩阵是相同的</span></span><br><span class="line">values = torch.arange(<span class="number">40</span>, dtype=torch.float32).reshape(<span class="number">1</span>, <span class="number">10</span>, <span class="number">4</span>).repeat(</span><br><span class="line">    <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">valid_lens = torch.tensor([<span class="number">2</span>, <span class="number">6</span>])</span><br><span class="line"></span><br><span class="line">attention = AdditiveAttention(key_size=<span class="number">2</span>, query_size=<span class="number">20</span>, num_hiddens=<span class="number">8</span>,</span><br><span class="line">                              dropout=<span class="number">0.1</span>)</span><br><span class="line">attention.<span class="built_in">eval</span>()</span><br><span class="line">attention(queries, keys, values, valid_lens)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[[ <span class="number">2.0000</span>,  <span class="number">3.0000</span>,  <span class="number">4.0000</span>,  <span class="number">5.0000</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">10.0000</span>, <span class="number">11.0000</span>, <span class="number">12.0000</span>, <span class="number">13.0000</span>]]], grad_fn=&lt;BmmBackward0&gt;)</span><br></pre></td></tr></table></figure><p>尽管加性注意力包含了可学习的参数，但由于本例子中每个键都是相同的， 所以注意力权重是均匀的，由指定的有效长度决定。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(attention.attention_weights.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">10</span>)),</span><br><span class="line">                  xlabel=<span class="string">&#x27;Keys&#x27;</span>, ylabel=<span class="string">&#x27;Queries&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_attention-scoring-functions.svg" alt="oasf"></p><h3 id="3-3-缩放点积注意力"><a href="#3-3-缩放点积注意力" class="headerlink" title="3.3. 缩放点积注意力"></a>3.3. 缩放点积注意力</h3><p>使用点积可以得到计算效率更高的评分函数，但是点积操作要求查询和键具有相同的长度$d$。假设查询和键的所有元素都是独立的随机变量，并且都满足零均值和单位方差，那么两个向量的点积的均值为$0$，方差为$d$。为确保无论向量长度如何，点积的方差在不考虑向量长度的情况下仍然是$1$，我们再将点积除以$\sqrt{d}$，则<em>缩放点积注意力</em>（scaled dot-product attention）评分函数为：</p><script type="math/tex; mode=display">a(\mathbf q, \mathbf k) = \mathbf{q}^\top \mathbf{k}  /\sqrt{d}</script><p>在实践中，我们通常从小批量的角度来考虑提高效率，例如基于$n$个查询和$m$个键－值对计算注意力，其中查询和键的长度为$d$，值的长度为$v$。查询$\mathbf Q\in\mathbb R^{n\times d}$、键$\mathbf K\in\mathbb R^{m\times d}$和值$\mathbf V\in\mathbb R^{m\times v}$的缩放点积注意力是：</p><script type="math/tex; mode=display">\mathrm{softmax}\left(\frac{\mathbf Q \mathbf K^\top }{\sqrt{d}}\right) \mathbf V \in \mathbb{R}^{n\times v}</script><p>下面的缩放点积注意力的实现使用了暂退法进行模型正则化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DotProductAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;缩放点积注意力&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dropout, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(DotProductAttention, self).__init__(**kwargs)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># queries的形状：(batch_size，查询的个数，d)</span></span><br><span class="line">    <span class="comment"># keys的形状：(batch_size，“键－值”对的个数，d)</span></span><br><span class="line">    <span class="comment"># values的形状：(batch_size，“键－值”对的个数，值的维度)</span></span><br><span class="line">    <span class="comment"># valid_lens的形状:(batch_size，)或者(batch_size，查询的个数)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, queries, keys, values, valid_lens=<span class="literal">None</span></span>):</span></span><br><span class="line">        d = queries.shape[-<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 设置transpose_b=True为了交换keys的最后两个维度</span></span><br><span class="line">        scores = torch.bmm(queries, keys.transpose(<span class="number">1</span>,<span class="number">2</span>)) / math.sqrt(d)</span><br><span class="line">        self.attention_weights = masked_softmax(scores, valid_lens)</span><br><span class="line">        <span class="keyword">return</span> torch.bmm(self.dropout(self.attention_weights), values)</span><br></pre></td></tr></table></figure><p>为了演示上述的DotProductAttention类， 我们使用与先前加性注意力例子中相同的键、值和有效长度。 对于点积操作，我们令查询的特征维度与键的特征维度大小相同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">queries = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">attention = DotProductAttention(dropout=<span class="number">0.5</span>)</span><br><span class="line">attention.<span class="built_in">eval</span>()</span><br><span class="line">attention(queries, keys, values, valid_lens)</span><br></pre></td></tr></table></figure><p>与加性注意力演示相同，由于键包含的是相同的元素， 而这些元素无法通过任何查询进行区分，因此获得了均匀的注意力权重。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(attention.attention_weights.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">10</span>)),</span><br><span class="line">                  xlabel=<span class="string">&#x27;Keys&#x27;</span>, ylabel=<span class="string">&#x27;Queries&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_attention-scoring-functions.svg" alt="oasf"></p><h2 id="4-Bahdanau-注意力"><a href="#4-Bahdanau-注意力" class="headerlink" title="4. Bahdanau 注意力"></a>4. Bahdanau 注意力</h2><p>之前章节中探讨了机器翻译问题：通过设计一个基于两个循环神经网络的编码器-解码器架构，用于序列到序列学习（seq2seq）。具体来说，循环神经网络编码器将长度可变的序列转换为固定形状的上下文变量，然后循环神经网络解码器根据生成的词元和上下文变量按词元生成输出（目标）序列词元。然而，即使并非所有输入（源）词元都对解码某个词元都有用，在每个解码步骤中仍使用编码<em>相同</em>的上下文变量。有什么方法能改变上下文变量呢？</p><p>试着从<code>Graves.2013</code>中找到灵感：在为给定文本序列生成手写的挑战中，Graves设计了一种可微注意力模型，将文本字符与更长的笔迹对齐，其中对齐方式仅向一个方向移动。受学习对齐想法的启发，Bahdanau等人提出了一个没有严格单向对齐限制的可微注意力模型 <code>Bahdanau.Cho.Bengio.2014</code>。在预测词元时，如果不是所有输入词元都相关，模型将仅对齐（或参与）输入序列中与当前预测相关的部分。这是通过将上下文变量视为注意力集中的输出来实现的。</p><p>由于这段是在NLP方向上加注意力机制，就粗略浏览一下，没做什么笔记。 —SZ</p><h3 id="4-1-模型"><a href="#4-1-模型" class="headerlink" title="4.1. 模型"></a>4.1. 模型</h3><p>下面描述的Bahdanau注意力模型将遵循之前seq2seq中的相同符号表达。这个新的基于注意力的模型与seq2seq中的模型相同，只不过上下文变量$\mathbf{c}$在任何解码时间步$t’$都会被$\mathbf{c}_{t’}$替换。假设输入序列中有$T$个词元，解码时间步$t’$的上下文变量是注意力集中的输出：</p><script type="math/tex; mode=display">\mathbf{c}_{t'} = \sum_{t=1}^T \alpha(\mathbf{s}_{t' - 1}, \mathbf{h}_t) \mathbf{h}_t</script><p>其中，时间步$t’ - 1$时的解码器隐状态$\mathbf{s}_{t’ - 1}$是查询，编码器隐状态$\mathbf{h}_t$既是键，也是值，注意力权重$\alpha$是使用<em>加性注意力打分函数</em>计算的。</p><p>与之前描述的循环神经网络编码器-解码器架构略有不同，下图描述了Bahdanau注意力的架构。</p><p><img src="/assets/post_img/article74/seq2seq-attention-details.svg" alt="一个带有Bahdanau注意力的循环神经网络编码器-解码器模型"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><h3 id="4-2-定义注意力解码器"><a href="#4-2-定义注意力解码器" class="headerlink" title="4.2. 定义注意力解码器"></a>4.2. 定义注意力解码器</h3><p>下面我们看看如何定义Bahdanau注意力，实现循环神经网络编码器-解码器。 其实，我们只需重新定义解码器即可。 为了更方便地显示学习的注意力权重， 以下AttentionDecoder类定义了带有注意力机制解码器的基本接口。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AttentionDecoder</span>(<span class="params">d2l.Decoder</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;带有注意力机制解码器的基本接口&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(AttentionDecoder, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">attention_weights</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure><p>接下来，让我们在接下来的<code>Seq2SeqAttentionDecoder</code>类中实现带有Bahdanau注意力的循环神经网络解码器。首先，初始化解码器的状态，需要下面的输入：</p><ol><li>编码器在所有时间步的最终层隐状态，将作为注意力的键和值；</li><li>上一时间步的编码器全层隐状态，将作为初始化解码器的隐状态；</li><li>编码器有效长度（排除在注意力池中填充词元）。</li></ol><p>在每个解码时间步骤中，解码器上一个时间步的最终层隐状态将用作查询。<br>因此，注意力输出和输入嵌入都连结为循环神经网络解码器的输入。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Seq2SeqAttentionDecoder</span>(<span class="params">AttentionDecoder</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embed_size, num_hiddens, num_layers,</span></span></span><br><span class="line"><span class="params"><span class="function">                 dropout=<span class="number">0</span>, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Seq2SeqAttentionDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.attention = d2l.AdditiveAttention(</span><br><span class="line">            num_hiddens, num_hiddens, num_hiddens, dropout)</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.rnn = nn.GRU(</span><br><span class="line">            embed_size + num_hiddens, num_hiddens, num_layers,</span><br><span class="line">            dropout=dropout)</span><br><span class="line">        self.dense = nn.Linear(num_hiddens, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_state</span>(<span class="params">self, enc_outputs, enc_valid_lens, *args</span>):</span></span><br><span class="line">        <span class="comment"># outputs的形状为(batch_size，num_steps，num_hiddens).</span></span><br><span class="line">        <span class="comment"># hidden_state的形状为(num_layers，batch_size，num_hiddens)</span></span><br><span class="line">        outputs, hidden_state = enc_outputs</span><br><span class="line">        <span class="keyword">return</span> (outputs.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>), hidden_state, enc_valid_lens)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, state</span>):</span></span><br><span class="line">        <span class="comment"># enc_outputs的形状为(batch_size,num_steps,num_hiddens).</span></span><br><span class="line">        <span class="comment"># hidden_state的形状为(num_layers,batch_size,</span></span><br><span class="line">        <span class="comment"># num_hiddens)</span></span><br><span class="line">        enc_outputs, hidden_state, enc_valid_lens = state</span><br><span class="line">        <span class="comment"># 输出X的形状为(num_steps,batch_size,embed_size)</span></span><br><span class="line">        X = self.embedding(X).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        outputs, self._attention_weights = [], []</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> X:</span><br><span class="line">            <span class="comment"># query的形状为(batch_size,1,num_hiddens)</span></span><br><span class="line">            query = torch.unsqueeze(hidden_state[-<span class="number">1</span>], dim=<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># context的形状为(batch_size,1,num_hiddens)</span></span><br><span class="line">            context = self.attention(</span><br><span class="line">                query, enc_outputs, enc_outputs, enc_valid_lens)</span><br><span class="line">            <span class="comment"># 在特征维度上连结</span></span><br><span class="line">            x = torch.cat((context, torch.unsqueeze(x, dim=<span class="number">1</span>)), dim=-<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 将x变形为(1,batch_size,embed_size+num_hiddens)</span></span><br><span class="line">            out, hidden_state = self.rnn(x.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>), hidden_state)</span><br><span class="line">            outputs.append(out)</span><br><span class="line">            self._attention_weights.append(self.attention.attention_weights)</span><br><span class="line">        <span class="comment"># 全连接层变换后，outputs的形状为</span></span><br><span class="line">        <span class="comment"># (num_steps,batch_size,vocab_size)</span></span><br><span class="line">        outputs = self.dense(torch.cat(outputs, dim=<span class="number">0</span>))</span><br><span class="line">        <span class="keyword">return</span> outputs.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>), [enc_outputs, hidden_state,</span><br><span class="line">                                          enc_valid_lens]</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">attention_weights</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self._attention_weights</span><br></pre></td></tr></table></figure><p>接下来，我们使用包含7个时间步的4个序列输入的小批量测试Bahdanau注意力解码器。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">encoder = d2l.Seq2SeqEncoder(vocab_size=<span class="number">10</span>, embed_size=<span class="number">8</span>, num_hiddens=<span class="number">16</span>,</span><br><span class="line">                             num_layers=<span class="number">2</span>)</span><br><span class="line">encoder.<span class="built_in">eval</span>()</span><br><span class="line">decoder = Seq2SeqAttentionDecoder(vocab_size=<span class="number">10</span>, embed_size=<span class="number">8</span>, num_hiddens=<span class="number">16</span>,</span><br><span class="line">                                  num_layers=<span class="number">2</span>)</span><br><span class="line">decoder.<span class="built_in">eval</span>()</span><br><span class="line">X = torch.zeros((<span class="number">4</span>, <span class="number">7</span>), dtype=torch.long)  <span class="comment"># (batch_size,num_steps)</span></span><br><span class="line">state = decoder.init_state(encoder(X), <span class="literal">None</span>)</span><br><span class="line">output, state = decoder(X, state)</span><br><span class="line">output.shape, <span class="built_in">len</span>(state), state[<span class="number">0</span>].shape, <span class="built_in">len</span>(state[<span class="number">1</span>]), state[<span class="number">1</span>][<span class="number">0</span>].shape</span><br></pre></td></tr></table></figure><h3 id="4-3-训练"><a href="#4-3-训练" class="headerlink" title="4.3. 训练"></a>4.3. 训练</h3><p>在这里指定超参数，实例化一个带有Bahdanau注意力的编码器和解码器， 并对这个模型进行机器翻译训练。 由于新增的注意力机制，训练要比没有注意力机制的慢得多。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">embed_size, num_hiddens, num_layers, dropout = <span class="number">32</span>, <span class="number">32</span>, <span class="number">2</span>, <span class="number">0.1</span></span><br><span class="line">batch_size, num_steps = <span class="number">64</span>, <span class="number">10</span></span><br><span class="line">lr, num_epochs, device = <span class="number">0.005</span>, <span class="number">250</span>, d2l.try_gpu()</span><br><span class="line"></span><br><span class="line">train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)</span><br><span class="line">encoder = d2l.Seq2SeqEncoder(</span><br><span class="line">    <span class="built_in">len</span>(src_vocab), embed_size, num_hiddens, num_layers, dropout)</span><br><span class="line">decoder = Seq2SeqAttentionDecoder(</span><br><span class="line">    <span class="built_in">len</span>(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)</span><br><span class="line">net = d2l.EncoderDecoder(encoder, decoder)</span><br><span class="line">d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)</span><br></pre></td></tr></table></figure><p>模型训练后，我们用它将几个英语句子翻译成法语并计算它们的BLEU分数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">engs = [<span class="string">&#x27;go .&#x27;</span>, <span class="string">&quot;i lost .&quot;</span>, <span class="string">&#x27;he\&#x27;s calm .&#x27;</span>, <span class="string">&#x27;i\&#x27;m home .&#x27;</span>]</span><br><span class="line">fras = [<span class="string">&#x27;va !&#x27;</span>, <span class="string">&#x27;j\&#x27;ai perdu .&#x27;</span>, <span class="string">&#x27;il est calme .&#x27;</span>, <span class="string">&#x27;je suis chez moi .&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> eng, fra <span class="keyword">in</span> <span class="built_in">zip</span>(engs, fras):</span><br><span class="line">    translation, dec_attention_weight_seq = d2l.predict_seq2seq(</span><br><span class="line">        net, eng, src_vocab, tgt_vocab, num_steps, device, <span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;eng&#125;</span> =&gt; <span class="subst">&#123;translation&#125;</span>, &#x27;</span>,</span><br><span class="line">          <span class="string">f&#x27;bleu <span class="subst">&#123;d2l.bleu(translation, fra, k=<span class="number">2</span>):<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">go . =&gt; va !,  bleu <span class="number">1.000</span></span><br><span class="line">i lost . =&gt; j<span class="string">&#x27;ai perdu .,  bleu 1.000</span></span><br><span class="line"><span class="string">he&#x27;</span>s calm . =&gt; je suis ici .,  bleu <span class="number">0.000</span></span><br><span class="line">i<span class="string">&#x27;m home . =&gt; je suis chez moi .,  bleu 1.000</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">attention_weights = torch.cat([step[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>] <span class="keyword">for</span> step <span class="keyword">in</span> dec_attention_weight_seq], <span class="number">0</span>).reshape((</span><br><span class="line">    <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, num_steps))</span><br></pre></td></tr></table></figure><p>训练结束后，通过可视化注意力权重你会发现，每个查询都会在键值对上分配不同的权重，这说明 在每个解码步中，输入序列的不同部分被选择性地聚集在注意力池中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加上一个包含序列结束词元</span></span><br><span class="line">d2l.show_heatmaps(</span><br><span class="line">    attention_weights[:, :, :, :<span class="built_in">len</span>(engs[-<span class="number">1</span>].split()) + <span class="number">1</span>].cpu(),</span><br><span class="line">    xlabel=<span class="string">&#x27;Key positions&#x27;</span>, ylabel=<span class="string">&#x27;Query positions&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_bahdanau-attention_seq.svg" alt="seq"></p><h2 id="5-多头注意力"><a href="#5-多头注意力" class="headerlink" title="5. 多头注意力"></a>5. 多头注意力</h2><p>在实践中，当给定相同的查询、键和值的集合时，我们希望模型可以基于相同的注意力机制学习到不同的行为，然后将不同的行为作为知识组合起来，捕获序列内各种范围的依赖关系（例如，短距离依赖和长距离依赖关系）。因此，允许注意力机制组合使用查询、键和值的不同<em>子空间表示</em>（representation subspaces）可能是有益的。</p><p>为此，与其只使用单独一个注意力池化，我们可以用独立学习得到的$h$组不同的<em>线性投影</em>（linear projections）来变换查询、键和值。然后，这$h$组变换后的查询、键和值将并行地送到注意力池化中。最后，将这$h$个注意力池化的输出拼接在一起，并且通过另一个可以学习的线性投影进行变换，以产生最终输出。这种设计被称为<em>多头注意力</em>（multihead attention）<code>Vaswani.Shazeer.Parmar.ea.2017</code>。对于$h$个注意力池化输出，每一个注意力池化都被称作一个<em>头</em>（head）。 下图展示了使用全连接层来实现可学习的线性变换的多头注意力。</p><p><img src="/assets/post_img/article74/multi-head-attention.svg" alt="多头注意力：多个头连结然后线性变换"></p><h3 id="5-1-模型"><a href="#5-1-模型" class="headerlink" title="5.1. 模型"></a>5.1. 模型</h3><p>在实现多头注意力之前，让我们用数学语言将这个模型形式化地描述出来。给定查询$\mathbf{q} \in \mathbb{R}^{d_q}$、键$\mathbf{k} \in \mathbb{R}^{d_k}$和值$\mathbf{v} \in \mathbb{R}^{d_v}$，每个注意力头$\mathbf{h}_i$（$i = 1, \ldots, h$）的计算方法为：</p><script type="math/tex; mode=display">\mathbf{h}_i = f(\mathbf W_i^{(q)}\mathbf q, \mathbf W_i^{(k)}\mathbf k,\mathbf W_i^{(v)}\mathbf v) \in \mathbb R^{p_v}</script><p>其中，可学习的参数包括$\mathbf W_i^{(q)}\in\mathbb R^{p_q\times d_q}$、$\mathbf W_i^{(k)}\in\mathbb R^{p_k\times d_k}$和$\mathbf W_i^{(v)}\in\mathbb R^{p_v\times d_v}$，以及代表注意力池化的函数$f$。$f$可以是<a href="#3-注意力评分函数">注意力评分函数</a>中的加性注意力和缩放点积注意力。多头注意力的输出需要经过另一个线性转换，它对应着$h$个头连结后的结果，因此其可学习参数是$\mathbf W_o\in\mathbb R^{p_o\times h p_v}$：</p><script type="math/tex; mode=display">\mathbf W_o \begin{bmatrix}\mathbf h_1\\\vdots\\\mathbf h_h\end{bmatrix} \in \mathbb{R}^{p_o}</script><p>基于这种设计，每个头都可能会关注输入的不同部分，可以表示比简单加权平均值更复杂的函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><h3 id="5-2-实现"><a href="#5-2-实现" class="headerlink" title="5.2. 实现"></a>5.2. 实现</h3><p>在实现过程中通常选择缩放点积注意力作为每一个注意力头。为了避免计算代价和参数代价的大幅增长，我们设定$p_q = p_k = p_v = p_o / h$。值得注意的是，如果将查询、键和值的线性变换的输出数量设置为$p_q h = p_k h = p_v h = p_o$，则可以并行计算$h$个头。在下面的实现中，$p_o$是通过参数<code>num_hiddens</code>指定的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;多头注意力&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, key_size, query_size, value_size, num_hiddens,</span></span></span><br><span class="line"><span class="params"><span class="function">                 num_heads, dropout, bias=<span class="literal">False</span>, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttention, self).__init__(**kwargs)</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        self.attention = d2l.DotProductAttention(dropout)</span><br><span class="line">        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)</span><br><span class="line">        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)</span><br><span class="line">        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)</span><br><span class="line">        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, queries, keys, values, valid_lens</span>):</span></span><br><span class="line">        <span class="comment"># queries，keys，values的形状:</span></span><br><span class="line">        <span class="comment"># (batch_size，查询或者“键－值”对的个数，num_hiddens)</span></span><br><span class="line">        <span class="comment"># valid_lens　的形状:</span></span><br><span class="line">        <span class="comment"># (batch_size，)或(batch_size，查询的个数)</span></span><br><span class="line">        <span class="comment"># 经过变换后，输出的queries，keys，values　的形状:</span></span><br><span class="line">        <span class="comment"># (batch_size*num_heads，查询或者“键－值”对的个数，</span></span><br><span class="line">        <span class="comment"># num_hiddens/num_heads)</span></span><br><span class="line">        queries = transpose_qkv(self.W_q(queries), self.num_heads)</span><br><span class="line">        keys = transpose_qkv(self.W_k(keys), self.num_heads)</span><br><span class="line">        values = transpose_qkv(self.W_v(values), self.num_heads)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> valid_lens <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 在轴0，将第一项（标量或者矢量）复制num_heads次，</span></span><br><span class="line">            <span class="comment"># 然后如此复制第二项，然后诸如此类。</span></span><br><span class="line">            valid_lens = torch.repeat_interleave(</span><br><span class="line">                valid_lens, repeats=self.num_heads, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># output的形状:(batch_size*num_heads，查询的个数，</span></span><br><span class="line">        <span class="comment"># num_hiddens/num_heads)</span></span><br><span class="line">        output = self.attention(queries, keys, values, valid_lens)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># output_concat的形状:(batch_size，查询的个数，num_hiddens)</span></span><br><span class="line">        output_concat = transpose_output(output, self.num_heads)</span><br><span class="line">        <span class="keyword">return</span> self.W_o(output_concat)</span><br></pre></td></tr></table></figure><p>为了能够使多个头并行计算，上面的<code>MultiHeadAttention</code>类将使用下面定义的两个转置函数。具体来说，<code>transpose_output</code>函数反转了<code>transpose_qkv</code>函数的操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transpose_qkv</span>(<span class="params">X, num_heads</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;为了多注意力头的并行计算而变换形状&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 输入X的形状:(batch_size，查询或者“键－值”对的个数，num_hiddens)</span></span><br><span class="line">    <span class="comment"># 输出X的形状:(batch_size，查询或者“键－值”对的个数，num_heads，</span></span><br><span class="line">    <span class="comment"># num_hiddens/num_heads)</span></span><br><span class="line">    X = X.reshape(X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>], num_heads, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出X的形状:(batch_size，num_heads，查询或者“键－值”对的个数,</span></span><br><span class="line">    <span class="comment"># num_hiddens/num_heads)</span></span><br><span class="line">    X = X.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 最终输出的形状:(batch_size*num_heads,查询或者“键－值”对的个数,</span></span><br><span class="line">    <span class="comment"># num_hiddens/num_heads)</span></span><br><span class="line">    <span class="keyword">return</span> X.reshape(-<span class="number">1</span>, X.shape[<span class="number">2</span>], X.shape[<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transpose_output</span>(<span class="params">X, num_heads</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;逆转transpose_qkv函数的操作&quot;&quot;&quot;</span></span><br><span class="line">    X = X.reshape(-<span class="number">1</span>, num_heads, X.shape[<span class="number">1</span>], X.shape[<span class="number">2</span>])</span><br><span class="line">    X = X.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">    <span class="keyword">return</span> X.reshape(X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>], -<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>下面我们使用键和值相同的小例子来测试我们编写的MultiHeadAttention类。 多头注意力输出的形状是（batch_size，num_queries，num_hiddens）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens, num_heads = <span class="number">100</span>, <span class="number">5</span></span><br><span class="line">attention = MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens,</span><br><span class="line">                               num_hiddens, num_heads, <span class="number">0.5</span>)</span><br><span class="line">attention.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">MultiHeadAttention(</span><br><span class="line">  (attention): DotProductAttention(</span><br><span class="line">    (dropout): Dropout(p=<span class="number">0.5</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">  )</span><br><span class="line">  (W_q): Linear(in_features=<span class="number">100</span>, out_features=<span class="number">100</span>, bias=<span class="literal">False</span>)</span><br><span class="line">  (W_k): Linear(in_features=<span class="number">100</span>, out_features=<span class="number">100</span>, bias=<span class="literal">False</span>)</span><br><span class="line">  (W_v): Linear(in_features=<span class="number">100</span>, out_features=<span class="number">100</span>, bias=<span class="literal">False</span>)</span><br><span class="line">  (W_o): Linear(in_features=<span class="number">100</span>, out_features=<span class="number">100</span>, bias=<span class="literal">False</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">batch_size, num_queries = <span class="number">2</span>, <span class="number">4</span></span><br><span class="line">num_kvpairs, valid_lens =  <span class="number">6</span>, torch.tensor([<span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">X = torch.ones((batch_size, num_queries, num_hiddens))</span><br><span class="line">Y = torch.ones((batch_size, num_kvpairs, num_hiddens))</span><br><span class="line">attention(X, Y, Y, valid_lens).shape</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">4</span>, <span class="number">100</span>])</span><br></pre></td></tr></table></figure><h2 id="6-自注意力和位置编码"><a href="#6-自注意力和位置编码" class="headerlink" title="6. 自注意力和位置编码"></a>6. 自注意力和位置编码</h2><p>在深度学习中，经常使用卷积神经网络（CNN）或循环神经网络（RNN）对序列进行编码。想象一下，有了注意力机制之后，我们将词元序列输入注意力池化中，以便同一组词元同时充当查询、键和值。具体来说，每个查询都会关注所有的键－值对并生成一个注意力输出。由于查询、键和值来自同一组输入，因此被称为<em>自注意力</em>（self-attention）<code>Lin.Feng.Santos.ea.2017,Vaswani.Shazeer.Parmar.ea.2017</code>，也被称为<em>内部注意力</em>（intra-attention）<code>Cheng.Dong.Lapata.2016,Parikh.Tackstrom.Das.ea.2016,Paulus.Xiong.Socher.2017</code>。本节将使用自注意力进行序列编码，以及如何使用序列的顺序作为补充信息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><h3 id="6-1-自注意力"><a href="#6-1-自注意力" class="headerlink" title="6.1. 自注意力"></a>6.1. 自注意力</h3><p>给定一个由词元组成的输入序列$\mathbf{x}_1, \ldots, \mathbf{x}_n$，其中任意$\mathbf{x}_i \in \mathbb{R}^d$（$1 \leq i \leq n$）。该序列的自注意力输出为一个长度相同的序列<br>$\mathbf{y}_1, \ldots, \mathbf{y}_n$，其中：</p><script type="math/tex; mode=display">\mathbf{y}_i = f(\mathbf{x}_i, (\mathbf{x}_1, \mathbf{x}_1), \ldots, (\mathbf{x}_n, \mathbf{x}_n)) \in \mathbb{R}^d</script><p>根据<a href="#24-带参数注意力池化">2.4</a>中定义的注意力池化函数$f$。下面的代码片段是基于多头注意力对一个张量完成自注意力的计算，张量的形状为（批量大小，时间步的数目或词元序列的长度，$d$）。输出与输入的张量形状相同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens, num_heads = <span class="number">100</span>, <span class="number">5</span></span><br><span class="line">attention = d2l.MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens,</span><br><span class="line">                                   num_hiddens, num_heads, <span class="number">0.5</span>)</span><br><span class="line">attention.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">MultiHeadAttention(</span><br><span class="line">  (attention): DotProductAttention(</span><br><span class="line">    (dropout): Dropout(p=<span class="number">0.5</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">  )</span><br><span class="line">  (W_q): Linear(in_features=<span class="number">100</span>, out_features=<span class="number">100</span>, bias=<span class="literal">False</span>)</span><br><span class="line">  (W_k): Linear(in_features=<span class="number">100</span>, out_features=<span class="number">100</span>, bias=<span class="literal">False</span>)</span><br><span class="line">  (W_v): Linear(in_features=<span class="number">100</span>, out_features=<span class="number">100</span>, bias=<span class="literal">False</span>)</span><br><span class="line">  (W_o): Linear(in_features=<span class="number">100</span>, out_features=<span class="number">100</span>, bias=<span class="literal">False</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">batch_size, num_queries, valid_lens = <span class="number">2</span>, <span class="number">4</span>, torch.tensor([<span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">X = torch.ones((batch_size, num_queries, num_hiddens))</span><br><span class="line">attention(X, X, X, valid_lens).shape</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">4</span>, <span class="number">100</span>])</span><br></pre></td></tr></table></figure><h3 id="6-2-比较卷积神经网络、循环神经网络和自注意力"><a href="#6-2-比较卷积神经网络、循环神经网络和自注意力" class="headerlink" title="6.2. 比较卷积神经网络、循环神经网络和自注意力"></a>6.2. 比较卷积神经网络、循环神经网络和自注意力</h3><p>接下来比较下面几个架构，目标都是将由$n$个词元组成的序列映射到另一个长度相等的序列，其中的每个输入词元或输出词元都由$d$维向量表示。具体来说，将比较的是卷积神经网络、循环神经网络和自注意力这几个架构的计算复杂性、顺序操作和最大路径长度。请注意，顺序操作会妨碍并行计算，而任意的序列位置组合之间的路径越短，则能更轻松地学习序列中的远距离依赖关系 <code>Hochreiter.Bengio.Frasconi.ea.2001</code>。</p><p><img src="/assets/post_img/article74/cnn-rnn-self-attention.svg" alt="比较卷积神经网络（填充词元被忽略）、循环神经网络和自注意力三种架构"></p><p>考虑一个卷积核大小为$k$的卷积层。在后面的章节将提供关于使用卷积神经网络处理序列的更多详细信息。目前只需要知道的是，由于序列长度是$n$，输入和输出的通道数量都是$d$，所以卷积层的计算复杂度为$\mathcal{O}(knd^2)$。如上图所示，卷积神经网络是分层的，因此为有$\mathcal{O}(1)$个顺序操作，最大路径长度为$\mathcal{O}(n/k)$。例如，$\mathbf{x}_1$和$\mathbf{x}_5$处于上图中卷积核大小为3的双层卷积神经网络的感受野内。</p><p>当更新循环神经网络的隐状态时，$d \times d$权重矩阵和$d$维隐状态的乘法计算复杂度为$\mathcal{O}(d^2)$。由于序列长度为$n$，因此循环神经网络层的计算复杂度为$\mathcal{O}(nd^2)$。根据上图，有$\mathcal{O}(n)$个顺序操作无法并行化，最大路径长度也是$\mathcal{O}(n)$。</p><p>在自注意力中，查询、键和值都是$n \times d$矩阵。考虑<a href="#33-缩放点积注意力">3.3</a>中缩放的”点－积“注意力，其中$n \times d$矩阵乘以$d \times n$矩阵。之后输出的$n \times n$矩阵乘以$n \times d$矩阵。因此，自注意力具有$\mathcal{O}(n^2d)$计算复杂性。正如在上图中所讲，每个词元都通过自注意力直接连接到任何其他词元。因此，有$\mathcal{O}(1)$个顺序操作可以并行计算，最大路径长度也是$\mathcal{O}(1)$。</p><p>总而言之，卷积神经网络和自注意力都拥有并行计算的优势，而且自注意力的最大路径长度最短。但是因为其计算复杂度是关于序列长度的二次方，所以在很长的序列中计算会非常慢。</p><h3 id="6-3-位置编码"><a href="#6-3-位置编码" class="headerlink" title="6.3. 位置编码"></a>6.3. 位置编码</h3><p>在处理词元序列时，循环神经网络是逐个的重复地处理词元的，而自注意力则因为并行计算而放弃了顺序操作。为了使用序列的顺序信息，通过在输入表示中添加<em>位置编码</em>（positional encoding）来注入绝对的或相对的位置信息。位置编码可以通过学习得到也可以直接固定得到。接下来描述的是基于正弦函数和余弦函数的固定位置编码<code>Vaswani.Shazeer.Parmar.ea.2017</code>。</p><p>假设输入表示 $\mathbf{X} \in \mathbb{R}^{n \times d}$ 包含一个序列中$n$个词元的$d$维嵌入表示。位置编码使用相同形状的位置嵌入矩阵 $\mathbf{P} \in \mathbb{R}^{n \times d}$输出$\mathbf{X} + \mathbf{P}$，矩阵第$i$行、第$2j$列和$2j+1$列上的元素为：</p><script type="math/tex; mode=display">\begin{aligned} p_{i, 2j} &= \sin\left(\frac{i}{10000^{2j/d}}\right),\\p_{i, 2j+1} &= \cos\left(\frac{i}{10000^{2j/d}}\right).\end{aligned}</script><p>乍一看，这种基于三角函数的设计看起来很奇怪。在解释这个设计之前，让我们先在下面的<code>PositionalEncoding</code>类中实现它。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;位置编码&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_hiddens, dropout, max_len=<span class="number">1000</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="comment"># 创建一个足够长的P</span></span><br><span class="line">        self.P = torch.zeros((<span class="number">1</span>, max_len, num_hiddens))</span><br><span class="line">        X = torch.arange(max_len, dtype=torch.float32).reshape(</span><br><span class="line">            -<span class="number">1</span>, <span class="number">1</span>) / torch.<span class="built_in">pow</span>(<span class="number">10000</span>, torch.arange(</span><br><span class="line">            <span class="number">0</span>, num_hiddens, <span class="number">2</span>, dtype=torch.float32) / num_hiddens)</span><br><span class="line">        self.P[:, :, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(X)</span><br><span class="line">        self.P[:, :, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(X)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        X = X + self.P[:, :X.shape[<span class="number">1</span>], :].to(X.device)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(X)</span><br></pre></td></tr></table></figure><p>在位置嵌入矩阵$\mathbf{P}$中，行代表词元在序列中的位置，列代表位置编码的不同维度。从下面的例子中可以看到位置嵌入矩阵的第$6$列和第$7$列的频率高于第$8$列和第$9$列。第$6$列和第$7$列之间的偏移量（第$8$列和第$9$列相同）是由于正弦函数和余弦函数的交替。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">encoding_dim, num_steps = <span class="number">32</span>, <span class="number">60</span></span><br><span class="line">pos_encoding = PositionalEncoding(encoding_dim, <span class="number">0</span>)</span><br><span class="line">pos_encoding.<span class="built_in">eval</span>()</span><br><span class="line">X = pos_encoding(torch.zeros((<span class="number">1</span>, num_steps, encoding_dim)))</span><br><span class="line">P = pos_encoding.P[:, :X.shape[<span class="number">1</span>], :]</span><br><span class="line">d2l.plot(torch.arange(num_steps), P[<span class="number">0</span>, :, <span class="number">6</span>:<span class="number">10</span>].T, xlabel=<span class="string">&#x27;Row (position)&#x27;</span>,</span><br><span class="line">         figsize=(<span class="number">6</span>, <span class="number">2.5</span>), legend=[<span class="string">&quot;Col %d&quot;</span> % d <span class="keyword">for</span> d <span class="keyword">in</span> torch.arange(<span class="number">6</span>, <span class="number">10</span>)])</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_self-attention-and-positional-encoding.svg" alt="row（pos）"></p><h4 id="6-3-1-绝对位置信息"><a href="#6-3-1-绝对位置信息" class="headerlink" title="6.3.1. 绝对位置信息"></a>6.3.1. 绝对位置信息</h4><p>为了明白沿着编码维度单调降低的频率与绝对位置信息的关系，让我们打印出$0, 1, \ldots, 7$的[<strong>二进制表示</strong>]形式。正如所看到的，每个数字、每两个数字和每四个数字上的比特值在第一个最低位、第二个最低位和第三个最低位上分别交替。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;i&#125;</span>的二进制是：<span class="subst">&#123;i:&gt;03b&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line"><span class="number">0</span>的二进制是：<span class="number">000</span></span><br><span class="line"><span class="number">1</span>的二进制是：001</span><br><span class="line"><span class="number">2</span>的二进制是：010</span><br><span class="line"><span class="number">3</span>的二进制是：011</span><br><span class="line"><span class="number">4</span>的二进制是：<span class="number">100</span></span><br><span class="line"><span class="number">5</span>的二进制是：<span class="number">101</span></span><br><span class="line"><span class="number">6</span>的二进制是：<span class="number">110</span></span><br><span class="line"><span class="number">7</span>的二进制是：<span class="number">111</span></span><br></pre></td></tr></table></figure><p>在二进制表示中，较高比特位的交替频率低于较低比特位， 与下面的热图所示相似，只是位置编码通过使用三角函数在编码维度上降低频率。 由于输出是浮点数，因此此类连续表示比二进制表示法更节省空间。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">P = P[<span class="number">0</span>, :, :].unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>)</span><br><span class="line">d2l.show_heatmaps(P, xlabel=<span class="string">&#x27;Column (encoding dimension)&#x27;</span>,</span><br><span class="line">                  ylabel=<span class="string">&#x27;Row (position)&#x27;</span>, figsize=(<span class="number">3.5</span>, <span class="number">4</span>), cmap=<span class="string">&#x27;Blues&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_self-attention-and-positional-encoding_2.svg" alt=""></p><h4 id="6-3-2-相对位置信息"><a href="#6-3-2-相对位置信息" class="headerlink" title="6.3.2. 相对位置信息"></a>6.3.2. 相对位置信息</h4><p>除了捕获绝对位置信息之外，上述的位置编码还允许模型学习得到输入序列中相对位置信息。这是因为对于任何确定的位置偏移$\delta$，位置$i + \delta$处的位置编码可以线性投影位置$i$处的位置编码来表示。</p><p>这种投影的数学解释是，令$\omega_j = 1/10000^{2j/d}$，对于任何确定的位置偏移$\delta$，<a href="#63-位置编码">位置编码</a>中的任何一对$(p_{i, 2j}, p_{i, 2j+1})$都可以线性投影到$(p_{i+\delta, 2j}, p_{i+\delta, 2j+1})$：</p><script type="math/tex; mode=display">\begin{aligned}&\begin{bmatrix} \cos(\delta \omega_j) & \sin(\delta \omega_j) \\  -\sin(\delta \omega_j) & \cos(\delta \omega_j) \\ \end{bmatrix}\begin{bmatrix} p_{i, 2j} \\  p_{i, 2j+1} \\ \end{bmatrix}\\=&\begin{bmatrix} \cos(\delta \omega_j) \sin(i \omega_j) + \sin(\delta \omega_j) \cos(i \omega_j) \\  -\sin(\delta \omega_j) \sin(i \omega_j) + \cos(\delta \omega_j) \cos(i \omega_j) \\ \end{bmatrix}\\=&\begin{bmatrix} \sin\left((i+\delta) \omega_j\right) \\  \cos\left((i+\delta) \omega_j\right) \\ \end{bmatrix}\\=& \begin{bmatrix} p_{i+\delta, 2j} \\  p_{i+\delta, 2j+1} \\ \end{bmatrix},\end{aligned}</script><p>$2\times 2$投影矩阵不依赖于任何位置的索引$i$。</p><h2 id="7-Transformer"><a href="#7-Transformer" class="headerlink" title="7. Transformer"></a>7. Transformer</h2><p>在<a href="#62-比较卷积神经网络循环神经网络和自注意力">6.2</a>中比较了卷积神经网络（CNN）、循环神经网络（RNN）和自注意力（self-attention）。值得注意的是，自注意力同时具有并行计算和最短的最大路径长度这两个优势。因此，使用自注意力来设计深度架构是很有吸引力的。对比之前仍然依赖循环神经网络实现输入表示的自注意力模型 <code>Cheng.Dong.Lapata.2016,Lin.Feng.Santos.ea.2017,Paulus.Xiong.Socher.2017</code>，Transformer模型完全基于注意力机制，没有任何卷积层或循环神经网络层 <code>Vaswani.Shazeer.Parmar.ea.2017</code>。尽管Transformer最初是应用于在文本数据上的序列到序列学习，但现在已经推广到各种现代的深度学习中，例如语言、视觉、语音和强化学习领域。</p><h3 id="7-1-模型"><a href="#7-1-模型" class="headerlink" title="7.1. 模型"></a>7.1. 模型</h3><p>Transformer作为编码器－解码器架构的一个实例，其整体架构图如下所示。正如所见到的，Transformer是由编码器和解码器组成的。与<a href="#41-模型">带有Bahdanau注意力的循环神经网络编码器-解码器模型</a>中基于Bahdanau注意力实现的序列到序列的学习相比，Transformer的编码器和解码器是基于自注意力的模块叠加而成的，源（输入）序列和目标（输出）序列的<em>嵌入</em>（embedding）表示将加上<em>位置编码</em>（positional encoding），再分别输入到编码器和解码器中。</p><p><img src="/assets/post_img/article74/transformer.svg" alt="transformer架构"></p><p>上图概述了Transformer的架构。从宏观角度来看，Transformer的编码器是由多个相同的层叠加而成的，每个层都有两个子层（子层表示为$\mathrm{sublayer}$）。第一个子层是<em>多头自注意力</em>（multi-head self-attention）池化；第二个子层是<em>基于位置的前馈网络</em>（positionwise feed-forward network）。具体来说，在计算编码器的自注意力时，查询、键和值都来自前一个编码器层的输出。受ResNet中残差网络的启发，每个子层都采用了<em>残差连接</em>（residual connection）。在Transformer中，对于序列中任何位置的任何输入$\mathbf{x} \in \mathbb{R}^d$，都要求满足$\mathrm{sublayer}(\mathbf{x}) \in \mathbb{R}^d$，以便残差连接满足$\mathbf{x} + \mathrm{sublayer}(\mathbf{x}) \in \mathbb{R}^d$。在残差连接的加法计算之后，紧接着应用<em>层规范化</em>（layer normalization）<code>Ba.Kiros.Hinton.2016</code>。因此，输入序列对应的每个位置，Transformer编码器都将输出一个$d$维表示向量。</p><p>Transformer解码器也是由多个相同的层叠加而成的，并且层中使用了残差连接和层规范化。除了编码器中描述的两个子层之外，解码器还在这两个子层之间插入了第三个子层，称为<em>编码器－解码器注意力</em>（encoder-decoder attention）层。在编码器－解码器注意力中，查询来自前一个解码器层的输出，而键和值来自整个编码器的输出。在解码器自注意力中，查询、键和值都来自上一个解码器层的输出。但是，解码器中的每个位置只能考虑该位置之前的所有位置。这种<em>掩蔽</em>（masked）注意力保留了<em>自回归</em>（auto-regressive）属性，确保预测仅依赖于已生成的输出词元。</p><p>在此之前已经描述并实现了基于缩放点积多头注意力和位置编码。接下来将实现Transformer模型的剩余部分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><h3 id="7-2-基于位置的前馈网络"><a href="#7-2-基于位置的前馈网络" class="headerlink" title="7.2. 基于位置的前馈网络"></a>7.2. 基于位置的前馈网络</h3><p>基于位置的前馈网络对序列中的所有位置的表示进行变换时使用的是同一个多层感知机（MLP），这就是称前馈网络是<em>基于位置的</em>（positionwise）的原因。在下面的实现中，输入<code>X</code>的形状（批量大小，时间步数或序列长度，隐单元数或特征维度）将被一个两层的感知机转换成形状为（批量大小，时间步数，<code>ffn_num_outputs</code>）的输出张量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionWiseFFN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;基于位置的前馈网络&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs,</span></span></span><br><span class="line"><span class="params"><span class="function">                 **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PositionWiseFFN, self).__init__(**kwargs)</span><br><span class="line">        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.dense2(self.relu(self.dense1(X)))</span><br></pre></td></tr></table></figure><p>下面的例子显示，改变张量的最里层维度的尺寸，会改变成基于位置的前馈网络的输出尺寸。因为用同一个多层感知机对所有位置上的输入进行变换，所以当所有这些位置的输入相同时，它们的输出也是相同的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ffn = PositionWiseFFN(<span class="number">4</span>, <span class="number">4</span>, <span class="number">8</span>)</span><br><span class="line">ffn.<span class="built_in">eval</span>()</span><br><span class="line">ffn(torch.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)))[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[-<span class="number">1.2386</span>, -<span class="number">0.9917</span>,  <span class="number">0.4708</span>, -<span class="number">0.4516</span>, -<span class="number">0.3069</span>,  <span class="number">0.0447</span>,  <span class="number">0.9740</span>,  <span class="number">0.1313</span>],</span><br><span class="line">        [-<span class="number">1.2386</span>, -<span class="number">0.9917</span>,  <span class="number">0.4708</span>, -<span class="number">0.4516</span>, -<span class="number">0.3069</span>,  <span class="number">0.0447</span>,  <span class="number">0.9740</span>,  <span class="number">0.1313</span>],</span><br><span class="line">        [-<span class="number">1.2386</span>, -<span class="number">0.9917</span>,  <span class="number">0.4708</span>, -<span class="number">0.4516</span>, -<span class="number">0.3069</span>,  <span class="number">0.0447</span>,  <span class="number">0.9740</span>,  <span class="number">0.1313</span>]],</span><br><span class="line">       grad_fn=&lt;SelectBackward0&gt;)</span><br></pre></td></tr></table></figure><h3 id="7-3-残差连接和层规范化"><a href="#7-3-残差连接和层规范化" class="headerlink" title="7.3. 残差连接和层规范化"></a>7.3. 残差连接和层规范化</h3><p>现在让我们关注<em>加法和规范化</em>（add&amp;norm）组件。正如在本节开头所述，这是由残差连接和紧随其后的层规范化组成的。两者都是构建有效的深度架构的关键。</p><p>“批量规范化”章节中解释了在一个小批量的样本内基于批量规范化对数据进行重新中心化和重新缩放的调整。层规范化和批量规范化的目标相同，但层规范化是基于特征维度进行规范化。尽管批量规范化在计算机视觉中被广泛应用，但在自然语言处理任务中（输入通常是变长序列）批量规范化通常不如层规范化的效果好。</p><p>以下代码对比不同维度的层规范化和批量规范化的效果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">ln = nn.LayerNorm(<span class="number">2</span>)</span><br><span class="line">bn = nn.BatchNorm1d(<span class="number">2</span>)</span><br><span class="line">X = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">3</span>]], dtype=torch.float32)</span><br><span class="line"><span class="comment"># 在训练模式下计算X的均值和方差</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;layer norm:&#x27;</span>, ln(X), <span class="string">&#x27;\nbatch norm:&#x27;</span>, bn(X))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># layer norm: tensor([[-1.0000,  1.0000],</span></span><br><span class="line"><span class="comment">#         [-1.0000,  1.0000]], grad_fn=&lt;NativeLayerNormBackward0&gt;)</span></span><br><span class="line"><span class="comment"># batch norm: tensor([[-1.0000, -1.0000],</span></span><br><span class="line"><span class="comment">#         [ 1.0000,  1.0000]], grad_fn=&lt;NativeBatchNormBackward0&gt;)</span></span><br></pre></td></tr></table></figure><p>现在我们可以使用残差连接和层规范化来实现AddNorm类。暂退法也被作为正则化方法使用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AddNorm</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;残差连接后进行层规范化&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, normalized_shape, dropout, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(AddNorm, self).__init__(**kwargs)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.ln = nn.LayerNorm(normalized_shape)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, Y</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.ln(self.dropout(Y) + X)</span><br></pre></td></tr></table></figure><p>残差连接要求两个输入的形状相同，以便加法操作后输出张量的形状相同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">add_norm = AddNorm([<span class="number">3</span>, <span class="number">4</span>], <span class="number">0.5</span>)</span><br><span class="line">add_norm.<span class="built_in">eval</span>()</span><br><span class="line">add_norm(torch.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)), torch.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))).shape</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure><h3 id="7-4-编码器"><a href="#7-4-编码器" class="headerlink" title="7.4. 编码器"></a>7.4. 编码器</h3><p>有了组成transformer编码器的基础组件，现在可以先实现编码器中的一个层。下面的EncoderBlock类包含两个子层：多头自注意力和基于位置的前馈网络，这两个子层都使用了残差连接和紧随的层规范化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;transformer编码器块&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, key_size, query_size, value_size, num_hiddens,</span></span></span><br><span class="line"><span class="params"><span class="function">                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span></span></span><br><span class="line"><span class="params"><span class="function">                 dropout, use_bias=<span class="literal">False</span>, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(EncoderBlock, self).__init__(**kwargs)</span><br><span class="line">        self.attention = d2l.MultiHeadAttention(</span><br><span class="line">            key_size, query_size, value_size, num_hiddens, num_heads, dropout,</span><br><span class="line">            use_bias)</span><br><span class="line">        self.addnorm1 = AddNorm(norm_shape, dropout)</span><br><span class="line">        self.ffn = PositionWiseFFN(</span><br><span class="line">            ffn_num_input, ffn_num_hiddens, num_hiddens)</span><br><span class="line">        self.addnorm2 = AddNorm(norm_shape, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, valid_lens</span>):</span></span><br><span class="line">        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))</span><br><span class="line">        <span class="keyword">return</span> self.addnorm2(Y, self.ffn(Y))</span><br></pre></td></tr></table></figure><p>正如我们所看到的，transformer编码器中的任何层都不会改变其输入的形状。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">X = torch.ones((<span class="number">2</span>, <span class="number">100</span>, <span class="number">24</span>))</span><br><span class="line">valid_lens = torch.tensor([<span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">encoder_blk = EncoderBlock(<span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, [<span class="number">100</span>, <span class="number">24</span>], <span class="number">24</span>, <span class="number">48</span>, <span class="number">8</span>, <span class="number">0.5</span>)</span><br><span class="line">encoder_blk.<span class="built_in">eval</span>()</span><br><span class="line">encoder_blk(X, valid_lens).shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># torch.Size([2, 100, 24])</span></span><br></pre></td></tr></table></figure><p>下面实现的Transformer编码器的代码中，堆叠了<code>num_layers</code>个<code>EncoderBlock</code>类的实例。由于这里使用的是值范围在$-1$和$1$之间的固定位置编码，因此通过学习得到的输入的嵌入表示的值需要先乘以嵌入维度的平方根进行重新缩放，然后再与位置编码相加。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerEncoder</span>(<span class="params">d2l.Encoder</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;transformer编码器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, key_size, query_size, value_size,</span></span></span><br><span class="line"><span class="params"><span class="function">                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,</span></span></span><br><span class="line"><span class="params"><span class="function">                 num_heads, num_layers, dropout, use_bias=<span class="literal">False</span>, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(TransformerEncoder, self).__init__(**kwargs)</span><br><span class="line">        self.num_hiddens = num_hiddens</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, num_hiddens)</span><br><span class="line">        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)</span><br><span class="line">        self.blks = nn.Sequential()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            self.blks.add_module(<span class="string">&quot;block&quot;</span>+<span class="built_in">str</span>(i),</span><br><span class="line">                EncoderBlock(key_size, query_size, value_size, num_hiddens,</span><br><span class="line">                             norm_shape, ffn_num_input, ffn_num_hiddens,</span><br><span class="line">                             num_heads, dropout, use_bias))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, valid_lens, *args</span>):</span></span><br><span class="line">        <span class="comment"># 因为位置编码值在-1和1之间，</span></span><br><span class="line">        <span class="comment"># 因此嵌入值乘以嵌入维度的平方根进行缩放，</span></span><br><span class="line">        <span class="comment"># 然后再与位置编码相加。</span></span><br><span class="line">        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))</span><br><span class="line">        self.attention_weights = [<span class="literal">None</span>] * <span class="built_in">len</span>(self.blks)</span><br><span class="line">        <span class="keyword">for</span> i, blk <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.blks):</span><br><span class="line">            X = blk(X, valid_lens)</span><br><span class="line">            self.attention_weights[</span><br><span class="line">                i] = blk.attention.attention.attention_weights</span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><p>下面我们指定了超参数来创建一个两层的transformer编码器。 Transformer编码器输出的形状是（批量大小，时间步数目，num_hiddens）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">encoder = TransformerEncoder(</span><br><span class="line">    <span class="number">200</span>, <span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, [<span class="number">100</span>, <span class="number">24</span>], <span class="number">24</span>, <span class="number">48</span>, <span class="number">8</span>, <span class="number">2</span>, <span class="number">0.5</span>)</span><br><span class="line">encoder.<span class="built_in">eval</span>()</span><br><span class="line">encoder(torch.ones((<span class="number">2</span>, <span class="number">100</span>), dtype=torch.long), valid_lens).shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># torch.Size([2, 100, 24])</span></span><br></pre></td></tr></table></figure><h3 id="7-5-解码器"><a href="#7-5-解码器" class="headerlink" title="7.5. 解码器"></a>7.5. 解码器</h3><p>如模型图所示，Transformer解码器也是由多个相同的层组成。在<code>DecoderBlock</code>类中实现的每个层包含了三个子层：解码器自注意力、“编码器-解码器”注意力和基于位置的前馈网络。这些子层也都被残差连接和紧随的层规范化围绕。</p><p>正如在本节前面所述，在掩蔽多头解码器自注意力层（第一个子层）中，查询、键和值都来自上一个解码器层的输出。关于<em>序列到序列模型</em>（sequence-to-sequence model），在训练阶段，其输出序列的所有位置（时间步）的词元都是已知的；然而，在预测阶段，其输出序列的词元是逐个生成的。因此，在任何解码器时间步中，只有生成的词元才能用于解码器的自注意力计算中。为了在解码器中保留自回归的属性，其掩蔽自注意力设定了参数<code>dec_valid_lens</code>，以便任何查询都只会与解码器中所有已经生成词元的位置（即直到该查询位置为止）进行注意力计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;解码器中第i个块&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, key_size, query_size, value_size, num_hiddens,</span></span></span><br><span class="line"><span class="params"><span class="function">                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span></span></span><br><span class="line"><span class="params"><span class="function">                 dropout, i, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(DecoderBlock, self).__init__(**kwargs)</span><br><span class="line">        self.i = i</span><br><span class="line">        self.attention1 = d2l.MultiHeadAttention(</span><br><span class="line">            key_size, query_size, value_size, num_hiddens, num_heads, dropout)</span><br><span class="line">        self.addnorm1 = AddNorm(norm_shape, dropout)</span><br><span class="line">        self.attention2 = d2l.MultiHeadAttention(</span><br><span class="line">            key_size, query_size, value_size, num_hiddens, num_heads, dropout)</span><br><span class="line">        self.addnorm2 = AddNorm(norm_shape, dropout)</span><br><span class="line">        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens,</span><br><span class="line">                                   num_hiddens)</span><br><span class="line">        self.addnorm3 = AddNorm(norm_shape, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, state</span>):</span></span><br><span class="line">        enc_outputs, enc_valid_lens = state[<span class="number">0</span>], state[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 训练阶段，输出序列的所有词元都在同一时间处理，</span></span><br><span class="line">        <span class="comment"># 因此state[2][self.i]初始化为None。</span></span><br><span class="line">        <span class="comment"># 预测阶段，输出序列是通过词元一个接着一个解码的，</span></span><br><span class="line">        <span class="comment"># 因此state[2][self.i]包含着直到当前时间步第i个块解码的输出表示</span></span><br><span class="line">        <span class="keyword">if</span> state[<span class="number">2</span>][self.i] <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            key_values = X</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            key_values = torch.cat((state[<span class="number">2</span>][self.i], X), axis=<span class="number">1</span>)</span><br><span class="line">        state[<span class="number">2</span>][self.i] = key_values</span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            batch_size, num_steps, _ = X.shape</span><br><span class="line">            <span class="comment"># dec_valid_lens的开头:(batch_size,num_steps),</span></span><br><span class="line">            <span class="comment"># 其中每一行是[1,2,...,num_steps]</span></span><br><span class="line">            dec_valid_lens = torch.arange(</span><br><span class="line">                <span class="number">1</span>, num_steps + <span class="number">1</span>, device=X.device).repeat(batch_size, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            dec_valid_lens = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 自注意力</span></span><br><span class="line">        X2 = self.attention1(X, key_values, key_values, dec_valid_lens)</span><br><span class="line">        Y = self.addnorm1(X, X2)</span><br><span class="line">        <span class="comment"># 编码器－解码器注意力。</span></span><br><span class="line">        <span class="comment"># enc_outputs的开头:(batch_size,num_steps,num_hiddens)</span></span><br><span class="line">        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)</span><br><span class="line">        Z = self.addnorm2(Y, Y2)</span><br><span class="line">        <span class="keyword">return</span> self.addnorm3(Z, self.ffn(Z)), state</span><br></pre></td></tr></table></figure><p>为了便于在“编码器－解码器”注意力中进行缩放点积计算和残差连接中进行加法计算，编码器和解码器的特征维度都是num_hiddens。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">decoder_blk = DecoderBlock(<span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, [<span class="number">100</span>, <span class="number">24</span>], <span class="number">24</span>, <span class="number">48</span>, <span class="number">8</span>, <span class="number">0.5</span>, <span class="number">0</span>)</span><br><span class="line">decoder_blk.<span class="built_in">eval</span>()</span><br><span class="line">X = torch.ones((<span class="number">2</span>, <span class="number">100</span>, <span class="number">24</span>))</span><br><span class="line">state = [encoder_blk(X, valid_lens), valid_lens, [<span class="literal">None</span>]]</span><br><span class="line">decoder_blk(X, state)[<span class="number">0</span>].shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># torch.Size([2, 100, 24])</span></span><br></pre></td></tr></table></figure><p>现在我们构建了由num_layers个DecoderBlock实例组成的完整的transformer解码器。最后，通过一个全连接层计算所有vocab_size个可能的输出词元的预测值。解码器的自注意力权重和编码器解码器注意力权重都被存储下来，方便日后可视化的需要。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerDecoder</span>(<span class="params">d2l.AttentionDecoder</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, key_size, query_size, value_size,</span></span></span><br><span class="line"><span class="params"><span class="function">                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,</span></span></span><br><span class="line"><span class="params"><span class="function">                 num_heads, num_layers, dropout, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(TransformerDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.num_hiddens = num_hiddens</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, num_hiddens)</span><br><span class="line">        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)</span><br><span class="line">        self.blks = nn.Sequential()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            self.blks.add_module(<span class="string">&quot;block&quot;</span>+<span class="built_in">str</span>(i),</span><br><span class="line">                DecoderBlock(key_size, query_size, value_size, num_hiddens,</span><br><span class="line">                             norm_shape, ffn_num_input, ffn_num_hiddens,</span><br><span class="line">                             num_heads, dropout, i))</span><br><span class="line">        self.dense = nn.Linear(num_hiddens, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_state</span>(<span class="params">self, enc_outputs, enc_valid_lens, *args</span>):</span></span><br><span class="line">        <span class="keyword">return</span> [enc_outputs, enc_valid_lens, [<span class="literal">None</span>] * self.num_layers]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, state</span>):</span></span><br><span class="line">        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))</span><br><span class="line">        self._attention_weights = [[<span class="literal">None</span>] * <span class="built_in">len</span>(self.blks) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span> (<span class="number">2</span>)]</span><br><span class="line">        <span class="keyword">for</span> i, blk <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.blks):</span><br><span class="line">            X, state = blk(X, state)</span><br><span class="line">            <span class="comment"># 解码器自注意力权重</span></span><br><span class="line">            self._attention_weights[<span class="number">0</span>][</span><br><span class="line">                i] = blk.attention1.attention.attention_weights</span><br><span class="line">            <span class="comment"># “编码器－解码器”自注意力权重</span></span><br><span class="line">            self._attention_weights[<span class="number">1</span>][</span><br><span class="line">                i] = blk.attention2.attention.attention_weights</span><br><span class="line">        <span class="keyword">return</span> self.dense(X), state</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">attention_weights</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self._attention_weights</span><br></pre></td></tr></table></figure><h3 id="7-6-训练"><a href="#7-6-训练" class="headerlink" title="7.6. 训练"></a>7.6. 训练</h3><p>依照Transformer架构来实例化编码器－解码器模型。在这里，指定Transformer的编码器和解码器都是2层，都使用4头注意力。为了进行序列到序列的学习，下面在“英语－法语”机器翻译数据集上训练Transformer模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens, num_layers, dropout, batch_size, num_steps = <span class="number">32</span>, <span class="number">2</span>, <span class="number">0.1</span>, <span class="number">64</span>, <span class="number">10</span></span><br><span class="line">lr, num_epochs, device = <span class="number">0.005</span>, <span class="number">200</span>, d2l.try_gpu()</span><br><span class="line">ffn_num_input, ffn_num_hiddens, num_heads = <span class="number">32</span>, <span class="number">64</span>, <span class="number">4</span></span><br><span class="line">key_size, query_size, value_size = <span class="number">32</span>, <span class="number">32</span>, <span class="number">32</span></span><br><span class="line">norm_shape = [<span class="number">32</span>]</span><br><span class="line"></span><br><span class="line">train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)</span><br><span class="line"></span><br><span class="line">encoder = TransformerEncoder(</span><br><span class="line">    <span class="built_in">len</span>(src_vocab), key_size, query_size, value_size, num_hiddens,</span><br><span class="line">    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span><br><span class="line">    num_layers, dropout)</span><br><span class="line">decoder = TransformerDecoder(</span><br><span class="line">    <span class="built_in">len</span>(tgt_vocab), key_size, query_size, value_size, num_hiddens,</span><br><span class="line">    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span><br><span class="line">    num_layers, dropout)</span><br><span class="line">net = d2l.EncoderDecoder(encoder, decoder)</span><br><span class="line">d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)</span><br></pre></td></tr></table></figure><p>训练结束后，使用transformer模型将一些英语句子翻译成法语，并且计算它们的BLEU分数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">engs = [<span class="string">&#x27;go .&#x27;</span>, <span class="string">&quot;i lost .&quot;</span>, <span class="string">&#x27;he\&#x27;s calm .&#x27;</span>, <span class="string">&#x27;i\&#x27;m home .&#x27;</span>]</span><br><span class="line">fras = [<span class="string">&#x27;va !&#x27;</span>, <span class="string">&#x27;j\&#x27;ai perdu .&#x27;</span>, <span class="string">&#x27;il est calme .&#x27;</span>, <span class="string">&#x27;je suis chez moi .&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> eng, fra <span class="keyword">in</span> <span class="built_in">zip</span>(engs, fras):</span><br><span class="line">    translation, dec_attention_weight_seq = d2l.predict_seq2seq(</span><br><span class="line">        net, eng, src_vocab, tgt_vocab, num_steps, device, <span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;eng&#125;</span> =&gt; <span class="subst">&#123;translation&#125;</span>, &#x27;</span>,</span><br><span class="line">          <span class="string">f&#x27;bleu <span class="subst">&#123;d2l.bleu(translation, fra, k=<span class="number">2</span>):<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># go . =&gt; va !,  bleu 1.000</span></span><br><span class="line"><span class="comment"># i lost . =&gt; je suis avons été battues .,  bleu 0.000</span></span><br><span class="line"><span class="comment"># he&#x27;s calm . =&gt; il est malade .,  bleu 0.658</span></span><br><span class="line"><span class="comment"># i&#x27;m home . =&gt; je suis chez moi .,  bleu 1.000</span></span><br></pre></td></tr></table></figure><p>当进行最后一个英语到法语的句子翻译工作时，让我们可视化transformer的注意力权重。编码器自注意力权重的形状为（编码器层数，注意力头数，num_steps或查询的数目，num_steps或“键－值”对的数目）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">enc_attention_weights = torch.cat(net.encoder.attention_weights, <span class="number">0</span>).reshape((num_layers, num_heads,</span><br><span class="line">    -<span class="number">1</span>, num_steps))</span><br><span class="line">enc_attention_weights.shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># torch.Size([2, 4, 10, 10])</span></span><br></pre></td></tr></table></figure><p>在编码器的自注意力中，查询和键都来自相同的输入序列。因为填充词元是不携带信息的，因此通过指定输入序列的有效长度可以避免查询与使用填充词元的位置计算注意力。接下来，将逐行呈现两层多头注意力的权重。每个注意力头都根据查询、键和值的不同的表示子空间来表示不同的注意力。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(</span><br><span class="line">    enc_attention_weights.cpu(), xlabel=<span class="string">&#x27;Key positions&#x27;</span>,</span><br><span class="line">    ylabel=<span class="string">&#x27;Query positions&#x27;</span>, titles=[<span class="string">&#x27;Head %d&#x27;</span> % i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">5</span>)],</span><br><span class="line">    figsize=(<span class="number">7</span>, <span class="number">3.5</span>))</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_transformer_1.svg" alt="o1"></p><p>为了可视化解码器的自注意力权重和“编码器－解码器”的注意力权重，我们需要完成更多的数据操作工作。例如用零填充被掩蔽住的注意力权重。值得注意的是，解码器的自注意力权重和“编码器－解码器”的注意力权重都有相同的查询：即以<em>序列开始词元</em>（beginning-of-sequence,BOS）打头，再与后续输出的词元共同组成序列。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">dec_attention_weights_2d = [head[<span class="number">0</span>].tolist()</span><br><span class="line">                            <span class="keyword">for</span> step <span class="keyword">in</span> dec_attention_weight_seq</span><br><span class="line">                            <span class="keyword">for</span> attn <span class="keyword">in</span> step <span class="keyword">for</span> blk <span class="keyword">in</span> attn <span class="keyword">for</span> head <span class="keyword">in</span> blk]</span><br><span class="line">dec_attention_weights_filled = torch.tensor(</span><br><span class="line">    pd.DataFrame(dec_attention_weights_2d).fillna(<span class="number">0.0</span>).values)</span><br><span class="line">dec_attention_weights = dec_attention_weights_filled.reshape((-<span class="number">1</span>, <span class="number">2</span>, num_layers, num_heads, num_steps))</span><br><span class="line">dec_self_attention_weights, dec_inter_attention_weights = \</span><br><span class="line">    dec_attention_weights.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">4</span>)</span><br><span class="line">dec_self_attention_weights.shape, dec_inter_attention_weights.shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># (torch.Size([2, 4, 6, 10]), torch.Size([2, 4, 6, 10]))</span></span><br></pre></td></tr></table></figure><p>由于解码器自注意力的自回归属性，查询不会对当前位置之后的“键－值”对进行注意力计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plusonetoincludethebeginning-of-sequencetoken</span></span><br><span class="line">d2l.show_heatmaps(</span><br><span class="line">    dec_self_attention_weights[:, :, :, :<span class="built_in">len</span>(translation.split()) + <span class="number">1</span>],</span><br><span class="line">    xlabel=<span class="string">&#x27;Key positions&#x27;</span>, ylabel=<span class="string">&#x27;Query positions&#x27;</span>,</span><br><span class="line">    titles=[<span class="string">&#x27;Head %d&#x27;</span> % i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">5</span>)], figsize=(<span class="number">7</span>, <span class="number">3.5</span>))</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_transformer_2.svg" alt="o2"></p><p>与编码器的自注意力的情况类似，通过指定输入序列的有效长度，输出序列的查询不会与输入序列中填充位置的词元进行注意力计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(</span><br><span class="line">    dec_inter_attention_weights, xlabel=<span class="string">&#x27;Key positions&#x27;</span>,</span><br><span class="line">    ylabel=<span class="string">&#x27;Query positions&#x27;</span>, titles=[<span class="string">&#x27;Head %d&#x27;</span> % i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">5</span>)],</span><br><span class="line">    figsize=(<span class="number">7</span>, <span class="number">3.5</span>))</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_transformer_3.svg" alt="o3"></p><p>尽管transformer架构是为了“序列到序列”的学习而提出的，但正如我们将在本书后面提及的那样，transformer编码器或transformer解码器通常被单独用于不同的深度学习任务中。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;p&gt;灵长类动物的视觉系统接受了大量的感官输入，这些感官输入远远超过了大脑能够完全处理的程度。然而，并非所有刺激的影响都是相等的。意识的聚集和专注使灵长类动物能够在复杂的视觉环境中将注意力引向感兴趣的物体，例如猎物和天敌。只关注一小部分信息的能力对进化更加有意义，使人类得以生存和成功。&lt;/p&gt;
&lt;p&gt;自19世纪以来，科学家们一直致力于研究认知神经科学领域的注意力。本章的很多章节将涉及到一些研究。&lt;br&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://silencezheng.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>导向滤波</title>
    <link href="http://silencezheng.top/2022/11/04/article73/"/>
    <id>http://silencezheng.top/2022/11/04/article73/</id>
    <published>2022-11-04T13:27:06.000Z</published>
    <updated>2022-11-04T13:28:52.059Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>导向滤波学习，也可以说是论文笔记了。<br><span id="more"></span></p><h2 id="各向同性（isotropy）滤波与各向异性（anisotropy）滤波"><a href="#各向同性（isotropy）滤波与各向异性（anisotropy）滤波" class="headerlink" title="各向同性（isotropy）滤波与各向异性（anisotropy）滤波"></a>各向同性（isotropy）滤波与各向异性（anisotropy）滤波</h2><p>对图像来说，各向同性滤波就是说滤波器在各方向上的梯度变化是相同的，对各个方向一视同仁的进行滤波。各向异性滤波是指滤波器会对各方向进行区别对待，有选择的进行滤波。</p><p>高斯滤波属于<strong>各向同性滤波</strong>，根据二维高斯的图像就可以看出。</p><p>双边滤波属于<strong>各向异性滤波</strong>，因为它会根据图像梯度变化情况改变滤波器形状。</p><h2 id="导向滤波（Guided-Filter）"><a href="#导向滤波（Guided-Filter）" class="headerlink" title="导向滤波（Guided Filter）"></a>导向滤波（Guided Filter）</h2><p>导向滤波也属于各向异性滤波，也可以作为一种保边（Edge-preserving）滤波算法。</p><blockquote><p>Derived from a local linear model, the guided filter generates the filtering output by considering the content of a guidance image, which can be the input image itself or another different image.</p><p>We demonstrate that the guided filter is both effective and efficient in a great variety of computer vision and computer graphics applications including noise reduction, detail smoothing/enhancement, HDR compression, image matting/feathering, haze removal, and joint upsampling.</p></blockquote><p>导向滤波通过考虑<strong>引导图像</strong>的内容来生成滤波输出，引导图像可以是输入图像本身或另一个不同的图像。[1]中提到了双边滤波相比于导向滤波的两点缺陷，“have unwanted gradient reversal artifacts near edges”和难以进行维持精度的快速计算，这也是导向滤波的优势。</p><p>文中还提到了<strong>联合双边滤波器</strong>（joint bilateral filter），它也是利用了引导图来改善双边滤波<strong>权值不稳定</strong>的问题（双边滤波边缘出现<strong>梯度翻转</strong>现象的原因）。</p><p>文中先定义了一个<strong>通用的平移不变的线性滤波过程</strong>，包括一个引导图像 $\textit{I}$、一个输入图像 $\textit{p}$ 和一个输出图像 $\textit{q}$。</p><script type="math/tex; mode=display">\begin{equation}q_i=\sum_j W_{i j}(I) p_j \tag{1}\end{equation}</script><p>这里 $i$ 和 $j$ 表示的是两个像素，而不是像素的横纵坐标！像素 $i$ 的位置坐标表示为 $\mathbb{x}_i$，$q_i$ 表示输出图的像素 $i$ 处的值，滤波核 $W_{i j}(I)$ 是关于导向图和输入图的函数，当然也与 $i$ 有关。</p><p>联合双边滤波器符合上述的线性滤波过程，其滤波核 $W^{\mathrm{bf}}$ 如下：</p><script type="math/tex; mode=display">\begin{equation}W_{i j}^{\mathrm{bf}}(I)=\frac{1}{K_i} \exp \left(-\frac{\left|\mathbf{x}_i-\mathbf{x}_j\right|^2}{\sigma_{\mathrm{s}}^2}\right) \exp \left(-\frac{\left|I_i-I_j\right|^2}{\sigma_{\mathrm{r}}^2}\right) \tag{2}\end{equation}</script><p>其中，$K_i$是归一化参数，用于保证$\sum_j W_{i j}^{\mathrm{bf}} = 1$，$\sigma_{\mathrm{s}}$ 和 $\sigma_{\mathrm{r}}$ 分别代表空域和值域（原文说值可以是 intensity 或 color）的对应参数，分别用于调整空域和值域的滤波程度（原文为similarity，我理解就和高斯中的标准差类似）。原文说当导向图和输入图相同时，联合双边滤波就退化为双边滤波，我理解里的双边滤波应该是要加两个2倍在分母的，但是效果应该一样。</p><p>下面简单讨论下导向滤波的推导，主要还是学习如何计算。 文中首先假设导向滤波是一个导向图 $I$ 和 输出图 $q$ 间的<strong>局部线性模型</strong>。令 $w_k$ 是 $I$ 中的一个正方形窗口，其中心为像素 $k$，输出 $q$ 为 $I$ 在 $w_k$ 上的一个线性变换：</p><script type="math/tex; mode=display">\begin{equation}q_i=a_k I_i+b_k, \forall i \in \omega_k \tag{3}\end{equation}</script><p>这个稍微想一下就可以理解，假设 $w_k$ 是一个在 $I$ 上移动的九宫格，由于 $q$、$I$ 和 $p$都是尺寸一致的，那么每次移动产生的九个线性变化值就是 $q$ 对应 $I$ 位置上的值。当然这会引出一个问题，即同一个像素可能会被不同的窗口计算出多个值，如何确定最终输出的 $q_i$ 呢？文中提出了一种简单的处理办法，即取所有这些输出的平均。</p><p>$a_k$ 和 $b_k$ 是窗口 $k$ 中的常量线性系数，顺便一提 $w_k$ 的半径定义为 $r$。 关于为何使用局部线性模型，是因为它确保了输出和导向图的边缘一致，具体可以看原文。</p><p>为确定上述的两个线性系数，原文将输出建模为输入减去不想要的内容（噪声、纹理等）:</p><script type="math/tex; mode=display">q_i = p_i - n_i</script><p>然后通过最小化输入和输出的差异来实现，具体来说，最小化如下函数(cost function in $w_k$)：</p><script type="math/tex; mode=display">\begin{equation}E\left(a_k, b_k\right)=\sum_{i \in \omega_k}\left(\left(a_k I_i+b_k-p_i\right)^2+\epsilon a_k^2\right) \tag{4}\end{equation}</script><p>其中 $\epsilon$ 是正则化参数，用于防止 $a_k$ 变得太大。通过线性回归求解$(4)$可得如下：</p><script type="math/tex; mode=display">\begin{equation}a_k=\frac{\frac{1}{|\omega|} \sum_{i \in \omega_k} I_i p_i-\mu_k \bar{p}_k}{\sigma_k^2+\epsilon} \tag{5}\end{equation}</script><script type="math/tex; mode=display">\begin{equation}b_k=\bar{p}_k-a_k \mu_k \tag{6}\end{equation}</script><p>其中 $\mu_k$ 和 $\sigma_k^2$ 分别是导向图在窗口 $k$ 中部分的均值和方差（像素值），$|\omega|$ 是窗口 $k$ 中的像素数，$\bar{p}_k$ 是 输入图 $p$ 在窗口 $k$ 中部分的均值，表示为 $\bar{p}_k=\frac{1}{|\omega|} \sum_{i \in \omega_k} p_i$。</p><p>解释完各个符号的含义，让我们思考将上述的局部线性模型<strong>应用在整幅图像上</strong>，将图像中每一个能放置 $w_k$ 的区域进行运算（考虑将 $I$ 和 $p$ 叠放），并采用之前提到的取均值的方式，可得到滤波器输出如下：</p><script type="math/tex; mode=display">\begin{equation}q_i =\frac{1}{|\omega|} \sum_{k: i \in \omega_k}\left(a_k I_i+b_k\right) \tag{7}\end{equation}</script><script type="math/tex; mode=display">\begin{equation}=\bar{a}_i I_i+\bar{b}_i \tag{8}\end{equation}</script><p>其中$\bar{a}_i=\frac{1}{|\omega|} \sum_{k \in \omega_i} a_k$，$\bar{b}_i=\frac{1}{|\omega|} \sum_{k \in \omega_i} b_k$。</p><p>下面原文对上述输出的保边性进行了一些论证，并指出$(5), (6), (8)$中的关系是在图像滤波中真实存在的，并且这三个关系（公式）可以分别重写为输入的加权和，如下：</p><script type="math/tex; mode=display">a_k=\sum_j A_{k j}(I) p_j \\b_k=\sum_j B_{k j}(I) p_j \\q_i=\sum_j W_{i j}(I) p_j</script><p>其中 $A_{i j}, B_{i j}, W_{i j}$ 为三个仅依赖于导向图 $I$ 的权重，可以看到最终推出了一个输出和导向图与输入的<strong>平移不变线性滤波</strong>：$q_i=\sum_j W_{i j}(I) p_j$。那么我们只需要关心权重如何计算即可：</p><script type="math/tex; mode=display">\begin{equation}W_{i j}(I)=\frac{1}{|\omega|^2} \sum_{k:(i, j) \in \omega_k}\left(1+\frac{\left(I_i-\mu_k\right)\left(I_j-\mu_k\right)}{\sigma_k^2+\epsilon}\right) \tag{9}\end{equation}</script><p>解读一下这个权重，对于给定的输出像素 $i$，我们先寻找重叠图像（$I$ 和 $p$）中包含该像素的所有窗口 $k$，对于每个窗口 $k$，在 $I$ 上计算 $\mu_k$ 和 $\sigma_k^2$ 后代入计算权重，然后求得加权和。由于可以证明 $\sum_j W_{i j}(I)=1$，所以不需要对权重再进行归一化处理。</p><p>总之，导向滤波符合一个平移不变的线性滤波模型，其权重通过导向图计算，最终结合输入图产生输出。</p><p><img src="/assets/post_img/article73/idea.jpeg" alt="idea"></p><h2 id="计算方式"><a href="#计算方式" class="headerlink" title="计算方式"></a>计算方式</h2><p>$(5), (6), (8)$ 的三个方程是导向滤波器的一种定义，令 $f_{mean}$ 表示半径为 $r$ 的均值滤波器，correlation（corr，<strong>相关系数</strong>），variance（var，<strong>方差</strong>）和covariance（cov，<strong>协方差</strong>）分别表示其本身含义。则可给出算法的伪代码如下：</p><p><img src="/assets/post_img/article73/pseudocode-normal.png" alt="normal"></p><p>算法中出现的<code>./ .*</code>等分别代表逐像素除、逐像素乘等操作。</p><h2 id="个人的一点理解"><a href="#个人的一点理解" class="headerlink" title="个人的一点理解"></a>个人的一点理解</h2><p>文中总共提出了两种对导向滤波的定义方式，先是得到全图范围上的线性模型 $q_i=\bar{a}_i I_i+\bar{b}_i$，后又重新整理成线性滤波的形式 $q_i=\sum_j W_{i j}(I) p_j$，并求出了仅依赖导向图的滤波核权重。然后从线性模型和滤波核的角度分别分析了导向滤波Edge-Preserving的性质，考虑 $I \equiv p$ 的情况。</p><h2 id="快速导向滤波（Fast-Guided-Filter）"><a href="#快速导向滤波（Fast-Guided-Filter）" class="headerlink" title="快速导向滤波（Fast Guided Filter）"></a>快速导向滤波（Fast Guided Filter）</h2><p>还没看[3]，先贴一个伪代码，日后需要再回来整理。</p><p><img src="/assets/post_img/article73/pseudocode-fast.png" alt="fast"></p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]Kaiming He, Jian Sun, Xiaoou Tang, Guided Image Filtering. IEEE Transactions on Pattern Analysis and Machine Intelligence, Volume 35, Issue 6, pp. 1397-1409, June 2013<br>[2]<a href="https://zhuanlan.zhihu.com/p/161666126">https://zhuanlan.zhihu.com/p/161666126</a><br>[3]<a href="https://arxiv.org/abs/1505.00996v1">https://arxiv.org/abs/1505.00996v1</a><br>[4]<a href="http://kaiminghe.com/eccv10/">http://kaiminghe.com/eccv10/</a><br>[5]Guided Image Filtering, by Kaiming He, Jian Sun, and Xiaoou Tang, in ECCV 2010 (Oral).</p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于 <a href="https://silencezheng.top">silencezheng.top</a>。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;导向滤波学习，也可以说是论文笔记了。&lt;br&gt;</summary>
    
    
    
    
    <category term="计算机视觉" scheme="http://silencezheng.top/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>目标检测常用评价指标</title>
    <link href="http://silencezheng.top/2022/10/24/article72/"/>
    <id>http://silencezheng.top/2022/10/24/article72/</id>
    <published>2022-10-23T16:50:28.000Z</published>
    <updated>2022-10-23T16:52:21.268Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>关于目标检测领域的常用评价指标进行一些个人解读，欢迎批评指正。</p><p>评价指标系列第二篇～上一次说了语义分割的内容，这次来搞目标检测，看了十余篇内容后终于搞明白些了，记录一下。本文说的内容比较浅，一方面是笔者水平实在菜鸡，另一方面写文的主要目的是让不了解目标检测的人读完能上手去做，而不用查三十个资料后再开始，就像博客的Slogan一样，“帮你在学习技术的路上节约半小时”。<br><span id="more"></span></p><h2 id="前置理解"><a href="#前置理解" class="headerlink" title="前置理解"></a>前置理解</h2><p>上一篇中，介绍了混淆矩阵和一些语义分割常用评价指标，例如P、R、IoU，这些在目标检测中都会用到，但是相对于语义分割来说，更加抽象一些。</p><p>目标检测对我个人来说是一个相对陌生的领域，所以本文准备先从目标检测的一般过程说起。首先，目标检测中的基本单元是框，我把Ground Truth称为<strong>真实框</strong>（标签），把Prediction称为<strong>预测框</strong>。框中的内容可能是<strong>目标</strong>（需要），也可能是<strong>背景</strong>（不需要）。</p><p>对于目标检测来说，模型需要完成的任务有两个，一是产生目标的预测框，二是对框内目标的类别进行预测，这又称为<strong>回归分支</strong>（连续）和<strong>分类分支</strong>（离散）。模型的预测输出通常如下所示（假设三分类，<strong>实际上不同模型的输出格式也不尽相同的</strong>。）：</p><script type="math/tex; mode=display">\mathrm{y}=\left[\begin{array}{l}\mathrm{p}_{\mathrm{c}} \\\mathrm{b}_{\mathrm{x}} \\\mathrm{b}_{\mathrm{y}} \\\mathrm{b}_{\mathrm{w}} \\\mathrm{b}_{\mathrm{h}} \\\mathrm{C}_1 \\\mathrm{C}_2 \\\mathrm{C}_3\end{array}\right], \mathrm{y}_{\text {true }}=\left[\begin{array}{c}1 \\40 \\45 \\80 \\60 \\0 \\1 \\0\end{array}\right], \mathrm{y}_{\text {pred }}=\left[\begin{array}{c}0.88 \\41 \\46 \\82 \\59 \\0.01 \\0.95 \\0.04\end{array}\right]</script><p>其中, $\mathrm{p}_{\mathrm{c}}$ 为预测结果的置信度（Confidence），表达预测框内包含目标的概率。$\mathrm{b}_{\mathrm{x}}, \mathrm{b}_{\mathrm{y}}, \mathrm{b}_{\mathrm{w}}, \mathrm{b}_{\mathrm{h}}$ 分别为预测框左上点$x$坐标和$y$坐标以及预测框的宽度、长度，也可以是预测框的左上、右下两点的$x$坐标和$y$坐标，表达的意思是相同的。 $\mathrm{C}_1, \mathrm{C}_2, \mathrm{C}_3$ 为目标属于某个类别的概率。</p><p>真实框和预测框的数量可能是不对等的，这是目标检测与语义分割的一大区别。现在我准备把这些内容再简化，归类为三个信息，<strong>目标置信度</strong>、<strong>定位信息</strong>和<strong>分类置信度</strong>，这里面有些事情需要讲清楚。</p><p>第一，<strong>分类置信度</strong>可能有互斥和不互斥两种，取决于是否使用了softmax计算，本文中，默认分类置信度是互斥的，也就是说当模型输出了一个预测框，框内“目标”的类别就固定了（不论是否真的存在目标）。</p><p>第二，预测框与真实框的贴合程度可以由<strong>定位信息</strong>计算IoU表示，上次提到了。</p><p>以上，前置理解完毕，可以开始进行模型评估了。</p><h2 id="重返混淆矩阵"><a href="#重返混淆矩阵" class="headerlink" title="重返混淆矩阵"></a>重返混淆矩阵</h2><p>从我们现有的信息来看，我们只有两个数值：<strong>目标置信度</strong>和<strong>定位信息</strong>。定位信息可以求IoU，但它依然是数值。基于mAP是目标检测中热门的评估指标的现状，我们希望能够继续用混淆矩阵来对预测框进行分类。众所周知，对数值分类的最简单方式就是在集合中画一条界限，将数值集分为两块，这条界限我们称为<strong>阈值</strong>。</p><p>我们有两个数值，自然产生了两个阈值，即<strong>置信度阈值</strong>和<strong>IoU阈值</strong>。显然，前者能把预测分成里面大概率是目标的框和里面大概率是背景的框，后者则把预测分成了很像某个目标的框和与某个目标不沾边的框。好了，现在我们来说混淆矩阵的事。</p><p>由于预测框类别已经固定，目标置信度和定位信息就是用来确定这框里面到底有没有目标，两个阈值用来判断一件事，这不就是二分类吗？没错，目标检测中我们还是用二分类，和之前多分类按照每个类别来看依然是二分类问题一样。</p><p>那现在事情就变得简单了，只需要搞懂如何区别正负就可以了，这是由目标置信度和定位信息共同决定的。下面给出方法，首先我们假设测试集有$N$张图片，我们从中取出一张图片$Img_n$，取该图片中属于类别$C_1$的目标的真实框和预测为类别$C_1$的预测框作为待分类集合。</p><ul><li>TP：与某一真实框的IoU值大于IoU阈值，且目标置信度大于置信度阈值的预测框数量。每个真实框仅能匹配一个预测框，这意味这TP的最大值为真实框的数量。</li><li>FN：漏检测的真实框数量。</li><li>FP：目标置信度大于置信度阈值但不满足TP条件的预测框数量。</li><li>TN：不考虑，因为Negative的样本想画可以画无数个，没有价值。</li></ul><p>初次看到这个判断方法多少还是有点懵，笔者解释一下，首先我们希望预测结果对每个目标至多只有一个预测框，这导致了即便有多个符合双阈值要求的预测框，对同一目标也只能<strong>选择一个置信度最高的预测框作为TP</strong>，其余重复预测框（虽然满足置信度要求）都进入FP。这样以后，能进入TP的目标也被确定了，剩下的真实框自然就成为了FN。</p><h2 id="准确率-召回率曲线（Precision-Recall-Curve）"><a href="#准确率-召回率曲线（Precision-Recall-Curve）" class="headerlink" title="准确率-召回率曲线（Precision-Recall Curve）"></a>准确率-召回率曲线（Precision-Recall Curve）</h2><p>先回顾一下Precision和Recall的计算方式，这里就只考虑Positive了：</p><script type="math/tex; mode=display">Precision = \frac{TP}{TP+FP} \\Recall = \frac{TP}{TP+FN}</script><p>由上面的分析可以看出，双阈值会影响P和R，通过调整阈值，就可以获得多个$(R,P)$对，通常我们通过指定IoU阈值（不小于0.5），调整置信度阈值的方式获得P-R曲线。事实上，精确率和召回率是一对由置信度阈值控制的冲突的变量，如果想要精准率提高，召回率则会下降，如果要召回率提高，精准率则会下降。当置信度阈值下降时，Recall单调上升，Precision总体呈下降趋势，这也很好理解，查的更全但是查的不准。</p><p><img src="/assets/post_img/article72/p-r.webp" alt="pr"></p><p>需要注意的是，在计算P-R曲线的过程中，置信度阈值的调整方式通常是对所有预测框按置信度降序排序，依次将置信度阈值设为某一预测框的置信度。并且<strong>P-R曲线的计算是以全测试集为域，按类别划分的</strong>，也就是说，计算类别$C_1$的P-R曲线时，预测框的基数为$C_1$在所有图片中的预测框数量之和，$100$个预测框可以计算$100$组$(R,P)$。另外，由于FP也需要满足置信度大于阈值的条件，<strong>参与整个P-R计算的预测框数量应该是持续增加的，最终达到$C_1$在所有图片中的预测框数量之和</strong>。（这里主要的意思就是粗体的地方，前面的原因可以有很多，主要就是需要按照前面定义的规则计算。）</p><p>更重要的是，<strong>TP、FP和FN却需要在单张图片的范围内按类别计算，再以图片为单位求和构成总TP、FP和FN值，进而求得类别在数据集上的一个$(R,P)$</strong>。</p><h2 id="AP（Average-Precision）"><a href="#AP（Average-Precision）" class="headerlink" title="AP（Average Precision）"></a>AP（Average Precision）</h2><p>经历了前面曲折艰难的分析，终于能够接近AP了。首先从字面上看，平均精确度，很容易联想到一种计算方式（笔者就这么干过）：对数据集中的每张图片单独计算Precision，然后求平均值，这不就是AP吗？实际上这是不对的，错误出在了对于Precision的理解上，Precision是一个<strong>Rate</strong>，我们现在的目的是评估模型<strong>对类别</strong>的检测效果，一个类别的查准率是建立在全数据集的样本之上的，显然，上述计算无法准确的表达这一内涵。</p><p>如果有读者想不明白这一点，我可以再举一个例子来验证，例如有三堆球摆在我们面前，其中两堆相同，都由$3$个白球和$1$个红球构成，另外一堆由$1$个红球和$1$个白球构成，我希望得到这些球中的红球率。如果按照上述理解，我们可以分别计算三堆中的红球率，得到$\frac{1}{4},\frac{1}{4},\frac{1}{2}$，再求它们的算数平均值得到$\frac{1}{3}$。而实际上呢，红球率是$\frac{3}{10}$，因为红球率是指以球为基本单位的红球比率（也不知道我解释清楚没）。</p><p>总之，Precision就是这样一个指标，要计算类别的AP，就要先获得P-R曲线。得到P-R曲线后，AP的计算方式就有很多了，根据不同的流行数据集，有几种常见的方式，罗列如下：</p><ul><li><p>按照VOC2007的方法，是先平滑曲线，对于每个点取其右边最大的P值，连成直线，然后等间距取11个点的最大P值，AP就是这11个Precision的平均值。</p></li><li><p>VOC2012，还是先按07的方法平滑曲线，然后计算PR曲线下面积作为AP值，因为本身P和R就是Rate，构成的正方形区域面积就是1，求曲线积分就完事了。</p></li><li><p>COCO数据集，设定多个IoU阈值（0.5至0.95，0.05为步长），在每一个IoU阈值下都有某一类别的AP值，然后求所有IOU阈值下的平均AP，以该平均AP值作为最终的某类别的AP值。</p></li></ul><p>总的来说，就是一个精度越来越高的过程。</p><h2 id="mAP（mean-Average-Precision）"><a href="#mAP（mean-Average-Precision）" class="headerlink" title="mAP（mean Average Precision）"></a>mAP（mean Average Precision）</h2><p>mAP和速度是最常用的目标检测模型评价指标，mAP顾名思义就是对所有分类的AP再求平均值。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]<a href="https://blog.csdn.net/yegeli/article/details/109861867">https://blog.csdn.net/yegeli/article/details/109861867</a><br>[2]<a href="https://zhuanlan.zhihu.com/p/70306015">https://zhuanlan.zhihu.com/p/70306015</a><br>[3]<a href="https://www.jianshu.com/p/86b8208f634f">https://www.jianshu.com/p/86b8208f634f</a><br>[4]<a href="https://zhuanlan.zhihu.com/p/94597205">https://zhuanlan.zhihu.com/p/94597205</a><br>[5]<a href="https://zhuanlan.zhihu.com/p/101566089">https://zhuanlan.zhihu.com/p/101566089</a><br>[6]<a href="https://www.jianshu.com/p/fd9b1e89f983">https://www.jianshu.com/p/fd9b1e89f983</a></p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于<a href="https://silencezheng.top/">silencezheng.top</a>。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;关于目标检测领域的常用评价指标进行一些个人解读，欢迎批评指正。&lt;/p&gt;
&lt;p&gt;评价指标系列第二篇～上一次说了语义分割的内容，这次来搞目标检测，看了十余篇内容后终于搞明白些了，记录一下。本文说的内容比较浅，一方面是笔者水平实在菜鸡，另一方面写文的主要目的是让不了解目标检测的人读完能上手去做，而不用查三十个资料后再开始，就像博客的Slogan一样，“帮你在学习技术的路上节约半小时”。&lt;br&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://silencezheng.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="计算机视觉" scheme="http://silencezheng.top/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>语义分割常用评价指标</title>
    <link href="http://silencezheng.top/2022/10/23/article71/"/>
    <id>http://silencezheng.top/2022/10/23/article71/</id>
    <published>2022-10-22T16:26:05.000Z</published>
    <updated>2022-10-22T16:33:20.039Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>关于语义分割领域的常用评价指标进行一些个人解读，欢迎批评指正。</p><p>本来是想连目标检测的一起说了的，但是关于AP的一些东西始终有疑问，留着后面搞懂了再写一篇吧。<br><span id="more"></span></p><h2 id="前置理解"><a href="#前置理解" class="headerlink" title="前置理解"></a>前置理解</h2><p>在解读这些评价指标之前，需要对深度学习方法有一个基础认识。</p><p>首先，评价指标，是指测试阶段将预测结果与真实值进行比对得到的量化结论。<strong>评价指标（Metric）和损失函数（Loss）有联系也有区别</strong>，目前在我个人理解来说，评价指标是以实用的角度评价模型，只关心模型的结果，而损失函数是从数学的角度收敛模型，但损失往往应该与一个你最关心的评价指标相对应，通过收敛损失能够达到向指标增大的方向靠拢。并且，损失应该是容易优化的，很多时候它们对模型参数可微，甚至是凸的。下面引[10]中的例子做说明。</p><blockquote><p>假设某同学备战高考，他给自己定下了一个奋斗的方向，即每周要把自己的各科总成绩提高5分；经过多年的准备，终于在高考中取得了好成绩（710分，总分750），被北大录取。<br>分析该例子，该同学“每周要把自己的各科总成绩提高5分”这个指导原则相当于目标函数，在这个指导原则的指引下，想必该同学的总分会越来越高，即模型被训练的越来越好。<br>最终，该同学高考成绩优异，相当于模型的测试效果良好，至于用从哪个角度评价这名同学，可以用其高考总分与750分的差距来衡量，也可以用其被录取的大学的水平来衡量，这就如同模型的评估指标是多种多样的，比如分类问题中的准确率、召回率等。<br>当然，模型的评估指标多样，模型的损失函数也是多样的；该例中，该同学可以将“每周要把自己的各科总成绩提高5分”作为指导原则，也可将“每周比之前多学2个知识点”作为指导原则。<br>另外，如果该同学将“每周模拟高考总分与750分的差距”同时作为指导原则与评价角度，则类似于线性回归模型将“MSE均方误差”同时作为损失函数与评估指标。<br>该例中，备考的“指导原则”相当于“损失函数”，“评价角度”相当于“评估指标”，该同学相当于一个机器学习模型。</p></blockquote><p>其次，在多分类任务中，通常包含$n$个类别，而对于某一样本的最终预测只能是$n$个类别中的一个。但是，算法对一个样本的类别预测通常以置信度的形式表示，最终选择置信度最高的类别作为预测输出。</p><p>最后，多分类任务对于每个类来看，可以看作是一个二分类问题，以样本对于该类别预测是否正确作为区分。</p><h2 id="混淆矩阵（Confusion-Matrix）"><a href="#混淆矩阵（Confusion-Matrix）" class="headerlink" title="混淆矩阵（Confusion Matrix）"></a>混淆矩阵（Confusion Matrix）</h2><p>混淆矩阵用于直观的显示模型预测结果的情形。 混淆矩阵中的横纵轴都是类别，对于$p_{ij}$（横坐标为$i$,纵坐标为$j$处的值），其含义为属于类别$i$并被预测为类别$j$的<strong>样本数量</strong>（在语义分割中通常样本等同于像素）。也就是说，每个位置的<strong>横坐标表示模型的预测，纵坐标表示真实标签</strong>。</p><p>对于二分类问题，混淆矩阵可以表示如下：</p><p><img src="/assets/post_img/article71/confusion-matrix-2classes.jpeg" alt="cm2"></p><p>若令其中$1$表示正类，$0$表示负类，则可以定义如下四个量：</p><ul><li>TP(True Positive)：将正样本预测为正类的数量，即图中的$a$。</li><li>FN(False Negative)：将正样本预测为负类的数量，即图中的$b$。</li><li>FP(False Positive)：将负样本预测为正类的数量，即图中的$c$。</li><li>TN(True Negative)：将负样本预测为负类的数量，即图中的$d$。</li></ul><p>对于多分类问题，只是把该矩阵由$2 \times 2$变化为$n \times n$，其中$n$表示类别数量。</p><p>从混淆矩阵中我们可以获得一些基础信息，如：</p><ul><li>$i$行的和$\sum^n_{j=1}p_{ij}$表示数据集中属于类别$i$的样本个数</li><li>$j$列的和$\sum^n_{i=1}p_{ij}$表示模型预测中属于类别$j$的样本个数</li><li>矩阵中所有元素的和$\sum^n_{i=1}\sum^n_{j=1}p_{ij}$表示图像中的总样本个数</li><li>…</li></ul><h2 id="精确率（Precision）和召回率（Recall）"><a href="#精确率（Precision）和召回率（Recall）" class="headerlink" title="精确率（Precision）和召回率（Recall）"></a>精确率（Precision）和召回率（Recall）</h2><p>这两个指标都是<strong>针对某一类别</strong>而言的，是分类任务的常用评价指标。</p><p>精确率又称查准率，含义是对于模型预测中属于类别$j$的样本，预测结果正确的比例。例如对于二分类问题，正类的精确率$Precision_{positive} = \frac{TP}{TP+FP} = \frac{a}{a+c}$。</p><p>召回率又称查全率，如果说精准率是站在预测的角度看问题，那么召回率就是站在现实的角度看问题，其含义是对于数据集中属于类别$i$的样本，被正确预测的比例。例如对于二分类问题，负类的召回率$Recall_{negative} = \frac{TN}{FP+TN} = \frac{d}{c+d}$。</p><h2 id="准确率（Accuracy）"><a href="#准确率（Accuracy）" class="headerlink" title="准确率（Accuracy）"></a>准确率（Accuracy）</h2><p>准确率需要和精确率区别开，准确率是站在预测的整体角度看问题，其含义是预测正确的样本占所有样本的比例。例如对于二分类问题，预测的准确率$Accuracy_{predict} = \frac{TP+TN}{TP+FN+FP+TN} = \frac{a+d}{a+b+c+d}$。可以看出，准确率其实就是混淆矩阵对角线元素和与所有元素和的比值。</p><h2 id="F1指标（F1-Score）和F-Beta指标（F-Beta-Score）"><a href="#F1指标（F1-Score）和F-Beta指标（F-Beta-Score）" class="headerlink" title="F1指标（F1 Score）和F-Beta指标（F-Beta Score）"></a>F1指标（F1 Score）和F-Beta指标（F-Beta Score）</h2><p>单独用精确率或召回率有时不能很好的评估模型，例如在二分类问题中，模型选择对所有样本预测为正类，此时所有正类样本都被“准确”的预测了，正类召回率为$1$，但模型实际上很差。</p><p>F1指标就是用来平衡精确率和召回率的重要程度的度量指标，它被定义为两者的<strong>调和平均值</strong>，表示二者重要程度一致。F1指标的计算公式如下：</p><script type="math/tex; mode=display">F_1 = 2 \times \frac{precision \cdot recall}{precision + recall}</script><p>调和平均值的一个重要特性就是如果两者极度不平衡，调和平均值会很小，只有当两者都较高时，调和平均才会比较高。</p><p>而F-Beta指标则是更一般的形式，他的计算方式如下：</p><script type="math/tex; mode=display">F_{\beta} = (1+\beta^2) \cdot \frac{precision \cdot recall}{(\beta^2 \cdot precision) + recall}</script><p>其中参数$\beta$决定了精确率和召回率的重要程度比值，当$\beta&gt;1$时召回率比重更大，当$\beta&lt;1$时精确率比重更大。</p><h2 id="特异性（Specificity）和敏感性（Sensitivity）"><a href="#特异性（Specificity）和敏感性（Sensitivity）" class="headerlink" title="特异性（Specificity）和敏感性（Sensitivity）"></a>特异性（Specificity）和敏感性（Sensitivity）</h2><p>关于这两个指标，似乎是仅针对于二分类问题而言的，这里只谈一些个人理解。</p><p>还是参照二分类的混淆矩阵，特异性实际上指的就是负类的召回率$Recall_{negative}$，而敏感性则指的是正类的召回率$Recall_{positive}$，看了网上许多解释，都是聚焦在医疗领域，把患病作为正类，健康作为负类，说什么敏感性越高，漏诊概率越低；特异性越高，确诊概率越高。</p><p>个人理解，实际上就是召回率在特定情况下的应用吧。</p><h2 id="交并比（Intersection-over-Union，IoU）和平均交并比（mIoU）"><a href="#交并比（Intersection-over-Union，IoU）和平均交并比（mIoU）" class="headerlink" title="交并比（Intersection over Union，IoU）和平均交并比（mIoU）"></a>交并比（Intersection over Union，IoU）和平均交并比（mIoU）</h2><p>给定两个区域$A$和$B$，IoU就是两区域的交集与两区域并集的比值：</p><script type="math/tex; mode=display">IoU = \frac{A \cap B}{A \cup B}</script><p><img src="/assets/post_img/article71/IoU.png" alt="iou"></p><p>在分类任务中，可以对某一类别的预测结果和真实标签求IoU，例如对于二分类求正类的IoU如下：</p><script type="math/tex; mode=display">IoU_{positive} = \frac{TP}{TP+FP+FN} = \frac{a}{a+b+c}</script><p>也就是说，混淆矩阵中$i$行和$i$列的交集比上它们的并集。</p><p><strong>平均交并比</strong>（mean IoU）就是对每一个类别求IoU，再求和求平均得到的值。</p><p>对于目标检测，IoU还有一个重要的应用，就是判断预测框与真实框的贴合程度，两部分重合面积越大，则IoU值越大。IoU是一个比较严格的评价指标，当两区域稍微有偏差时，IoU值也可能变得相当小，于是通常认为IoU大于$0.5$时就获得了一个比较不错的预测框。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]<a href="https://zhuanlan.zhihu.com/p/111234566">https://zhuanlan.zhihu.com/p/111234566</a><br>[2]<a href="https://zhuanlan.zhihu.com/p/101566089">https://zhuanlan.zhihu.com/p/101566089</a><br>[3]<a href="https://blog.csdn.net/h1yupyp/article/details/80842172">https://blog.csdn.net/h1yupyp/article/details/80842172</a><br>[4]<a href="https://blog.csdn.net/lhxez6868/article/details/108150777">https://blog.csdn.net/lhxez6868/article/details/108150777</a><br>[5]<a href="https://zhuanlan.zhihu.com/p/371819054">https://zhuanlan.zhihu.com/p/371819054</a><br>[6]<a href="https://www.jianshu.com/p/22d947ffb71e">https://www.jianshu.com/p/22d947ffb71e</a><br>[7]<a href="https://zhuanlan.zhihu.com/p/372402161">https://zhuanlan.zhihu.com/p/372402161</a><br>[8]<a href="https://zhuanlan.zhihu.com/p/373658488">https://zhuanlan.zhihu.com/p/373658488</a><br>[9]<a href="https://zhuanlan.zhihu.com/p/373032887">https://zhuanlan.zhihu.com/p/373032887</a><br>[10]<a href="https://www.cnblogs.com/pythonfl/p/13705143.html">https://www.cnblogs.com/pythonfl/p/13705143.html</a></p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于<a href="https://silencezheng.top/">silencezheng.top</a>。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;关于语义分割领域的常用评价指标进行一些个人解读，欢迎批评指正。&lt;/p&gt;
&lt;p&gt;本来是想连目标检测的一起说了的，但是关于AP的一些东西始终有疑问，留着后面搞懂了再写一篇吧。&lt;br&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://silencezheng.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="计算机视觉" scheme="http://silencezheng.top/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>M1 Mac weasyprint安装使用</title>
    <link href="http://silencezheng.top/2022/10/19/article70/"/>
    <id>http://silencezheng.top/2022/10/19/article70/</id>
    <published>2022-10-19T15:54:48.000Z</published>
    <updated>2022-10-19T15:57:18.582Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>为了把HTML转成PDF（带很多css的），折腾了一晚上。试了一下pdfkit是基本没戏（<a href="https://pypi.org/project/pdfkit/">pdfkit css bug</a>），转头想用weasyprint，没想到适配更差，记录一下。<br><span id="more"></span></p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p><code>brew install cairo pango gdk-pixbuf libffi</code></p><p><code>pip install weasyprint</code></p><h2 id="在conda环境下使用weasyprint"><a href="#在conda环境下使用weasyprint" class="headerlink" title="在conda环境下使用weasyprint"></a>在conda环境下使用weasyprint</h2><p>本来如果在brew的python3下使用应该没什么问题，但是如果要用conda环境的解释器就会报错：<code>OSError: cannot load library &#39;gobject-2.0-0&#39;</code></p><p>解决方案如下：<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo ln -s <span class="regexp">/opt/</span>homebrew<span class="regexp">/opt/g</span>lib<span class="regexp">/lib/</span>libgobject-<span class="number">2.0</span>.<span class="number">0</span>.dylib <span class="regexp">/usr/</span>local<span class="regexp">/lib/g</span>object-<span class="number">2.0</span></span><br><span class="line">sudo ln -s <span class="regexp">/opt/</span>homebrew<span class="regexp">/opt/</span>pango<span class="regexp">/lib/</span>libpango-<span class="number">1.0</span>.dylib <span class="regexp">/usr/</span>local<span class="regexp">/lib/</span>pango-<span class="number">1.0</span></span><br><span class="line">sudo ln -s <span class="regexp">/opt/</span>homebrew<span class="regexp">/opt/</span>harfbuzz<span class="regexp">/lib/</span>libharfbuzz.dylib <span class="regexp">/usr/</span>local<span class="regexp">/lib/</span>harfbuzz</span><br><span class="line">sudo ln -s <span class="regexp">/opt/</span>homebrew<span class="regexp">/opt/</span>fontconfig<span class="regexp">/lib/</span>libfontconfig.<span class="number">1</span>.dylib <span class="regexp">/usr/</span>local<span class="regexp">/lib/</span>fontconfig-<span class="number">1</span></span><br><span class="line">sudo ln -s <span class="regexp">/opt/</span>homebrew<span class="regexp">/opt/</span>pango<span class="regexp">/lib/</span>libpangoft2-<span class="number">1.0</span>.dylib <span class="regexp">/usr/</span>local<span class="regexp">/lib/</span>pangoft2-<span class="number">1.0</span></span><br></pre></td></tr></table></figure></p><p>创建对应位置的软链接，<a href="https://github.com/Kozea/WeasyPrint/issues/1448">issue在这</a>。</p><p>这样以后在终端用是没什么问题了，<code>weasyprint url xx.pdf</code>，中文支持不佳，css部分支持不好。</p><h2 id="仍然报错"><a href="#仍然报错" class="headerlink" title="仍然报错"></a>仍然报错</h2><p>在正常Python调用中仍然会报错，<code>RuntimeError: cannot use unpack() on &lt;cdata &#39;char *&#39; NULL&gt;</code>，定位到<code>cffi/api.py</code>，一个空指针，暂时不知道怎么解决。</p><p>CFFI(C Foreign Function Interface) 是Python的C语言外部函数接口。通过CFFI，Python可以与几乎任何C语言代码进行交互。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;为了把HTML转成PDF（带很多css的），折腾了一晚上。试了一下pdfkit是基本没戏（&lt;a href=&quot;https://pypi.org/project/pdfkit/&quot;&gt;pdfkit css bug&lt;/a&gt;），转头想用weasyprint，没想到适配更差，记录一下。&lt;br&gt;</summary>
    
    
    
    
    <category term="MacOS" scheme="http://silencezheng.top/tags/MacOS/"/>
    
  </entry>
  
</feed>
