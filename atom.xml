<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>SilenceZheng66-BLOG</title>
  
  <subtitle>log</subtitle>
  <link href="http://silencezheng.top/atom.xml" rel="self"/>
  
  <link href="http://silencezheng.top/"/>
  <updated>2022-11-18T04:52:50.466Z</updated>
  <id>http://silencezheng.top/</id>
  
  <author>
    <name>SilenceZheng66</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>MySQL中的Join查询</title>
    <link href="http://silencezheng.top/2022/11/18/article77/"/>
    <id>http://silencezheng.top/2022/11/18/article77/</id>
    <published>2022-11-18T04:50:46.000Z</published>
    <updated>2022-11-18T04:52:50.466Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>业精于勤，荒于嬉。</p><span id="more"></span><h2 id="闲谈"><a href="#闲谈" class="headerlink" title="闲谈"></a>闲谈</h2><p>在整理Join相关的内容时，我提出了几个问题，整理到一个部分记录一下，想看“干货”的读者可以跳过了。</p><p>问题一：<strong>Join和Key有啥关系？</strong><br>Key无非主、外、候选、公共之类的内容，一个row的identifier罢了，无非是对内对外，同时它也是一个field。那么我们在规划表结构、塞数据的时候就有了一个方便的方法，把想表达的一个row的数据用一个key概括，需要获取所有数据时通过多表查询即可。总之，笔者认为，Join和Key可以说没关系，Key在任何时候都发挥着identifier的作用。</p><p>问题二：<strong>MySQL中不用Key也能Join，为什么？</strong><br>其实这个问题本身有点奇怪（我突然想出来的），首先关系代数中连接（Join）也没有要求一定要用键做连接，其次上面也说了这俩没多大关系。但我一搜吧，还真有个<a href="https://blog.csdn.net/lamanchas/article/details/121366276">回答</a>，主要是说外键约束有成本，对高并发情况不合适之类的，一时不知道是我有问题还是理解不到位，知道的大神可以告诉我，感谢。</p><p>问题三：<strong>为什么不能用Where替代Join?</strong><br>关于这个问题，我简单思考了一下，首先就拿纯<code>WHERE</code>、<code>JOIN</code>和<code>LEFT JOIN</code>来说，我写了以下三个查询：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">select</span> *</span><br><span class="line"><span class="attribute">from</span> node_graph<span class="number">1</span> left join edge_graph<span class="number">1</span> <span class="literal">on</span> node_graph<span class="number">1</span>.node_list = edge_graph<span class="number">1</span>.node<span class="number">1</span>_name;</span><br><span class="line"></span><br><span class="line"><span class="attribute">select</span> *</span><br><span class="line"><span class="attribute">from</span> node_graph<span class="number">1</span>, edge_graph<span class="number">1</span> where node_graph<span class="number">1</span>.node_list = edge_graph<span class="number">1</span>.node<span class="number">1</span>_name;</span><br><span class="line"></span><br><span class="line"><span class="attribute">select</span> *</span><br><span class="line"><span class="attribute">from</span> node_graph<span class="number">1</span> join edge_graph<span class="number">1</span> <span class="literal">on</span> node_graph<span class="number">1</span>.node_list = edge_graph<span class="number">1</span>.node<span class="number">1</span>_name;</span><br></pre></td></tr></table></figure><p>其中后两个的效果是一致的，而<code>LEFT JOIN</code>会返回node_graph1的所有结果，即使没有match到，这可能是<code>WHERE</code>做不到的一个地方。更多还是要在实践中发掘。</p><h2 id="Join查询"><a href="#Join查询" class="headerlink" title="Join查询"></a>Join查询</h2><p>SQL中Join用于根据两个或多个表中的列之间的关系，从这些表中查询数据。日常使用中对多表查询有广泛的需求，Join查询自然是必不可少。</p><p>用Join联合表时需要在每个表中选择一个<strong>字段</strong>，并对这些字段的值进行比较，值相同的两条记录将合并为一条。联合表的本质就是将不同表的记录合并起来，形成一张新表。当然，这张新表只是临时的，它仅存在于本次查询期间。</p><p>那么SQL中的Join都有哪些呢？先上一张总览：</p><p><img src="/assets/post_img/article77/SQL-Join.png" alt="overview"></p><p>下面开始逐个说一下。</p><h3 id="1-内连接（Inner-Join"><a href="#1-内连接（Inner-Join" class="headerlink" title="1. 内连接（Inner Join)"></a>1. 内连接（Inner Join)</h3><p>INNER JOIN 是 SQL 中最重要、最常用的表连接形式，只有当连接的两个或者多个表中都存在满足条件的记录时，才返回行。任何一条只存在于某一张表中的数据，都不会返回。</p><h3 id="2-左外连接（Left-Outer-Join"><a href="#2-左外连接（Left-Outer-Join" class="headerlink" title="2. 左外连接（Left Outer Join)"></a>2. 左外连接（Left Outer Join)</h3><p>LEFT OUTER JOIN 以左表为主，即左表中的<strong>所有记录</strong>都会被返回，具体分为以下三种情况：</p><ul><li>如果 TableA 中的某条记录在 TableB 中刚好只有一条记录可以匹配，那么在返回的结果中会生成一个新的行。</li><li>如果 TableA 中的某条记录在 TableB 中有 N 条记录可以匹配，那么在返回结果中也会生成 N 个新的行，这些行所包含的 TableA 的字段值是重复的。</li><li>如果 TableA 中的某条记录在 TableB 中没有匹配的记录，那么在返回结果中仍然会生成一个新的行，只是该行所包含的 TableB 的字段值都是 NULL。<h3 id="3-右外连接（Right-Outer-Join"><a href="#3-右外连接（Right-Outer-Join" class="headerlink" title="3. 右外连接（Right Outer Join)"></a>3. 右外连接（Right Outer Join)</h3>RIGHT OUTER JOIN 以右表为主，即右表中的<strong>所有记录</strong>都会被返回，具体分为以下三种情况：</li><li>如果 TableB 中的某条记录在 TableA 中刚好只有一条记录可以匹配，那么在返回的结果中会生成一个新的行。</li><li>如果 TableB 中的某条记录在 TableA 中有 N 条记录可以匹配，那么在返回的结果中也会生成 N 个新的行，这些行所包含的 TableB 的字段值是重复的。</li><li>如果 TableB 中的某条记录在 TableA 中没有匹配记录，那么在返回结果中仍然会生成一个新的行，只是该行所包含的 TableA 的字段值都是 NULL。<h3 id="4-左外连接-with-exclusion（Left-Outer-Join-with-exclusion"><a href="#4-左外连接-with-exclusion（Left-Outer-Join-with-exclusion" class="headerlink" title="4. 左外连接 with exclusion（Left Outer Join with exclusion)"></a>4. 左外连接 with exclusion（Left Outer Join with exclusion)</h3>在左外连接的基础上，去除TableB可匹配到的部分，只返回B.Key为NULL的记录。<h3 id="5-右外连接-with-exclusion（Right-Outer-Join-with-exclusion"><a href="#5-右外连接-with-exclusion（Right-Outer-Join-with-exclusion" class="headerlink" title="5. 右外连接 with exclusion（Right Outer Join with exclusion)"></a>5. 右外连接 with exclusion（Right Outer Join with exclusion)</h3>在右外连接的基础上，去除TableA可匹配到的部分，只返回A.Key为NULL的记录。<h3 id="6-全外连接（Full-Outer-Join）"><a href="#6-全外连接（Full-Outer-Join）" class="headerlink" title="6. 全外连接（Full Outer Join）"></a>6. 全外连接（Full Outer Join）</h3>FULL OUTER JOIN 先执行 LEFT OUTER JOIN 遍历左表，再执行 RIGHT OUTER JOIN 遍历右表，最后将 RIGHT OUTER JOIN 的结果直接追加到 LEFT OUTER JOIN 后面。注意，FULL OUTER JOIN 会返回重复的行，它们会被保留，不会被删除。<h3 id="7-全外连接-with-exclusion（Full-Outer-Join-with-exclusion）"><a href="#7-全外连接-with-exclusion（Full-Outer-Join-with-exclusion）" class="headerlink" title="7. 全外连接 with exclusion（Full Outer Join with exclusion）"></a>7. 全外连接 with exclusion（Full Outer Join with exclusion）</h3>两表的FULL OUTER JOIN去除重合部分，也就是返回 左外连接 with exclusion 和 右外连接 with exclusion 的 FULL OUTER JOIN 记录。</li></ul><h2 id="MySQL支持的Join方式"><a href="#MySQL支持的Join方式" class="headerlink" title="MySQL支持的Join方式"></a>MySQL支持的Join方式</h2><p>在聊这个之前，先简单了解一下<strong>驱动表和被驱动表</strong>的概念。在LEFT OUTER JOIN时，左表为驱动表，右表为被驱动表；在RIGHT OUTER JOIN时，右表为驱动表，左表为被驱动表。关于驱动表和被驱动表的作用，实际上是与MySQL表关联算法和SQL优化有关的，通常来说，用小表<strong>驱动</strong>大表能够获得更高的效率，这里不详细展开了。</p><p>以MySQL8.0.11为例，MySQL提供的JOIN关键字有：<code>JOIN</code>、<code>INNER JOIN</code>、<code>LEFT JOIN</code>、<code>LEFT OUTER JOIN</code>、<code>RIGHT JOIN</code>、<code>RIGHT OUTER JOIN</code>、<code>CROSS JOIN</code>和<code>STRAIGHT_JOIN</code>。</p><p>其中，<code>JOIN</code>和<code>INNER JOIN</code>为内连接，<code>LEFT JOIN</code>与<code>LEFT OUTER JOIN</code>是等价的，都对应着左外连接（右也是一样的道理）。也就是说，上面提到的七种JOIN方式，MySQL关键字只支持前三种，对于4、5可以结合WHERE来实现，但不提供全外连接关键字。</p><p>这三种（或六个）关键字的通用Join查询结构如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">&lt;</span>row_list<span class="operator">&gt;</span> </span><br><span class="line">  <span class="keyword">FROM</span> <span class="operator">&lt;</span>left_table<span class="operator">&gt;</span> </span><br><span class="line">    <span class="operator">&lt;</span><span class="keyword">inner</span><span class="operator">|</span><span class="keyword">left</span><span class="operator">|</span><span class="keyword">right</span><span class="operator">&gt;</span> <span class="keyword">JOIN</span> <span class="operator">&lt;</span>right_table<span class="operator">&gt;</span> </span><br><span class="line">      <span class="keyword">ON</span> <span class="operator">&lt;</span>join_condition<span class="operator">&gt;</span> </span><br><span class="line">        <span class="keyword">WHERE</span> <span class="operator">&lt;</span>where_condition<span class="operator">&gt;</span></span><br></pre></td></tr></table></figure><p>可以看到这里有两种条件，分别是<strong>join_condition</strong>和<strong>where_condition</strong>，两者执行存在先后顺序。数据库通过JOIN关键字返回记录时会先生成一张临时表，通过临时表返回记录，<strong>join_condition</strong>是在生成临时表时使用的条件，而<strong>where_condition</strong>是在临时表生成后再对其进行过滤的条件。以<code>LEFT JOIN</code>为例，在生成临时表时无论<strong>join_condition</strong>是否为真都会将左表记录加入到临时表中，所以“左表中的<strong>所有记录</strong>都会被返回”。</p><p>那么<code>CROSS JOIN</code>和<code>STRAIGHT_JOIN</code>又是什么呢？</p><p><code>CROSS JOIN</code>子句从连接的表返回行的笛卡儿积，它的通用查询结构如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">&lt;</span>row_list<span class="operator">&gt;</span> </span><br><span class="line">  <span class="keyword">FROM</span> <span class="operator">&lt;</span>left_table<span class="operator">&gt;</span> </span><br><span class="line">    <span class="keyword">CROSS</span> <span class="keyword">JOIN</span> <span class="operator">&lt;</span>right_table<span class="operator">&gt;</span> </span><br><span class="line">        <span class="keyword">ON</span> <span class="operator">&lt;</span>join_condition<span class="operator">&gt;</span></span><br><span class="line">            <span class="keyword">WHERE</span> <span class="operator">&lt;</span>where_condition<span class="operator">&gt;</span></span><br></pre></td></tr></table></figure><p>注意，仅当不添加<strong>join_condition</strong>和<strong>where_condition</strong>的时候，<code>CROSS JOIN</code>才能返回笛卡尔积，如果添加了这些条件，那么工作方式将和<code>JOIN</code>相同。</p><p>至于<code>STRAIGHT_JOIN</code>，其实是提供给用户一种自主决定驱动表与被驱动表关系的方式，它的用法与<code>JOIN</code>相同，只是<code>STRAIGHT_JOIN</code>前面的表一定是驱动表，后面的表一定是被驱动表。而在MySQL中，<code>JOIN</code>会自动选择小表作为驱动表，大表作为被驱动表。用户可以使用<code>STRAIGHT_JOIN</code>来解决MySQL优化器不能解决的部分。</p><p>关于全外连接以及其他各种连接方式在MySQL中的实现，我找到了一张图，是由Steve Stedman制作的，供读者参考。</p><p><img src="/assets/post_img/article77/MySQL-Join.png" alt="MySQLJoinType"></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1]<a href="https://blog.csdn.net/lamanchas/article/details/121366276">https://blog.csdn.net/lamanchas/article/details/121366276</a><br>[2]<a href="https://blog.csdn.net/asd051377305/article/details/115320564">https://blog.csdn.net/asd051377305/article/details/115320564</a><br>[3]<a href="http://c.biancheng.net/sql/join.html">http://c.biancheng.net/sql/join.html</a><br>[4]<a href="https://cloud.tencent.com/developer/article/1167929">https://cloud.tencent.com/developer/article/1167929</a><br>[5]<a href="https://www.jianshu.com/p/76c90b03b7bd">https://www.jianshu.com/p/76c90b03b7bd</a><br>[6]<a href="https://blog.csdn.net/javaanddonet/article/details/109693672">https://blog.csdn.net/javaanddonet/article/details/109693672</a><br>[7]<a href="https://blog.csdn.net/weixin_37692493/article/details/106970429">https://blog.csdn.net/weixin_37692493/article/details/106970429</a></p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于 <a href="https://silencezheng.top">silencezheng.top</a>，转载请注明出处。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;业精于勤，荒于嬉。&lt;/p&gt;</summary>
    
    
    
    
    <category term="MySQL" scheme="http://silencezheng.top/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>二叉树非递归遍历</title>
    <link href="http://silencezheng.top/2022/11/09/article76/"/>
    <id>http://silencezheng.top/2022/11/09/article76/</id>
    <published>2022-11-09T13:41:12.000Z</published>
    <updated>2022-11-11T02:21:08.306Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>不积跬步，无以至千里。</p><span id="more"></span><p>用Java写一下二叉树的非递归遍历，用print表示操作了，主要关注算法。</p><p>树定义如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TreeNode</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> val;</span><br><span class="line">        TreeNode left;</span><br><span class="line">        TreeNode right;</span><br><span class="line">        TreeNode() &#123;&#125;</span><br><span class="line">        TreeNode(<span class="keyword">int</span> val) &#123; <span class="keyword">this</span>.val = val; &#125;</span><br><span class="line">        TreeNode(<span class="keyword">int</span> val, TreeNode left, TreeNode right) &#123;</span><br><span class="line">          <span class="keyword">this</span>.val = val;</span><br><span class="line">          <span class="keyword">this</span>.left = left;</span><br><span class="line">          <span class="keyword">this</span>.right = right;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h2 id="先序遍历（直觉版）"><a href="#先序遍历（直觉版）" class="headerlink" title="先序遍历（直觉版）"></a>先序遍历（直觉版）</h2><p>思路：根左右，从根节点开始先走到最左下节点，然后依次出栈，如果出栈的节点有右子节点则对右子节点再走到最左下，直至栈空。</p><p>步骤：</p><ol><li>对当前节点走到最左下，每次对当前节点操作+入栈。</li><li>元素出栈，若有右子节点则重复1，若无则重复2。</li><li>栈空结束。</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">preorderTraversal</span><span class="params">(TreeNode root)</span></span>&#123;</span><br><span class="line">        Stack&lt;TreeNode&gt; nodeStack = <span class="keyword">new</span> Stack&lt;&gt;();</span><br><span class="line">        <span class="keyword">if</span>(root == <span class="keyword">null</span>)&#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;Null root error!&quot;</span>);</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            TreeNode temp = root;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> (temp!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                <span class="comment">// 操作</span></span><br><span class="line">                System.out.println(temp.val);</span><br><span class="line">                nodeStack.add(temp);</span><br><span class="line">                temp = temp.left;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> (!nodeStack.isEmpty())&#123;</span><br><span class="line">                temp = nodeStack.pop();</span><br><span class="line">                <span class="keyword">if</span> (temp.right!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                    temp = temp.right;</span><br><span class="line">                    <span class="keyword">while</span> (temp!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                        <span class="comment">// 操作</span></span><br><span class="line">                        System.out.println(temp.val);</span><br><span class="line">                        nodeStack.add(temp);</span><br><span class="line">                        temp = temp.left;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>这个版本的基础上可以进行各种优化。</p><h2 id="中序遍历（直觉版）"><a href="#中序遍历（直觉版）" class="headerlink" title="中序遍历（直觉版）"></a>中序遍历（直觉版）</h2><p>思路：左根右，从根节点走到最左下节点，然后依次出栈并操作，如果出栈的节点有右节点则对右节点再走到最左下，直至栈空。</p><p>步骤：</p><ol><li>对当前节点走到最左下，每次对当前节点入栈。</li><li>元素出栈，操作当前节点，若有右子节点则重复1，若无则重复2。</li><li>栈空结束。</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">inorderTraversal</span><span class="params">(TreeNode root)</span></span>&#123;</span><br><span class="line">        Stack&lt;TreeNode&gt; nodeStack = <span class="keyword">new</span> Stack&lt;&gt;();</span><br><span class="line">        <span class="keyword">if</span>(root == <span class="keyword">null</span>)&#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;Null root error!&quot;</span>);</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            TreeNode temp = root;</span><br><span class="line">            <span class="keyword">while</span> (temp!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                nodeStack.add(temp);</span><br><span class="line">                temp = temp.left;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            temp = nodeStack.pop();</span><br><span class="line">            <span class="keyword">while</span> (temp!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                <span class="comment">// 操作</span></span><br><span class="line">                System.out.println(temp.val);</span><br><span class="line">                <span class="keyword">if</span>(temp.right!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                    temp = temp.right;</span><br><span class="line">                    <span class="keyword">while</span> (temp!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                        nodeStack.add(temp);</span><br><span class="line">                        temp = temp.left;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (nodeStack.isEmpty())&#123;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                temp = nodeStack.pop();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>这个版本的基础上可以进行各种优化。</p><h2 id="后序遍历（直觉版）"><a href="#后序遍历（直觉版）" class="headerlink" title="后序遍历（直觉版）"></a>后序遍历（直觉版）</h2><p>思路：左右根，后序不能采用先序和中序的同款算法的主要原因是判断到当前节点存在右子树时，则不能对当前节点进行操作，而需要先对右子树做后序遍历，而即便是保留当前节点，并把右子树遍历完毕后，再对当前节点进行操作，仍然需要对当前节点的右子树是否已被遍历的状态进行判断，判断的依据是上一次操作的节点是否是右子节点。</p><p>步骤：</p><ol><li>对当前节点走到最左下，每次对当前节点入栈。</li><li>元素出栈，判断当前节点，若无右子树则操作并标记当前节点，若有右子树则判断右子树是否被访问过，若未被访问则保留当前节点状态，并依次入栈右子树左支，标记右子节点；若已访问过则操作当前节点，并标记当前节点。重复2。</li><li>栈空结束。</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">postorderTraversal</span><span class="params">(TreeNode root)</span></span>&#123;</span><br><span class="line">        Stack&lt;TreeNode&gt; nodeStack = <span class="keyword">new</span> Stack&lt;&gt;();</span><br><span class="line">        <span class="keyword">if</span>(root == <span class="keyword">null</span>)&#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;Null root error!&quot;</span>);</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            TreeNode temp = root;</span><br><span class="line">            TreeNode mark = <span class="keyword">null</span>;</span><br><span class="line">            <span class="keyword">while</span> (temp!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                nodeStack.add(temp);</span><br><span class="line">                temp = temp.left;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            temp = nodeStack.pop();</span><br><span class="line">            <span class="keyword">while</span> (temp!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                <span class="keyword">if</span>(temp.right==<span class="keyword">null</span>)&#123;</span><br><span class="line">                    <span class="comment">// 操作</span></span><br><span class="line">                    System.out.println(temp.val);</span><br><span class="line">                    <span class="comment">// 标记当前节点</span></span><br><span class="line">                    mark = temp;</span><br><span class="line">                &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                    <span class="comment">// 当前节点含右子树的情况，判断前次处理节点是否是右子节点。</span></span><br><span class="line">                    <span class="keyword">if</span>(temp.right==mark)&#123;</span><br><span class="line">                        <span class="comment">// 操作</span></span><br><span class="line">                        System.out.println(temp.val);</span><br><span class="line">                        <span class="comment">// 标记当前节点</span></span><br><span class="line">                        mark = temp;</span><br><span class="line">                    &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">                        <span class="comment">// 保留状态</span></span><br><span class="line">                        nodeStack.add(temp);</span><br><span class="line">                        <span class="comment">// 入栈右子树的左支，并标记右子节点。</span></span><br><span class="line">                        temp = temp.right;</span><br><span class="line">                        mark = temp;</span><br><span class="line">                        <span class="keyword">while</span> (temp!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                            nodeStack.add(temp);</span><br><span class="line">                            temp = temp.left;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (nodeStack.isEmpty())&#123;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                temp = nodeStack.pop();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>这个版本的基础上可以进行各种优化。</p><h2 id="层序遍历"><a href="#层序遍历" class="headerlink" title="层序遍历"></a>层序遍历</h2><p>思路：按从上到下，从左到右的顺序遍历。用队列实现，先入当前节点，出队再入左、右两子节点，然后每出一个就入队该节点的左、右子节点，直到队空。</p><p>步骤：</p><ol><li>入队当前节点。</li><li>出队一个节点，入队该节点的左、右子节点，重复2。</li><li>队空结束。</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">layerSequenceTraversal</span><span class="params">(TreeNode root)</span></span>&#123;</span><br><span class="line">        LinkedBlockingQueue&lt;TreeNode&gt; que =  <span class="keyword">new</span> LinkedBlockingQueue&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(root == <span class="keyword">null</span>)&#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;Null root error!&quot;</span>);</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            que.offer(root);</span><br><span class="line">            TreeNode temp = <span class="keyword">null</span>;</span><br><span class="line">            <span class="keyword">while</span> (!que.isEmpty())&#123;</span><br><span class="line">                temp = que.poll();</span><br><span class="line">                System.out.println(temp.val);</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (temp.left!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                    que.offer(temp.left);</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (temp.right!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                    que.offer(temp.right);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>先把我个人认为符合直觉的遍历方法写一下，看起来比较复杂但是容易理解，后面再进行优化补充。</p><p>首发于 <a href="https://silencezheng.top">silencezheng.top</a>。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;不积跬步，无以至千里。&lt;/p&gt;</summary>
    
    
    
    
    <category term="数据结构与算法" scheme="http://silencezheng.top/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>深度学习常用术语解释</title>
    <link href="http://silencezheng.top/2022/11/08/article75/"/>
    <id>http://silencezheng.top/2022/11/08/article75/</id>
    <published>2022-11-08T14:00:16.000Z</published>
    <updated>2022-11-08T14:00:52.846Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>深度学习常用术语解释，持续更新～<br><span id="more"></span></p><h2 id="Backbone"><a href="#Backbone" class="headerlink" title="Backbone"></a>Backbone</h2><p>主干网络，或称骨干网络，通常是网络的一部分，大多时候指的是提取特征的网络，其作用就是提取图片中的信息，供后面的网络使用。这些网络经常使用的是ResNet、VGG等，而不是我们自己设计的网络，因为这些网络已经证明了在分类等问题上的特征提取能力是很强的。在用这些网络作为Backbone的时候，都是直接加载官方已经训练好的模型参数，后面接着我们自己的网络。让网络的这两个部分同时进行训练，因为加载的Backbone模型已经具有提取特征的能力了，在我们的训练过程中，会对它进行微调，使得其更适合于我们自己的任务。</p><h2 id="Head"><a href="#Head" class="headerlink" title="Head"></a>Head</h2><p>Head即整个网络的头部，是获取网络输出内容的网络，利用之前（Backbone）提取的特征，做出预测。</p><h2 id="Neck"><a href="#Neck" class="headerlink" title="Neck"></a>Neck</h2><p>是指放在Backbone和Head之间的层，是为了更好的利用Backbone提取的特征。</p><h2 id="Pretext-task"><a href="#Pretext-task" class="headerlink" title="Pretext task"></a>Pretext task</h2><p>用于预训练的任务，可以翻译为前置任务或代理任务。</p><h2 id="Downstream-task"><a href="#Downstream-task" class="headerlink" title="Downstream task"></a>Downstream task</h2><p>下游任务，用于微调的任务。</p><h2 id="Warm-up"><a href="#Warm-up" class="headerlink" title="Warm up"></a>Warm up</h2><p>用一个小的学习率先训练几个epoch，这是因为网络的参数是随机初始化的，一开始就采用较大的学习率容易数值不稳定。</p><h2 id="End-to-End"><a href="#End-to-End" class="headerlink" title="End to End"></a>End to End</h2><p>端到端，给一个输入，获得一个输出，中间的处理过程处于黑箱中，相当于打包成应用了。</p><h2 id="Standardization"><a href="#Standardization" class="headerlink" title="Standardization"></a>Standardization</h2><p>标准化，指将数据按比例缩放，使其落入一个小区间中，缩放后均值为$0$，方差为$1$。在某些比较和评价的指标处理中经常会用到，去除数据的单位限制，将其转化为无量纲的纯数值，便于不同单位或量级的指标能够进行比较和加权。</p><p>例如在数据预处理时为了将所有特征放在一个共同的尺度上，会通过<strong>将特征重新缩放到零均值和单位方差</strong>来标准化数据。这既能方便优化，又能避免惩罚分配给某一特征的系数超过其他特征（一视同仁）。</p><p>在训练过程中，对输入进行规范化可以加速深度网络权重参数的收敛速度。</p><h2 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h2><p>这个词真的需要好好理解一下，其实首先应该想到翻译为“规范化”，它包括归一化、标准化甚至正则化，作为一个统称。比如Batch Normalization其实做的是Standardization的事，所以翻译成批量规范化或者批量标准化。</p><p>其次这个词又可以指归一化，即把数值放缩到$0$到$1$的小区间中。归一化可以消除量纲对最终结果的影响，使不同变量具有可比性。</p><p>关于这个词的解读是要具体问题具体分析了，甚至有时Standardization也被作为统称。</p><h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><p>正则化，一般形式是在整个平均损失函数的最后增加一个正则项（比如L2范数正则化，也有其他形式的正则化，作用不同）。正则项越大表明惩罚力度越大，等于0表示不做惩罚。</p><h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p>[1]<a href="https://zhuanlan.zhihu.com/p/348800083">https://zhuanlan.zhihu.com/p/348800083</a><br>[2]<a href="https://zhuanlan.zhihu.com/p/343692147">https://zhuanlan.zhihu.com/p/343692147</a><br>[3]<a href="https://blog.csdn.net/u014381464/article/details/81101551">https://blog.csdn.net/u014381464/article/details/81101551</a></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;深度学习常用术语解释，持续更新～&lt;br&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://silencezheng.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="机器学习" scheme="http://silencezheng.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>注意力机制--《动手学深度学习》笔记0x0B</title>
    <link href="http://silencezheng.top/2022/11/07/article74/"/>
    <id>http://silencezheng.top/2022/11/07/article74/</id>
    <published>2022-11-07T07:25:27.000Z</published>
    <updated>2022-11-07T07:27:27.834Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><p>灵长类动物的视觉系统接受了大量的感官输入，这些感官输入远远超过了大脑能够完全处理的程度。然而，并非所有刺激的影响都是相等的。意识的聚集和专注使灵长类动物能够在复杂的视觉环境中将注意力引向感兴趣的物体，例如猎物和天敌。只关注一小部分信息的能力对进化更加有意义，使人类得以生存和成功。</p><p>自19世纪以来，科学家们一直致力于研究认知神经科学领域的注意力。本章的很多章节将涉及到一些研究。<br><span id="more"></span><br>本章首先回顾一个经典注意力框架，解释如何在视觉场景中展开注意力。受此框架中的<em>注意力提示</em>（attention cues）的启发，我们将设计能够利用这些注意力提示的模型。1964年的Nadaraya-Waston核回归（kernel regression）正是具有<em>注意力机制</em>（attention mechanism）的机器学习的简单演示。</p><p>然后继续介绍注意力函数，它们在深度学习的注意力模型设计中被广泛使用。具体来说将展示如何使用这些函数来设计<em>Bahdanau注意力</em>。Bahdanau注意力是深度学习中的具有突破性价值的注意力模型，它双向对齐并且可以微分。</p><p>最后将描述仅仅基于注意力机制的<em>Transformer</em>架构，该架构中使用了<em>多头注意力</em>（multi-head attention）和<em>自注意力</em>（self-attention）。自2017年横空出世，Transformer一直都普遍存在于现代的深度学习应用中，例如语言、视觉、语音和强化学习领域。</p><p>这一章目前只做了解，关于NLP的内容没有实验，也没有详细调查。</p><h3 id="0-1-小结"><a href="#0-1-小结" class="headerlink" title="0.1. 小结"></a>0.1. 小结</h3><ul><li>人类的注意力是有限的、有价值和稀缺的资源。</li><li>受试者使用非自主性和自主性提示有选择性地引导注意力。前者基于突出性，后者则依赖于主体的意识。</li><li>注意力机制与全连接层或者池化层的区别源于增加的自主提示。</li><li>由于包含了自主性提示，注意力机制与全连接的层或池化层不同。</li><li>注意力机制通过注意力池化使选择偏向于值（感官输入），其中包含查询（自主性提示）和键（非自主性提示）。键和值是成对的。</li><li>我们可以可视化查询和键之间的注意力权重。</li><li>Nadaraya-Watson核回归是具有注意力机制的机器学习范例。</li><li>Nadaraya-Watson核回归的注意力池化是对训练数据中输出的加权平均。从注意力的角度来看，分配给每个值的注意力权重取决于将值所对应的键和查询作为输入的函数。</li><li>注意力池化可以分为非参数型和带参数型</li><li>注意力池化的输出可以计算为值的加权平均，选择不同的注意力评分函数会带来不同的注意力池化操作。</li><li>当查询和键是不同长度的矢量时，可以使用<em>加性注意力评分函数</em>。当它们的长度相同时，使用<em>缩放的“点－积”注意力评分函数</em>的计算效率更高。</li><li>在预测词元时，如果不是所有输入词元都是相关的，那么具有Bahdanau注意力的循环神经网络编码器-解码器会有选择地统计输入序列的不同部分。这是通过将上下文变量视为加性注意力池化的输出来实现的。</li><li>在循环神经网络编码器-解码器中，Bahdanau注意力将上一时间步的解码器隐状态视为查询，在所有时间步的编码器隐状态同时视为键和值。</li><li>多头注意力融合了来自于多个注意力池化的不同知识，这些知识的不同来源于相同的查询、键和值的不同的子空间表示。</li><li>基于适当的张量操作，可以实现多头注意力的并行计算。</li><li>在自注意力中，查询、键和值都来自同一组输入。</li><li>卷积神经网络和自注意力都拥有并行计算的优势，而且自注意力的最大路径长度最短。但是因为其计算复杂度是关于序列长度的二次方，所以在很长的序列中计算会非常慢。</li><li>为了使用序列的顺序信息，我们可以通过在输入表示中添加位置编码，来注入绝对的或相对的位置信息。</li><li>transformer是编码器－解码器架构的一个实践，尽管在实际情况中编码器或解码器可以单独使用。</li><li>在transformer中，多头自注意力用于表示输入序列和输出序列，不过解码器必须通过掩蔽机制来保留自回归属性。</li><li>transformer中的残差连接和层规范化是训练非常深度模型的重要工具。</li><li>transformer模型中基于位置的前馈网络使用同一个多层感知机，作用是对所有序列位置的表示进行转换。</li></ul><h2 id="1-注意力提示"><a href="#1-注意力提示" class="headerlink" title="1. 注意力提示"></a>1. 注意力提示</h2><p>感谢读者对本书的关注，因为读者的注意力是一种稀缺的资源：此刻读者正在阅读本书（而忽略了其他的书），因此读者的注意力是用机会成本（与金钱类似）来支付的。为了确保读者现在投入的注意力是值得的，作者们尽全力（全部的注意力）创作一本好书。</p><p>自经济学研究稀缺资源分配以来，人们正处在“注意力经济”时代，即人类的注意力被视为可以交换的、有限的、有价值的且稀缺的商品。许多商业模式也被开发出来去利用这一点：在音乐或视频流媒体服务上，人们要么消耗注意力在广告上，要么付钱来隐藏广告；为了在网络游戏世界的成长，人们要么消耗注意力在游戏战斗中，从而帮助吸引新的玩家，要么付钱立即变得强大。总之，注意力不是免费的。</p><p>注意力是稀缺的，而环境中的干扰注意力的信息却并不少。比如人类的视觉神经系统大约每秒收到$10^8$位的信息，这远远超过了大脑能够完全处理的水平。幸运的是，人类的祖先已经从经验（也称为数据）中认识到“并非感官的所有输入都是一样的”。在整个人类历史中，这种只将注意力引向感兴趣的一小部分信息的能力，使人类的大脑能够更明智地分配资源来生存、成长和社交，例如发现天敌、找寻食物和伴侣。</p><h3 id="1-1-生物学中的注意力提示"><a href="#1-1-生物学中的注意力提示" class="headerlink" title="1.1. 生物学中的注意力提示"></a>1.1. 生物学中的注意力提示</h3><p>注意力是如何应用于视觉世界中的呢？这要从当今十分普及的<em>双组件</em>（two-component）的框架开始讲起：这个框架的出现可以追溯到19世纪90年代的威廉·詹姆斯，他被认为是“美国心理学之父” [<code>James.2007</code>]。在这个框架中，受试者基于<em>非自主性提示</em>和<em>自主性提示</em>有选择地引导注意力的焦点。</p><p>非自主性提示是基于环境中物体的突出性和易见性。想象一下，假如我们面前有五个物品：一份报纸、一篇研究论文、一杯咖啡、一本笔记本和一本书，就像下图。所有纸制品都是黑白印刷的，但咖啡杯是红色的。换句话说，这个咖啡杯在这种视觉环境中是突出和显眼的，不由自主地引起人们的注意。所以我们会把视力最敏锐的地方放到咖啡上，如下图所示。</p><p><img src="/assets/post_img/article74/eye-coffee.svg" alt="由于突出性的非自主性提示（红杯子），注意力不自主地指向了咖啡杯"></p><p>喝咖啡后，我们会变得兴奋并想读书，所以转过头，重新聚焦眼睛，然后看看书，就像下图中描述那样。与上图中由于突出性导致的选择不同，此时选择书是受到了认知和意识的控制，因此注意力在基于自主性提示去辅助选择时将更为谨慎。受试者的主观意愿推动，选择的力量也就更强大。</p><p><img src="/assets/post_img/article74/eye-book.svg" alt="依赖于任务的意志提示（想读一本书），注意力被自主引导到书上"></p><h3 id="1-2-查询、键和值"><a href="#1-2-查询、键和值" class="headerlink" title="1.2. 查询、键和值"></a>1.2. 查询、键和值</h3><p>自主性的与非自主性的注意力提示解释了人类注意力的方式，下面来看看如何通过这两种注意力提示，用神经网络来设计注意力机制的框架。</p><p>首先，对于只使用非自主性提示的情况。要想将选择偏向于感官输入，则可以简单地使用参数化的全连接层，甚至是非参数化的最大池化层或平均池化层。个人理解，这就是说，这种情况下不需要在以往的神经网络上做出修改，因为非自主性提示来自客体的差异。</p><p>因此，“是否包含自主性提示”将<strong>注意力机制</strong>与全连接层或池化层区别开来。在注意力机制的背景下，自主性提示被称为<em>查询</em>（query）。给定任何查询，注意力机制通过<em>注意力池化</em>（attention pooling）将选择引导至<em>感官输入</em>（sensory inputs，例如中间特征表示）。在注意力机制中，这些感官输入被称为<em>值</em>（value）。更通俗的解释是，每个值都与一个感官输入的非自主提示配对，这些对应的非自主性提示称为<em>键</em>（key）。如下图所示，可以通过设计注意力池化的方式，使给定的查询（自主性提示）与键（非自主性提示）进行匹配，这将引导得出最匹配的值（感官输入）。</p><p><img src="/assets/post_img/article74/qkv.svg" alt="注意力机制通过注意力池化将*查询*（自主性提示）和*键*（非自主性提示）结合在一起，实现对*值*（感官输入）的选择倾向"></p><p>鉴于上面所提框架在上图中的主导地位，因此这个框架下的模型将成为本章的中心。然而，注意力机制的设计有许多替代方案。例如可以设计一个不可微的注意力模型，该模型可以使用强化学习方法 [<code>Mnih.Heess.Graves.ea.2014</code>]进行训练。</p><h3 id="1-3-注意力的可视化"><a href="#1-3-注意力的可视化" class="headerlink" title="1.3. 注意力的可视化"></a>1.3. 注意力的可视化</h3><p>平均池化层可以被视为输入的加权平均值，其中各输入的权重是一样的。实际上，注意力池化得到的是加权平均的总和值，其中权重是在给定的查询和不同的键之间计算得出的。</p><p>为了可视化注意力权重，需要定义一个<code>show_heatmaps</code>函数。其输入<code>matrices</code>的形状是（要显示的行数，要显示的列数，查询的数目，键的数目）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_heatmaps</span>(<span class="params">matrices, xlabel, ylabel, titles=<span class="literal">None</span>, figsize=(<span class="params"><span class="number">2.5</span>, <span class="number">2.5</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="function">                  cmap=<span class="string">&#x27;Reds&#x27;</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;显示矩阵热图&quot;&quot;&quot;</span></span><br><span class="line">    d2l.use_svg_display()</span><br><span class="line">    num_rows, num_cols = matrices.shape[<span class="number">0</span>], matrices.shape[<span class="number">1</span>]</span><br><span class="line">    fig, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize,</span><br><span class="line">                                 sharex=<span class="literal">True</span>, sharey=<span class="literal">True</span>, squeeze=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">for</span> i, (row_axes, row_matrices) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(axes, matrices)):</span><br><span class="line">        <span class="keyword">for</span> j, (ax, matrix) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(row_axes, row_matrices)):</span><br><span class="line">            pcm = ax.imshow(matrix.detach().numpy(), cmap=cmap)</span><br><span class="line">            <span class="keyword">if</span> i == num_rows - <span class="number">1</span>:</span><br><span class="line">                ax.set_xlabel(xlabel)</span><br><span class="line">            <span class="keyword">if</span> j == <span class="number">0</span>:</span><br><span class="line">                ax.set_ylabel(ylabel)</span><br><span class="line">            <span class="keyword">if</span> titles:</span><br><span class="line">                ax.set_title(titles[j])</span><br><span class="line">    fig.colorbar(pcm, ax=axes, shrink=<span class="number">0.6</span>);</span><br></pre></td></tr></table></figure><p>下面使用一个简单的例子进行演示，本例中，仅当查询和键相同时（即客体特征符合主体意识时），注意力权重为1，否则为0。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">attention_weights = torch.eye(<span class="number">10</span>).reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">show_heatmaps(attention_weights, xlabel=<span class="string">&#x27;Keys&#x27;</span>, ylabel=<span class="string">&#x27;Queries&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_attention-cues_1.svg" alt="输出"></p><p>后面的章节将经常调用<code>show_heatmaps</code>函数来显示注意力权重。</p><h2 id="2-注意力池化：Nadaraya-Watson-核回归"><a href="#2-注意力池化：Nadaraya-Watson-核回归" class="headerlink" title="2. 注意力池化：Nadaraya-Watson 核回归"></a>2. 注意力池化：Nadaraya-Watson 核回归</h2><p>上节介绍了框架下的注意力机制的主要成分：查询（自主提示）和键（非自主提示）之间的交互形成了注意力池化；注意力池化有选择地聚合了值（感官输入）以生成最终的输出。本节将介绍注意力池化的更多细节，以便从宏观上了解注意力机制在实践中的运作方式。1964年提出的Nadaraya-Watson核回归模型是一个简单但完整的例子，可以用于演示具有注意力机制的机器学习。</p><h3 id="2-1-生成数据集"><a href="#2-1-生成数据集" class="headerlink" title="2.1. 生成数据集"></a>2.1. 生成数据集</h3><p>简单起见，考虑这个回归问题：给定的成对的“输入－输出”数据集${(x_1, y_1), \ldots, (x_n, y_n)}$，如何学习$f$来预测任意新输入$x$的输出$\hat{y} = f(x)$？</p><p>根据下面的非线性函数生成一个人工数据集，其中加入的噪声项为$\epsilon$：</p><script type="math/tex; mode=display">y_i = 2\sin(x_i) + x_i^{0.8} + \epsilon,</script><p>其中$\epsilon$服从均值为$0$和标准差为$0.5$的正态分布。在这里生成了$50$个训练样本和$50$个测试样本。为了更好地可视化之后的注意力模式，需要将训练样本进行排序。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">n_train = <span class="number">50</span>  <span class="comment"># 训练样本数</span></span><br><span class="line"><span class="comment"># torch.sort返回排序后的张量和原张量在排序后张量中的对应索引</span></span><br><span class="line">x_train, _ = torch.sort(torch.rand(n_train) * <span class="number">5</span>)   <span class="comment"># 排序后的训练样本</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * torch.sin(x) + x**<span class="number">0.8</span></span><br><span class="line"></span><br><span class="line">y_train = f(x_train) + torch.normal(<span class="number">0.0</span>, <span class="number">0.5</span>, (n_train,))  <span class="comment"># 训练样本的输出</span></span><br><span class="line">x_test = torch.arange(<span class="number">0</span>, <span class="number">5</span>, <span class="number">0.1</span>)  <span class="comment"># 测试样本</span></span><br><span class="line">y_truth = f(x_test)  <span class="comment"># 测试样本的真实输出</span></span><br><span class="line">n_test = <span class="built_in">len</span>(x_test)  <span class="comment"># 测试样本数</span></span><br></pre></td></tr></table></figure><p>下面的函数将绘制所有的训练样本（样本由圆圈表示），不带噪声项的真实数据生成函数$f$（标记为“Truth”），以及学习得到的预测函数（标记为“Pred”）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_kernel_reg</span>(<span class="params">y_hat</span>):</span></span><br><span class="line">    d2l.plot(x_test, [y_truth, y_hat], <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;y&#x27;</span>, legend=[<span class="string">&#x27;Truth&#x27;</span>, <span class="string">&#x27;Pred&#x27;</span>],</span><br><span class="line">             xlim=[<span class="number">0</span>, <span class="number">5</span>], ylim=[-<span class="number">1</span>, <span class="number">5</span>])</span><br><span class="line">    d2l.plt.plot(x_train, y_train, <span class="string">&#x27;o&#x27;</span>, alpha=<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure><h3 id="2-2-平均池化"><a href="#2-2-平均池化" class="headerlink" title="2.2. 平均池化"></a>2.2. 平均池化</h3><p>先使用最简单的估计器来解决回归问题。基于平均池化来计算所有训练样本输出值的平均值：</p><script type="math/tex; mode=display">f(x) = \frac{1}{n}\sum_{i=1}^n y_i,</script><p>如下图所示，这个估计器确实不够聪明。真实函数$f$（“Truth”）和预测函数（“Pred”）相差很大。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里先求了一下训练集标签的平均值，得到一个单值张量（无维度）。</span></span><br><span class="line"><span class="comment"># torch.repeat_interleave(): 将输入张量按照指定维度进行扩展，若未指定维度则会将输入拉张开为1维向量再进行扩展。</span></span><br><span class="line"><span class="comment"># 这里没有指定dim，故先将单值张量转为1维张量，然后在该维度上复制成n_test个元素。</span></span><br><span class="line">y_hat = torch.repeat_interleave(y_train.mean(), n_test)</span><br><span class="line">plot_kernel_reg(y_hat)</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_nadaraya-waston_1.svg" alt="输出"></p><h3 id="2-3-非参数注意力池化"><a href="#2-3-非参数注意力池化" class="headerlink" title="2.3. 非参数注意力池化"></a>2.3. 非参数注意力池化</h3><p>显然，平均池化忽略了输入$x_i$。于是Nadaraya[<code>Nadaraya.1964</code>]和Watson[<code>Watson.1964</code>]提出了一个更好的想法，根据输入的位置对输出$y_i$进行加权：</p><script type="math/tex; mode=display">f(x) = \sum_{i=1}^n \frac{K(x - x_i)}{\sum_{j=1}^n K(x - x_j)} y_i,</script><p>其中$K$是<em>核</em>（kernel）。上式所描述的估计器被称为<em>Nadaraya-Watson核回归</em>（Nadaraya-Watson kernel regression）。这里不会深入讨论核函数的细节，但受此启发，我们可以从<a href="#12-查询键和值">第一节图中</a>的注意力机制框架的角度重写上式，成为一个更加通用的<em>注意力池化</em>（attention pooling）公式：</p><script type="math/tex; mode=display">f(x) = \sum_{i=1}^n \alpha(x, x_i) y_i,</script><p>其中$x$是查询，$(x_i, y_i)$是键值对。比较两个公式，注意力池化是$y_i$的加权平均。将查询$x$和键$x_i$之间的关系建模为<em>注意力权重</em>（attention weight）$\alpha(x, x_i)$，如上式所示，这个权重将被分配给每一个对应值$y_i$。对于任何查询，模型在所有键值对注意力权重都是一个有效的概率分布：它们是非负的，并且总和为1。</p><p>为了更好地理解注意力池化，考虑一个<em>高斯核</em>（Gaussian kernel），其定义为：</p><script type="math/tex; mode=display">K(u) = \frac{1}{\sqrt{2\pi}} \exp(-\frac{u^2}{2}).</script><p>将高斯核代入<em>Nadaraya-Watson核回归公式</em> 和 <em>注意力池化公式</em> 可以得到：</p><script type="math/tex; mode=display">\begin{aligned} f(x) &=\sum_{i=1}^n \alpha(x, x_i) y_i\\ &= \sum_{i=1}^n \frac{\exp\left(-\frac{1}{2}(x - x_i)^2\right)}{\sum_{j=1}^n \exp\left(-\frac{1}{2}(x - x_j)^2\right)} y_i \\&= \sum_{i=1}^n \mathrm{softmax}\left(-\frac{1}{2}(x - x_i)^2\right) y_i. \end{aligned}</script><p>在上式中，如果一个键$x_i$越是接近给定的查询$x$，那么分配给这个键对应值$y_i$的注意力权重就会越大，也就“获得了更多的注意力”。</p><p>这里穿插解释一下参数模型和非参数模型。</p><p>参数模型<br>: 在统计学中，参数模型通常假设总体服从某个分布，这个分布可以由一些参数确定，如正态分布由均值和标准差确定，在此基础上构建的模型称为参数模型。</p><p>非参数模型<br>: 非参数模型对于总体的分布不做任何假设或者说是数据分布假设自由，只知道其分布是存在的，所以就无法得到其分布的相关参数，只能通过非参数统计的方法进行推断。</p><p>总之，参数模型和非参数模型中的“参数”并不是模型中的参数，而是数据分布的参数。</p><p>Nadaraya-Watson核回归是一个非参数模型。因此，上式是<em>非参数的注意力池化</em>（nonparametric attention pooling）模型。接下来将基于这个非参数的注意力池化模型来绘制预测结果。从绘制的结果会发现新的模型预测线是平滑的，并且比平均池化的预测更接近真实。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># X_repeat的形状:(n_test,n_train),</span></span><br><span class="line"><span class="comment"># 每一行都包含着相同的测试输入（例如：同样的查询）</span></span><br><span class="line">X_repeat = x_test.repeat_interleave(n_train).reshape((-<span class="number">1</span>, n_train))</span><br><span class="line"><span class="comment"># x_train包含着键。attention_weights的形状：(n_test,n_train),</span></span><br><span class="line"><span class="comment"># 每一行都包含着要在给定的每个查询的值（y_train）之间分配的注意力权重</span></span><br><span class="line">attention_weights = nn.functional.softmax(-(X_repeat - x_train)**<span class="number">2</span> / <span class="number">2</span>, dim=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># y_hat的每个元素都是值的加权平均值，其中的权重是注意力权重</span></span><br><span class="line">y_hat = torch.matmul(attention_weights, y_train)</span><br><span class="line">plot_kernel_reg(y_hat)</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_nadaraya-waston_2.svg" alt="输出"></p><p>现在来观察注意力的权重。 这里测试数据的输入相当于查询，而训练数据的输入相当于键。 因为两个输入都是经过排序的，因此由观察可知“查询-键”对越接近，注意力池化的注意力权重就越高。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(attention_weights.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>),</span><br><span class="line">                  xlabel=<span class="string">&#x27;Sorted training inputs&#x27;</span>,</span><br><span class="line">                  ylabel=<span class="string">&#x27;Sorted testing inputs&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_nadaraya-waston_3.svg" alt="输出"></p><h3 id="2-4-带参数注意力池化"><a href="#2-4-带参数注意力池化" class="headerlink" title="2.4. 带参数注意力池化"></a>2.4. 带参数注意力池化</h3><p>非参数的Nadaraya-Watson核回归具有<em>一致性</em>（consistency）的优点：如果有足够的数据，此模型会收敛到最优结果。尽管如此，我们还是可以轻松地将可学习的参数集成到注意力池化中。</p><p>例如，与上一节略有不同，在下面的查询$x$和键$x_i$之间的距离乘以可学习参数$w$：</p><script type="math/tex; mode=display">\begin{aligned}f(x) &= \sum_{i=1}^n \alpha(x, x_i) y_i \\&= \sum_{i=1}^n \frac{\exp\left(-\frac{1}{2}((x - x_i)w)^2\right)}{\sum_{j=1}^n \exp\left(-\frac{1}{2}((x - x_j)w)^2\right)} y_i \\&= \sum_{i=1}^n \mathrm{softmax}\left(-\frac{1}{2}((x - x_i)w)^2\right) y_i \end{aligned}</script><p>本节的余下部分将通过训练这个模型来学习注意力池化的参数。</p><h4 id="2-4-1-批量矩阵乘法"><a href="#2-4-1-批量矩阵乘法" class="headerlink" title="2.4.1. 批量矩阵乘法"></a>2.4.1. 批量矩阵乘法</h4><p>为了更有效地计算小批量数据的注意力，可以利用深度学习开发框架中提供的批量矩阵乘法。</p><p>假设第一个小批量数据包含$n$个矩阵$\mathbf{X}_1,\ldots, \mathbf{X}_n$，第二个小批量包含$n$个矩阵$\mathbf{Y}_1, \ldots, \mathbf{Y}_n$，形状为$a\times b$，形状为$b\times c$。它们的批量矩阵乘法得到$n$个矩阵$\mathbf{X}_1\mathbf{Y}_1, \ldots, \mathbf{X}_n\mathbf{Y}_n$，形状为$a\times c$。因此，假定两个张量的形状分别是$(n,a,b)$和$(n,b,c)$，它们的批量矩阵乘法输出的形状为$(n,a,c)$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X = torch.ones((<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>))</span><br><span class="line">Y = torch.ones((<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>))</span><br><span class="line">torch.bmm(X, Y).shape</span><br></pre></td></tr></table></figure><p>在注意力机制的背景中，我们可以使用小批量矩阵乘法来计算小批量数据中的加权平均值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">weights = torch.ones((<span class="number">2</span>, <span class="number">10</span>)) * <span class="number">0.1</span></span><br><span class="line">values = torch.arange(<span class="number">20.0</span>).reshape((<span class="number">2</span>, <span class="number">10</span>))</span><br><span class="line"><span class="comment"># 权重在第一维升维，值在第二维升维，这里unsqueeze(-1) = unsqueeze(2)</span></span><br><span class="line">torch.bmm(weights.unsqueeze(<span class="number">1</span>), values.unsqueeze(-<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[[ <span class="number">4.5000</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">14.5000</span>]]])</span><br></pre></td></tr></table></figure><h4 id="2-4-2-定义模型"><a href="#2-4-2-定义模型" class="headerlink" title="2.4.2. 定义模型"></a>2.4.2. 定义模型</h4><p>基于上述的带参数的注意力池化，使用小批量矩阵乘法，定义Nadaraya-Watson核回归的带参数版本为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NWKernelRegression</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line">        self.w = nn.Parameter(torch.rand((<span class="number">1</span>,), requires_grad=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, queries, keys, values</span>):</span></span><br><span class="line">        <span class="comment"># queries和attention_weights的形状为(查询个数，“键－值”对个数)</span></span><br><span class="line">        queries = queries.repeat_interleave(keys.shape[<span class="number">1</span>]).reshape((-<span class="number">1</span>, keys.shape[<span class="number">1</span>]))</span><br><span class="line">        self.attention_weights = nn.functional.softmax(</span><br><span class="line">            -((queries - keys) * self.w)**<span class="number">2</span> / <span class="number">2</span>, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># values的形状为(查询个数，“键－值”对个数)</span></span><br><span class="line">        <span class="keyword">return</span> torch.bmm(self.attention_weights.unsqueeze(<span class="number">1</span>),</span><br><span class="line">                         values.unsqueeze(-<span class="number">1</span>)).reshape(-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h4 id="2-4-3-训练"><a href="#2-4-3-训练" class="headerlink" title="2.4.3. 训练"></a>2.4.3. 训练</h4><p>接下来，将训练数据集变换为键和值用于训练注意力模型。 在带参数的注意力池化模型中， 任何一个训练样本的输入都会和除自己以外的所有训练样本的“键－值”对进行计算， 从而得到其对应的预测输出。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># X_tile的形状:(n_train，n_train)，每一行都包含着相同的训练输入</span></span><br><span class="line">X_tile = x_train.repeat((n_train, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># Y_tile的形状:(n_train，n_train)，每一行都包含着相同的训练输出</span></span><br><span class="line">Y_tile = y_train.repeat((n_train, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># eye函数为了生成对角线全1，其余部分全0的二维数组。然后转化为对角线False的矩阵，从X_tile中去除了对角线元素后reshape为新的二维数组。</span></span><br><span class="line"><span class="comment"># keys的形状:(&#x27;n_train&#x27;，&#x27;n_train&#x27;-1)</span></span><br><span class="line">keys = X_tile[(<span class="number">1</span> - torch.eye(n_train)).<span class="built_in">type</span>(torch.<span class="built_in">bool</span>)].reshape((n_train, -<span class="number">1</span>))</span><br><span class="line"><span class="comment"># values的形状:(&#x27;n_train&#x27;，&#x27;n_train&#x27;-1)</span></span><br><span class="line">values = Y_tile[(<span class="number">1</span> - torch.eye(n_train)).<span class="built_in">type</span>(torch.<span class="built_in">bool</span>)].reshape((n_train, -<span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>训练带参数的注意力池化模型时，使用平方损失函数和随机梯度下降。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">net = NWKernelRegression()</span><br><span class="line">loss = nn.MSELoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.5</span>)</span><br><span class="line">animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>, xlim=[<span class="number">1</span>, <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    trainer.zero_grad()</span><br><span class="line">    l = loss(net(x_train, keys, values), y_train)</span><br><span class="line">    l.<span class="built_in">sum</span>().backward()</span><br><span class="line">    trainer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;<span class="built_in">float</span>(l.<span class="built_in">sum</span>()):<span class="number">.6</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    animator.add(epoch + <span class="number">1</span>, <span class="built_in">float</span>(l.<span class="built_in">sum</span>()))</span><br></pre></td></tr></table></figure><p>如下所示，训练完带参数的注意力池化模型后，我们发现： 在尝试拟合带噪声的训练数据时， 预测结果绘制的线不如之前非参数模型的平滑。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># keys的形状:(n_test，n_train)，每一行包含着相同的训练输入（例如，相同的键）</span></span><br><span class="line">keys = x_train.repeat((n_test, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># value的形状:(n_test，n_train)</span></span><br><span class="line">values = y_train.repeat((n_test, <span class="number">1</span>))</span><br><span class="line">y_hat = net(x_test, keys, values).unsqueeze(<span class="number">1</span>).detach()</span><br><span class="line">plot_kernel_reg(y_hat)</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_nadaraya-waston_4.svg" alt="输出"></p><p>为什么新的模型更不平滑了呢？ 来看一下输出结果的绘制图： 与非参数的注意力池化模型相比，带参数的模型加入可学习的参数后， 曲线在注意力权重较大的区域变得更不平滑。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(net.attention_weights.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>),</span><br><span class="line">                  xlabel=<span class="string">&#x27;Sorted training inputs&#x27;</span>,</span><br><span class="line">                  ylabel=<span class="string">&#x27;Sorted testing inputs&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_nadaraya-waston_5.svg" alt="5"></p><h2 id="3-注意力评分函数"><a href="#3-注意力评分函数" class="headerlink" title="3. 注意力评分函数"></a>3. 注意力评分函数</h2><p>上一节中，我们使用高斯核来对查询和键之间的关系建模。可以将其中的高斯核指数部分视为<strong>注意力评分函数</strong>（attention scoring function）， 简称<em>评分函数</em>（scoring function），然后把这个函数的输出结果输入到softmax函数中进行运算。 通过上述步骤，我们将得到与键对应的值的概率分布（即注意力权重）。 最后，注意力池化的输出就是基于这些注意力权重的值的加权和。</p><p>从宏观来看，我们可以使用上述算法来实现<a href="#12-查询键和值">1.2</a>中的注意力机制框架。下图说明了如何将注意力池化的输出计算成为值的加权和，其中 $a$ 表示注意力评分函数。 由于注意力权重是概率分布， 因此加权和其本质上是加权平均值。</p><p><img src="/assets/post_img/article74/attention-output.svg" alt="attention output"></p><p>用数学语言描述，假设有一个查询 $\mathbf{q} \in \mathbb{R}^q$ 和 $m$ 个“键－值”对 $(\mathbf{k}_1, \mathbf{v}_1), \ldots, (\mathbf{k}_m, \mathbf{v}_m)$，其中$\mathbf{k}_i \in \mathbb{R}^k$，$\mathbf{v}_i \in \mathbb{R}^v$。注意力池化函数 $f$ 就被表示成值的加权和：</p><script type="math/tex; mode=display">f(\mathbf{q}, (\mathbf{k}_1, \mathbf{v}_1), \ldots, (\mathbf{k}_m, \mathbf{v}_m)) = \sum_{i=1}^m \alpha(\mathbf{q}, \mathbf{k}_i) \mathbf{v}_i \in \mathbb{R}^v</script><p>其中查询$\mathbf{q}$和键$\mathbf{k}_i$的注意力权重（标量）是通过注意力评分函数$a$将两个向量映射成标量，再经过softmax运算得到的：</p><script type="math/tex; mode=display">\alpha(\mathbf{q}, \mathbf{k}_i) = \mathrm{softmax}(a(\mathbf{q}, \mathbf{k}_i)) = \frac{\exp(a(\mathbf{q}, \mathbf{k}_i))}{\sum_{j=1}^m \exp(a(\mathbf{q}, \mathbf{k}_j))} \in \mathbb{R}</script><p>正如上图所示，选择不同的注意力评分函数 $a$ 会导致不同的注意力池化操作。本节将介绍两个流行的评分函数，稍后将用他们来实现更复杂的注意力机制。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><h3 id="3-1-掩蔽softmax操作"><a href="#3-1-掩蔽softmax操作" class="headerlink" title="3.1. 掩蔽softmax操作"></a>3.1. 掩蔽softmax操作</h3><p>正如上面提到的，softmax操作用于输出一个概率分布作为注意力权重。在某些情况下，并非所有的值都应该被纳入到注意力池化中。例如，为了在“机器翻译与数据集”一节中高效处理小批量数据集，某些文本序列被填充了没有意义的特殊词元。为了仅将有意义的词元作为值来获取注意力池化，可以指定一个有效序列长度（即词元的个数），以便在计算softmax时过滤掉超出指定范围的位置。下面的<code>masked_softmax</code>函数实现了这样的<em>掩蔽softmax操作</em>（masked softmax operation），其中任何超出有效长度的位置都被掩蔽并置为0。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">masked_softmax</span>(<span class="params">X, valid_lens</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;通过在最后一个轴上掩蔽元素来执行softmax操作&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># X:3D张量，valid_lens:1D或2D张量</span></span><br><span class="line">    <span class="keyword">if</span> valid_lens <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> nn.functional.softmax(X, dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        shape = X.shape</span><br><span class="line">        <span class="keyword">if</span> valid_lens.dim() == <span class="number">1</span>:</span><br><span class="line">            valid_lens = torch.repeat_interleave(valid_lens, shape[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            valid_lens = valid_lens.reshape(-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 最后一轴上被掩蔽的元素使用一个非常大的负值替换，从而其softmax输出为0</span></span><br><span class="line">        X = d2l.sequence_mask(X.reshape(-<span class="number">1</span>, shape[-<span class="number">1</span>]), valid_lens,</span><br><span class="line">                              value=-<span class="number">1e6</span>)</span><br><span class="line">        <span class="keyword">return</span> nn.functional.softmax(X.reshape(shape), dim=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>为了演示此函数是如何工作的，考虑由两个$2 \times 4$矩阵表示的样本，这两个样本的有效长度分别为$2$和$3$。经过掩蔽softmax操作，超出有效长度的值都被掩蔽为0。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">masked_softmax(torch.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>), torch.tensor([<span class="number">2</span>, <span class="number">3</span>]))</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[[<span class="number">0.5423</span>, <span class="number">0.4577</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>],</span><br><span class="line">         [<span class="number">0.6133</span>, <span class="number">0.3867</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">0.3324</span>, <span class="number">0.2348</span>, <span class="number">0.4329</span>, <span class="number">0.0000</span>],</span><br><span class="line">         [<span class="number">0.2444</span>, <span class="number">0.3943</span>, <span class="number">0.3613</span>, <span class="number">0.0000</span>]]])</span><br></pre></td></tr></table></figure><p>也可以使用二维张量，为矩阵样本中的每一行指定有效长度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">masked_softmax(torch.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>), torch.tensor([[<span class="number">1</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">4</span>]]))</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[[<span class="number">1.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>],</span><br><span class="line">         [<span class="number">0.4142</span>, <span class="number">0.3582</span>, <span class="number">0.2275</span>, <span class="number">0.0000</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">0.5565</span>, <span class="number">0.4435</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>],</span><br><span class="line">         [<span class="number">0.3305</span>, <span class="number">0.2070</span>, <span class="number">0.2827</span>, <span class="number">0.1798</span>]]])</span><br></pre></td></tr></table></figure><h3 id="3-2-加性注意力"><a href="#3-2-加性注意力" class="headerlink" title="3.2. 加性注意力"></a>3.2. 加性注意力</h3><p>一般来说，当查询和键是<strong>不同长度</strong>的矢量时，可以使用加性注意力作为评分函数。给定查询$\mathbf{q} \in \mathbb{R}^q$和键$\mathbf{k} \in \mathbb{R}^k$，<em>加性注意力</em>（additive attention）的评分函数为</p><script type="math/tex; mode=display">a(\mathbf q, \mathbf k) = \mathbf w_v^\top \text{tanh}(\mathbf W_q\mathbf q + \mathbf W_k \mathbf k) \in \mathbb{R}</script><p>其中可学习的参数是$\mathbf W_q\in\mathbb R^{h\times q}$、$\mathbf W_k\in\mathbb R^{h\times k}$和$\mathbf w_v\in\mathbb R^{h}$。如上式所示，将查询和键连结起来后输入到一个多层感知机（MLP）中，感知机包含一个隐藏层，其隐藏单元数是一个超参数$h$。通过使用$\tanh$作为激活函数，并且禁用偏置项。</p><p>下面来实现加性注意力。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AdditiveAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;加性注意力&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, key_size, query_size, num_hiddens, dropout, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(AdditiveAttention, self).__init__(**kwargs)</span><br><span class="line">        self.W_k = nn.Linear(key_size, num_hiddens, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_q = nn.Linear(query_size, num_hiddens, bias=<span class="literal">False</span>)</span><br><span class="line">        self.w_v = nn.Linear(num_hiddens, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, queries, keys, values, valid_lens</span>):</span></span><br><span class="line">        queries, keys = self.W_q(queries), self.W_k(keys)</span><br><span class="line">        <span class="comment"># 在维度扩展后，</span></span><br><span class="line">        <span class="comment"># queries的形状：(batch_size，查询的个数，1，num_hidden)</span></span><br><span class="line">        <span class="comment"># key的形状：(batch_size，1，“键－值”对的个数，num_hiddens)</span></span><br><span class="line">        <span class="comment"># 使用广播方式进行求和</span></span><br><span class="line">        features = queries.unsqueeze(<span class="number">2</span>) + keys.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        features = torch.tanh(features)</span><br><span class="line">        <span class="comment"># self.w_v仅有一个输出，因此从形状中移除最后那个维度。</span></span><br><span class="line">        <span class="comment"># scores的形状：(batch_size，查询的个数，“键-值”对的个数)</span></span><br><span class="line">        scores = self.w_v(features).squeeze(-<span class="number">1</span>)</span><br><span class="line">        self.attention_weights = masked_softmax(scores, valid_lens)</span><br><span class="line">        <span class="comment"># values的形状：(batch_size，“键－值”对的个数，值的维度)</span></span><br><span class="line">        <span class="keyword">return</span> torch.bmm(self.dropout(self.attention_weights), values)</span><br></pre></td></tr></table></figure><p>用一个小例子来演示上面的<code>AdditiveAttention</code>类，其中查询、键和值的形状为（批量大小，步数或词元序列长度，特征大小），实际输出为$(2,1,20)$、$(2,10,2)$和$(2,10,4)$。注意力池化输出的形状为（批量大小，查询的步数，值的维度）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">queries, keys = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">1</span>, <span class="number">20</span>)), torch.ones((<span class="number">2</span>, <span class="number">10</span>, <span class="number">2</span>))</span><br><span class="line"><span class="comment"># values的小批量，两个值矩阵是相同的</span></span><br><span class="line">values = torch.arange(<span class="number">40</span>, dtype=torch.float32).reshape(<span class="number">1</span>, <span class="number">10</span>, <span class="number">4</span>).repeat(</span><br><span class="line">    <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">valid_lens = torch.tensor([<span class="number">2</span>, <span class="number">6</span>])</span><br><span class="line"></span><br><span class="line">attention = AdditiveAttention(key_size=<span class="number">2</span>, query_size=<span class="number">20</span>, num_hiddens=<span class="number">8</span>,</span><br><span class="line">                              dropout=<span class="number">0.1</span>)</span><br><span class="line">attention.<span class="built_in">eval</span>()</span><br><span class="line">attention(queries, keys, values, valid_lens)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[[ <span class="number">2.0000</span>,  <span class="number">3.0000</span>,  <span class="number">4.0000</span>,  <span class="number">5.0000</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">10.0000</span>, <span class="number">11.0000</span>, <span class="number">12.0000</span>, <span class="number">13.0000</span>]]], grad_fn=&lt;BmmBackward0&gt;)</span><br></pre></td></tr></table></figure><p>尽管加性注意力包含了可学习的参数，但由于本例子中每个键都是相同的， 所以注意力权重是均匀的，由指定的有效长度决定。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(attention.attention_weights.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">10</span>)),</span><br><span class="line">                  xlabel=<span class="string">&#x27;Keys&#x27;</span>, ylabel=<span class="string">&#x27;Queries&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_attention-scoring-functions.svg" alt="oasf"></p><h3 id="3-3-缩放点积注意力"><a href="#3-3-缩放点积注意力" class="headerlink" title="3.3. 缩放点积注意力"></a>3.3. 缩放点积注意力</h3><p>使用点积可以得到计算效率更高的评分函数，但是点积操作要求查询和键具有相同的长度$d$。假设查询和键的所有元素都是独立的随机变量，并且都满足零均值和单位方差，那么两个向量的点积的均值为$0$，方差为$d$。为确保无论向量长度如何，点积的方差在不考虑向量长度的情况下仍然是$1$，我们再将点积除以$\sqrt{d}$，则<em>缩放点积注意力</em>（scaled dot-product attention）评分函数为：</p><script type="math/tex; mode=display">a(\mathbf q, \mathbf k) = \mathbf{q}^\top \mathbf{k}  /\sqrt{d}</script><p>在实践中，我们通常从小批量的角度来考虑提高效率，例如基于$n$个查询和$m$个键－值对计算注意力，其中查询和键的长度为$d$，值的长度为$v$。查询$\mathbf Q\in\mathbb R^{n\times d}$、键$\mathbf K\in\mathbb R^{m\times d}$和值$\mathbf V\in\mathbb R^{m\times v}$的缩放点积注意力是：</p><script type="math/tex; mode=display">\mathrm{softmax}\left(\frac{\mathbf Q \mathbf K^\top }{\sqrt{d}}\right) \mathbf V \in \mathbb{R}^{n\times v}</script><p>下面的缩放点积注意力的实现使用了暂退法进行模型正则化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DotProductAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;缩放点积注意力&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dropout, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(DotProductAttention, self).__init__(**kwargs)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># queries的形状：(batch_size，查询的个数，d)</span></span><br><span class="line">    <span class="comment"># keys的形状：(batch_size，“键－值”对的个数，d)</span></span><br><span class="line">    <span class="comment"># values的形状：(batch_size，“键－值”对的个数，值的维度)</span></span><br><span class="line">    <span class="comment"># valid_lens的形状:(batch_size，)或者(batch_size，查询的个数)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, queries, keys, values, valid_lens=<span class="literal">None</span></span>):</span></span><br><span class="line">        d = queries.shape[-<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 设置transpose_b=True为了交换keys的最后两个维度</span></span><br><span class="line">        scores = torch.bmm(queries, keys.transpose(<span class="number">1</span>,<span class="number">2</span>)) / math.sqrt(d)</span><br><span class="line">        self.attention_weights = masked_softmax(scores, valid_lens)</span><br><span class="line">        <span class="keyword">return</span> torch.bmm(self.dropout(self.attention_weights), values)</span><br></pre></td></tr></table></figure><p>为了演示上述的DotProductAttention类， 我们使用与先前加性注意力例子中相同的键、值和有效长度。 对于点积操作，我们令查询的特征维度与键的特征维度大小相同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">queries = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">attention = DotProductAttention(dropout=<span class="number">0.5</span>)</span><br><span class="line">attention.<span class="built_in">eval</span>()</span><br><span class="line">attention(queries, keys, values, valid_lens)</span><br></pre></td></tr></table></figure><p>与加性注意力演示相同，由于键包含的是相同的元素， 而这些元素无法通过任何查询进行区分，因此获得了均匀的注意力权重。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(attention.attention_weights.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">10</span>)),</span><br><span class="line">                  xlabel=<span class="string">&#x27;Keys&#x27;</span>, ylabel=<span class="string">&#x27;Queries&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_attention-scoring-functions.svg" alt="oasf"></p><h2 id="4-Bahdanau-注意力"><a href="#4-Bahdanau-注意力" class="headerlink" title="4. Bahdanau 注意力"></a>4. Bahdanau 注意力</h2><p>之前章节中探讨了机器翻译问题：通过设计一个基于两个循环神经网络的编码器-解码器架构，用于序列到序列学习（seq2seq）。具体来说，循环神经网络编码器将长度可变的序列转换为固定形状的上下文变量，然后循环神经网络解码器根据生成的词元和上下文变量按词元生成输出（目标）序列词元。然而，即使并非所有输入（源）词元都对解码某个词元都有用，在每个解码步骤中仍使用编码<em>相同</em>的上下文变量。有什么方法能改变上下文变量呢？</p><p>试着从<code>Graves.2013</code>中找到灵感：在为给定文本序列生成手写的挑战中，Graves设计了一种可微注意力模型，将文本字符与更长的笔迹对齐，其中对齐方式仅向一个方向移动。受学习对齐想法的启发，Bahdanau等人提出了一个没有严格单向对齐限制的可微注意力模型 <code>Bahdanau.Cho.Bengio.2014</code>。在预测词元时，如果不是所有输入词元都相关，模型将仅对齐（或参与）输入序列中与当前预测相关的部分。这是通过将上下文变量视为注意力集中的输出来实现的。</p><p>由于这段是在NLP方向上加注意力机制，就粗略浏览一下，没做什么笔记。 —SZ</p><h3 id="4-1-模型"><a href="#4-1-模型" class="headerlink" title="4.1. 模型"></a>4.1. 模型</h3><p>下面描述的Bahdanau注意力模型将遵循之前seq2seq中的相同符号表达。这个新的基于注意力的模型与seq2seq中的模型相同，只不过上下文变量$\mathbf{c}$在任何解码时间步$t’$都会被$\mathbf{c}_{t’}$替换。假设输入序列中有$T$个词元，解码时间步$t’$的上下文变量是注意力集中的输出：</p><script type="math/tex; mode=display">\mathbf{c}_{t'} = \sum_{t=1}^T \alpha(\mathbf{s}_{t' - 1}, \mathbf{h}_t) \mathbf{h}_t</script><p>其中，时间步$t’ - 1$时的解码器隐状态$\mathbf{s}_{t’ - 1}$是查询，编码器隐状态$\mathbf{h}_t$既是键，也是值，注意力权重$\alpha$是使用<em>加性注意力打分函数</em>计算的。</p><p>与之前描述的循环神经网络编码器-解码器架构略有不同，下图描述了Bahdanau注意力的架构。</p><p><img src="/assets/post_img/article74/seq2seq-attention-details.svg" alt="一个带有Bahdanau注意力的循环神经网络编码器-解码器模型"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><h3 id="4-2-定义注意力解码器"><a href="#4-2-定义注意力解码器" class="headerlink" title="4.2. 定义注意力解码器"></a>4.2. 定义注意力解码器</h3><p>下面我们看看如何定义Bahdanau注意力，实现循环神经网络编码器-解码器。 其实，我们只需重新定义解码器即可。 为了更方便地显示学习的注意力权重， 以下AttentionDecoder类定义了带有注意力机制解码器的基本接口。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AttentionDecoder</span>(<span class="params">d2l.Decoder</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;带有注意力机制解码器的基本接口&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(AttentionDecoder, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">attention_weights</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure><p>接下来，让我们在接下来的<code>Seq2SeqAttentionDecoder</code>类中实现带有Bahdanau注意力的循环神经网络解码器。首先，初始化解码器的状态，需要下面的输入：</p><ol><li>编码器在所有时间步的最终层隐状态，将作为注意力的键和值；</li><li>上一时间步的编码器全层隐状态，将作为初始化解码器的隐状态；</li><li>编码器有效长度（排除在注意力池中填充词元）。</li></ol><p>在每个解码时间步骤中，解码器上一个时间步的最终层隐状态将用作查询。<br>因此，注意力输出和输入嵌入都连结为循环神经网络解码器的输入。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Seq2SeqAttentionDecoder</span>(<span class="params">AttentionDecoder</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embed_size, num_hiddens, num_layers,</span></span></span><br><span class="line"><span class="params"><span class="function">                 dropout=<span class="number">0</span>, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Seq2SeqAttentionDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.attention = d2l.AdditiveAttention(</span><br><span class="line">            num_hiddens, num_hiddens, num_hiddens, dropout)</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.rnn = nn.GRU(</span><br><span class="line">            embed_size + num_hiddens, num_hiddens, num_layers,</span><br><span class="line">            dropout=dropout)</span><br><span class="line">        self.dense = nn.Linear(num_hiddens, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_state</span>(<span class="params">self, enc_outputs, enc_valid_lens, *args</span>):</span></span><br><span class="line">        <span class="comment"># outputs的形状为(batch_size，num_steps，num_hiddens).</span></span><br><span class="line">        <span class="comment"># hidden_state的形状为(num_layers，batch_size，num_hiddens)</span></span><br><span class="line">        outputs, hidden_state = enc_outputs</span><br><span class="line">        <span class="keyword">return</span> (outputs.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>), hidden_state, enc_valid_lens)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, state</span>):</span></span><br><span class="line">        <span class="comment"># enc_outputs的形状为(batch_size,num_steps,num_hiddens).</span></span><br><span class="line">        <span class="comment"># hidden_state的形状为(num_layers,batch_size,</span></span><br><span class="line">        <span class="comment"># num_hiddens)</span></span><br><span class="line">        enc_outputs, hidden_state, enc_valid_lens = state</span><br><span class="line">        <span class="comment"># 输出X的形状为(num_steps,batch_size,embed_size)</span></span><br><span class="line">        X = self.embedding(X).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        outputs, self._attention_weights = [], []</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> X:</span><br><span class="line">            <span class="comment"># query的形状为(batch_size,1,num_hiddens)</span></span><br><span class="line">            query = torch.unsqueeze(hidden_state[-<span class="number">1</span>], dim=<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># context的形状为(batch_size,1,num_hiddens)</span></span><br><span class="line">            context = self.attention(</span><br><span class="line">                query, enc_outputs, enc_outputs, enc_valid_lens)</span><br><span class="line">            <span class="comment"># 在特征维度上连结</span></span><br><span class="line">            x = torch.cat((context, torch.unsqueeze(x, dim=<span class="number">1</span>)), dim=-<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 将x变形为(1,batch_size,embed_size+num_hiddens)</span></span><br><span class="line">            out, hidden_state = self.rnn(x.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>), hidden_state)</span><br><span class="line">            outputs.append(out)</span><br><span class="line">            self._attention_weights.append(self.attention.attention_weights)</span><br><span class="line">        <span class="comment"># 全连接层变换后，outputs的形状为</span></span><br><span class="line">        <span class="comment"># (num_steps,batch_size,vocab_size)</span></span><br><span class="line">        outputs = self.dense(torch.cat(outputs, dim=<span class="number">0</span>))</span><br><span class="line">        <span class="keyword">return</span> outputs.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>), [enc_outputs, hidden_state,</span><br><span class="line">                                          enc_valid_lens]</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">attention_weights</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self._attention_weights</span><br></pre></td></tr></table></figure><p>接下来，我们使用包含7个时间步的4个序列输入的小批量测试Bahdanau注意力解码器。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">encoder = d2l.Seq2SeqEncoder(vocab_size=<span class="number">10</span>, embed_size=<span class="number">8</span>, num_hiddens=<span class="number">16</span>,</span><br><span class="line">                             num_layers=<span class="number">2</span>)</span><br><span class="line">encoder.<span class="built_in">eval</span>()</span><br><span class="line">decoder = Seq2SeqAttentionDecoder(vocab_size=<span class="number">10</span>, embed_size=<span class="number">8</span>, num_hiddens=<span class="number">16</span>,</span><br><span class="line">                                  num_layers=<span class="number">2</span>)</span><br><span class="line">decoder.<span class="built_in">eval</span>()</span><br><span class="line">X = torch.zeros((<span class="number">4</span>, <span class="number">7</span>), dtype=torch.long)  <span class="comment"># (batch_size,num_steps)</span></span><br><span class="line">state = decoder.init_state(encoder(X), <span class="literal">None</span>)</span><br><span class="line">output, state = decoder(X, state)</span><br><span class="line">output.shape, <span class="built_in">len</span>(state), state[<span class="number">0</span>].shape, <span class="built_in">len</span>(state[<span class="number">1</span>]), state[<span class="number">1</span>][<span class="number">0</span>].shape</span><br></pre></td></tr></table></figure><h3 id="4-3-训练"><a href="#4-3-训练" class="headerlink" title="4.3. 训练"></a>4.3. 训练</h3><p>在这里指定超参数，实例化一个带有Bahdanau注意力的编码器和解码器， 并对这个模型进行机器翻译训练。 由于新增的注意力机制，训练要比没有注意力机制的慢得多。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">embed_size, num_hiddens, num_layers, dropout = <span class="number">32</span>, <span class="number">32</span>, <span class="number">2</span>, <span class="number">0.1</span></span><br><span class="line">batch_size, num_steps = <span class="number">64</span>, <span class="number">10</span></span><br><span class="line">lr, num_epochs, device = <span class="number">0.005</span>, <span class="number">250</span>, d2l.try_gpu()</span><br><span class="line"></span><br><span class="line">train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)</span><br><span class="line">encoder = d2l.Seq2SeqEncoder(</span><br><span class="line">    <span class="built_in">len</span>(src_vocab), embed_size, num_hiddens, num_layers, dropout)</span><br><span class="line">decoder = Seq2SeqAttentionDecoder(</span><br><span class="line">    <span class="built_in">len</span>(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)</span><br><span class="line">net = d2l.EncoderDecoder(encoder, decoder)</span><br><span class="line">d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)</span><br></pre></td></tr></table></figure><p>模型训练后，我们用它将几个英语句子翻译成法语并计算它们的BLEU分数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">engs = [<span class="string">&#x27;go .&#x27;</span>, <span class="string">&quot;i lost .&quot;</span>, <span class="string">&#x27;he\&#x27;s calm .&#x27;</span>, <span class="string">&#x27;i\&#x27;m home .&#x27;</span>]</span><br><span class="line">fras = [<span class="string">&#x27;va !&#x27;</span>, <span class="string">&#x27;j\&#x27;ai perdu .&#x27;</span>, <span class="string">&#x27;il est calme .&#x27;</span>, <span class="string">&#x27;je suis chez moi .&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> eng, fra <span class="keyword">in</span> <span class="built_in">zip</span>(engs, fras):</span><br><span class="line">    translation, dec_attention_weight_seq = d2l.predict_seq2seq(</span><br><span class="line">        net, eng, src_vocab, tgt_vocab, num_steps, device, <span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;eng&#125;</span> =&gt; <span class="subst">&#123;translation&#125;</span>, &#x27;</span>,</span><br><span class="line">          <span class="string">f&#x27;bleu <span class="subst">&#123;d2l.bleu(translation, fra, k=<span class="number">2</span>):<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">go . =&gt; va !,  bleu <span class="number">1.000</span></span><br><span class="line">i lost . =&gt; j<span class="string">&#x27;ai perdu .,  bleu 1.000</span></span><br><span class="line"><span class="string">he&#x27;</span>s calm . =&gt; je suis ici .,  bleu <span class="number">0.000</span></span><br><span class="line">i<span class="string">&#x27;m home . =&gt; je suis chez moi .,  bleu 1.000</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">attention_weights = torch.cat([step[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>] <span class="keyword">for</span> step <span class="keyword">in</span> dec_attention_weight_seq], <span class="number">0</span>).reshape((</span><br><span class="line">    <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, num_steps))</span><br></pre></td></tr></table></figure><p>训练结束后，通过可视化注意力权重你会发现，每个查询都会在键值对上分配不同的权重，这说明 在每个解码步中，输入序列的不同部分被选择性地聚集在注意力池中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加上一个包含序列结束词元</span></span><br><span class="line">d2l.show_heatmaps(</span><br><span class="line">    attention_weights[:, :, :, :<span class="built_in">len</span>(engs[-<span class="number">1</span>].split()) + <span class="number">1</span>].cpu(),</span><br><span class="line">    xlabel=<span class="string">&#x27;Key positions&#x27;</span>, ylabel=<span class="string">&#x27;Query positions&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_bahdanau-attention_seq.svg" alt="seq"></p><h2 id="5-多头注意力"><a href="#5-多头注意力" class="headerlink" title="5. 多头注意力"></a>5. 多头注意力</h2><p>在实践中，当给定相同的查询、键和值的集合时，我们希望模型可以基于相同的注意力机制学习到不同的行为，然后将不同的行为作为知识组合起来，捕获序列内各种范围的依赖关系（例如，短距离依赖和长距离依赖关系）。因此，允许注意力机制组合使用查询、键和值的不同<em>子空间表示</em>（representation subspaces）可能是有益的。</p><p>为此，与其只使用单独一个注意力池化，我们可以用独立学习得到的$h$组不同的<em>线性投影</em>（linear projections）来变换查询、键和值。然后，这$h$组变换后的查询、键和值将并行地送到注意力池化中。最后，将这$h$个注意力池化的输出拼接在一起，并且通过另一个可以学习的线性投影进行变换，以产生最终输出。这种设计被称为<em>多头注意力</em>（multihead attention）<code>Vaswani.Shazeer.Parmar.ea.2017</code>。对于$h$个注意力池化输出，每一个注意力池化都被称作一个<em>头</em>（head）。 下图展示了使用全连接层来实现可学习的线性变换的多头注意力。</p><p><img src="/assets/post_img/article74/multi-head-attention.svg" alt="多头注意力：多个头连结然后线性变换"></p><h3 id="5-1-模型"><a href="#5-1-模型" class="headerlink" title="5.1. 模型"></a>5.1. 模型</h3><p>在实现多头注意力之前，让我们用数学语言将这个模型形式化地描述出来。给定查询$\mathbf{q} \in \mathbb{R}^{d_q}$、键$\mathbf{k} \in \mathbb{R}^{d_k}$和值$\mathbf{v} \in \mathbb{R}^{d_v}$，每个注意力头$\mathbf{h}_i$（$i = 1, \ldots, h$）的计算方法为：</p><script type="math/tex; mode=display">\mathbf{h}_i = f(\mathbf W_i^{(q)}\mathbf q, \mathbf W_i^{(k)}\mathbf k,\mathbf W_i^{(v)}\mathbf v) \in \mathbb R^{p_v}</script><p>其中，可学习的参数包括$\mathbf W_i^{(q)}\in\mathbb R^{p_q\times d_q}$、$\mathbf W_i^{(k)}\in\mathbb R^{p_k\times d_k}$和$\mathbf W_i^{(v)}\in\mathbb R^{p_v\times d_v}$，以及代表注意力池化的函数$f$。$f$可以是<a href="#3-注意力评分函数">注意力评分函数</a>中的加性注意力和缩放点积注意力。多头注意力的输出需要经过另一个线性转换，它对应着$h$个头连结后的结果，因此其可学习参数是$\mathbf W_o\in\mathbb R^{p_o\times h p_v}$：</p><script type="math/tex; mode=display">\mathbf W_o \begin{bmatrix}\mathbf h_1\\\vdots\\\mathbf h_h\end{bmatrix} \in \mathbb{R}^{p_o}</script><p>基于这种设计，每个头都可能会关注输入的不同部分，可以表示比简单加权平均值更复杂的函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><h3 id="5-2-实现"><a href="#5-2-实现" class="headerlink" title="5.2. 实现"></a>5.2. 实现</h3><p>在实现过程中通常选择缩放点积注意力作为每一个注意力头。为了避免计算代价和参数代价的大幅增长，我们设定$p_q = p_k = p_v = p_o / h$。值得注意的是，如果将查询、键和值的线性变换的输出数量设置为$p_q h = p_k h = p_v h = p_o$，则可以并行计算$h$个头。在下面的实现中，$p_o$是通过参数<code>num_hiddens</code>指定的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;多头注意力&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, key_size, query_size, value_size, num_hiddens,</span></span></span><br><span class="line"><span class="params"><span class="function">                 num_heads, dropout, bias=<span class="literal">False</span>, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttention, self).__init__(**kwargs)</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        self.attention = d2l.DotProductAttention(dropout)</span><br><span class="line">        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)</span><br><span class="line">        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)</span><br><span class="line">        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)</span><br><span class="line">        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, queries, keys, values, valid_lens</span>):</span></span><br><span class="line">        <span class="comment"># queries，keys，values的形状:</span></span><br><span class="line">        <span class="comment"># (batch_size，查询或者“键－值”对的个数，num_hiddens)</span></span><br><span class="line">        <span class="comment"># valid_lens　的形状:</span></span><br><span class="line">        <span class="comment"># (batch_size，)或(batch_size，查询的个数)</span></span><br><span class="line">        <span class="comment"># 经过变换后，输出的queries，keys，values　的形状:</span></span><br><span class="line">        <span class="comment"># (batch_size*num_heads，查询或者“键－值”对的个数，</span></span><br><span class="line">        <span class="comment"># num_hiddens/num_heads)</span></span><br><span class="line">        queries = transpose_qkv(self.W_q(queries), self.num_heads)</span><br><span class="line">        keys = transpose_qkv(self.W_k(keys), self.num_heads)</span><br><span class="line">        values = transpose_qkv(self.W_v(values), self.num_heads)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> valid_lens <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 在轴0，将第一项（标量或者矢量）复制num_heads次，</span></span><br><span class="line">            <span class="comment"># 然后如此复制第二项，然后诸如此类。</span></span><br><span class="line">            valid_lens = torch.repeat_interleave(</span><br><span class="line">                valid_lens, repeats=self.num_heads, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># output的形状:(batch_size*num_heads，查询的个数，</span></span><br><span class="line">        <span class="comment"># num_hiddens/num_heads)</span></span><br><span class="line">        output = self.attention(queries, keys, values, valid_lens)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># output_concat的形状:(batch_size，查询的个数，num_hiddens)</span></span><br><span class="line">        output_concat = transpose_output(output, self.num_heads)</span><br><span class="line">        <span class="keyword">return</span> self.W_o(output_concat)</span><br></pre></td></tr></table></figure><p>为了能够使多个头并行计算，上面的<code>MultiHeadAttention</code>类将使用下面定义的两个转置函数。具体来说，<code>transpose_output</code>函数反转了<code>transpose_qkv</code>函数的操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transpose_qkv</span>(<span class="params">X, num_heads</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;为了多注意力头的并行计算而变换形状&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 输入X的形状:(batch_size，查询或者“键－值”对的个数，num_hiddens)</span></span><br><span class="line">    <span class="comment"># 输出X的形状:(batch_size，查询或者“键－值”对的个数，num_heads，</span></span><br><span class="line">    <span class="comment"># num_hiddens/num_heads)</span></span><br><span class="line">    X = X.reshape(X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>], num_heads, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出X的形状:(batch_size，num_heads，查询或者“键－值”对的个数,</span></span><br><span class="line">    <span class="comment"># num_hiddens/num_heads)</span></span><br><span class="line">    X = X.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 最终输出的形状:(batch_size*num_heads,查询或者“键－值”对的个数,</span></span><br><span class="line">    <span class="comment"># num_hiddens/num_heads)</span></span><br><span class="line">    <span class="keyword">return</span> X.reshape(-<span class="number">1</span>, X.shape[<span class="number">2</span>], X.shape[<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transpose_output</span>(<span class="params">X, num_heads</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;逆转transpose_qkv函数的操作&quot;&quot;&quot;</span></span><br><span class="line">    X = X.reshape(-<span class="number">1</span>, num_heads, X.shape[<span class="number">1</span>], X.shape[<span class="number">2</span>])</span><br><span class="line">    X = X.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">    <span class="keyword">return</span> X.reshape(X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>], -<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>下面我们使用键和值相同的小例子来测试我们编写的MultiHeadAttention类。 多头注意力输出的形状是（batch_size，num_queries，num_hiddens）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens, num_heads = <span class="number">100</span>, <span class="number">5</span></span><br><span class="line">attention = MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens,</span><br><span class="line">                               num_hiddens, num_heads, <span class="number">0.5</span>)</span><br><span class="line">attention.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">MultiHeadAttention(</span><br><span class="line">  (attention): DotProductAttention(</span><br><span class="line">    (dropout): Dropout(p=<span class="number">0.5</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">  )</span><br><span class="line">  (W_q): Linear(in_features=<span class="number">100</span>, out_features=<span class="number">100</span>, bias=<span class="literal">False</span>)</span><br><span class="line">  (W_k): Linear(in_features=<span class="number">100</span>, out_features=<span class="number">100</span>, bias=<span class="literal">False</span>)</span><br><span class="line">  (W_v): Linear(in_features=<span class="number">100</span>, out_features=<span class="number">100</span>, bias=<span class="literal">False</span>)</span><br><span class="line">  (W_o): Linear(in_features=<span class="number">100</span>, out_features=<span class="number">100</span>, bias=<span class="literal">False</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">batch_size, num_queries = <span class="number">2</span>, <span class="number">4</span></span><br><span class="line">num_kvpairs, valid_lens =  <span class="number">6</span>, torch.tensor([<span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">X = torch.ones((batch_size, num_queries, num_hiddens))</span><br><span class="line">Y = torch.ones((batch_size, num_kvpairs, num_hiddens))</span><br><span class="line">attention(X, Y, Y, valid_lens).shape</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">4</span>, <span class="number">100</span>])</span><br></pre></td></tr></table></figure><h2 id="6-自注意力和位置编码"><a href="#6-自注意力和位置编码" class="headerlink" title="6. 自注意力和位置编码"></a>6. 自注意力和位置编码</h2><p>在深度学习中，经常使用卷积神经网络（CNN）或循环神经网络（RNN）对序列进行编码。想象一下，有了注意力机制之后，我们将词元序列输入注意力池化中，以便同一组词元同时充当查询、键和值。具体来说，每个查询都会关注所有的键－值对并生成一个注意力输出。由于查询、键和值来自同一组输入，因此被称为<em>自注意力</em>（self-attention）<code>Lin.Feng.Santos.ea.2017,Vaswani.Shazeer.Parmar.ea.2017</code>，也被称为<em>内部注意力</em>（intra-attention）<code>Cheng.Dong.Lapata.2016,Parikh.Tackstrom.Das.ea.2016,Paulus.Xiong.Socher.2017</code>。本节将使用自注意力进行序列编码，以及如何使用序列的顺序作为补充信息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><h3 id="6-1-自注意力"><a href="#6-1-自注意力" class="headerlink" title="6.1. 自注意力"></a>6.1. 自注意力</h3><p>给定一个由词元组成的输入序列$\mathbf{x}_1, \ldots, \mathbf{x}_n$，其中任意$\mathbf{x}_i \in \mathbb{R}^d$（$1 \leq i \leq n$）。该序列的自注意力输出为一个长度相同的序列<br>$\mathbf{y}_1, \ldots, \mathbf{y}_n$，其中：</p><script type="math/tex; mode=display">\mathbf{y}_i = f(\mathbf{x}_i, (\mathbf{x}_1, \mathbf{x}_1), \ldots, (\mathbf{x}_n, \mathbf{x}_n)) \in \mathbb{R}^d</script><p>根据<a href="#24-带参数注意力池化">2.4</a>中定义的注意力池化函数$f$。下面的代码片段是基于多头注意力对一个张量完成自注意力的计算，张量的形状为（批量大小，时间步的数目或词元序列的长度，$d$）。输出与输入的张量形状相同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens, num_heads = <span class="number">100</span>, <span class="number">5</span></span><br><span class="line">attention = d2l.MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens,</span><br><span class="line">                                   num_hiddens, num_heads, <span class="number">0.5</span>)</span><br><span class="line">attention.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">MultiHeadAttention(</span><br><span class="line">  (attention): DotProductAttention(</span><br><span class="line">    (dropout): Dropout(p=<span class="number">0.5</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">  )</span><br><span class="line">  (W_q): Linear(in_features=<span class="number">100</span>, out_features=<span class="number">100</span>, bias=<span class="literal">False</span>)</span><br><span class="line">  (W_k): Linear(in_features=<span class="number">100</span>, out_features=<span class="number">100</span>, bias=<span class="literal">False</span>)</span><br><span class="line">  (W_v): Linear(in_features=<span class="number">100</span>, out_features=<span class="number">100</span>, bias=<span class="literal">False</span>)</span><br><span class="line">  (W_o): Linear(in_features=<span class="number">100</span>, out_features=<span class="number">100</span>, bias=<span class="literal">False</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">batch_size, num_queries, valid_lens = <span class="number">2</span>, <span class="number">4</span>, torch.tensor([<span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">X = torch.ones((batch_size, num_queries, num_hiddens))</span><br><span class="line">attention(X, X, X, valid_lens).shape</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">4</span>, <span class="number">100</span>])</span><br></pre></td></tr></table></figure><h3 id="6-2-比较卷积神经网络、循环神经网络和自注意力"><a href="#6-2-比较卷积神经网络、循环神经网络和自注意力" class="headerlink" title="6.2. 比较卷积神经网络、循环神经网络和自注意力"></a>6.2. 比较卷积神经网络、循环神经网络和自注意力</h3><p>接下来比较下面几个架构，目标都是将由$n$个词元组成的序列映射到另一个长度相等的序列，其中的每个输入词元或输出词元都由$d$维向量表示。具体来说，将比较的是卷积神经网络、循环神经网络和自注意力这几个架构的计算复杂性、顺序操作和最大路径长度。请注意，顺序操作会妨碍并行计算，而任意的序列位置组合之间的路径越短，则能更轻松地学习序列中的远距离依赖关系 <code>Hochreiter.Bengio.Frasconi.ea.2001</code>。</p><p><img src="/assets/post_img/article74/cnn-rnn-self-attention.svg" alt="比较卷积神经网络（填充词元被忽略）、循环神经网络和自注意力三种架构"></p><p>考虑一个卷积核大小为$k$的卷积层。在后面的章节将提供关于使用卷积神经网络处理序列的更多详细信息。目前只需要知道的是，由于序列长度是$n$，输入和输出的通道数量都是$d$，所以卷积层的计算复杂度为$\mathcal{O}(knd^2)$。如上图所示，卷积神经网络是分层的，因此为有$\mathcal{O}(1)$个顺序操作，最大路径长度为$\mathcal{O}(n/k)$。例如，$\mathbf{x}_1$和$\mathbf{x}_5$处于上图中卷积核大小为3的双层卷积神经网络的感受野内。</p><p>当更新循环神经网络的隐状态时，$d \times d$权重矩阵和$d$维隐状态的乘法计算复杂度为$\mathcal{O}(d^2)$。由于序列长度为$n$，因此循环神经网络层的计算复杂度为$\mathcal{O}(nd^2)$。根据上图，有$\mathcal{O}(n)$个顺序操作无法并行化，最大路径长度也是$\mathcal{O}(n)$。</p><p>在自注意力中，查询、键和值都是$n \times d$矩阵。考虑<a href="#33-缩放点积注意力">3.3</a>中缩放的”点－积“注意力，其中$n \times d$矩阵乘以$d \times n$矩阵。之后输出的$n \times n$矩阵乘以$n \times d$矩阵。因此，自注意力具有$\mathcal{O}(n^2d)$计算复杂性。正如在上图中所讲，每个词元都通过自注意力直接连接到任何其他词元。因此，有$\mathcal{O}(1)$个顺序操作可以并行计算，最大路径长度也是$\mathcal{O}(1)$。</p><p>总而言之，卷积神经网络和自注意力都拥有并行计算的优势，而且自注意力的最大路径长度最短。但是因为其计算复杂度是关于序列长度的二次方，所以在很长的序列中计算会非常慢。</p><h3 id="6-3-位置编码"><a href="#6-3-位置编码" class="headerlink" title="6.3. 位置编码"></a>6.3. 位置编码</h3><p>在处理词元序列时，循环神经网络是逐个的重复地处理词元的，而自注意力则因为并行计算而放弃了顺序操作。为了使用序列的顺序信息，通过在输入表示中添加<em>位置编码</em>（positional encoding）来注入绝对的或相对的位置信息。位置编码可以通过学习得到也可以直接固定得到。接下来描述的是基于正弦函数和余弦函数的固定位置编码<code>Vaswani.Shazeer.Parmar.ea.2017</code>。</p><p>假设输入表示 $\mathbf{X} \in \mathbb{R}^{n \times d}$ 包含一个序列中$n$个词元的$d$维嵌入表示。位置编码使用相同形状的位置嵌入矩阵 $\mathbf{P} \in \mathbb{R}^{n \times d}$输出$\mathbf{X} + \mathbf{P}$，矩阵第$i$行、第$2j$列和$2j+1$列上的元素为：</p><script type="math/tex; mode=display">\begin{aligned} p_{i, 2j} &= \sin\left(\frac{i}{10000^{2j/d}}\right),\\p_{i, 2j+1} &= \cos\left(\frac{i}{10000^{2j/d}}\right).\end{aligned}</script><p>乍一看，这种基于三角函数的设计看起来很奇怪。在解释这个设计之前，让我们先在下面的<code>PositionalEncoding</code>类中实现它。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;位置编码&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_hiddens, dropout, max_len=<span class="number">1000</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="comment"># 创建一个足够长的P</span></span><br><span class="line">        self.P = torch.zeros((<span class="number">1</span>, max_len, num_hiddens))</span><br><span class="line">        X = torch.arange(max_len, dtype=torch.float32).reshape(</span><br><span class="line">            -<span class="number">1</span>, <span class="number">1</span>) / torch.<span class="built_in">pow</span>(<span class="number">10000</span>, torch.arange(</span><br><span class="line">            <span class="number">0</span>, num_hiddens, <span class="number">2</span>, dtype=torch.float32) / num_hiddens)</span><br><span class="line">        self.P[:, :, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(X)</span><br><span class="line">        self.P[:, :, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(X)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        X = X + self.P[:, :X.shape[<span class="number">1</span>], :].to(X.device)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(X)</span><br></pre></td></tr></table></figure><p>在位置嵌入矩阵$\mathbf{P}$中，行代表词元在序列中的位置，列代表位置编码的不同维度。从下面的例子中可以看到位置嵌入矩阵的第$6$列和第$7$列的频率高于第$8$列和第$9$列。第$6$列和第$7$列之间的偏移量（第$8$列和第$9$列相同）是由于正弦函数和余弦函数的交替。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">encoding_dim, num_steps = <span class="number">32</span>, <span class="number">60</span></span><br><span class="line">pos_encoding = PositionalEncoding(encoding_dim, <span class="number">0</span>)</span><br><span class="line">pos_encoding.<span class="built_in">eval</span>()</span><br><span class="line">X = pos_encoding(torch.zeros((<span class="number">1</span>, num_steps, encoding_dim)))</span><br><span class="line">P = pos_encoding.P[:, :X.shape[<span class="number">1</span>], :]</span><br><span class="line">d2l.plot(torch.arange(num_steps), P[<span class="number">0</span>, :, <span class="number">6</span>:<span class="number">10</span>].T, xlabel=<span class="string">&#x27;Row (position)&#x27;</span>,</span><br><span class="line">         figsize=(<span class="number">6</span>, <span class="number">2.5</span>), legend=[<span class="string">&quot;Col %d&quot;</span> % d <span class="keyword">for</span> d <span class="keyword">in</span> torch.arange(<span class="number">6</span>, <span class="number">10</span>)])</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_self-attention-and-positional-encoding.svg" alt="row（pos）"></p><h4 id="6-3-1-绝对位置信息"><a href="#6-3-1-绝对位置信息" class="headerlink" title="6.3.1. 绝对位置信息"></a>6.3.1. 绝对位置信息</h4><p>为了明白沿着编码维度单调降低的频率与绝对位置信息的关系，让我们打印出$0, 1, \ldots, 7$的[<strong>二进制表示</strong>]形式。正如所看到的，每个数字、每两个数字和每四个数字上的比特值在第一个最低位、第二个最低位和第三个最低位上分别交替。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;i&#125;</span>的二进制是：<span class="subst">&#123;i:&gt;03b&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line"><span class="number">0</span>的二进制是：<span class="number">000</span></span><br><span class="line"><span class="number">1</span>的二进制是：001</span><br><span class="line"><span class="number">2</span>的二进制是：010</span><br><span class="line"><span class="number">3</span>的二进制是：011</span><br><span class="line"><span class="number">4</span>的二进制是：<span class="number">100</span></span><br><span class="line"><span class="number">5</span>的二进制是：<span class="number">101</span></span><br><span class="line"><span class="number">6</span>的二进制是：<span class="number">110</span></span><br><span class="line"><span class="number">7</span>的二进制是：<span class="number">111</span></span><br></pre></td></tr></table></figure><p>在二进制表示中，较高比特位的交替频率低于较低比特位， 与下面的热图所示相似，只是位置编码通过使用三角函数在编码维度上降低频率。 由于输出是浮点数，因此此类连续表示比二进制表示法更节省空间。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">P = P[<span class="number">0</span>, :, :].unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>)</span><br><span class="line">d2l.show_heatmaps(P, xlabel=<span class="string">&#x27;Column (encoding dimension)&#x27;</span>,</span><br><span class="line">                  ylabel=<span class="string">&#x27;Row (position)&#x27;</span>, figsize=(<span class="number">3.5</span>, <span class="number">4</span>), cmap=<span class="string">&#x27;Blues&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_self-attention-and-positional-encoding_2.svg" alt=""></p><h4 id="6-3-2-相对位置信息"><a href="#6-3-2-相对位置信息" class="headerlink" title="6.3.2. 相对位置信息"></a>6.3.2. 相对位置信息</h4><p>除了捕获绝对位置信息之外，上述的位置编码还允许模型学习得到输入序列中相对位置信息。这是因为对于任何确定的位置偏移$\delta$，位置$i + \delta$处的位置编码可以线性投影位置$i$处的位置编码来表示。</p><p>这种投影的数学解释是，令$\omega_j = 1/10000^{2j/d}$，对于任何确定的位置偏移$\delta$，<a href="#63-位置编码">位置编码</a>中的任何一对$(p_{i, 2j}, p_{i, 2j+1})$都可以线性投影到$(p_{i+\delta, 2j}, p_{i+\delta, 2j+1})$：</p><script type="math/tex; mode=display">\begin{aligned}&\begin{bmatrix} \cos(\delta \omega_j) & \sin(\delta \omega_j) \\  -\sin(\delta \omega_j) & \cos(\delta \omega_j) \\ \end{bmatrix}\begin{bmatrix} p_{i, 2j} \\  p_{i, 2j+1} \\ \end{bmatrix}\\=&\begin{bmatrix} \cos(\delta \omega_j) \sin(i \omega_j) + \sin(\delta \omega_j) \cos(i \omega_j) \\  -\sin(\delta \omega_j) \sin(i \omega_j) + \cos(\delta \omega_j) \cos(i \omega_j) \\ \end{bmatrix}\\=&\begin{bmatrix} \sin\left((i+\delta) \omega_j\right) \\  \cos\left((i+\delta) \omega_j\right) \\ \end{bmatrix}\\=& \begin{bmatrix} p_{i+\delta, 2j} \\  p_{i+\delta, 2j+1} \\ \end{bmatrix},\end{aligned}</script><p>$2\times 2$投影矩阵不依赖于任何位置的索引$i$。</p><h2 id="7-Transformer"><a href="#7-Transformer" class="headerlink" title="7. Transformer"></a>7. Transformer</h2><p>在<a href="#62-比较卷积神经网络循环神经网络和自注意力">6.2</a>中比较了卷积神经网络（CNN）、循环神经网络（RNN）和自注意力（self-attention）。值得注意的是，自注意力同时具有并行计算和最短的最大路径长度这两个优势。因此，使用自注意力来设计深度架构是很有吸引力的。对比之前仍然依赖循环神经网络实现输入表示的自注意力模型 <code>Cheng.Dong.Lapata.2016,Lin.Feng.Santos.ea.2017,Paulus.Xiong.Socher.2017</code>，Transformer模型完全基于注意力机制，没有任何卷积层或循环神经网络层 <code>Vaswani.Shazeer.Parmar.ea.2017</code>。尽管Transformer最初是应用于在文本数据上的序列到序列学习，但现在已经推广到各种现代的深度学习中，例如语言、视觉、语音和强化学习领域。</p><h3 id="7-1-模型"><a href="#7-1-模型" class="headerlink" title="7.1. 模型"></a>7.1. 模型</h3><p>Transformer作为编码器－解码器架构的一个实例，其整体架构图如下所示。正如所见到的，Transformer是由编码器和解码器组成的。与<a href="#41-模型">带有Bahdanau注意力的循环神经网络编码器-解码器模型</a>中基于Bahdanau注意力实现的序列到序列的学习相比，Transformer的编码器和解码器是基于自注意力的模块叠加而成的，源（输入）序列和目标（输出）序列的<em>嵌入</em>（embedding）表示将加上<em>位置编码</em>（positional encoding），再分别输入到编码器和解码器中。</p><p><img src="/assets/post_img/article74/transformer.svg" alt="transformer架构"></p><p>上图概述了Transformer的架构。从宏观角度来看，Transformer的编码器是由多个相同的层叠加而成的，每个层都有两个子层（子层表示为$\mathrm{sublayer}$）。第一个子层是<em>多头自注意力</em>（multi-head self-attention）池化；第二个子层是<em>基于位置的前馈网络</em>（positionwise feed-forward network）。具体来说，在计算编码器的自注意力时，查询、键和值都来自前一个编码器层的输出。受ResNet中残差网络的启发，每个子层都采用了<em>残差连接</em>（residual connection）。在Transformer中，对于序列中任何位置的任何输入$\mathbf{x} \in \mathbb{R}^d$，都要求满足$\mathrm{sublayer}(\mathbf{x}) \in \mathbb{R}^d$，以便残差连接满足$\mathbf{x} + \mathrm{sublayer}(\mathbf{x}) \in \mathbb{R}^d$。在残差连接的加法计算之后，紧接着应用<em>层规范化</em>（layer normalization）<code>Ba.Kiros.Hinton.2016</code>。因此，输入序列对应的每个位置，Transformer编码器都将输出一个$d$维表示向量。</p><p>Transformer解码器也是由多个相同的层叠加而成的，并且层中使用了残差连接和层规范化。除了编码器中描述的两个子层之外，解码器还在这两个子层之间插入了第三个子层，称为<em>编码器－解码器注意力</em>（encoder-decoder attention）层。在编码器－解码器注意力中，查询来自前一个解码器层的输出，而键和值来自整个编码器的输出。在解码器自注意力中，查询、键和值都来自上一个解码器层的输出。但是，解码器中的每个位置只能考虑该位置之前的所有位置。这种<em>掩蔽</em>（masked）注意力保留了<em>自回归</em>（auto-regressive）属性，确保预测仅依赖于已生成的输出词元。</p><p>在此之前已经描述并实现了基于缩放点积多头注意力和位置编码。接下来将实现Transformer模型的剩余部分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><h3 id="7-2-基于位置的前馈网络"><a href="#7-2-基于位置的前馈网络" class="headerlink" title="7.2. 基于位置的前馈网络"></a>7.2. 基于位置的前馈网络</h3><p>基于位置的前馈网络对序列中的所有位置的表示进行变换时使用的是同一个多层感知机（MLP），这就是称前馈网络是<em>基于位置的</em>（positionwise）的原因。在下面的实现中，输入<code>X</code>的形状（批量大小，时间步数或序列长度，隐单元数或特征维度）将被一个两层的感知机转换成形状为（批量大小，时间步数，<code>ffn_num_outputs</code>）的输出张量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionWiseFFN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;基于位置的前馈网络&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs,</span></span></span><br><span class="line"><span class="params"><span class="function">                 **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PositionWiseFFN, self).__init__(**kwargs)</span><br><span class="line">        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.dense2(self.relu(self.dense1(X)))</span><br></pre></td></tr></table></figure><p>下面的例子显示，改变张量的最里层维度的尺寸，会改变成基于位置的前馈网络的输出尺寸。因为用同一个多层感知机对所有位置上的输入进行变换，所以当所有这些位置的输入相同时，它们的输出也是相同的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ffn = PositionWiseFFN(<span class="number">4</span>, <span class="number">4</span>, <span class="number">8</span>)</span><br><span class="line">ffn.<span class="built_in">eval</span>()</span><br><span class="line">ffn(torch.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)))[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[-<span class="number">1.2386</span>, -<span class="number">0.9917</span>,  <span class="number">0.4708</span>, -<span class="number">0.4516</span>, -<span class="number">0.3069</span>,  <span class="number">0.0447</span>,  <span class="number">0.9740</span>,  <span class="number">0.1313</span>],</span><br><span class="line">        [-<span class="number">1.2386</span>, -<span class="number">0.9917</span>,  <span class="number">0.4708</span>, -<span class="number">0.4516</span>, -<span class="number">0.3069</span>,  <span class="number">0.0447</span>,  <span class="number">0.9740</span>,  <span class="number">0.1313</span>],</span><br><span class="line">        [-<span class="number">1.2386</span>, -<span class="number">0.9917</span>,  <span class="number">0.4708</span>, -<span class="number">0.4516</span>, -<span class="number">0.3069</span>,  <span class="number">0.0447</span>,  <span class="number">0.9740</span>,  <span class="number">0.1313</span>]],</span><br><span class="line">       grad_fn=&lt;SelectBackward0&gt;)</span><br></pre></td></tr></table></figure><h3 id="7-3-残差连接和层规范化"><a href="#7-3-残差连接和层规范化" class="headerlink" title="7.3. 残差连接和层规范化"></a>7.3. 残差连接和层规范化</h3><p>现在让我们关注<em>加法和规范化</em>（add&amp;norm）组件。正如在本节开头所述，这是由残差连接和紧随其后的层规范化组成的。两者都是构建有效的深度架构的关键。</p><p>“批量规范化”章节中解释了在一个小批量的样本内基于批量规范化对数据进行重新中心化和重新缩放的调整。层规范化和批量规范化的目标相同，但层规范化是基于特征维度进行规范化。尽管批量规范化在计算机视觉中被广泛应用，但在自然语言处理任务中（输入通常是变长序列）批量规范化通常不如层规范化的效果好。</p><p>以下代码对比不同维度的层规范化和批量规范化的效果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">ln = nn.LayerNorm(<span class="number">2</span>)</span><br><span class="line">bn = nn.BatchNorm1d(<span class="number">2</span>)</span><br><span class="line">X = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">3</span>]], dtype=torch.float32)</span><br><span class="line"><span class="comment"># 在训练模式下计算X的均值和方差</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;layer norm:&#x27;</span>, ln(X), <span class="string">&#x27;\nbatch norm:&#x27;</span>, bn(X))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># layer norm: tensor([[-1.0000,  1.0000],</span></span><br><span class="line"><span class="comment">#         [-1.0000,  1.0000]], grad_fn=&lt;NativeLayerNormBackward0&gt;)</span></span><br><span class="line"><span class="comment"># batch norm: tensor([[-1.0000, -1.0000],</span></span><br><span class="line"><span class="comment">#         [ 1.0000,  1.0000]], grad_fn=&lt;NativeBatchNormBackward0&gt;)</span></span><br></pre></td></tr></table></figure><p>现在我们可以使用残差连接和层规范化来实现AddNorm类。暂退法也被作为正则化方法使用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AddNorm</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;残差连接后进行层规范化&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, normalized_shape, dropout, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(AddNorm, self).__init__(**kwargs)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.ln = nn.LayerNorm(normalized_shape)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, Y</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.ln(self.dropout(Y) + X)</span><br></pre></td></tr></table></figure><p>残差连接要求两个输入的形状相同，以便加法操作后输出张量的形状相同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">add_norm = AddNorm([<span class="number">3</span>, <span class="number">4</span>], <span class="number">0.5</span>)</span><br><span class="line">add_norm.<span class="built_in">eval</span>()</span><br><span class="line">add_norm(torch.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)), torch.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))).shape</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure><h3 id="7-4-编码器"><a href="#7-4-编码器" class="headerlink" title="7.4. 编码器"></a>7.4. 编码器</h3><p>有了组成transformer编码器的基础组件，现在可以先实现编码器中的一个层。下面的EncoderBlock类包含两个子层：多头自注意力和基于位置的前馈网络，这两个子层都使用了残差连接和紧随的层规范化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;transformer编码器块&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, key_size, query_size, value_size, num_hiddens,</span></span></span><br><span class="line"><span class="params"><span class="function">                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span></span></span><br><span class="line"><span class="params"><span class="function">                 dropout, use_bias=<span class="literal">False</span>, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(EncoderBlock, self).__init__(**kwargs)</span><br><span class="line">        self.attention = d2l.MultiHeadAttention(</span><br><span class="line">            key_size, query_size, value_size, num_hiddens, num_heads, dropout,</span><br><span class="line">            use_bias)</span><br><span class="line">        self.addnorm1 = AddNorm(norm_shape, dropout)</span><br><span class="line">        self.ffn = PositionWiseFFN(</span><br><span class="line">            ffn_num_input, ffn_num_hiddens, num_hiddens)</span><br><span class="line">        self.addnorm2 = AddNorm(norm_shape, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, valid_lens</span>):</span></span><br><span class="line">        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))</span><br><span class="line">        <span class="keyword">return</span> self.addnorm2(Y, self.ffn(Y))</span><br></pre></td></tr></table></figure><p>正如我们所看到的，transformer编码器中的任何层都不会改变其输入的形状。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">X = torch.ones((<span class="number">2</span>, <span class="number">100</span>, <span class="number">24</span>))</span><br><span class="line">valid_lens = torch.tensor([<span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">encoder_blk = EncoderBlock(<span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, [<span class="number">100</span>, <span class="number">24</span>], <span class="number">24</span>, <span class="number">48</span>, <span class="number">8</span>, <span class="number">0.5</span>)</span><br><span class="line">encoder_blk.<span class="built_in">eval</span>()</span><br><span class="line">encoder_blk(X, valid_lens).shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># torch.Size([2, 100, 24])</span></span><br></pre></td></tr></table></figure><p>下面实现的Transformer编码器的代码中，堆叠了<code>num_layers</code>个<code>EncoderBlock</code>类的实例。由于这里使用的是值范围在$-1$和$1$之间的固定位置编码，因此通过学习得到的输入的嵌入表示的值需要先乘以嵌入维度的平方根进行重新缩放，然后再与位置编码相加。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerEncoder</span>(<span class="params">d2l.Encoder</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;transformer编码器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, key_size, query_size, value_size,</span></span></span><br><span class="line"><span class="params"><span class="function">                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,</span></span></span><br><span class="line"><span class="params"><span class="function">                 num_heads, num_layers, dropout, use_bias=<span class="literal">False</span>, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(TransformerEncoder, self).__init__(**kwargs)</span><br><span class="line">        self.num_hiddens = num_hiddens</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, num_hiddens)</span><br><span class="line">        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)</span><br><span class="line">        self.blks = nn.Sequential()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            self.blks.add_module(<span class="string">&quot;block&quot;</span>+<span class="built_in">str</span>(i),</span><br><span class="line">                EncoderBlock(key_size, query_size, value_size, num_hiddens,</span><br><span class="line">                             norm_shape, ffn_num_input, ffn_num_hiddens,</span><br><span class="line">                             num_heads, dropout, use_bias))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, valid_lens, *args</span>):</span></span><br><span class="line">        <span class="comment"># 因为位置编码值在-1和1之间，</span></span><br><span class="line">        <span class="comment"># 因此嵌入值乘以嵌入维度的平方根进行缩放，</span></span><br><span class="line">        <span class="comment"># 然后再与位置编码相加。</span></span><br><span class="line">        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))</span><br><span class="line">        self.attention_weights = [<span class="literal">None</span>] * <span class="built_in">len</span>(self.blks)</span><br><span class="line">        <span class="keyword">for</span> i, blk <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.blks):</span><br><span class="line">            X = blk(X, valid_lens)</span><br><span class="line">            self.attention_weights[</span><br><span class="line">                i] = blk.attention.attention.attention_weights</span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><p>下面我们指定了超参数来创建一个两层的transformer编码器。 Transformer编码器输出的形状是（批量大小，时间步数目，num_hiddens）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">encoder = TransformerEncoder(</span><br><span class="line">    <span class="number">200</span>, <span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, [<span class="number">100</span>, <span class="number">24</span>], <span class="number">24</span>, <span class="number">48</span>, <span class="number">8</span>, <span class="number">2</span>, <span class="number">0.5</span>)</span><br><span class="line">encoder.<span class="built_in">eval</span>()</span><br><span class="line">encoder(torch.ones((<span class="number">2</span>, <span class="number">100</span>), dtype=torch.long), valid_lens).shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># torch.Size([2, 100, 24])</span></span><br></pre></td></tr></table></figure><h3 id="7-5-解码器"><a href="#7-5-解码器" class="headerlink" title="7.5. 解码器"></a>7.5. 解码器</h3><p>如模型图所示，Transformer解码器也是由多个相同的层组成。在<code>DecoderBlock</code>类中实现的每个层包含了三个子层：解码器自注意力、“编码器-解码器”注意力和基于位置的前馈网络。这些子层也都被残差连接和紧随的层规范化围绕。</p><p>正如在本节前面所述，在掩蔽多头解码器自注意力层（第一个子层）中，查询、键和值都来自上一个解码器层的输出。关于<em>序列到序列模型</em>（sequence-to-sequence model），在训练阶段，其输出序列的所有位置（时间步）的词元都是已知的；然而，在预测阶段，其输出序列的词元是逐个生成的。因此，在任何解码器时间步中，只有生成的词元才能用于解码器的自注意力计算中。为了在解码器中保留自回归的属性，其掩蔽自注意力设定了参数<code>dec_valid_lens</code>，以便任何查询都只会与解码器中所有已经生成词元的位置（即直到该查询位置为止）进行注意力计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;解码器中第i个块&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, key_size, query_size, value_size, num_hiddens,</span></span></span><br><span class="line"><span class="params"><span class="function">                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span></span></span><br><span class="line"><span class="params"><span class="function">                 dropout, i, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(DecoderBlock, self).__init__(**kwargs)</span><br><span class="line">        self.i = i</span><br><span class="line">        self.attention1 = d2l.MultiHeadAttention(</span><br><span class="line">            key_size, query_size, value_size, num_hiddens, num_heads, dropout)</span><br><span class="line">        self.addnorm1 = AddNorm(norm_shape, dropout)</span><br><span class="line">        self.attention2 = d2l.MultiHeadAttention(</span><br><span class="line">            key_size, query_size, value_size, num_hiddens, num_heads, dropout)</span><br><span class="line">        self.addnorm2 = AddNorm(norm_shape, dropout)</span><br><span class="line">        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens,</span><br><span class="line">                                   num_hiddens)</span><br><span class="line">        self.addnorm3 = AddNorm(norm_shape, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, state</span>):</span></span><br><span class="line">        enc_outputs, enc_valid_lens = state[<span class="number">0</span>], state[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 训练阶段，输出序列的所有词元都在同一时间处理，</span></span><br><span class="line">        <span class="comment"># 因此state[2][self.i]初始化为None。</span></span><br><span class="line">        <span class="comment"># 预测阶段，输出序列是通过词元一个接着一个解码的，</span></span><br><span class="line">        <span class="comment"># 因此state[2][self.i]包含着直到当前时间步第i个块解码的输出表示</span></span><br><span class="line">        <span class="keyword">if</span> state[<span class="number">2</span>][self.i] <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            key_values = X</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            key_values = torch.cat((state[<span class="number">2</span>][self.i], X), axis=<span class="number">1</span>)</span><br><span class="line">        state[<span class="number">2</span>][self.i] = key_values</span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            batch_size, num_steps, _ = X.shape</span><br><span class="line">            <span class="comment"># dec_valid_lens的开头:(batch_size,num_steps),</span></span><br><span class="line">            <span class="comment"># 其中每一行是[1,2,...,num_steps]</span></span><br><span class="line">            dec_valid_lens = torch.arange(</span><br><span class="line">                <span class="number">1</span>, num_steps + <span class="number">1</span>, device=X.device).repeat(batch_size, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            dec_valid_lens = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 自注意力</span></span><br><span class="line">        X2 = self.attention1(X, key_values, key_values, dec_valid_lens)</span><br><span class="line">        Y = self.addnorm1(X, X2)</span><br><span class="line">        <span class="comment"># 编码器－解码器注意力。</span></span><br><span class="line">        <span class="comment"># enc_outputs的开头:(batch_size,num_steps,num_hiddens)</span></span><br><span class="line">        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)</span><br><span class="line">        Z = self.addnorm2(Y, Y2)</span><br><span class="line">        <span class="keyword">return</span> self.addnorm3(Z, self.ffn(Z)), state</span><br></pre></td></tr></table></figure><p>为了便于在“编码器－解码器”注意力中进行缩放点积计算和残差连接中进行加法计算，编码器和解码器的特征维度都是num_hiddens。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">decoder_blk = DecoderBlock(<span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, [<span class="number">100</span>, <span class="number">24</span>], <span class="number">24</span>, <span class="number">48</span>, <span class="number">8</span>, <span class="number">0.5</span>, <span class="number">0</span>)</span><br><span class="line">decoder_blk.<span class="built_in">eval</span>()</span><br><span class="line">X = torch.ones((<span class="number">2</span>, <span class="number">100</span>, <span class="number">24</span>))</span><br><span class="line">state = [encoder_blk(X, valid_lens), valid_lens, [<span class="literal">None</span>]]</span><br><span class="line">decoder_blk(X, state)[<span class="number">0</span>].shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># torch.Size([2, 100, 24])</span></span><br></pre></td></tr></table></figure><p>现在我们构建了由num_layers个DecoderBlock实例组成的完整的transformer解码器。最后，通过一个全连接层计算所有vocab_size个可能的输出词元的预测值。解码器的自注意力权重和编码器解码器注意力权重都被存储下来，方便日后可视化的需要。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerDecoder</span>(<span class="params">d2l.AttentionDecoder</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, key_size, query_size, value_size,</span></span></span><br><span class="line"><span class="params"><span class="function">                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,</span></span></span><br><span class="line"><span class="params"><span class="function">                 num_heads, num_layers, dropout, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(TransformerDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.num_hiddens = num_hiddens</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, num_hiddens)</span><br><span class="line">        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)</span><br><span class="line">        self.blks = nn.Sequential()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            self.blks.add_module(<span class="string">&quot;block&quot;</span>+<span class="built_in">str</span>(i),</span><br><span class="line">                DecoderBlock(key_size, query_size, value_size, num_hiddens,</span><br><span class="line">                             norm_shape, ffn_num_input, ffn_num_hiddens,</span><br><span class="line">                             num_heads, dropout, i))</span><br><span class="line">        self.dense = nn.Linear(num_hiddens, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_state</span>(<span class="params">self, enc_outputs, enc_valid_lens, *args</span>):</span></span><br><span class="line">        <span class="keyword">return</span> [enc_outputs, enc_valid_lens, [<span class="literal">None</span>] * self.num_layers]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, state</span>):</span></span><br><span class="line">        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))</span><br><span class="line">        self._attention_weights = [[<span class="literal">None</span>] * <span class="built_in">len</span>(self.blks) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span> (<span class="number">2</span>)]</span><br><span class="line">        <span class="keyword">for</span> i, blk <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.blks):</span><br><span class="line">            X, state = blk(X, state)</span><br><span class="line">            <span class="comment"># 解码器自注意力权重</span></span><br><span class="line">            self._attention_weights[<span class="number">0</span>][</span><br><span class="line">                i] = blk.attention1.attention.attention_weights</span><br><span class="line">            <span class="comment"># “编码器－解码器”自注意力权重</span></span><br><span class="line">            self._attention_weights[<span class="number">1</span>][</span><br><span class="line">                i] = blk.attention2.attention.attention_weights</span><br><span class="line">        <span class="keyword">return</span> self.dense(X), state</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">attention_weights</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self._attention_weights</span><br></pre></td></tr></table></figure><h3 id="7-6-训练"><a href="#7-6-训练" class="headerlink" title="7.6. 训练"></a>7.6. 训练</h3><p>依照Transformer架构来实例化编码器－解码器模型。在这里，指定Transformer的编码器和解码器都是2层，都使用4头注意力。为了进行序列到序列的学习，下面在“英语－法语”机器翻译数据集上训练Transformer模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens, num_layers, dropout, batch_size, num_steps = <span class="number">32</span>, <span class="number">2</span>, <span class="number">0.1</span>, <span class="number">64</span>, <span class="number">10</span></span><br><span class="line">lr, num_epochs, device = <span class="number">0.005</span>, <span class="number">200</span>, d2l.try_gpu()</span><br><span class="line">ffn_num_input, ffn_num_hiddens, num_heads = <span class="number">32</span>, <span class="number">64</span>, <span class="number">4</span></span><br><span class="line">key_size, query_size, value_size = <span class="number">32</span>, <span class="number">32</span>, <span class="number">32</span></span><br><span class="line">norm_shape = [<span class="number">32</span>]</span><br><span class="line"></span><br><span class="line">train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)</span><br><span class="line"></span><br><span class="line">encoder = TransformerEncoder(</span><br><span class="line">    <span class="built_in">len</span>(src_vocab), key_size, query_size, value_size, num_hiddens,</span><br><span class="line">    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span><br><span class="line">    num_layers, dropout)</span><br><span class="line">decoder = TransformerDecoder(</span><br><span class="line">    <span class="built_in">len</span>(tgt_vocab), key_size, query_size, value_size, num_hiddens,</span><br><span class="line">    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span><br><span class="line">    num_layers, dropout)</span><br><span class="line">net = d2l.EncoderDecoder(encoder, decoder)</span><br><span class="line">d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)</span><br></pre></td></tr></table></figure><p>训练结束后，使用transformer模型将一些英语句子翻译成法语，并且计算它们的BLEU分数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">engs = [<span class="string">&#x27;go .&#x27;</span>, <span class="string">&quot;i lost .&quot;</span>, <span class="string">&#x27;he\&#x27;s calm .&#x27;</span>, <span class="string">&#x27;i\&#x27;m home .&#x27;</span>]</span><br><span class="line">fras = [<span class="string">&#x27;va !&#x27;</span>, <span class="string">&#x27;j\&#x27;ai perdu .&#x27;</span>, <span class="string">&#x27;il est calme .&#x27;</span>, <span class="string">&#x27;je suis chez moi .&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> eng, fra <span class="keyword">in</span> <span class="built_in">zip</span>(engs, fras):</span><br><span class="line">    translation, dec_attention_weight_seq = d2l.predict_seq2seq(</span><br><span class="line">        net, eng, src_vocab, tgt_vocab, num_steps, device, <span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;eng&#125;</span> =&gt; <span class="subst">&#123;translation&#125;</span>, &#x27;</span>,</span><br><span class="line">          <span class="string">f&#x27;bleu <span class="subst">&#123;d2l.bleu(translation, fra, k=<span class="number">2</span>):<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># go . =&gt; va !,  bleu 1.000</span></span><br><span class="line"><span class="comment"># i lost . =&gt; je suis avons été battues .,  bleu 0.000</span></span><br><span class="line"><span class="comment"># he&#x27;s calm . =&gt; il est malade .,  bleu 0.658</span></span><br><span class="line"><span class="comment"># i&#x27;m home . =&gt; je suis chez moi .,  bleu 1.000</span></span><br></pre></td></tr></table></figure><p>当进行最后一个英语到法语的句子翻译工作时，让我们可视化transformer的注意力权重。编码器自注意力权重的形状为（编码器层数，注意力头数，num_steps或查询的数目，num_steps或“键－值”对的数目）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">enc_attention_weights = torch.cat(net.encoder.attention_weights, <span class="number">0</span>).reshape((num_layers, num_heads,</span><br><span class="line">    -<span class="number">1</span>, num_steps))</span><br><span class="line">enc_attention_weights.shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># torch.Size([2, 4, 10, 10])</span></span><br></pre></td></tr></table></figure><p>在编码器的自注意力中，查询和键都来自相同的输入序列。因为填充词元是不携带信息的，因此通过指定输入序列的有效长度可以避免查询与使用填充词元的位置计算注意力。接下来，将逐行呈现两层多头注意力的权重。每个注意力头都根据查询、键和值的不同的表示子空间来表示不同的注意力。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(</span><br><span class="line">    enc_attention_weights.cpu(), xlabel=<span class="string">&#x27;Key positions&#x27;</span>,</span><br><span class="line">    ylabel=<span class="string">&#x27;Query positions&#x27;</span>, titles=[<span class="string">&#x27;Head %d&#x27;</span> % i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">5</span>)],</span><br><span class="line">    figsize=(<span class="number">7</span>, <span class="number">3.5</span>))</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_transformer_1.svg" alt="o1"></p><p>为了可视化解码器的自注意力权重和“编码器－解码器”的注意力权重，我们需要完成更多的数据操作工作。例如用零填充被掩蔽住的注意力权重。值得注意的是，解码器的自注意力权重和“编码器－解码器”的注意力权重都有相同的查询：即以<em>序列开始词元</em>（beginning-of-sequence,BOS）打头，再与后续输出的词元共同组成序列。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">dec_attention_weights_2d = [head[<span class="number">0</span>].tolist()</span><br><span class="line">                            <span class="keyword">for</span> step <span class="keyword">in</span> dec_attention_weight_seq</span><br><span class="line">                            <span class="keyword">for</span> attn <span class="keyword">in</span> step <span class="keyword">for</span> blk <span class="keyword">in</span> attn <span class="keyword">for</span> head <span class="keyword">in</span> blk]</span><br><span class="line">dec_attention_weights_filled = torch.tensor(</span><br><span class="line">    pd.DataFrame(dec_attention_weights_2d).fillna(<span class="number">0.0</span>).values)</span><br><span class="line">dec_attention_weights = dec_attention_weights_filled.reshape((-<span class="number">1</span>, <span class="number">2</span>, num_layers, num_heads, num_steps))</span><br><span class="line">dec_self_attention_weights, dec_inter_attention_weights = \</span><br><span class="line">    dec_attention_weights.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">4</span>)</span><br><span class="line">dec_self_attention_weights.shape, dec_inter_attention_weights.shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># (torch.Size([2, 4, 6, 10]), torch.Size([2, 4, 6, 10]))</span></span><br></pre></td></tr></table></figure><p>由于解码器自注意力的自回归属性，查询不会对当前位置之后的“键－值”对进行注意力计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plusonetoincludethebeginning-of-sequencetoken</span></span><br><span class="line">d2l.show_heatmaps(</span><br><span class="line">    dec_self_attention_weights[:, :, :, :<span class="built_in">len</span>(translation.split()) + <span class="number">1</span>],</span><br><span class="line">    xlabel=<span class="string">&#x27;Key positions&#x27;</span>, ylabel=<span class="string">&#x27;Query positions&#x27;</span>,</span><br><span class="line">    titles=[<span class="string">&#x27;Head %d&#x27;</span> % i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">5</span>)], figsize=(<span class="number">7</span>, <span class="number">3.5</span>))</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_transformer_2.svg" alt="o2"></p><p>与编码器的自注意力的情况类似，通过指定输入序列的有效长度，输出序列的查询不会与输入序列中填充位置的词元进行注意力计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(</span><br><span class="line">    dec_inter_attention_weights, xlabel=<span class="string">&#x27;Key positions&#x27;</span>,</span><br><span class="line">    ylabel=<span class="string">&#x27;Query positions&#x27;</span>, titles=[<span class="string">&#x27;Head %d&#x27;</span> % i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">5</span>)],</span><br><span class="line">    figsize=(<span class="number">7</span>, <span class="number">3.5</span>))</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_transformer_3.svg" alt="o3"></p><p>尽管transformer架构是为了“序列到序列”的学习而提出的，但正如我们将在本书后面提及的那样，transformer编码器或transformer解码器通常被单独用于不同的深度学习任务中。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;p&gt;灵长类动物的视觉系统接受了大量的感官输入，这些感官输入远远超过了大脑能够完全处理的程度。然而，并非所有刺激的影响都是相等的。意识的聚集和专注使灵长类动物能够在复杂的视觉环境中将注意力引向感兴趣的物体，例如猎物和天敌。只关注一小部分信息的能力对进化更加有意义，使人类得以生存和成功。&lt;/p&gt;
&lt;p&gt;自19世纪以来，科学家们一直致力于研究认知神经科学领域的注意力。本章的很多章节将涉及到一些研究。&lt;br&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://silencezheng.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>导向滤波</title>
    <link href="http://silencezheng.top/2022/11/04/article73/"/>
    <id>http://silencezheng.top/2022/11/04/article73/</id>
    <published>2022-11-04T13:27:06.000Z</published>
    <updated>2022-11-04T13:28:52.059Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>导向滤波学习，也可以说是论文笔记了。<br><span id="more"></span></p><h2 id="各向同性（isotropy）滤波与各向异性（anisotropy）滤波"><a href="#各向同性（isotropy）滤波与各向异性（anisotropy）滤波" class="headerlink" title="各向同性（isotropy）滤波与各向异性（anisotropy）滤波"></a>各向同性（isotropy）滤波与各向异性（anisotropy）滤波</h2><p>对图像来说，各向同性滤波就是说滤波器在各方向上的梯度变化是相同的，对各个方向一视同仁的进行滤波。各向异性滤波是指滤波器会对各方向进行区别对待，有选择的进行滤波。</p><p>高斯滤波属于<strong>各向同性滤波</strong>，根据二维高斯的图像就可以看出。</p><p>双边滤波属于<strong>各向异性滤波</strong>，因为它会根据图像梯度变化情况改变滤波器形状。</p><h2 id="导向滤波（Guided-Filter）"><a href="#导向滤波（Guided-Filter）" class="headerlink" title="导向滤波（Guided Filter）"></a>导向滤波（Guided Filter）</h2><p>导向滤波也属于各向异性滤波，也可以作为一种保边（Edge-preserving）滤波算法。</p><blockquote><p>Derived from a local linear model, the guided filter generates the filtering output by considering the content of a guidance image, which can be the input image itself or another different image.</p><p>We demonstrate that the guided filter is both effective and efficient in a great variety of computer vision and computer graphics applications including noise reduction, detail smoothing/enhancement, HDR compression, image matting/feathering, haze removal, and joint upsampling.</p></blockquote><p>导向滤波通过考虑<strong>引导图像</strong>的内容来生成滤波输出，引导图像可以是输入图像本身或另一个不同的图像。[1]中提到了双边滤波相比于导向滤波的两点缺陷，“have unwanted gradient reversal artifacts near edges”和难以进行维持精度的快速计算，这也是导向滤波的优势。</p><p>文中还提到了<strong>联合双边滤波器</strong>（joint bilateral filter），它也是利用了引导图来改善双边滤波<strong>权值不稳定</strong>的问题（双边滤波边缘出现<strong>梯度翻转</strong>现象的原因）。</p><p>文中先定义了一个<strong>通用的平移不变的线性滤波过程</strong>，包括一个引导图像 $\textit{I}$、一个输入图像 $\textit{p}$ 和一个输出图像 $\textit{q}$。</p><script type="math/tex; mode=display">\begin{equation}q_i=\sum_j W_{i j}(I) p_j \tag{1}\end{equation}</script><p>这里 $i$ 和 $j$ 表示的是两个像素，而不是像素的横纵坐标！像素 $i$ 的位置坐标表示为 $\mathbb{x}_i$，$q_i$ 表示输出图的像素 $i$ 处的值，滤波核 $W_{i j}(I)$ 是关于导向图和输入图的函数，当然也与 $i$ 有关。</p><p>联合双边滤波器符合上述的线性滤波过程，其滤波核 $W^{\mathrm{bf}}$ 如下：</p><script type="math/tex; mode=display">\begin{equation}W_{i j}^{\mathrm{bf}}(I)=\frac{1}{K_i} \exp \left(-\frac{\left|\mathbf{x}_i-\mathbf{x}_j\right|^2}{\sigma_{\mathrm{s}}^2}\right) \exp \left(-\frac{\left|I_i-I_j\right|^2}{\sigma_{\mathrm{r}}^2}\right) \tag{2}\end{equation}</script><p>其中，$K_i$是归一化参数，用于保证$\sum_j W_{i j}^{\mathrm{bf}} = 1$，$\sigma_{\mathrm{s}}$ 和 $\sigma_{\mathrm{r}}$ 分别代表空域和值域（原文说值可以是 intensity 或 color）的对应参数，分别用于调整空域和值域的滤波程度（原文为similarity，我理解就和高斯中的标准差类似）。原文说当导向图和输入图相同时，联合双边滤波就退化为双边滤波，我理解里的双边滤波应该是要加两个2倍在分母的，但是效果应该一样。</p><p>下面简单讨论下导向滤波的推导，主要还是学习如何计算。 文中首先假设导向滤波是一个导向图 $I$ 和 输出图 $q$ 间的<strong>局部线性模型</strong>。令 $w_k$ 是 $I$ 中的一个正方形窗口，其中心为像素 $k$，输出 $q$ 为 $I$ 在 $w_k$ 上的一个线性变换：</p><script type="math/tex; mode=display">\begin{equation}q_i=a_k I_i+b_k, \forall i \in \omega_k \tag{3}\end{equation}</script><p>这个稍微想一下就可以理解，假设 $w_k$ 是一个在 $I$ 上移动的九宫格，由于 $q$、$I$ 和 $p$都是尺寸一致的，那么每次移动产生的九个线性变化值就是 $q$ 对应 $I$ 位置上的值。当然这会引出一个问题，即同一个像素可能会被不同的窗口计算出多个值，如何确定最终输出的 $q_i$ 呢？文中提出了一种简单的处理办法，即取所有这些输出的平均。</p><p>$a_k$ 和 $b_k$ 是窗口 $k$ 中的常量线性系数，顺便一提 $w_k$ 的半径定义为 $r$。 关于为何使用局部线性模型，是因为它确保了输出和导向图的边缘一致，具体可以看原文。</p><p>为确定上述的两个线性系数，原文将输出建模为输入减去不想要的内容（噪声、纹理等）:</p><script type="math/tex; mode=display">q_i = p_i - n_i</script><p>然后通过最小化输入和输出的差异来实现，具体来说，最小化如下函数(cost function in $w_k$)：</p><script type="math/tex; mode=display">\begin{equation}E\left(a_k, b_k\right)=\sum_{i \in \omega_k}\left(\left(a_k I_i+b_k-p_i\right)^2+\epsilon a_k^2\right) \tag{4}\end{equation}</script><p>其中 $\epsilon$ 是正则化参数，用于防止 $a_k$ 变得太大。通过线性回归求解$(4)$可得如下：</p><script type="math/tex; mode=display">\begin{equation}a_k=\frac{\frac{1}{|\omega|} \sum_{i \in \omega_k} I_i p_i-\mu_k \bar{p}_k}{\sigma_k^2+\epsilon} \tag{5}\end{equation}</script><script type="math/tex; mode=display">\begin{equation}b_k=\bar{p}_k-a_k \mu_k \tag{6}\end{equation}</script><p>其中 $\mu_k$ 和 $\sigma_k^2$ 分别是导向图在窗口 $k$ 中部分的均值和方差（像素值），$|\omega|$ 是窗口 $k$ 中的像素数，$\bar{p}_k$ 是 输入图 $p$ 在窗口 $k$ 中部分的均值，表示为 $\bar{p}_k=\frac{1}{|\omega|} \sum_{i \in \omega_k} p_i$。</p><p>解释完各个符号的含义，让我们思考将上述的局部线性模型<strong>应用在整幅图像上</strong>，将图像中每一个能放置 $w_k$ 的区域进行运算（考虑将 $I$ 和 $p$ 叠放），并采用之前提到的取均值的方式，可得到滤波器输出如下：</p><script type="math/tex; mode=display">\begin{equation}q_i =\frac{1}{|\omega|} \sum_{k: i \in \omega_k}\left(a_k I_i+b_k\right) \tag{7}\end{equation}</script><script type="math/tex; mode=display">\begin{equation}=\bar{a}_i I_i+\bar{b}_i \tag{8}\end{equation}</script><p>其中$\bar{a}_i=\frac{1}{|\omega|} \sum_{k \in \omega_i} a_k$，$\bar{b}_i=\frac{1}{|\omega|} \sum_{k \in \omega_i} b_k$。</p><p>下面原文对上述输出的保边性进行了一些论证，并指出$(5), (6), (8)$中的关系是在图像滤波中真实存在的，并且这三个关系（公式）可以分别重写为输入的加权和，如下：</p><script type="math/tex; mode=display">a_k=\sum_j A_{k j}(I) p_j \\b_k=\sum_j B_{k j}(I) p_j \\q_i=\sum_j W_{i j}(I) p_j</script><p>其中 $A_{i j}, B_{i j}, W_{i j}$ 为三个仅依赖于导向图 $I$ 的权重，可以看到最终推出了一个输出和导向图与输入的<strong>平移不变线性滤波</strong>：$q_i=\sum_j W_{i j}(I) p_j$。那么我们只需要关心权重如何计算即可：</p><script type="math/tex; mode=display">\begin{equation}W_{i j}(I)=\frac{1}{|\omega|^2} \sum_{k:(i, j) \in \omega_k}\left(1+\frac{\left(I_i-\mu_k\right)\left(I_j-\mu_k\right)}{\sigma_k^2+\epsilon}\right) \tag{9}\end{equation}</script><p>解读一下这个权重，对于给定的输出像素 $i$，我们先寻找重叠图像（$I$ 和 $p$）中包含该像素的所有窗口 $k$，对于每个窗口 $k$，在 $I$ 上计算 $\mu_k$ 和 $\sigma_k^2$ 后代入计算权重，然后求得加权和。由于可以证明 $\sum_j W_{i j}(I)=1$，所以不需要对权重再进行归一化处理。</p><p>总之，导向滤波符合一个平移不变的线性滤波模型，其权重通过导向图计算，最终结合输入图产生输出。</p><p><img src="/assets/post_img/article73/idea.jpeg" alt="idea"></p><h2 id="计算方式"><a href="#计算方式" class="headerlink" title="计算方式"></a>计算方式</h2><p>$(5), (6), (8)$ 的三个方程是导向滤波器的一种定义，令 $f_{mean}$ 表示半径为 $r$ 的均值滤波器，correlation（corr，<strong>相关系数</strong>），variance（var，<strong>方差</strong>）和covariance（cov，<strong>协方差</strong>）分别表示其本身含义。则可给出算法的伪代码如下：</p><p><img src="/assets/post_img/article73/pseudocode-normal.png" alt="normal"></p><p>算法中出现的<code>./ .*</code>等分别代表逐像素除、逐像素乘等操作。</p><h2 id="个人的一点理解"><a href="#个人的一点理解" class="headerlink" title="个人的一点理解"></a>个人的一点理解</h2><p>文中总共提出了两种对导向滤波的定义方式，先是得到全图范围上的线性模型 $q_i=\bar{a}_i I_i+\bar{b}_i$，后又重新整理成线性滤波的形式 $q_i=\sum_j W_{i j}(I) p_j$，并求出了仅依赖导向图的滤波核权重。然后从线性模型和滤波核的角度分别分析了导向滤波Edge-Preserving的性质，考虑 $I \equiv p$ 的情况。</p><h2 id="快速导向滤波（Fast-Guided-Filter）"><a href="#快速导向滤波（Fast-Guided-Filter）" class="headerlink" title="快速导向滤波（Fast Guided Filter）"></a>快速导向滤波（Fast Guided Filter）</h2><p>还没看[3]，先贴一个伪代码，日后需要再回来整理。</p><p><img src="/assets/post_img/article73/pseudocode-fast.png" alt="fast"></p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]Kaiming He, Jian Sun, Xiaoou Tang, Guided Image Filtering. IEEE Transactions on Pattern Analysis and Machine Intelligence, Volume 35, Issue 6, pp. 1397-1409, June 2013<br>[2]<a href="https://zhuanlan.zhihu.com/p/161666126">https://zhuanlan.zhihu.com/p/161666126</a><br>[3]<a href="https://arxiv.org/abs/1505.00996v1">https://arxiv.org/abs/1505.00996v1</a><br>[4]<a href="http://kaiminghe.com/eccv10/">http://kaiminghe.com/eccv10/</a><br>[5]Guided Image Filtering, by Kaiming He, Jian Sun, and Xiaoou Tang, in ECCV 2010 (Oral).</p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于 <a href="https://silencezheng.top">silencezheng.top</a>。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;导向滤波学习，也可以说是论文笔记了。&lt;br&gt;</summary>
    
    
    
    
    <category term="计算机视觉" scheme="http://silencezheng.top/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>目标检测常用评价指标</title>
    <link href="http://silencezheng.top/2022/10/24/article72/"/>
    <id>http://silencezheng.top/2022/10/24/article72/</id>
    <published>2022-10-23T16:50:28.000Z</published>
    <updated>2022-10-23T16:52:21.268Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>关于目标检测领域的常用评价指标进行一些个人解读，欢迎批评指正。</p><p>评价指标系列第二篇～上一次说了语义分割的内容，这次来搞目标检测，看了十余篇内容后终于搞明白些了，记录一下。本文说的内容比较浅，一方面是笔者水平实在菜鸡，另一方面写文的主要目的是让不了解目标检测的人读完能上手去做，而不用查三十个资料后再开始，就像博客的Slogan一样，“帮你在学习技术的路上节约半小时”。<br><span id="more"></span></p><h2 id="前置理解"><a href="#前置理解" class="headerlink" title="前置理解"></a>前置理解</h2><p>上一篇中，介绍了混淆矩阵和一些语义分割常用评价指标，例如P、R、IoU，这些在目标检测中都会用到，但是相对于语义分割来说，更加抽象一些。</p><p>目标检测对我个人来说是一个相对陌生的领域，所以本文准备先从目标检测的一般过程说起。首先，目标检测中的基本单元是框，我把Ground Truth称为<strong>真实框</strong>（标签），把Prediction称为<strong>预测框</strong>。框中的内容可能是<strong>目标</strong>（需要），也可能是<strong>背景</strong>（不需要）。</p><p>对于目标检测来说，模型需要完成的任务有两个，一是产生目标的预测框，二是对框内目标的类别进行预测，这又称为<strong>回归分支</strong>（连续）和<strong>分类分支</strong>（离散）。模型的预测输出通常如下所示（假设三分类，<strong>实际上不同模型的输出格式也不尽相同的</strong>。）：</p><script type="math/tex; mode=display">\mathrm{y}=\left[\begin{array}{l}\mathrm{p}_{\mathrm{c}} \\\mathrm{b}_{\mathrm{x}} \\\mathrm{b}_{\mathrm{y}} \\\mathrm{b}_{\mathrm{w}} \\\mathrm{b}_{\mathrm{h}} \\\mathrm{C}_1 \\\mathrm{C}_2 \\\mathrm{C}_3\end{array}\right], \mathrm{y}_{\text {true }}=\left[\begin{array}{c}1 \\40 \\45 \\80 \\60 \\0 \\1 \\0\end{array}\right], \mathrm{y}_{\text {pred }}=\left[\begin{array}{c}0.88 \\41 \\46 \\82 \\59 \\0.01 \\0.95 \\0.04\end{array}\right]</script><p>其中, $\mathrm{p}_{\mathrm{c}}$ 为预测结果的置信度（Confidence），表达预测框内包含目标的概率。$\mathrm{b}_{\mathrm{x}}, \mathrm{b}_{\mathrm{y}}, \mathrm{b}_{\mathrm{w}}, \mathrm{b}_{\mathrm{h}}$ 分别为预测框左上点$x$坐标和$y$坐标以及预测框的宽度、长度，也可以是预测框的左上、右下两点的$x$坐标和$y$坐标，表达的意思是相同的。 $\mathrm{C}_1, \mathrm{C}_2, \mathrm{C}_3$ 为目标属于某个类别的概率。</p><p>真实框和预测框的数量可能是不对等的，这是目标检测与语义分割的一大区别。现在我准备把这些内容再简化，归类为三个信息，<strong>目标置信度</strong>、<strong>定位信息</strong>和<strong>分类置信度</strong>，这里面有些事情需要讲清楚。</p><p>第一，<strong>分类置信度</strong>可能有互斥和不互斥两种，取决于是否使用了softmax计算，本文中，默认分类置信度是互斥的，也就是说当模型输出了一个预测框，框内“目标”的类别就固定了（不论是否真的存在目标）。</p><p>第二，预测框与真实框的贴合程度可以由<strong>定位信息</strong>计算IoU表示，上次提到了。</p><p>以上，前置理解完毕，可以开始进行模型评估了。</p><h2 id="重返混淆矩阵"><a href="#重返混淆矩阵" class="headerlink" title="重返混淆矩阵"></a>重返混淆矩阵</h2><p>从我们现有的信息来看，我们只有两个数值：<strong>目标置信度</strong>和<strong>定位信息</strong>。定位信息可以求IoU，但它依然是数值。基于mAP是目标检测中热门的评估指标的现状，我们希望能够继续用混淆矩阵来对预测框进行分类。众所周知，对数值分类的最简单方式就是在集合中画一条界限，将数值集分为两块，这条界限我们称为<strong>阈值</strong>。</p><p>我们有两个数值，自然产生了两个阈值，即<strong>置信度阈值</strong>和<strong>IoU阈值</strong>。显然，前者能把预测分成里面大概率是目标的框和里面大概率是背景的框，后者则把预测分成了很像某个目标的框和与某个目标不沾边的框。好了，现在我们来说混淆矩阵的事。</p><p>由于预测框类别已经固定，目标置信度和定位信息就是用来确定这框里面到底有没有目标，两个阈值用来判断一件事，这不就是二分类吗？没错，目标检测中我们还是用二分类，和之前多分类按照每个类别来看依然是二分类问题一样。</p><p>那现在事情就变得简单了，只需要搞懂如何区别正负就可以了，这是由目标置信度和定位信息共同决定的。下面给出方法，首先我们假设测试集有$N$张图片，我们从中取出一张图片$Img_n$，取该图片中属于类别$C_1$的目标的真实框和预测为类别$C_1$的预测框作为待分类集合。</p><ul><li>TP：与某一真实框的IoU值大于IoU阈值，且目标置信度大于置信度阈值的预测框数量。每个真实框仅能匹配一个预测框，这意味这TP的最大值为真实框的数量。</li><li>FN：漏检测的真实框数量。</li><li>FP：目标置信度大于置信度阈值但不满足TP条件的预测框数量。</li><li>TN：不考虑，因为Negative的样本想画可以画无数个，没有价值。</li></ul><p>初次看到这个判断方法多少还是有点懵，笔者解释一下，首先我们希望预测结果对每个目标至多只有一个预测框，这导致了即便有多个符合双阈值要求的预测框，对同一目标也只能<strong>选择一个置信度最高的预测框作为TP</strong>，其余重复预测框（虽然满足置信度要求）都进入FP。这样以后，能进入TP的目标也被确定了，剩下的真实框自然就成为了FN。</p><h2 id="准确率-召回率曲线（Precision-Recall-Curve）"><a href="#准确率-召回率曲线（Precision-Recall-Curve）" class="headerlink" title="准确率-召回率曲线（Precision-Recall Curve）"></a>准确率-召回率曲线（Precision-Recall Curve）</h2><p>先回顾一下Precision和Recall的计算方式，这里就只考虑Positive了：</p><script type="math/tex; mode=display">Precision = \frac{TP}{TP+FP} \\Recall = \frac{TP}{TP+FN}</script><p>由上面的分析可以看出，双阈值会影响P和R，通过调整阈值，就可以获得多个$(R,P)$对，通常我们通过指定IoU阈值（不小于0.5），调整置信度阈值的方式获得P-R曲线。事实上，精确率和召回率是一对由置信度阈值控制的冲突的变量，如果想要精准率提高，召回率则会下降，如果要召回率提高，精准率则会下降。当置信度阈值下降时，Recall单调上升，Precision总体呈下降趋势，这也很好理解，查的更全但是查的不准。</p><p><img src="/assets/post_img/article72/p-r.webp" alt="pr"></p><p>需要注意的是，在计算P-R曲线的过程中，置信度阈值的调整方式通常是对所有预测框按置信度降序排序，依次将置信度阈值设为某一预测框的置信度。并且<strong>P-R曲线的计算是以全测试集为域，按类别划分的</strong>，也就是说，计算类别$C_1$的P-R曲线时，预测框的基数为$C_1$在所有图片中的预测框数量之和，$100$个预测框可以计算$100$组$(R,P)$。另外，由于FP也需要满足置信度大于阈值的条件，<strong>参与整个P-R计算的预测框数量应该是持续增加的，最终达到$C_1$在所有图片中的预测框数量之和</strong>。（这里主要的意思就是粗体的地方，前面的原因可以有很多，主要就是需要按照前面定义的规则计算。）</p><p>更重要的是，<strong>TP、FP和FN却需要在单张图片的范围内按类别计算，再以图片为单位求和构成总TP、FP和FN值，进而求得类别在数据集上的一个$(R,P)$</strong>。</p><h2 id="AP（Average-Precision）"><a href="#AP（Average-Precision）" class="headerlink" title="AP（Average Precision）"></a>AP（Average Precision）</h2><p>经历了前面曲折艰难的分析，终于能够接近AP了。首先从字面上看，平均精确度，很容易联想到一种计算方式（笔者就这么干过）：对数据集中的每张图片单独计算Precision，然后求平均值，这不就是AP吗？实际上这是不对的，错误出在了对于Precision的理解上，Precision是一个<strong>Rate</strong>，我们现在的目的是评估模型<strong>对类别</strong>的检测效果，一个类别的查准率是建立在全数据集的样本之上的，显然，上述计算无法准确的表达这一内涵。</p><p>如果有读者想不明白这一点，我可以再举一个例子来验证，例如有三堆球摆在我们面前，其中两堆相同，都由$3$个白球和$1$个红球构成，另外一堆由$1$个红球和$1$个白球构成，我希望得到这些球中的红球率。如果按照上述理解，我们可以分别计算三堆中的红球率，得到$\frac{1}{4},\frac{1}{4},\frac{1}{2}$，再求它们的算数平均值得到$\frac{1}{3}$。而实际上呢，红球率是$\frac{3}{10}$，因为红球率是指以球为基本单位的红球比率（也不知道我解释清楚没）。</p><p>总之，Precision就是这样一个指标，要计算类别的AP，就要先获得P-R曲线。得到P-R曲线后，AP的计算方式就有很多了，根据不同的流行数据集，有几种常见的方式，罗列如下：</p><ul><li><p>按照VOC2007的方法，是先平滑曲线，对于每个点取其右边最大的P值，连成直线，然后等间距取11个点的最大P值，AP就是这11个Precision的平均值。</p></li><li><p>VOC2012，还是先按07的方法平滑曲线，然后计算PR曲线下面积作为AP值，因为本身P和R就是Rate，构成的正方形区域面积就是1，求曲线积分就完事了。</p></li><li><p>COCO数据集，设定多个IoU阈值（0.5至0.95，0.05为步长），在每一个IoU阈值下都有某一类别的AP值，然后求所有IOU阈值下的平均AP，以该平均AP值作为最终的某类别的AP值。</p></li></ul><p>总的来说，就是一个精度越来越高的过程。</p><h2 id="mAP（mean-Average-Precision）"><a href="#mAP（mean-Average-Precision）" class="headerlink" title="mAP（mean Average Precision）"></a>mAP（mean Average Precision）</h2><p>mAP和速度是最常用的目标检测模型评价指标，mAP顾名思义就是对所有分类的AP再求平均值。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]<a href="https://blog.csdn.net/yegeli/article/details/109861867">https://blog.csdn.net/yegeli/article/details/109861867</a><br>[2]<a href="https://zhuanlan.zhihu.com/p/70306015">https://zhuanlan.zhihu.com/p/70306015</a><br>[3]<a href="https://www.jianshu.com/p/86b8208f634f">https://www.jianshu.com/p/86b8208f634f</a><br>[4]<a href="https://zhuanlan.zhihu.com/p/94597205">https://zhuanlan.zhihu.com/p/94597205</a><br>[5]<a href="https://zhuanlan.zhihu.com/p/101566089">https://zhuanlan.zhihu.com/p/101566089</a><br>[6]<a href="https://www.jianshu.com/p/fd9b1e89f983">https://www.jianshu.com/p/fd9b1e89f983</a></p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于<a href="https://silencezheng.top/">silencezheng.top</a>。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;关于目标检测领域的常用评价指标进行一些个人解读，欢迎批评指正。&lt;/p&gt;
&lt;p&gt;评价指标系列第二篇～上一次说了语义分割的内容，这次来搞目标检测，看了十余篇内容后终于搞明白些了，记录一下。本文说的内容比较浅，一方面是笔者水平实在菜鸡，另一方面写文的主要目的是让不了解目标检测的人读完能上手去做，而不用查三十个资料后再开始，就像博客的Slogan一样，“帮你在学习技术的路上节约半小时”。&lt;br&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://silencezheng.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="计算机视觉" scheme="http://silencezheng.top/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>语义分割常用评价指标</title>
    <link href="http://silencezheng.top/2022/10/23/article71/"/>
    <id>http://silencezheng.top/2022/10/23/article71/</id>
    <published>2022-10-22T16:26:05.000Z</published>
    <updated>2022-10-22T16:33:20.039Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>关于语义分割领域的常用评价指标进行一些个人解读，欢迎批评指正。</p><p>本来是想连目标检测的一起说了的，但是关于AP的一些东西始终有疑问，留着后面搞懂了再写一篇吧。<br><span id="more"></span></p><h2 id="前置理解"><a href="#前置理解" class="headerlink" title="前置理解"></a>前置理解</h2><p>在解读这些评价指标之前，需要对深度学习方法有一个基础认识。</p><p>首先，评价指标，是指测试阶段将预测结果与真实值进行比对得到的量化结论。<strong>评价指标（Metric）和损失函数（Loss）有联系也有区别</strong>，目前在我个人理解来说，评价指标是以实用的角度评价模型，只关心模型的结果，而损失函数是从数学的角度收敛模型，但损失往往应该与一个你最关心的评价指标相对应，通过收敛损失能够达到向指标增大的方向靠拢。并且，损失应该是容易优化的，很多时候它们对模型参数可微，甚至是凸的。下面引[10]中的例子做说明。</p><blockquote><p>假设某同学备战高考，他给自己定下了一个奋斗的方向，即每周要把自己的各科总成绩提高5分；经过多年的准备，终于在高考中取得了好成绩（710分，总分750），被北大录取。<br>分析该例子，该同学“每周要把自己的各科总成绩提高5分”这个指导原则相当于目标函数，在这个指导原则的指引下，想必该同学的总分会越来越高，即模型被训练的越来越好。<br>最终，该同学高考成绩优异，相当于模型的测试效果良好，至于用从哪个角度评价这名同学，可以用其高考总分与750分的差距来衡量，也可以用其被录取的大学的水平来衡量，这就如同模型的评估指标是多种多样的，比如分类问题中的准确率、召回率等。<br>当然，模型的评估指标多样，模型的损失函数也是多样的；该例中，该同学可以将“每周要把自己的各科总成绩提高5分”作为指导原则，也可将“每周比之前多学2个知识点”作为指导原则。<br>另外，如果该同学将“每周模拟高考总分与750分的差距”同时作为指导原则与评价角度，则类似于线性回归模型将“MSE均方误差”同时作为损失函数与评估指标。<br>该例中，备考的“指导原则”相当于“损失函数”，“评价角度”相当于“评估指标”，该同学相当于一个机器学习模型。</p></blockquote><p>其次，在多分类任务中，通常包含$n$个类别，而对于某一样本的最终预测只能是$n$个类别中的一个。但是，算法对一个样本的类别预测通常以置信度的形式表示，最终选择置信度最高的类别作为预测输出。</p><p>最后，多分类任务对于每个类来看，可以看作是一个二分类问题，以样本对于该类别预测是否正确作为区分。</p><h2 id="混淆矩阵（Confusion-Matrix）"><a href="#混淆矩阵（Confusion-Matrix）" class="headerlink" title="混淆矩阵（Confusion Matrix）"></a>混淆矩阵（Confusion Matrix）</h2><p>混淆矩阵用于直观的显示模型预测结果的情形。 混淆矩阵中的横纵轴都是类别，对于$p_{ij}$（横坐标为$i$,纵坐标为$j$处的值），其含义为属于类别$i$并被预测为类别$j$的<strong>样本数量</strong>（在语义分割中通常样本等同于像素）。也就是说，每个位置的<strong>横坐标表示模型的预测，纵坐标表示真实标签</strong>。</p><p>对于二分类问题，混淆矩阵可以表示如下：</p><p><img src="/assets/post_img/article71/confusion-matrix-2classes.jpeg" alt="cm2"></p><p>若令其中$1$表示正类，$0$表示负类，则可以定义如下四个量：</p><ul><li>TP(True Positive)：将正样本预测为正类的数量，即图中的$a$。</li><li>FN(False Negative)：将正样本预测为负类的数量，即图中的$b$。</li><li>FP(False Positive)：将负样本预测为正类的数量，即图中的$c$。</li><li>TN(True Negative)：将负样本预测为负类的数量，即图中的$d$。</li></ul><p>对于多分类问题，只是把该矩阵由$2 \times 2$变化为$n \times n$，其中$n$表示类别数量。</p><p>从混淆矩阵中我们可以获得一些基础信息，如：</p><ul><li>$i$行的和$\sum^n_{j=1}p_{ij}$表示数据集中属于类别$i$的样本个数</li><li>$j$列的和$\sum^n_{i=1}p_{ij}$表示模型预测中属于类别$j$的样本个数</li><li>矩阵中所有元素的和$\sum^n_{i=1}\sum^n_{j=1}p_{ij}$表示图像中的总样本个数</li><li>…</li></ul><h2 id="精确率（Precision）和召回率（Recall）"><a href="#精确率（Precision）和召回率（Recall）" class="headerlink" title="精确率（Precision）和召回率（Recall）"></a>精确率（Precision）和召回率（Recall）</h2><p>这两个指标都是<strong>针对某一类别</strong>而言的，是分类任务的常用评价指标。</p><p>精确率又称查准率，含义是对于模型预测中属于类别$j$的样本，预测结果正确的比例。例如对于二分类问题，正类的精确率$Precision_{positive} = \frac{TP}{TP+FP} = \frac{a}{a+c}$。</p><p>召回率又称查全率，如果说精准率是站在预测的角度看问题，那么召回率就是站在现实的角度看问题，其含义是对于数据集中属于类别$i$的样本，被正确预测的比例。例如对于二分类问题，负类的召回率$Recall_{negative} = \frac{TN}{FP+TN} = \frac{d}{c+d}$。</p><h2 id="准确率（Accuracy）"><a href="#准确率（Accuracy）" class="headerlink" title="准确率（Accuracy）"></a>准确率（Accuracy）</h2><p>准确率需要和精确率区别开，准确率是站在预测的整体角度看问题，其含义是预测正确的样本占所有样本的比例。例如对于二分类问题，预测的准确率$Accuracy_{predict} = \frac{TP+TN}{TP+FN+FP+TN} = \frac{a+d}{a+b+c+d}$。可以看出，准确率其实就是混淆矩阵对角线元素和与所有元素和的比值。</p><h2 id="F1指标（F1-Score）和F-Beta指标（F-Beta-Score）"><a href="#F1指标（F1-Score）和F-Beta指标（F-Beta-Score）" class="headerlink" title="F1指标（F1 Score）和F-Beta指标（F-Beta Score）"></a>F1指标（F1 Score）和F-Beta指标（F-Beta Score）</h2><p>单独用精确率或召回率有时不能很好的评估模型，例如在二分类问题中，模型选择对所有样本预测为正类，此时所有正类样本都被“准确”的预测了，正类召回率为$1$，但模型实际上很差。</p><p>F1指标就是用来平衡精确率和召回率的重要程度的度量指标，它被定义为两者的<strong>调和平均值</strong>，表示二者重要程度一致。F1指标的计算公式如下：</p><script type="math/tex; mode=display">F_1 = 2 \times \frac{precision \cdot recall}{precision + recall}</script><p>调和平均值的一个重要特性就是如果两者极度不平衡，调和平均值会很小，只有当两者都较高时，调和平均才会比较高。</p><p>而F-Beta指标则是更一般的形式，他的计算方式如下：</p><script type="math/tex; mode=display">F_{\beta} = (1+\beta^2) \cdot \frac{precision \cdot recall}{(\beta^2 \cdot precision) + recall}</script><p>其中参数$\beta$决定了精确率和召回率的重要程度比值，当$\beta&gt;1$时召回率比重更大，当$\beta&lt;1$时精确率比重更大。</p><h2 id="特异性（Specificity）和敏感性（Sensitivity）"><a href="#特异性（Specificity）和敏感性（Sensitivity）" class="headerlink" title="特异性（Specificity）和敏感性（Sensitivity）"></a>特异性（Specificity）和敏感性（Sensitivity）</h2><p>关于这两个指标，似乎是仅针对于二分类问题而言的，这里只谈一些个人理解。</p><p>还是参照二分类的混淆矩阵，特异性实际上指的就是负类的召回率$Recall_{negative}$，而敏感性则指的是正类的召回率$Recall_{positive}$，看了网上许多解释，都是聚焦在医疗领域，把患病作为正类，健康作为负类，说什么敏感性越高，漏诊概率越低；特异性越高，确诊概率越高。</p><p>个人理解，实际上就是召回率在特定情况下的应用吧。</p><h2 id="交并比（Intersection-over-Union，IoU）和平均交并比（mIoU）"><a href="#交并比（Intersection-over-Union，IoU）和平均交并比（mIoU）" class="headerlink" title="交并比（Intersection over Union，IoU）和平均交并比（mIoU）"></a>交并比（Intersection over Union，IoU）和平均交并比（mIoU）</h2><p>给定两个区域$A$和$B$，IoU就是两区域的交集与两区域并集的比值：</p><script type="math/tex; mode=display">IoU = \frac{A \cap B}{A \cup B}</script><p><img src="/assets/post_img/article71/IoU.png" alt="iou"></p><p>在分类任务中，可以对某一类别的预测结果和真实标签求IoU，例如对于二分类求正类的IoU如下：</p><script type="math/tex; mode=display">IoU_{positive} = \frac{TP}{TP+FP+FN} = \frac{a}{a+b+c}</script><p>也就是说，混淆矩阵中$i$行和$i$列的交集比上它们的并集。</p><p><strong>平均交并比</strong>（mean IoU）就是对每一个类别求IoU，再求和求平均得到的值。</p><p>对于目标检测，IoU还有一个重要的应用，就是判断预测框与真实框的贴合程度，两部分重合面积越大，则IoU值越大。IoU是一个比较严格的评价指标，当两区域稍微有偏差时，IoU值也可能变得相当小，于是通常认为IoU大于$0.5$时就获得了一个比较不错的预测框。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]<a href="https://zhuanlan.zhihu.com/p/111234566">https://zhuanlan.zhihu.com/p/111234566</a><br>[2]<a href="https://zhuanlan.zhihu.com/p/101566089">https://zhuanlan.zhihu.com/p/101566089</a><br>[3]<a href="https://blog.csdn.net/h1yupyp/article/details/80842172">https://blog.csdn.net/h1yupyp/article/details/80842172</a><br>[4]<a href="https://blog.csdn.net/lhxez6868/article/details/108150777">https://blog.csdn.net/lhxez6868/article/details/108150777</a><br>[5]<a href="https://zhuanlan.zhihu.com/p/371819054">https://zhuanlan.zhihu.com/p/371819054</a><br>[6]<a href="https://www.jianshu.com/p/22d947ffb71e">https://www.jianshu.com/p/22d947ffb71e</a><br>[7]<a href="https://zhuanlan.zhihu.com/p/372402161">https://zhuanlan.zhihu.com/p/372402161</a><br>[8]<a href="https://zhuanlan.zhihu.com/p/373658488">https://zhuanlan.zhihu.com/p/373658488</a><br>[9]<a href="https://zhuanlan.zhihu.com/p/373032887">https://zhuanlan.zhihu.com/p/373032887</a><br>[10]<a href="https://www.cnblogs.com/pythonfl/p/13705143.html">https://www.cnblogs.com/pythonfl/p/13705143.html</a></p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于<a href="https://silencezheng.top/">silencezheng.top</a>。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;关于语义分割领域的常用评价指标进行一些个人解读，欢迎批评指正。&lt;/p&gt;
&lt;p&gt;本来是想连目标检测的一起说了的，但是关于AP的一些东西始终有疑问，留着后面搞懂了再写一篇吧。&lt;br&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://silencezheng.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="计算机视觉" scheme="http://silencezheng.top/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>M1 Mac weasyprint安装使用</title>
    <link href="http://silencezheng.top/2022/10/19/article70/"/>
    <id>http://silencezheng.top/2022/10/19/article70/</id>
    <published>2022-10-19T15:54:48.000Z</published>
    <updated>2022-10-19T15:57:18.582Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>为了把HTML转成PDF（带很多css的），折腾了一晚上。试了一下pdfkit是基本没戏（<a href="https://pypi.org/project/pdfkit/">pdfkit css bug</a>），转头想用weasyprint，没想到适配更差，记录一下。<br><span id="more"></span></p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p><code>brew install cairo pango gdk-pixbuf libffi</code></p><p><code>pip install weasyprint</code></p><h2 id="在conda环境下使用weasyprint"><a href="#在conda环境下使用weasyprint" class="headerlink" title="在conda环境下使用weasyprint"></a>在conda环境下使用weasyprint</h2><p>本来如果在brew的python3下使用应该没什么问题，但是如果要用conda环境的解释器就会报错：<code>OSError: cannot load library &#39;gobject-2.0-0&#39;</code></p><p>解决方案如下：<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo ln -s <span class="regexp">/opt/</span>homebrew<span class="regexp">/opt/g</span>lib<span class="regexp">/lib/</span>libgobject-<span class="number">2.0</span>.<span class="number">0</span>.dylib <span class="regexp">/usr/</span>local<span class="regexp">/lib/g</span>object-<span class="number">2.0</span></span><br><span class="line">sudo ln -s <span class="regexp">/opt/</span>homebrew<span class="regexp">/opt/</span>pango<span class="regexp">/lib/</span>libpango-<span class="number">1.0</span>.dylib <span class="regexp">/usr/</span>local<span class="regexp">/lib/</span>pango-<span class="number">1.0</span></span><br><span class="line">sudo ln -s <span class="regexp">/opt/</span>homebrew<span class="regexp">/opt/</span>harfbuzz<span class="regexp">/lib/</span>libharfbuzz.dylib <span class="regexp">/usr/</span>local<span class="regexp">/lib/</span>harfbuzz</span><br><span class="line">sudo ln -s <span class="regexp">/opt/</span>homebrew<span class="regexp">/opt/</span>fontconfig<span class="regexp">/lib/</span>libfontconfig.<span class="number">1</span>.dylib <span class="regexp">/usr/</span>local<span class="regexp">/lib/</span>fontconfig-<span class="number">1</span></span><br><span class="line">sudo ln -s <span class="regexp">/opt/</span>homebrew<span class="regexp">/opt/</span>pango<span class="regexp">/lib/</span>libpangoft2-<span class="number">1.0</span>.dylib <span class="regexp">/usr/</span>local<span class="regexp">/lib/</span>pangoft2-<span class="number">1.0</span></span><br></pre></td></tr></table></figure></p><p>创建对应位置的软链接，<a href="https://github.com/Kozea/WeasyPrint/issues/1448">issue在这</a>。</p><p>这样以后在终端用是没什么问题了，<code>weasyprint url xx.pdf</code>，中文支持不佳，css部分支持不好。</p><h2 id="仍然报错"><a href="#仍然报错" class="headerlink" title="仍然报错"></a>仍然报错</h2><p>在正常Python调用中仍然会报错，<code>RuntimeError: cannot use unpack() on &lt;cdata &#39;char *&#39; NULL&gt;</code>，定位到<code>cffi/api.py</code>，一个空指针，暂时不知道怎么解决。</p><p>CFFI(C Foreign Function Interface) 是Python的C语言外部函数接口。通过CFFI，Python可以与几乎任何C语言代码进行交互。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;为了把HTML转成PDF（带很多css的），折腾了一晚上。试了一下pdfkit是基本没戏（&lt;a href=&quot;https://pypi.org/project/pdfkit/&quot;&gt;pdfkit css bug&lt;/a&gt;），转头想用weasyprint，没想到适配更差，记录一下。&lt;br&gt;</summary>
    
    
    
    
    <category term="MacOS" scheme="http://silencezheng.top/tags/MacOS/"/>
    
  </entry>
  
  <entry>
    <title>动态规划</title>
    <link href="http://silencezheng.top/2022/10/11/article69/"/>
    <id>http://silencezheng.top/2022/10/11/article69/</id>
    <published>2022-10-11T10:15:29.000Z</published>
    <updated>2022-10-11T10:20:49.606Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Dynamic Programming学习，由于动态规划问题很多，且没有固定套路，本文会随时进行增补、修改。</p><p>另外，这里写的动态规划是笔者两天速成理解的，和真实的动态规划可能相去甚远，希望大家多多指教。<br><span id="more"></span></p><h2 id="概念（随便找了一个）"><a href="#概念（随便找了一个）" class="headerlink" title="概念（随便找了一个）"></a>概念（随便找了一个）</h2><p>动态规划是研究多步决策过程<strong>最优化</strong>问题的一种数学方法。在动态规划中，为了寻找一个问题的最优解（即最优决策过程），将整个问题划分成若干个相应的阶段，并在每个阶段都根据先前所作出的决策作出当前阶段最优决策，进而得出整个问题的最优解。即<em>记住已知问题的答案，在已知的答案的基础上解决未知的问题。</em></p><h2 id="能解决的问题"><a href="#能解决的问题" class="headerlink" title="能解决的问题"></a>能解决的问题</h2><p>1、计数问题<br>例如“有多少种方式使得…”，或者“有多少种方法选出…”。</p><p>2、最值问题<br>例如经典的“最少用多少枚硬币能组合出目标面值”，或者“求最长上升子序列”。</p><p>3、存在性问题<br>例如“能不能选出k个数使得…”，或者“先手方是否必胜”。</p><p>注意，通常求所有解法的问题不能用DP方法来做，因为与最优解无关的解在动态规划中不会被全部计算。</p><h2 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h2><p>在上述问题中，可能会有许多可行解。每一个解都对应于一个值，我们希望找到具有最优值的解。动态规划算法的基本思想是将待求解问题分解成若干个子问题，先求解子问题，然后从这些子问题的解得到原问题的解。与分治法不同的是，适合于用动态规划求解的问题，经分解得到<strong>子问题往往不是互相独立的</strong>。如果能够保存已解决的子问题的答案，而在需要时再找出已求得的答案，这样就可以避免大量的重复计算，节省时间。</p><p>这种记住子问题的做法很容易联想到记忆化，但 <strong>动态规划</strong> 和 <strong>记忆化搜索</strong> 的一个重要区别是动态规划通常不使用递归实现。 另外在计算顺序方面，多数动态规划问题是自底向上的（这与状态转移有关），而记忆化搜索是自顶向下的。</p><p>通常可以用一个表来记录所有已解的子问题的答案。不管该子问题以后是否被用到，只要它被计算过，就将其最优解填入表中，方便在求解之后的子问题时可以方便调用，进而求出整个问题的最优解。这就是动态规划法的基本思路。具体的动态规划算法多种多样，但它们具有相同的填表格式。</p><p>也可以说，动态规划最核心的思想，就在于<strong>拆分子问题，记住过往，减少重复计算</strong>。</p><h2 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h2><p>通常通过动态规划求解问题由四个主要部分组成：<br>1、拆分子问题<br>2、确定状态转移方程<br>3、确定初状态和边界条件<br>4、确定计算顺序</p><p>这四个部分看起来简单易懂，但是在实践过程中却会遇到重重困难。下面分别详细的解释一下。</p><p>首先需要明确动态规划解决的是一个多步决策问题，该问题由初状态、若干中间决策和末状态组成。</p><h3 id="1、拆分子问题"><a href="#1、拆分子问题" class="headerlink" title="1、拆分子问题"></a>1、拆分子问题</h3><p>拆分子问题，即将原问题拆解成为<strong>范围更小但性质不变的子问题</strong>。</p><p>一般首先要做的是找出“最后一步”，也就是若干中间决策的最后一步，经历该决策后，问题转变到末状态。</p><p>找到“最后一步”后，我们研究“最后一步”前的状态，它应该能够构成一个子问题，i.e.，如果说“最后一步”使问题达到末状态，且这一系列决策构成问题的最优解，那么去掉“最后一步”的决策链应该是最优解的一个子集，同时“最后一步”前的状态构成一个子问题，该子集为该子问题的最优解。</p><p>这样一来，我们就从原问题中拆出了一个子问题。<strong>拆分子问题的目的是找出状态</strong>，状态是对问题各阶段的客观表述，同时需要满足<em>无后效性</em>，即当前状态之后的决策对该状态无影响。</p><h3 id="2、确定状态转移方程"><a href="#2、确定状态转移方程" class="headerlink" title="2、确定状态转移方程"></a>2、确定状态转移方程</h3><p>状态转移方程是动态规划的重中之重，一旦确定了状态转移方程，问题通常也就迎刃而解了。</p><p><strong>状态转移就是根据上一阶段的状态和之后的一次决策来导出本阶段的状态</strong>。所以如果确定了决策，状态转移方程也就可写出。 但事实上常常是反过来做，根据相邻两个阶段的状态之间的关系来确定决策方法和状态转移方程。 状态转移方程通常是一个递推公式。</p><p>同时，状态转移方程也影响着最终算法的计算顺序，通常靠后的状态会需求前方状态的信息，原问题为求到最后状态的最优解，因此动态规划问题通常都是自底向上计算的。</p><p>我们用一个数组或哈希表来记录状态的最优解，本文暂时称其为<strong>状态表</strong>。</p><h3 id="3、确定初状态和边界情况"><a href="#3、确定初状态和边界情况" class="headerlink" title="3、确定初状态和边界情况"></a>3、确定初状态和边界情况</h3><p>初状态就是问题的初始状态。 确定初状态主要是确定状态表如何初始化。</p><p>边界情况也称边界条件，即状态转移方程的边界，有时在递推公式的某一部分会出现不合理的或不属于问题范围内的项，需要通过边界条件来限制它。</p><h3 id="4、确定计算顺序"><a href="#4、确定计算顺序" class="headerlink" title="4、确定计算顺序"></a>4、确定计算顺序</h3><p>如上面所说的，根据状态转移公式来确定。</p><h2 id="案例一：摩天大楼"><a href="#案例一：摩天大楼" class="headerlink" title="案例一：摩天大楼"></a>案例一：摩天大楼</h2><p>题目如下：</p><p><img src="/assets/post_img/article69/question.jpeg" alt="skyscraper"></p><p>拿到题目我们首先考虑一下暴力法，从暴力法的实现中可以看出有没有优化的余地，下面是我手写的一个过程，以输入$[2, 5, 1, 4, 8]$为例。</p><p><img src="/assets/post_img/article69/brutal-force.png" alt="bf"></p><p>可以发现有很多重复计算，可以通过记忆化进行优化。 暴力法代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># author：SilenceZheng66</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">brutal_force</span>(<span class="params">L: <span class="built_in">list</span>[<span class="built_in">int</span>]</span>):</span></span><br><span class="line">    n = <span class="built_in">len</span>(L)</span><br><span class="line"></span><br><span class="line">    results = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> n &lt;= <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">to_top</span>(<span class="params">pos, times</span>):</span></span><br><span class="line">        <span class="keyword">if</span> pos == n - <span class="number">1</span>:</span><br><span class="line">            results.append(times)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(pos + <span class="number">1</span>, pos + L[pos] + <span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> j &lt; n:</span><br><span class="line">                    to_top(j, times + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> L[<span class="number">0</span>] &gt;= n - <span class="number">1</span>:</span><br><span class="line">        results.append(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, L[<span class="number">0</span>] + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> i &lt; n:</span><br><span class="line">                to_top(i, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">min</span>(results)</span><br></pre></td></tr></table></figure><p>下面我们开始用动态规划的解题步骤尝试去优化我们的算法。</p><h3 id="1、拆分子问题-1"><a href="#1、拆分子问题-1" class="headerlink" title="1、拆分子问题"></a>1、拆分子问题</h3><p>原问题是：找出从底层到楼顶的最少乘电梯数。我们以数组的下标$pos$来进行说明，底层的$pos = 0$，楼顶的$pos = n-1$。</p><p>假设“最后一步”前的状态为 $pos = m$ ，那么“最后一步”就是 $n-1$ 位于 $m + 1$ 和 $m + L[m]$ 之内，也可以表示为 $m + L[m] \geq n-1$。 此时原问题就被拆分成了子问题：找出从底层到$m+1$层的最少乘电梯数。 原问题的答案为该子问题答案$+1$。 </p><p>那么此时我们就可以描述状态了，$f(x) = 到达x层的最少乘电梯数$。</p><h3 id="2、确定状态转移方程-1"><a href="#2、确定状态转移方程-1" class="headerlink" title="2、确定状态转移方程"></a>2、确定状态转移方程</h3><p>第一步中我们拆分了子问题，确定了状态$f(x)$，则原问题可以描述为状态表中的最后一个状态，即$f(n-1)$。 根据“最后一步”中的推断，最理想的状态是能直接找到$f(n-1)$的前一个最优状态$f(m)$，然后再找到$f(m)$的前一最优个状态…直到正好找到$f(0)$。但是这是不可能实现的。</p><p>回到现实，还是考虑输入$[2, 5, 1, 4, 8]$，很自然的能够得出$f(4) = min(f(3), f(1)) + 1$，那么问题来了，为什么不是$min(f(3),f(2),f(1),f(0))$？很明显，我们需要考虑电梯的上升能力，即$5$层前有哪些层是能直接到达$5$层的？这些能直接到达的层里，哪些层的$f(x)$最小？找出他们，再加上$1$，就得到了$f(4)$。</p><p>基于以上分析，我们可以得出状态转移方程：</p><script type="math/tex; mode=display">f(x) = min(g(f(x-1), ... ,f(0))) + 1</script><p>其中$g(f(x-1), … ,f(0))$表示从$pos=0$到$pos=x-1$中选出那些满足$L[pos] + pos \geq x $的$f(pos)$。</p><p>得出了状态转移方程，这道题也就拿下来一多半了。</p><h3 id="3、确定初状态和边界情况-1"><a href="#3、确定初状态和边界情况-1" class="headerlink" title="3、确定初状态和边界情况"></a>3、确定初状态和边界情况</h3><p>初状态是好确定的，因为要做求最小运算，所以状态表初始化时所有元素应为系统能取到的最大正整数值。而$f(0)$应该被置为$0$，因为到达ground floor的最少乘电梯次数是$0$次。</p><p>关于边界情况，可以看到状态转移方程中有涉及到$L[pos] + pos$的运算，这可能会超出数组上界，故需要考虑进去。 同时如果大楼的层数$\leq 1$，则可以直接得出$f(n-1) = f(0) = 0$，也可以考虑进去。</p><h3 id="4、确定计算顺序-1"><a href="#4、确定计算顺序-1" class="headerlink" title="4、确定计算顺序"></a>4、确定计算顺序</h3><p>从状态转移公式可以看出，靠后的状态依赖前方状态信息，故应为自底向上。</p><p>最后就是写代码了，这里提供一个我写的示范。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># author：SilenceZheng66</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dp</span>(<span class="params">L: <span class="built_in">list</span>[<span class="built_in">int</span>]</span>):</span></span><br><span class="line">    n = <span class="built_in">len</span>(L)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 状态数组</span></span><br><span class="line">    conditions = [sys.maxsize <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始状态</span></span><br><span class="line">    conditions[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 边界情况</span></span><br><span class="line">    <span class="keyword">if</span> n &lt;= <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 自底向上</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n - <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 边界情况判断</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i + <span class="number">1</span>, i + L[i] + <span class="number">1</span>) <span class="keyword">if</span> i + L[i] + <span class="number">1</span> &lt;= n <span class="keyword">else</span> <span class="built_in">range</span>(i + <span class="number">1</span>, n):</span><br><span class="line">            conditions[j] = <span class="built_in">min</span>(conditions[i] + <span class="number">1</span>, conditions[j])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> conditions[-<span class="number">1</span>]</span><br></pre></td></tr></table></figure></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;Dynamic Programming学习，由于动态规划问题很多，且没有固定套路，本文会随时进行增补、修改。&lt;/p&gt;
&lt;p&gt;另外，这里写的动态规划是笔者两天速成理解的，和真实的动态规划可能相去甚远，希望大家多多指教。&lt;br&gt;</summary>
    
    
    
    
    <category term="数据结构与算法" scheme="http://silencezheng.top/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>复活Google翻译</title>
    <link href="http://silencezheng.top/2022/10/06/article68/"/>
    <id>http://silencezheng.top/2022/10/06/article68/</id>
    <published>2022-10-06T08:21:31.000Z</published>
    <updated>2022-11-07T08:01:45.097Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>9月末，谷歌旗下网页翻译工具谷歌翻译停止了中国区服务，现在访问<code>translate.google.cn</code>网页会指向谷歌香港站，此做法与此前谷歌搜索、谷歌地图等功能退出中国大陆时一致。</p><p>也就是说，Chrome的内置翻译也不能正常使用了。好在目前可以通过修改host文件的方式复活谷歌翻译。<br><span id="more"></span></p><h2 id="22年11月7日更新"><a href="#22年11月7日更新" class="headerlink" title="22年11月7日更新"></a>22年11月7日更新</h2><p>自 2022 年 10 月 21 日起，Google 断开了谷歌翻译与在华运营的其它网络服务 IP 的关联，因此通过可访问的 Google 域名获取 IP 不再可行，想要通过修改 hosts 恢复谷歌翻译功能，需要寻找其它可用 IP 地址。如有条件可以科学上网，没有则可尝试使用下面这些 IP 地址：<br><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">74.125.137.90</span></span><br><span class="line"><span class="number">74.125.193.186</span></span><br><span class="line"><span class="number">74.125.196.113</span></span><br><span class="line"><span class="number">108.177.97.100</span></span><br><span class="line"><span class="number">108.177.111.90</span></span><br><span class="line"><span class="number">108.177.122.90</span></span><br><span class="line"><span class="number">108.177.125.186</span></span><br><span class="line"><span class="number">108.177.126.90</span></span><br><span class="line"><span class="number">108.177.127.90</span></span><br><span class="line"><span class="number">142.250.0.90</span></span><br><span class="line"><span class="number">142.250.1.90</span></span><br><span class="line"><span class="number">142.250.4.90</span></span><br><span class="line"><span class="number">142.250.8.90</span></span><br><span class="line"><span class="number">142.250.9.90</span></span><br><span class="line"><span class="number">142.250.10.90</span></span><br><span class="line"><span class="number">142.250.11.90</span></span><br><span class="line"><span class="number">142.250.12.90</span></span><br><span class="line"><span class="number">142.250.13.90</span></span><br><span class="line"><span class="number">142.250.27.90</span></span><br><span class="line"><span class="number">142.250.28.90</span></span><br><span class="line"><span class="number">142.250.30.90</span></span><br><span class="line"><span class="number">142.250.31.90</span></span><br><span class="line"><span class="number">142.250.96.90</span></span><br><span class="line"><span class="number">142.250.97.90</span></span><br><span class="line"><span class="number">142.250.98.90</span></span><br><span class="line"><span class="number">142.250.99.90</span></span><br><span class="line"><span class="number">142.250.100.90</span></span><br><span class="line"><span class="number">142.250.101.90</span></span><br><span class="line"><span class="number">142.250.102.90</span></span><br><span class="line"><span class="number">142.250.103.90</span></span><br><span class="line"><span class="number">142.250.105.90</span></span><br><span class="line"><span class="number">142.250.107.90</span></span><br><span class="line"><span class="number">142.250.111.90</span></span><br><span class="line"><span class="number">142.250.112.90</span></span><br><span class="line"><span class="number">142.250.113.90</span></span><br><span class="line"><span class="number">142.250.114.90</span></span><br><span class="line"><span class="number">142.250.115.90</span></span><br><span class="line"><span class="number">142.250.123.90</span></span><br><span class="line"><span class="number">142.250.125.90</span></span><br><span class="line"><span class="number">142.250.126.90</span></span><br><span class="line"><span class="number">142.250.128.90</span></span><br><span class="line"><span class="number">142.250.138.90</span></span><br><span class="line"><span class="number">142.250.141.90</span></span><br><span class="line"><span class="number">142.250.142.90</span></span><br><span class="line"><span class="number">142.250.145.90</span></span><br><span class="line"><span class="number">142.250.152.90</span></span><br><span class="line"><span class="number">142.250.153.90</span></span><br><span class="line"><span class="number">142.250.157.90</span></span><br><span class="line"><span class="number">142.250.157.183</span></span><br><span class="line"><span class="number">142.250.157.184</span></span><br><span class="line"><span class="number">142.250.157.186</span></span><br><span class="line"><span class="number">142.250.158.90</span></span><br><span class="line"><span class="number">142.250.159.90</span></span><br><span class="line"><span class="number">142.251.1.90</span></span><br><span class="line"><span class="number">142.251.2.90</span></span><br><span class="line"><span class="number">142.251.4.90</span></span><br><span class="line"><span class="number">142.251.5.90</span></span><br><span class="line"><span class="number">142.251.6.90</span></span><br><span class="line"><span class="number">142.251.8.90</span></span><br><span class="line"><span class="number">142.251.9.90</span></span><br><span class="line"><span class="number">142.251.10.90</span></span><br><span class="line"><span class="number">142.251.12.90</span></span><br><span class="line"><span class="number">142.251.15.90</span></span><br><span class="line"><span class="number">142.251.16.90</span></span><br><span class="line"><span class="number">142.251.18.90</span></span><br><span class="line"><span class="number">142.251.107.90</span></span><br><span class="line"><span class="number">142.251.111.90</span></span><br><span class="line"><span class="number">142.251.112.90</span></span><br><span class="line"><span class="number">142.251.116.90</span></span><br><span class="line"><span class="number">142.251.117.90</span></span><br><span class="line"><span class="number">142.251.120.90</span></span><br><span class="line"><span class="number">142.251.160.90</span></span><br><span class="line"><span class="number">142.251.161.90</span></span><br><span class="line"><span class="number">142.251.162.90</span></span><br><span class="line"><span class="number">142.251.163.90</span></span><br><span class="line"><span class="number">142.251.166.90</span></span><br><span class="line"><span class="number">172.217.192.90</span></span><br><span class="line"><span class="number">172.217.195.90</span></span><br><span class="line"><span class="number">172.217.203.90</span></span><br><span class="line"><span class="number">172.217.204.90</span></span><br><span class="line"><span class="number">172.217.214.90</span></span><br><span class="line"><span class="number">172.217.215.90</span></span><br><span class="line"><span class="number">172.253.58.90</span></span><br><span class="line"><span class="number">172.253.62.90</span></span><br><span class="line"><span class="number">172.253.63.90</span></span><br><span class="line"><span class="number">172.253.112.90</span></span><br><span class="line"><span class="number">172.253.113.90</span></span><br><span class="line"><span class="number">172.253.114.90</span></span><br><span class="line"><span class="number">172.253.115.90</span></span><br><span class="line"><span class="number">172.253.116.90</span></span><br><span class="line"><span class="number">172.253.117.90</span></span><br><span class="line"><span class="number">172.253.118.90</span></span><br><span class="line"><span class="number">172.253.119.90</span></span><br><span class="line"><span class="number">172.253.123.90</span></span><br><span class="line"><span class="number">172.253.124.90</span></span><br><span class="line"><span class="number">172.253.125.90</span></span><br><span class="line"><span class="number">172.253.126.90</span></span><br><span class="line"><span class="number">172.253.127.90</span></span><br><span class="line"><span class="number">216.58.227.65</span></span><br><span class="line"><span class="number">216.58.227.66</span></span><br><span class="line"><span class="number">216.58.227.67</span></span><br></pre></td></tr></table></figure></p><h2 id="找可用IP"><a href="#找可用IP" class="headerlink" title="找可用IP"></a>找可用IP</h2><p>打开：<a href="https://ping.chinaz.com/translate.google.cn">https://ping.chinaz.com/translate.google.cn</a></p><p>随便找一个IP就行，如果想再精细一点，可以找到距离自己物理位置较近的同网络运营商的IP地址。</p><h2 id="修改host文件"><a href="#修改host文件" class="headerlink" title="修改host文件"></a>修改host文件</h2><p>把如下内容增加到host文件中，其中IP就是第一步找的IP。<br><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">220.181.174.98</span> translate.googleapis.com</span><br></pre></td></tr></table></figure></p><p>对于Windows，该文件的目录通常为<code>C:\Windows\System32\drivers\etc\hosts</code>；</p><p>对于MacOS，为<code>/private/etc/hosts</code>；</p><p>对于Linux，为<code>/etc/hosts</code>；</p><p>Android和ios也有对应方式，没啥用就不写了，详细可以看<a href="https://pangniao.net/google-translate.html">这里</a>。</p><p>修改完了source一下就可以使用了，如果不行就重启浏览器。</p><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>这个方法就是通过关联可用IP和翻译API的域名使翻译服务可用，如果没有可用IP了这个办法就行不通了。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;9月末，谷歌旗下网页翻译工具谷歌翻译停止了中国区服务，现在访问&lt;code&gt;translate.google.cn&lt;/code&gt;网页会指向谷歌香港站，此做法与此前谷歌搜索、谷歌地图等功能退出中国大陆时一致。&lt;/p&gt;
&lt;p&gt;也就是说，Chrome的内置翻译也不能正常使用了。好在目前可以通过修改host文件的方式复活谷歌翻译。&lt;br&gt;</summary>
    
    
    
    
    <category term="Chrome" scheme="http://silencezheng.top/tags/Chrome/"/>
    
  </entry>
  
  <entry>
    <title>高斯滤波与双边滤波</title>
    <link href="http://silencezheng.top/2022/10/05/article67/"/>
    <id>http://silencezheng.top/2022/10/05/article67/</id>
    <published>2022-10-05T15:14:18.000Z</published>
    <updated>2022-10-06T01:47:02.633Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>学习一下高斯滤波和双边滤波，及其需要的前置知识，仅记录一下个人理解，先后逻辑比较混乱，参考请谨慎。如有错误请评论区指正，感谢。</p><p>文中图片多数来自网络，我尽量擦去了水印，是为了提升观感，参考文献在文末有注明。本文的出发点，大概是在尽量用直观、快速的方式了解这些抽象概念吧。<br><span id="more"></span></p><h2 id="频域（Frequency-domain）、时域（Time-domain）"><a href="#频域（Frequency-domain）、时域（Time-domain）" class="headerlink" title="频域（Frequency domain）、时域（Time domain）"></a>频域（Frequency domain）、时域（Time domain）</h2><p><strong>时域</strong>（Time domain）是描述数学函数或物理信号对时间的关系。例如一个信号的时域波形可以表达信号随着时间的变化。</p><p><strong>频域</strong>（Frequency domain）是描述信号在频率方面特性时用到的一种坐标系。频域最重要的性质是：它不是真实的，而是一个数学构造。时域是惟一客观存在的域，而频域是一个遵循特定规则的数学范畴，频域也被一些学者称为上帝视角。</p><p>以上是对时域和频域下的初步定义，时域是好理解的，但要真正理解频域，还需要若干补充文字。</p><h3 id="补充一：频谱（Spectrum）"><a href="#补充一：频谱（Spectrum）" class="headerlink" title="补充一：频谱（Spectrum）"></a>补充一：频谱（Spectrum）</h3><p>频域即频率域，在频域图像（<strong>频谱</strong>）中，自变量是频率（横轴），纵轴是该频率信号的幅度（<strong>振幅</strong>）。频谱图描述了信号的频率结构 及 频率与该频率信号幅度的关系。</p><p><img src="/assets/post_img/article67/FD.png" alt="fm"></p><p>所以，以时间作为变量所进行的研究就是时域，以频率作为变量所进行的研究就是频域。下图生动的表示了这一点：</p><p><img src="/assets/post_img/article67/TDandFD.png" alt="fdandtd"></p><p>时域中的红色波形为三个正弦波相叠加的结果，频域中的三条蓝色竖线分别对应三个正弦波的频率与振幅关系。</p><h3 id="补充二：傅立叶分析（Fourier-analysis）"><a href="#补充二：傅立叶分析（Fourier-analysis）" class="headerlink" title="补充二：傅立叶分析（Fourier analysis）"></a>补充二：傅立叶分析（Fourier analysis）</h3><p>傅立叶分析是分析学中逐渐形成的一个重要分支，它研究并扩展<strong>傅立叶级数</strong>和<strong>傅立叶变换</strong>的概念，又称调和分析。</p><blockquote><p>Fourier在1807年发表的传记和1822年出版的《热分析理论》一书中指出：无论函数多么复杂，只要它是<strong>周期</strong>的，并且满足某些适度的条件，都可以表示为不同频率的正弦和（或）余弦函数之和的形式，每个正弦和（或）余弦函数都乘以不同的系数。</p></blockquote><p>我先简单说明一下傅立叶分析究竟用来干什么，仅对于图像方面。假如我现在有一个波形如下：</p><p><img src="/assets/post_img/article67/sin3sin5.webp" alt="sin35"></p><p>假如告诉你该<em>复杂周期信号</em>是由两个<em>简单周期信号</em>组成的（后面会提到这两个概念），其中一个是$\sin (3x)$，要求你把该信号从图中取出，并绘制出剩下的信号波形。显然，这个任务在时域几乎不可能完成。</p><p>但是在频域中，无非是几条竖线而已，取出其中一个再容易不过了。这是频域的意义。但是不要忘了，现实世界中只有时域，于是，<strong>傅立叶分析作为时域与频域的桥梁</strong>出现了，他允许我们从时域跨越到频域完成任务（取出$\sin (3x)$）。顺便一提，这个任务在工程上称为<strong>滤波</strong>。</p><p>傅立叶分析可以分为两块，傅立叶级数（Fourier series）用于处理连续周期函数，傅立叶变换(Fourier transformation)则可以处理连续非周期函数。这里的函数是数学上的说法，在物理上，称为<strong>信号</strong>。总而言之，傅立叶提出的两个重要观点如下：</p><ol><li>周期信号都可以表示为谐波关系的正弦信号的<strong>加权和</strong>（傅里叶级数）</li><li>非周期信号都可用正弦信号的<strong>加权积分</strong>表示（傅里叶变换）</li></ol><p>傅立叶选这个正余弦函数也是有原因的，具体可以看下面对<strong>正弦波</strong>的补充。其实信号不只能分解为正余弦信号，也可以是什么幂级数展开、泰勒展开之类的。</p><p>傅立叶变换（傅立叶级数是其特例）就是通过数学的方法反向分解复杂的信号，使之成为若干简单的正余弦信号，那么问题又来了，这么多简单的信号，每个正弦信号都在时域图上画出来也没有什么意义，那么人类只需要记住每个正弦信号的相位和角频率就可以了，记住后随时随地我们都可以制造这些简单的信号，把它们在时域上相加，就会再现相同的复杂信号。</p><p>这时，我们就需要考虑如何记录信号的相位和角频率。如我们所知，<strong>频谱</strong>只能反映角频率和振幅间的关系，但并未包含$\mathrm{A} \sin (\omega x + \phi)$中的全部信息。因此，还需要<strong>相位谱</strong>的参与。</p><h3 id="补充三：正弦波（Sine-wave）"><a href="#补充三：正弦波（Sine-wave）" class="headerlink" title="补充三：正弦波（Sine wave）"></a>补充三：正弦波（Sine wave）</h3><p>首先，正弦波就是一个圆周运动在一条直线上的投影。</p><p><img src="/assets/post_img/article67/circle-wave.gif" alt="s-w"></p><p><strong>正弦波</strong>是频域中唯一存在的波形，这是频域中最重要的规则，即正弦波是对频域的描述，因为频域中的任何波形都可用正弦波合成。这是正弦波的一个非常重要的性质。然而，它并不是正弦波的独有特性，还有许多其他的波形也有这样的性质。正弦波有四个性质使它可以有效地描述其他任一波形：</p><p>（1）频域中的任何波形都可以由正弦波的组合完全且惟一地描述。</p><p>（2）任何两个频率不同的正弦波都是<strong>正交</strong>的。如果将两个正弦波相乘并在整个时间轴上求积分，则积分值为零。这说明可以将不同的频率分量相互分离开。</p><p>（3）正弦波有精确的数学定义。</p><p>（4）正弦波及其微分值处处存在，没有上下边界。</p><p>使用正弦波作为频域中的函数形式有它特别的地方。如果变换到频域并使用正弦波描述，有时会比仅仅在时域中能更快地得到答案。</p><p>这里一个重要的意识是：<strong>正弦和余弦可以通过相位的转变相互转化</strong>。暂时把正余弦信号统称为正弦波。</p><p>正弦曲线可表示为：</p><script type="math/tex; mode=display">\mathrm{y}=\mathrm{A} \sin (\omega x + \phi)+\mathrm{k}</script><p>$k$、$\omega$ 和 $\phi$ 是常数 $(k, \omega, \phi \in R$ 且 $\omega \neq 0)$</p><p>A —— <strong>振幅</strong>，当物体作轨迹符合正弦曲线的直线往复运动时，其值为行程的 $\frac{1}{2}$ 。<br>$(\omega x + \phi)$ —— <strong>相位</strong>（Phase）， 反映变量y所处的状态。<br>$\phi$ —— 初相, $x=0$ 时的相位，反映在坐标系上则为图像的左右移动。<br>$\mathrm{k}$ —— 偏距, 反映在坐标系上则为图像的上移或下移。<br>$\omega$ —— 角速度，或<strong>角频率</strong>，控制正弦周期(单位弧度内震动的次数)。</p><p>由于正余弦信号只含有一个频率成分 $\omega$ ，当横轴上的$x$表示时间时，上式不仅是其时域描述也是其频域描述，无需进行变换，可见时域与频域是合二为一的，因此适合将其做为合成其他任意信号的基本信号。</p><h3 id="补充四：傅立叶级数与其频谱、相位谱"><a href="#补充四：傅立叶级数与其频谱、相位谱" class="headerlink" title="补充四：傅立叶级数与其频谱、相位谱"></a>补充四：傅立叶级数与其频谱、相位谱</h3><p>首先，周期信号可以分为 <strong>简单周期信号</strong> 和 <strong>复杂周期信号</strong>，两者的最大区别就是频率结构上分别是<strong>单频</strong>和<strong>多频</strong>。 对下面的例子来说，正余弦信号是单频信号，而方波信号是多频信号。</p><p>将若干个正余弦信号叠加，可得到方波信号，而通过傅立叶级数即可逆向将方波信号分解为多个正余弦信号之和。</p><p><img src="/assets/post_img/article67/F-series.gif" alt="f-series"></p><p>但若要形成标准的90度角矩形波，需要无穷多个正弦波叠加。我们需要接受一个设定：<strong>不仅仅是矩形，任何波形都是可以如此方法用正弦波叠加起来获得。</strong></p><p>下面再换一个角度看正弦波叠加：</p><p><img src="/assets/post_img/article67/rec-wave.png" alt="rec-wave"></p><p>在这几幅图中，最前面黑色的线就是所有正弦波叠加而成的总和，也就是越来越接近矩形波的那个图形。</p><p>后面依不同颜色排列而成的正弦波就是组合为矩形波的各个分量，称为<strong>频率分量</strong>。这些正弦波按照频率<em>从低到高</em>从前向后排列开来，且每一个波的振幅都是不同的。频率越高，波形越密集。</p><p>每两个正弦波之间都还有一条直线，那是<strong>振幅为0的正弦波</strong>。也就是说，为了组成特殊的曲线，有些正弦波成分是不需要的。</p><p>如果我们把第一个频率最低的频率分量看作<strong>基本单元</strong>，该正弦波的角频率为$w_0$，那么频域的基本单元就是$\omega_0$。 假设$\omega_0 = 10$，回到<a href="#补充一频谱">频谱</a>中，可以看到该频谱的基本单元正是10。</p><p>如果说频域的基本单元是“1”，那么频域的“0”就是一个周期无限长的正弦波（角频率为0），也就是一条直线，0频率也被称为<strong>直流分量</strong>。</p><p>上面已经把频域的事研究的差不多了，但如前文所述，频谱中的信息只是时域中信息的一部分，我们还需要反映相位的<strong>相位谱</strong>。那么相位谱在哪呢？请看下图，鉴于正弦波是周期的，需要设定一个用来标记正弦波位置的东西，即红色点，红点是距离频率轴最近的波峰。粉色点是红点在下平面的投影，<strong>粉点用于标注波峰距离频率轴的距离</strong>。</p><p><img src="/assets/post_img/article67/phase-detail.jpeg" alt="phase-detail"></p><p>如我们所见，如果说频谱为侧面投影的话，相位谱就是波形在下方的“投影”。但是这个投影不是直接投影，而是间接的。请看下图：</p><p><img src="/assets/post_img/article67/phase-spectrum.png" alt="phase-map"></p><p>说间接投影，就是因为<strong>相位不能直接用时间差表示</strong>，我们可以看到，时间差是粉点到时域纵轴的距离，我们记为$\varDelta t$，又可以得到当前频率分量的周期$T$，则有相位$p = \frac{\varDelta t}{T} \times 2\pi$，最终相位谱上的纵轴值（相位）是依据时间差和周期算出的。</p><p>最终的图如下所示，其实，频域图像也可以叫做<strong>幅度频谱</strong>，对应的，相位谱叫做<strong>相位频谱</strong>，因为他们的定义域都是频域，只是值域不同。</p><p><img src="/assets/post_img/article67/tfp.jpeg" alt="tfp"></p><h3 id="补充五：傅立叶变换与其频谱"><a href="#补充五：傅立叶变换与其频谱" class="headerlink" title="补充五：傅立叶变换与其频谱"></a>补充五：傅立叶变换与其频谱</h3><p>前面我们说到，傅立叶级数是傅立叶变换的一个特例，也就是说，傅立叶变换推广了傅立叶级数，这个“推广“体现在傅立叶变换可以处理<strong>时域上的非周期的连续信号</strong>，将其转换为一个在<strong>频域上的非周期连续信号</strong>。如下图：</p><p><img src="/assets/post_img/article67/Fourier-transformation.jpeg" alt="f-t"></p><p>图片右上角的<code>Frequency resolution</code>是频率分辨率，即数字信号处理系统将相距最近的两个频率分量区分开的能力，它与周期成反比，当一个连续信号是非周期信号时，可以将其周期看作无限大，则该信号近似看为周期无限大的周期连续信号。这代表<strong>非周期信号可以通过傅立叶变换分解为无限多个频率无限接近的正余弦信号的和</strong>。</p><p><img src="/assets/post_img/article67/discrete-spectrum.png" alt="ds"></p><p>试想一下，上面离散的频率分量逐渐靠近…直至连在一起，无穷无尽的分量铺满了空间。</p><p><img src="/assets/post_img/article67/continuous-spectrum.png" alt="cs"></p><p>变成了波涛汹涌的大海，这就是连续的频谱下发生的事情。那么，计算时离散频谱下分量的累加，自然也就变成了连续频谱上的<strong>积分</strong>。</p><p>至此，对于研究图像处理来说，似乎已经足够，数学上的东西，需要时再深究也不迟。</p><h3 id="补充六：虚数（Imaginary-number）-和-欧拉公式（Euler’s-formula）"><a href="#补充六：虚数（Imaginary-number）-和-欧拉公式（Euler’s-formula）" class="headerlink" title="补充六：虚数（Imaginary number） 和 欧拉公式（Euler’s formula）"></a>补充六：虚数（Imaginary number） 和 欧拉公式（Euler’s formula）</h3><p>我们知道，在实数轴上，乘$-1$其实就是使线段逆时针旋转了$180$度，那么乘一个$i$呢？显然，旋转$90$度。</p><p><img src="/assets/post_img/article67/imagine-real.jpeg" alt="i-r"></p><p>同时，我们获得了一个垂直的虚数轴。实数轴与虚数轴共同构成了一个复数的平面，也称复平面。这样我们就了解到，乘虚数i的一个功能——旋转。</p><p>欧拉公式如下（关于欧拉公式为何被称为“上帝创造的公式”此处不再赘述）：</p><script type="math/tex; mode=display">e^{i x}=\cos x+i \sin x</script><p>当$x = \pi$时，就有$e^{i x} + 1 = 0$。</p><p>这个公式的关键作用之一，是将正弦波统一成了简单的指数形式。</p><p><img src="/assets/post_img/article67/Euler.png" alt="euler"></p><p>欧拉公式所描绘的，是一个随着时间变化，在复平面上做圆周运动的点，随着时间的改变，在时间轴上就成了一条螺旋线。如果只看它的实数部分，也就是螺旋线在左侧的投影，就是一个最基础的余弦函数。而右侧的投影则是一个正弦函数。</p><p>欧拉公式的另一种形式是：</p><script type="math/tex; mode=display">\sin x=\frac{e^{i x}-e^{-i x}}{2 i}, \cos x=\frac{e^{i x}+e^{-i x}}{2}</script><h3 id="补充七：数学上的傅立叶级数和傅立叶变换"><a href="#补充七：数学上的傅立叶级数和傅立叶变换" class="headerlink" title="补充七：数学上的傅立叶级数和傅立叶变换"></a>补充七：数学上的傅立叶级数和傅立叶变换</h3><p>傅立叶级数的三角函数形式如下：</p><script type="math/tex; mode=display">f(t)=a_0+\sum_{n=1}^{\infty}\left[a_n \cos \left(n \omega t\right)+b_n \sin \left(n \omega t\right)\right], \\其中 \omega = \frac{2 \pi}{T}为基波频率, a_n和b_n为傅立叶系数，分别代表余弦分量振幅和正弦分量振幅.</script><p>傅立叶变换相关的，先放放吧。可以参考<a href="https://blog.csdn.net/Thera_qing/article/details/106154313">这</a><a href="https://blog.csdn.net/weixin_40851250/article/details/84780221">三</a><a href="https://blog.csdn.net/jack__linux/article/details/99671469">篇</a>。</p><h2 id="从信号的角度看图像"><a href="#从信号的角度看图像" class="headerlink" title="从信号的角度看图像"></a>从信号的角度看图像</h2><p>图像是信号，这是直觉上极难理解的。</p><p>首先，我们需要摆脱信号和时间“绑定”的固有直觉，信号只是由自变量和因变量构成的，自变量不一定是时间。</p><p>实际上，图像是一个沿空间分布的信号，他的定义域是图像的$x$轴或$y$轴。图像既然是沿着空间分布的信号，说明它是一个<strong>二维信号</strong>。如果一维数字信号可以看作是一组数字序列，那么二维数字信号则可以看作是一组数字阵列。图像中沿着水平方向的任何一行可以看作是像素点随着$x$轴变化的信号。而沿着竖直方向的任何一列则可以看作是像素点随着$y$轴变化的信号。单独提取一行或者是一列数据出来，按照像素值的大小随轴的变化画出来，就可以得到我们熟悉的信号波形，如下。</p><p><img src="/assets/post_img/article67/pic-signal.png" alt="pic-signal"></p><p>可以明显的看出，图片右方的波形代表黑色横线，因为除了塔尖以外像素变化不大。而下方波形则对应竖线，由上到下。</p><p>图像既然是信号，那么自然有频率、幅度和相位的特征。</p><p><strong>图像的频率对应到的是图像细节的多少</strong>，比如城市的部分频率高于天空的部分。再比如在灰度图中，图像的频率是<strong>表征图像中灰度变化剧烈程度的指标</strong>，是灰度在平面空间上的梯度。</p><p><strong>图像的幅度对应到的是图像中像素的大小</strong>，比如天空中太阳照亮的区域幅度高于其他区域。</p><p>假设有图片A和图片B，分别获取A、B的幅度频谱和相位频谱，然后将图片A的幅度谱与图片B的相位谱结合做逆傅立叶变换，最终结果的视觉效果会是图片B加上一些噪声，通过对比实验可以得到一个结论：<strong>图像的相位谱中，保留了图像的边缘以及整体结构的信息。</strong> 而错误的幅度谱看起来则像是噪声覆盖在原图上，但并没有影响图像的内容本身。</p><p>同时，和我们在一维信号中看到的一样，如果想要通过修改频域中相位或振幅的方式来间接修改原图像，则需要同时保留幅度频谱和相位频谱，来通过逆傅立叶变换得到修改后的图像。</p><h3 id="空域（Spatial-domain）"><a href="#空域（Spatial-domain）" class="headerlink" title="空域（Spatial domain）"></a>空域（Spatial domain）</h3><p>空间域也叫<strong>空域</strong>，即所说的像素域，在空域的处理就是在像素级的处理，即直接对图像上的像素值进行增加或减少。对应的另一种操作是在频域上的，空域通过傅立叶变换后，得到的是图像的频谱。表示图像的能量梯度。</p><h3 id="图像变换技术"><a href="#图像变换技术" class="headerlink" title="图像变换技术"></a>图像变换技术</h3><p>为了有效和快速地对图像进行处理和分析，需要将原定义在图像空间的图像以某种形式转换到另外的空间，利用空间的特有性质方便地进行一定的加工，最后再转换回图像空间以得到所需的效果。 如：空域与频域间的相互转化。</p><h3 id="信号分析在图像处理中的应用"><a href="#信号分析在图像处理中的应用" class="headerlink" title="信号分析在图像处理中的应用"></a>信号分析在图像处理中的应用</h3><p>很多图像处理事实上就是信号处理。比如图像去噪 (Denoise)，一般就是应用数字<strong>低通滤波器</strong>对图像进行滤波。图像增强 (Detail enhancement)，一般就是应用数字<strong>高通滤波器</strong>得到图像的高频信号，并对高频信号进行增强。对比度增强 (Contrast enhancement)，一般就是参考画面的亮暗程度 (图像的幅度)，并人为修改亮暗的一种处理。相位的概念一般会在图像的缩放 (Scaling) 中使用到。</p><h2 id="图像的灰度、亮度、强度"><a href="#图像的灰度、亮度、强度" class="headerlink" title="图像的灰度、亮度、强度"></a>图像的灰度、亮度、强度</h2><p><strong>图像灰度</strong>(Image grayscale)：把白色与黑色之间按对数关系分为若干等级，称为灰度，灰度分为256阶，从0到255。用灰度表示的图像称为<strong>灰度图</strong>，实际上，<strong>灰度表征的是单色的亮暗程度</strong>。</p><p><strong>图象亮度</strong>(Image brightness)：指画面的明亮程度，单位是 堪德拉每平米(cd/m2) 或称 nits。图象亮度是从白色表面到黑色表面的感觉连续体，由反射系数决定，亮度侧重物体，重在“反射”。在灰度图像中，亮度等于灰度，图像运算处理方式相同。但是在彩色图像中，亮度和对比度相关，即通过对RGB颜色分量的增加（增加亮度）或减少（减少亮度）相同的增量来显示，亮度的调整就是给每个分量乘以一个百分比值。</p><p><strong>图像强度</strong>(Image intensity)：表示单通道图像像素的强度（值的大小）。在灰度图像中，它是图像的灰度。在RGB颜色空间中，可以理解把它为是R通道的像素灰度值，G通道的像素灰度值，或是B通道的像素灰度值，也就是RGB中含三个图像强度。在其他颜色空间类似，也就是每个通道的图像的像素灰度值。</p><p>这里多说一下，为什么RGB也有灰度？实际上，<strong>RGB各单通道的图像也都是灰度图</strong>，只是最终成像时将他们分别放置在红、绿、蓝的单色通道中再叠加罢了，也就是说，一个RGB彩色图像，是由三个表示对应位置亮与暗程度的图像，通过放置在三个单色通道中再叠加形成的（这里属于理糙话不糙了）。</p><p><strong>认识到所有图片都是灰度图或其叠加后，很多图像处理方法的应用都可以举一反三了。</strong></p><h2 id="图像滤波"><a href="#图像滤波" class="headerlink" title="图像滤波"></a>图像滤波</h2><p>图像滤波，就是对图像的频率进行过滤。本文关注两类，即<strong>高通滤波</strong>和<strong>低通滤波</strong>。这两种滤波方式不难理解，从名字就可以看出，高通滤波就是<em>减弱或阻隔低频信号，保留高频信号</em>。而低通滤波则是<em>减弱或阻隔高频信号，保留低频信号</em>。</p><p>现在关键的问题是，图像中的高频信号和低频信号分别代表什么？结合前面所提到的，高（低）频信号也叫做高（低）频分量，图像的频率是灰度值变化剧烈程度的标准，可以推出<strong>高频信号就是灰度值变化剧烈的地方</strong>，同理，<strong>低频信号是图像灰度值变化平缓的地方</strong>。</p><p>具体些说，<em>高频信号就是相邻区域之间灰度相差很大的地方</em>，例如一个影像与背景的边缘部位，通常会有明显的差别，灰度变化很快，也就是变化频率高的部位。同样的，图像的细节处也是属于灰度值急剧变化的区域。另外，噪声（即噪点）也是这样，噪点之所以是噪点，就是因为它与附近的像素点灰度不一样了。</p><p>相对应的，<em>低频信号就是那些连续渐变的区域</em>。人眼对图像中的高频信号更为敏感，举例来说，在一张白纸上有一行字，那么人眼会直接聚焦在文字上，而不会太在意白纸本身，这里文字就是高频信号，而白纸就是低频信号。 </p><h3 id="频谱中心化"><a href="#频谱中心化" class="headerlink" title="频谱中心化"></a>频谱中心化</h3><p>研究数字图像有时需要变换到频域做处理，比如滤波等。但直接对数字图像进行二维DFT（离散傅立叶变换）得到的频谱图是<em>高频在中间，低频在四角</em>。频谱图比较亮的地方就是低频，因为<strong>图像的能量一般都是集中在低频部分</strong>。为了把能量集中起来便于使用滤波器，可以利用二维DFT的平移性质对频谱进行中心化。如下：</p><p><img src="/assets/post_img/article67/spectrum-central.png" alt="s-c"></p><p>经二维傅立叶变换的平移性质推导，结论是<strong>频谱中心化只需对数字图像的每个像素点的取值直接乘以$(-1)^{x+y}$就可以了，其中$x，y$为像素坐标</strong>。</p><h3 id="频域滤波"><a href="#频域滤波" class="headerlink" title="频域滤波"></a>频域滤波</h3><p>频域滤波的基本过程如下：<br>1、对原图像做频谱中心化处理，<br>2、对第一步的结果进行DFT，<br>3、使用某滤波器乘第二步的结果，<br>4、对第三步结果做反DFT，再做一次频谱中心化，得到滤波后图像。</p><h3 id="空域滤波"><a href="#空域滤波" class="headerlink" title="空域滤波"></a>空域滤波</h3><p><strong>空域滤波</strong>是指利用像素及像素邻域组成的空间进行图像增强的方法。之所以用“滤波”这个词，是因为借助了频域里的概念。并且，频率域的卷积与空间域的乘积存在对应关系，由卷积定理可知<strong>所有频域的滤波理论上都可以转化为空域的卷积操作</strong>。</p><p>空域滤波是在图像空间通过邻域操作完成的，邻域操作常借助<strong>模板运算</strong>来实现。由此也可以看出，空域滤波的算法比较简单，处理速度快。而频域滤波算法复杂，计算慢。（这里只是相比于做DFT来说，现在好像还有FFT可以加速运算，具体如何先不深究。）</p><p>图像的空域滤波还可以按照运算的方式分为<strong>线性滤波</strong>和<strong>非线性滤波</strong>，如果运算只是加权求和之类的操作，则为线性滤波。而若是在模版内进行求最值、绝对值等操作，则属于非线性滤波。</p><h3 id="模版运算"><a href="#模版运算" class="headerlink" title="模版运算"></a>模版运算</h3><p>模板也称为核（kernel）。模板运算的基本思路是将 某个像素的值 替换为 它本身灰度值和其相邻像素灰度值的函数值。</p><p>模板一般是$n \times n$的<strong>方阵</strong>（$n$通常是3、5、7、9等很小的<strong>奇数</strong>）。当$n$为奇数时，可以定义<em>模板的半径</em>$r = \frac{(n-1)}{2}$。模板中有一个<strong>锚点</strong>（anchor point），通常是矩阵中心点，和原图像中待计算点对应。整个模板对应的区域，就是原图像中像素点的相邻区域。一个$n \times n$的模板最多可有$n \times n$个系数，该模板的功能由这些系数的取值所决定。</p><p>以模版运算中最常用的<strong>模版卷积</strong>举例，其在空域实现的主要步骤如下：<br>1、将模板在图中漫游，并将模板锚点与图中某个像素位置重合（待计算点），<br>2、将模板上的各个系数与模板下各对应像素的灰度值相乘，<br>3、将所有乘积相加（为保持灰度范围，常将结果再除以模板的系数个数），<br>4、将上述运算结果（模板的输出响应）赋给待计算点的像素。</p><h2 id="高斯滤波（Gaussian-filter）"><a href="#高斯滤波（Gaussian-filter）" class="headerlink" title="高斯滤波（Gaussian filter）"></a>高斯滤波（Gaussian filter）</h2><p>高斯滤波是一种<strong>根据高斯函数的形状来选择权值</strong>的线性平滑滤波，适用于消除<em>高斯噪声</em>，广泛应用于图像处理的减噪过程。通俗的讲，高斯滤波就是对整幅图像进行加权平均的过程，每一个像素点的值，都由其本身和邻域内的其他像素值经过加权平均后得到。</p><p><strong>高斯噪声</strong>就是概率密度函数服从高斯分布的一类噪声。如果一个噪声，它的幅度分布服从高斯分布，而它的功率谱密度（频谱的绝对值平方？）又是均匀分布的，则称它为<strong>高斯白噪声</strong>。高斯白噪声的二阶矩不相关，一阶矩为常数，是指先后信号在时间上的相关性。</p><p>高斯滤波的空域实现步骤很简单，先获得<strong>高斯核</strong>，然后对图像做模版卷积即可（可以做padding）。关键还是在高斯核上，或者说，在于高斯核上的权值如何计算上。</p><p>当然了，<em>高斯滤波是指用高斯函数作为滤波函数</em>，具体上实现的是模糊还是锐化效果，要根据是高通还是低通来判别。高斯锐化的高斯核就是$1 - 高斯模糊卷积核$。</p><h3 id="高斯核"><a href="#高斯核" class="headerlink" title="高斯核"></a>高斯核</h3><p>一维和二维的高斯函数如下，标准差$\sigma$的大小决定了高斯函数的宽度：</p><p><img src="/assets/post_img/article67/gaussian.webp" alt="gaussian"></p><p>根据二维高斯函数图像来理解高斯滤波就非常的直观了，假设我们要实现<strong>高斯模糊</strong>（低通），在确定高斯核时，首先需要将锚点（核的中心点）作为高斯图像中的原点（山峰的中心点），然后确定一个标准差$\sigma$和核的半径$r$，根据$r$我们可以计算出高斯核中各点的坐标，通过坐标$(x,y)$和$\sigma$依次计算二维高斯函数值，并填入对应的位置，然后，还需要进行<em>归一化</em>处理，即将核中各值除去权重总和，使核的权重综合为$1$，这样就得到了最终的高斯核。</p><p>现在的问题是，如何选择$\sigma$和$r$呢？</p><p>对于标准差，可以从二维高斯函数图像来分析，当标准差越小时，图像中的“山峰”越“瘦高”，权重分布越向中心集中，平滑效果越差；当标准差越大时，“山峰”越“矮胖”，权重分布越均匀，则平滑效果越明显。（有理论说空域上的标准差和频域的标准差成负相关，以此可以进一步解释高斯核的高低通效果，但是笔者目前还没看明白，留待日后补充。）</p><p>半径的选择同样可以从图像来分析。理论上，高斯分布在定义域的所有地方都有非负值，这就需要一个无限大的核。然而钟形曲线在区间$(\mu - \sigma, \mu + \sigma)$范围内的面积占曲线下总面积的$68\%$，在区间$(\mu - 2\sigma, \mu + 2\sigma)$范围内的面积占$95\%$，在区间$(\mu - 3\sigma, \mu + 3\sigma)$范围内的面积占$99.7\%$，通常情况下$3\sigma$以外的区域所占面积已经很小，可以被忽略（认为该段分布近似包含所有情况）。所以，当半径取$3\sigma$时，就基本上可以近似使用了，此时核的大小为$(6\sigma + 1) \times (6\sigma + 1)$。</p><p>最后，对高斯核进行<em>归一化</em>处理的一个重要目的是确保卷积运算后，像素值处于$[0, 255]$的范围之内。</p><h3 id="实验-高斯模糊"><a href="#实验-高斯模糊" class="headerlink" title="实验-高斯模糊"></a>实验-高斯模糊</h3><p>光说不练，不是好文章，下面就实验一下实现高斯滤波。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gaussian_filter</span>(<span class="params">img, sigma=<span class="number">0.5</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(img.shape) == <span class="number">3</span>:</span><br><span class="line">        H, W, C = img.shape</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        img = np.expand_dims(img, axis=-<span class="number">1</span>)</span><br><span class="line">        H, W, C = img.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算半径，取4倍</span></span><br><span class="line">    radius = <span class="built_in">int</span>(sigma * <span class="number">4</span>)</span><br><span class="line">    <span class="comment"># 计算高斯核大小</span></span><br><span class="line">    n = <span class="number">2</span> * radius + <span class="number">1</span></span><br><span class="line">    <span class="comment"># padding大小</span></span><br><span class="line">    pad = n // <span class="number">2</span></span><br><span class="line">    <span class="comment"># padding的地方置为0</span></span><br><span class="line">    out = np.zeros((H + pad * <span class="number">2</span>, W + pad * <span class="number">2</span>, C), dtype=np.float64)</span><br><span class="line">    out[pad: pad + H, pad: pad + W] = img.copy().astype(np.float64)</span><br><span class="line">    <span class="comment"># 构造高斯核坐标</span></span><br><span class="line">    x_index, y_index = np.mgrid[-radius:radius + <span class="number">1</span>, -radius:radius + <span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 计算核值</span></span><br><span class="line">    k = np.exp(-(x_index ** <span class="number">2</span> + y_index ** <span class="number">2</span>) / (<span class="number">2</span> * sigma ** <span class="number">2</span>))</span><br><span class="line">    <span class="comment"># 归一化</span></span><br><span class="line">    k = k / k.<span class="built_in">sum</span>()</span><br><span class="line">    <span class="comment"># 原图像深拷贝</span></span><br><span class="line">    origin = out.copy()</span><br><span class="line">    <span class="comment"># 高斯核卷积，计算有点慢</span></span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(H):</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(W):</span><br><span class="line">            <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(C):</span><br><span class="line">                out[pad + y, pad + x, c] = np.<span class="built_in">sum</span>(k * origin[y: y + n, x: x + n, c])</span><br><span class="line">    <span class="comment"># 切除填充部分</span></span><br><span class="line">    out = out[pad: pad + H, pad: pad + W].astype(np.uint8)</span><br><span class="line">    <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 读取图像</span></span><br><span class="line">    img = cv2.imread(<span class="string">&quot;./data/bridge.jpeg&quot;</span>)</span><br><span class="line">    <span class="comment"># 滤波</span></span><br><span class="line">    result = gaussian_filter(img, sigma=<span class="number">2.5</span>)</span><br><span class="line">    <span class="comment"># 保存结果</span></span><br><span class="line">    cv2.imwrite(<span class="string">&quot;./data/bridge-blur.jpeg&quot;</span>, result)</span><br><span class="line">    <span class="comment"># 显示</span></span><br><span class="line">    cv2.imshow(<span class="string">&quot;bridge&quot;</span>, img)</span><br><span class="line">    cv2.imshow(<span class="string">&quot;bridge-blur&quot;</span>, result)</span><br><span class="line">    cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">    cv2.destroyAllWindows()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>输入（笔者在重庆随手拍的一个桥，叫什么忘记了）：<br><img src="/assets/post_img/article67/bridge.jpeg" alt="bridge"><br>输出：<br><img src="/assets/post_img/article67/bridge-blur.jpeg" alt="blur-bridge"></p><h2 id="双边滤波（Bilateral-filter）"><a href="#双边滤波（Bilateral-filter）" class="headerlink" title="双边滤波（Bilateral filter）"></a>双边滤波（Bilateral filter）</h2><p>开局先甩一个定义：</p><blockquote><p>双边滤波是一种非线性的保边滤波器，通常在计算机视觉中用作工作流中的简单降噪阶段。它将每个输出像素的强度计算为输入图像中附近像素的强度值的加权平均值。至关重要的是，权重不仅取决于当前像素和相邻像素之间的欧几里得距离，还取决于它们之间的辐射差异（例如颜色强度差异）。结果是保留了边缘，同时平滑了具有相似强度的区域。</p></blockquote><p>只看定义，好像懂了，又好像没懂，那么下面就来详细的解释一下。众所周知，图像的噪声和边缘都属于高频分量，高斯滤波在降低噪声的同时没有很好的保留边缘，而是一起“降”了，这对于人眼观看（或者特征提取）就不怎么友好了，形成的图片模糊成了一团。</p><p>这时，有人想了，能不能在降低噪声的同时，保持图像的边缘呢？这就是双边滤波干的事。双边滤波同样是基于模版运算的，他的特点在于<strong>模版的权值同时考虑了空域上距离的差别和灰度值上数值的差别</strong>。 这里后者也可以叫做<strong>灰度距离</strong>，与前者的欧式距离相呼应（没错前者实际上就是欧式距离）。</p><p>回顾高斯滤波中的高斯核，它的权值分配是按照二维高斯图像的形状分配的，这实际上是一种考虑空域上距离差别的方式，远离中心点的像素点会被分配到更少的权值。</p><p>那么为什么要考虑灰度值的差别呢？这是由于<strong>边缘的表现其实是灰度值的突变</strong>，如果想要保留边缘，则不能将高灰度值的区域与低灰度值的区域相“混合”。传统的高斯核无法考虑当前覆盖区域的灰度值信息。</p><p>由此，双边滤波的核函数是<strong>空域核</strong>与<strong>值域（灰度值域）核</strong>的综合：在图像的平坦区域，也就是灰度值变化小的区域，值域核权重趋于$1$，此时空域核权重起主要作用，与高斯模糊类似；在图像的含边缘区域，灰度值变化很大，则值域核权重会增大，从而起到保留边缘的效果。如下图：</p><p><img src="/assets/post_img/article67/bilateral-filter.png" alt="b-f"></p><p>图片左侧输入表示输入图像的灰度值形状，可以明显看到图像左侧整体灰度值低而右侧整体灰度值高，值变化很大，图像中部存在一个断层，这表示该区域属于<strong>含边缘区域</strong>。而断层两侧的部分，也有起起伏伏的小土包，这表示灰度值变化很小的区域，即<strong>平坦区域</strong>。当核移动到图中红色箭头所指位置的正上方时，空域核权重与值域核权重的情况如方框中所示，显然，该位置属于含边缘区域，值域核在形状上也表征为断层。它与空域核结合后的结果就是最终的核形状，即一个被切了一半的高斯图像，这个核的权重也很容易想象，整体上，左侧权重低而右侧权重高，起到了保留边缘的作用。</p><p>以上，应该能够理解双边滤波是如何做的了，那具体怎么做呢？接着往下看。</p><p>双边滤波器中，最终输出像素的值$g$同样依赖于领域像素值的加权组合：</p><script type="math/tex; mode=display">g(i, j)=\frac{\sum_{k, l} f(k, l) w(i, j, k, l)}{\sum_{k, l} w(i, j, k, l)}</script><p>这里，$(i,j)$是待计算位置的坐标，$(k,l)$是领域像素位置坐标，$函数f$表示计算灰度。核权重$w$就是重点了，它取决于空域核权重$s$与值域核权重$r$：</p><script type="math/tex; mode=display">s(i, j, k, l)=\exp \left(-\frac{(i-k)^2+(j-l)^2}{2 \sigma_s^2}\right), \\r(i, j, k, l)=\exp \left(- \frac{(f(i,j)-f(k,l))^2}{2 \sigma_r^2}\right)</script><p>$\sigma_s$ 和 $\sigma_r$ 分别是空域高斯（Spatial Gaussian）和值域高斯（Range Gaussian）的标准差，代表了对图片的滤波程度。$w$取决于$s$和$r$的乘积：</p><script type="math/tex; mode=display">w(i, j, k, l) = \exp \left(-\frac{(i-k)^2+(j-l)^2}{2 \sigma_s^2} - \frac{(f(i,j)-f(k,l))^2}{2 \sigma_r^2}\right)</script><p>根据该权重函数，就可以获得双边滤波的核了，然后再进行模版卷积，归一化处理即可。</p><p>正式的一个公式应该是：</p><script type="math/tex; mode=display">B F[I]_{\mathrm{p}}=\frac{1}{W_{\mathrm{p}}} \sum_{\mathbf{q} \in \mathcal{S}} G_{\sigma_s}(\|\mathbf{p}-\mathbf{q}\|) G_{\sigma_r}\left(\lvert I_{\mathbf{p}}-I_{\mathbf{q}} \rvert \right) I_{\mathbf{q}}</script><p>其中$W_{\mathrm{p}}$是一个归一化因子（normalization factor）：</p><script type="math/tex; mode=display">W_{\mathrm{p}} = \sum_{\mathbf{q} \in \mathcal{S}} G_{\sigma_s}(\|\mathbf{p}-\mathbf{q}\|) G_{\sigma_r}\left(\lvert I_{\mathbf{p}}-I_{\mathbf{q}} \rvert \right)</script><p>这里，$B F[I]_{\mathrm{p}}$为像素$p$滤波后的灰度值；$I_{\mathbf{p}}$为像素$p$原本的灰度值；$\mathrm{p}$则表示像素$p$的空间坐标；$G_{\sigma_s}$和$G_{\sigma_r}$分别为空域高斯和值域高斯；$|\mathbf{p}-\mathbf{q}|$表示取模，即欧式距离；$\lvert I_{\mathbf{p}}-I_{\mathbf{q}} \rvert$为灰度值差的绝对值；像素$q$为领域点；$\mathcal{S}$表示整个核区域内的点。</p><p>这样解读完以后，其实和一开始的公式是一样的。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]<a href="https://zhuanlan.zhihu.com/p/19763358">https://zhuanlan.zhihu.com/p/19763358</a><br>[2]<a href="https://www.zhihu.com/question/21817515/answer/472164871">https://www.zhihu.com/question/21817515/answer/472164871</a><br>[3]<a href="https://blog.csdn.net/jack__linux/article/details/99671469">https://blog.csdn.net/jack__linux/article/details/99671469</a><br>[4]<a href="https://zhuanlan.zhihu.com/p/53071745">https://zhuanlan.zhihu.com/p/53071745</a><br>[5]<a href="https://blog.csdn.net/u010430651/article/details/95340236">https://blog.csdn.net/u010430651/article/details/95340236</a><br>[6]<a href="https://zhuanlan.zhihu.com/p/40301384">https://zhuanlan.zhihu.com/p/40301384</a><br>[7]<a href="https://blog.csdn.net/jack__linux/article/details/99671469">https://blog.csdn.net/jack__linux/article/details/99671469</a><br>[8]<a href="https://blog.csdn.net/Chevy_cxw/article/details/110948262">https://blog.csdn.net/Chevy_cxw/article/details/110948262</a><br>[9]<a href="https://blog.csdn.net/dengheCSDN/article/details/78906126">https://blog.csdn.net/dengheCSDN/article/details/78906126</a><br>[10]<a href="https://www.zhihu.com/question/29246532/answer/1473209132">https://www.zhihu.com/question/29246532/answer/1473209132</a><br>[11]<a href="https://blog.csdn.net/rocketeerLi/article/details/87986751">https://blog.csdn.net/rocketeerLi/article/details/87986751</a><br>[12]<a href="https://blog.csdn.net/weixin_44479045/article/details/104948535">https://blog.csdn.net/weixin_44479045/article/details/104948535</a><br>[13]<a href="https://blog.csdn.net/silence2015/article/details/53789748">https://blog.csdn.net/silence2015/article/details/53789748</a><br>[14]<a href="https://blog.csdn.net/weixin_43135178/article/details/115453145">https://blog.csdn.net/weixin_43135178/article/details/115453145</a><br>[15]<a href="https://blog.csdn.net/jialeheyeshu/article/details/51097860">https://blog.csdn.net/jialeheyeshu/article/details/51097860</a><br>[16]<a href="https://blog.csdn.net/qq_36607894/article/details/92809731">https://blog.csdn.net/qq_36607894/article/details/92809731</a><br>[17]<a href="https://www.jianshu.com/p/fbde7bdb256d">https://www.jianshu.com/p/fbde7bdb256d</a><br>[18]<a href="https://blog.csdn.net/qimingxia/article/details/89111897">https://blog.csdn.net/qimingxia/article/details/89111897</a><br>[19]<a href="https://blog.csdn.net/qq_41603898/article/details/81674987">https://blog.csdn.net/qq_41603898/article/details/81674987</a><br>[20]<a href="https://blog.csdn.net/sunmc1204953974/article/details/50634652">https://blog.csdn.net/sunmc1204953974/article/details/50634652</a><br>[21]<a href="https://www.jianshu.com/p/73e6ccbd8f3f?u_atoken=80aa2d72-8ab8-4b03-893a-a25ecc6f099f&amp;u_asession=01etOGNYlCJWvXBvSQngE-TGud_9aeeQYAvlS2WuwpQOq_wctc1A74a7od4e-upYygX0KNBwm7Lovlpxjd_P_q4JsKWYrT3W_NKPr8w6oU7K8LeXkNNqHvfNEX4gbXYnPLg0pn3tpfEcqG8HZmzd6q3mBkFo3NEHBv0PZUm6pbxQU&amp;u_asig=05D7OEW9nUnnFJ-ggxnAnVvWOUdkfmuhIUCY6TZqOqY-eeonucHFTEdDgyfQgZ-VHHeWSFSyR-Vy_Q8xsXHLJZQp0NfuTWlhk0fCigqVK1DCFbYmyFkNL-jcmynjX_hmMoxm6kbvl94B967SOLkRJntrO6BEKNazId1s3Et_W-l5P9JS7q8ZD7Xtz2Ly-b0kmuyAKRFSVJkkdwVUnyHAIJzbYjwP9YcDA_b__QLeG_TK-M2bdIetoqxc5CZJ9nLRpI6xaDswPo-3_59so9Oh9f1-3h9VXwMyh6PgyDIVSG1W-t741JlKYWli8UYkMs4RIiQKranM7rBKraCitp69DNzOmP87UHXNQfNZeF1yHm4yNyHvomyVImJs8-nyATChPamWspDxyAEEo4kbsryBKb9Q&amp;u_aref=G%2BN03JEAOlDe%2B8ugCiZkCbuRn%2FU%3D">简单易懂的高斯滤波-简书（链接太长了）</a><br>[22]<a href="https://blog.csdn.net/weixin_42985978/article/details/126517088">https://blog.csdn.net/weixin_42985978/article/details/126517088</a><br>[23]<a href="https://blog.csdn.net/blogshinelee/article/details/82734769">https://blog.csdn.net/blogshinelee/article/details/82734769</a><br>[24]<a href="https://blog.csdn.net/lz0499/article/details/54015150">https://blog.csdn.net/lz0499/article/details/54015150</a><br>[25]<a href="https://blog.csdn.net/huachizi/article/details/88951061">https://blog.csdn.net/huachizi/article/details/88951061</a><br>[26]<a href="https://docs.nvidia.com/vpi/algo_bilat_filter.html">https://docs.nvidia.com/vpi/algo_bilat_filter.html</a><br>[27]<a href="http://www.360doc.com/content/17/0306/14/28838452_634420847.shtml">http://www.360doc.com/content/17/0306/14/28838452_634420847.shtml</a><br>[28]<a href="https://people.csail.mit.edu/sparis/bf_course/course_notes.pdf">https://people.csail.mit.edu/sparis/bf_course/course_notes.pdf</a><br>[29]<a href="https://zhuanlan.zhihu.com/p/127023952">https://zhuanlan.zhihu.com/p/127023952</a><br>[30]<a href="https://blog.csdn.net/MoFMan/article/details/77482794">https://blog.csdn.net/MoFMan/article/details/77482794</a><br>[31]<a href="https://zhuanlan.zhihu.com/p/180497579">https://zhuanlan.zhihu.com/p/180497579</a></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;学习一下高斯滤波和双边滤波，及其需要的前置知识，仅记录一下个人理解，先后逻辑比较混乱，参考请谨慎。如有错误请评论区指正，感谢。&lt;/p&gt;
&lt;p&gt;文中图片多数来自网络，我尽量擦去了水印，是为了提升观感，参考文献在文末有注明。本文的出发点，大概是在尽量用直观、快速的方式了解这些抽象概念吧。&lt;br&gt;</summary>
    
    
    
    
    <category term="计算机视觉" scheme="http://silencezheng.top/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>各种“监督”学习区分</title>
    <link href="http://silencezheng.top/2022/10/01/article66/"/>
    <id>http://silencezheng.top/2022/10/01/article66/</id>
    <published>2022-10-01T12:52:54.000Z</published>
    <updated>2022-10-01T12:53:12.630Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>区分一下各种含“监督”的学习，包括无监督、弱监督、半监督、监督、自监督。<br><span id="more"></span></p><h2 id="无监督学习-unsupervised-learning"><a href="#无监督学习-unsupervised-learning" class="headerlink" title="无监督学习(unsupervised learning)"></a>无监督学习(unsupervised learning)</h2><blockquote><p>无监督学习是机器学习的一种方法，没有给定事先标记过的训练示例，自动对输入的资料进行分类或分群。无监督学习的主要运用包含：聚类分析（cluster analysis）、关系规则（association rule）、维度缩减（dimensionality reduce）。</p></blockquote><p>可以根据特点来认识无监督学习：</p><ol><li>无监督学习是没有明确目的的训练方式，你无法提前知道结果是什么。</li><li>无监督学习<strong>不需要给数据打标签</strong>。</li><li>无监督学习几乎无法量化效果如何。</li></ol><p>无监督学习是一种机器学习的训练方式，它本质上是一个统计手段，在没有标签的数据里可以发现潜在的一些结构的一种训练方式。</p><p>无监督学习的应用，比如对用户行为进行分类，筛选异常行为用户等等。</p><h2 id="监督学习-supervised-learning"><a href="#监督学习-supervised-learning" class="headerlink" title="监督学习(supervised learning)"></a>监督学习(supervised learning)</h2><p>已知数据和其一一对应的标签，训练一个智能算法，将输入数据映射到标签的过程。监督学习是最常见的学习问题之一，例如给定一组猪的图片，并作图像级标注分类为猪，用监督学习训练一个算法可以判断新输入的图片是否是猪。</p><p>与无监督学习相比：</p><ol><li>监督学习有明确的训练目的</li><li>监督学习的训练数据必须有标签</li><li>监督学习可以量化效果</li></ol><h2 id="弱监督学习-weakly-supervised-learning"><a href="#弱监督学习-weakly-supervised-learning" class="headerlink" title="弱监督学习(weakly supervised learning)"></a>弱监督学习(weakly supervised learning)</h2><p>已知数据和其一一对应的<strong>弱标签</strong>，训练一个智能算法，将输入数据映射到一组更强的标签的过程。</p><p><strong>标签的强弱指的是标签蕴含的信息量的多少</strong>，比如相对于分割的标签来说，分类的标签就是弱标签。再比如对于弱监督目标检测，就是数据只有图像级标注，比如图片的分类，要求算法获取目标边界框的学习任务。</p><h2 id="半监督学习-semi-supervised-learning"><a href="#半监督学习-semi-supervised-learning" class="headerlink" title="半监督学习(semi-supervised learning)"></a>半监督学习(semi-supervised learning)</h2><p>已知数据和部分数据一一对应的标签，有一部分数据的标签未知，训练一个智能算法，学习已知标签和未知标签的数据，将输入数据映射到标签的过程。半监督通常是一个数据的标注非常困难，比如说医院的检查结果，医生也需要一段时间来判断健康与否，可能只有几组数据知道是健康还是非健康，其他的只有数据不知道是不是健康。那么通过有监督学习和无监督的结合的半监督学习就在这里发挥作用了。</p><p>总之，是在数据标注困难的情况下，<strong>使用少量标注数据和其他未标注数据进行学习</strong>的训练方式。</p><h2 id="自监督学习-self-supervised-learning"><a href="#自监督学习-self-supervised-learning" class="headerlink" title="自监督学习(self-supervised learning)"></a>自监督学习(self-supervised learning)</h2><p>基于监督学习当前的主要瓶颈是 标签生成和标注 的现状，人们提出了一个问题：</p><blockquote><p>我们是否可以通过特定的方式设计任务，即可以从现有图像中生成几乎无限的标签，并以此来学习特征表示？</p></blockquote><p>这道出了自监督学习的理想状态，我们希望同时拥有监督学习的明确性和无监督学习的自由性，<strong>理想的自监督学习能够在给定的无标注数据中自动生成我们需要的标签，并根据该标签和数据进行监督学习。</strong></p><p>自监督学习的核心是<strong>如何给输入数据自动生成标签</strong>。之前的很多工作都是围绕这个核心展开的。一般的套路是：首先提出一个新的自动打标签的<strong>辅助任务</strong>（pretext task），用辅助任务自动生成标签，然后做实验、测性能、发文章。每年都有新的辅助任务被提出来，自监督学习的性能也在不断提高，有的甚至已经接近监督学习的性能。总体上说，或者是提出一种完全新的辅助任务，或者是把多个旧的辅助任务组合到一起作为一个“新”的辅助任务。</p><p>自监督学习的应用，有图像着色、图像超分辨率、图像修补等等，其学习到的特征表示通常可以用于下游任务。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;区分一下各种含“监督”的学习，包括无监督、弱监督、半监督、监督、自监督。&lt;br&gt;</summary>
    
    
    
    
    <category term="机器学习" scheme="http://silencezheng.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>什么是语义分割、实例分割、全景分割</title>
    <link href="http://silencezheng.top/2022/09/29/article65/"/>
    <id>http://silencezheng.top/2022/09/29/article65/</id>
    <published>2022-09-29T13:19:15.000Z</published>
    <updated>2022-09-29T13:20:47.640Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>看见一篇好文，不得不转，<a href="https://zhuanlan.zhihu.com/p/368904941">原地址在这</a>，一文分清语义分割问题。<br><span id="more"></span></p><h2 id="图像分类（image-classification）"><a href="#图像分类（image-classification）" class="headerlink" title="图像分类（image classification）"></a>图像分类（image classification）</h2><p>识别图像中存在的内容，如下图，有人（person）、树（tree）、草地（grass）、天空（sky）。</p><p><img src="/assets/post_img/article65/classification.webp" alt="ic"></p><h2 id="目标检测（object-detection）"><a href="#目标检测（object-detection）" class="headerlink" title="目标检测（object detection）"></a>目标检测（object detection）</h2><p>识别图像中存在的内容和检测其位置，如下图，以识别和检测人（person）为例。</p><p><img src="/assets/post_img/article65/object-detection.webp" alt="od"></p><h2 id="语义分割（semantic-segmentation）"><a href="#语义分割（semantic-segmentation）" class="headerlink" title="语义分割（semantic segmentation）"></a>语义分割（semantic segmentation）</h2><p>对图像中的每个像素打上类别标签，如下图，把图像分为人（红色）、树木（深绿）、草地（浅绿）、天空（蓝色）标签。</p><p><img src="/assets/post_img/article65/semantic-segmentation.webp" alt="ss"></p><h2 id="实例分割（instance-segmentation）"><a href="#实例分割（instance-segmentation）" class="headerlink" title="实例分割（instance segmentation）"></a>实例分割（instance segmentation）</h2><p>目标检测和语义分割的结合，在图像中将目标检测出来（目标检测），然后对每个像素打上标签（语义分割）。对比上图、下图，如以人（person）为目标，语义分割不区分属于相同类别的不同实例（所有人都标为红色），实例分割区分同类的不同实例（使用不同颜色区分不同的人）。</p><p><img src="/assets/post_img/article65/instance-segmentation.webp" alt="is"></p><h2 id="全景分割（panoptic-segmentation）"><a href="#全景分割（panoptic-segmentation）" class="headerlink" title="全景分割（panoptic segmentation）"></a>全景分割（panoptic segmentation）</h2><p>语义分割和实例分割的结合，即要对所有目标都检测出来，又要区分出同个类别中的不同实例。对比上图、下图，实例分割只对图像中的目标（如上图中的人）进行检测和按像素分割，区分不同实例（使用不同颜色），而全景分割是对图中的所有物体包括背景都要进行检测和分割，区分不同实例（使用不同颜色）。</p><p><img src="/assets/post_img/article65/panoptic-segmentation.webp" alt="ps"></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;看见一篇好文，不得不转，&lt;a href=&quot;https://zhuanlan.zhihu.com/p/368904941&quot;&gt;原地址在这&lt;/a&gt;，一文分清语义分割问题。&lt;br&gt;</summary>
    
    
    
    
    <category term="机器学习" scheme="http://silencezheng.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>M系芯片配置TensorFlow环境</title>
    <link href="http://silencezheng.top/2022/09/28/article64/"/>
    <id>http://silencezheng.top/2022/09/28/article64/</id>
    <published>2022-09-28T10:09:13.000Z</published>
    <updated>2022-10-03T03:07:52.462Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>如题，需求MacOS版本12+，主要参考<a href="https://developer.apple.com/metal/tensorflow-plugin/">官方教程</a>。<br><span id="more"></span></p><h2 id="前置"><a href="#前置" class="headerlink" title="前置"></a>前置</h2><p>有一个conda环境，进入。</p><h2 id="安装TensorFlow"><a href="#安装TensorFlow" class="headerlink" title="安装TensorFlow"></a>安装TensorFlow</h2><h3 id="1-安装TensorFlow依赖"><a href="#1-安装TensorFlow依赖" class="headerlink" title="1. 安装TensorFlow依赖"></a>1. 安装TensorFlow依赖</h3><p><code>conda install -c apple tensorflow-deps</code></p><h3 id="2-安装TensorFlow"><a href="#2-安装TensorFlow" class="headerlink" title="2. 安装TensorFlow"></a>2. 安装TensorFlow</h3><p><code>python -m pip install tensorflow-macos</code></p><h3 id="3-安装tensorflow-metal插件"><a href="#3-安装tensorflow-metal插件" class="headerlink" title="3. 安装tensorflow-metal插件"></a>3. 安装tensorflow-metal插件</h3><p><code>python -m pip install tensorflow-metal</code></p><h3 id="4-安装keras"><a href="#4-安装keras" class="headerlink" title="4. 安装keras"></a>4. 安装keras</h3><p><code>python -m pip install keras</code></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;如题，需求MacOS版本12+，主要参考&lt;a href=&quot;https://developer.apple.com/metal/tensorflow-plugin/&quot;&gt;官方教程&lt;/a&gt;。&lt;br&gt;</summary>
    
    
    
    
    <category term="MacOS" scheme="http://silencezheng.top/tags/MacOS/"/>
    
  </entry>
  
  <entry>
    <title>利用VSCode编写LaTeX文件</title>
    <link href="http://silencezheng.top/2022/09/22/article63/"/>
    <id>http://silencezheng.top/2022/09/22/article63/</id>
    <published>2022-09-22T04:00:51.000Z</published>
    <updated>2022-09-22T04:04:26.932Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在MacOS上安装LaTeX环境，并利用VSCode编写LaTeX文档。<br><span id="more"></span></p><h2 id="在MacOS上安装LaTeX环境"><a href="#在MacOS上安装LaTeX环境" class="headerlink" title="在MacOS上安装LaTeX环境"></a>在MacOS上安装LaTeX环境</h2><p>有两种环境可供安装，完整的MacTeX和BasicTeX。</p><p>完整的 MacTeX-2022 安装包 包含四个部分：</p><ul><li>TeX Live 2022：包含超过 4 GB 材料的完整发行版。</li><li>GUI 应用程序：前端、实用程序和少量启动文档。</li><li>Ghostscript 9.55</li><li>Ghostscript 库 libgs，仅由 TeX Live 中的一个程序 dvisvgm 使用。</li></ul><p>BasicTeX 仅包含完整发行版的 TeX Live 片段，即编写 TeX 文档所需的所有标准工具，包括 TeX、LaTeX、pdfTeX、MetaFont、dvips、MetaPost 和 XeTeX。且包含 AMSTeX、拉丁现代字体、用于从 TeX Live 添加和更新包的 TeX Live 管理器以及 SyncTeX。</p><p>BasicTeX 不包含 GUI 程序和 Ghostscript。要在 Mac 上使用 TeX，安装 BasicTeX 和 前端就足够了。BasicTeX不会覆盖整个发行版，它安装在<code>/usr/local/texlive/2022basic</code>中。</p><p>这里选择安装BasicTeX，可以在<a href="https://tug.org/mactex/">这里</a>下载，也可以直接使用<code>brew install basictex</code>安装。</p><p>下载完了以后<code>tlmgr --help</code>有反应证明安装成功，<code>tlmgr</code>是TeX Live中的包和配置管理器，完全独立于操作系统可能提供的任何包管理器。</p><h2 id="TeX-LaTeX-XeLaTeX"><a href="#TeX-LaTeX-XeLaTeX" class="headerlink" title="TeX, LaTeX, XeLaTeX"></a>TeX, LaTeX, XeLaTeX</h2><p>LaTeX 是 TeX 中的一种格式(format) ，是建立在 TeX 基础上的宏语言，每一个 LaTeX 命令实际上最后都会被转换解释成几个甚至上百个TeX 命令。</p><p>XeLaTeX是使用LaTeX的排版引擎，命令下直接使用<code>xelatex ***.tex</code>就会产生对应的PDF文件。</p><h2 id="cls-sty"><a href="#cls-sty" class="headerlink" title=".cls, .sty"></a>.cls, .sty</h2><p><code>.cls</code>和<code>.sty</code>文件都是增加 LaTeX 功能的补足文件。它们在我们排版文章时对应的使用<code>\documentclass&#123;&#125;</code>和<code>\usepackage&#123;&#125;</code>加载，在包内部则对应的使用<code>\LoadClass, \LoadClassWithOptions</code>和<code>\RequirePackage, \RequirePackageWithOptions</code>加载。我们通常将<code>.cls</code>文件称之为类（classes）文件，将<code>.sty</code>文件称之为风格（style）文件或者包（package）。</p><p>虽然它们都可以包含任意的 TeX 和 LaTeX 代码，但它们的使用方式不同。我们必须通过<code>\documentclass&#123;&#125;</code>加载一个类文件，并且在一个 LaTeX 文件中只能出现一次，通常也是第一个出现的命令。而包是一个可选项，它可以根据我们的需求加载任意多个（在开始文档之前）。</p><h3 id="sty文件缺失问题"><a href="#sty文件缺失问题" class="headerlink" title=".sty文件缺失问题"></a>.sty文件缺失问题</h3><p>报错：LaTeX Error: File `xxx.sty‘ not found，则需要安装对应包。</p><p>自动安装：<br><code>sudo tlmgr install xxx</code></p><p>手动安装：<br>1、放置文件到<code>/usr/local/texlive/2022basic/texmf-dist/tex/latex</code>下<br>2、<code>sudo texhash</code><br>3、<code>sudo mktexlsr</code></p><h2 id="tlmgr命令"><a href="#tlmgr命令" class="headerlink" title="tlmgr命令"></a>tlmgr命令</h2><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tlmgr install <span class="symbol">&lt;packagename&gt;</span>     安装宏包</span><br><span class="line">tlmgr <span class="built_in">remove</span> <span class="symbol">&lt;packagename&gt;</span>      移除</span><br><span class="line">tlmgr <span class="keyword">update</span> --<span class="keyword">list</span>         查看所有更新的宏包指令</span><br><span class="line">tlmgr <span class="keyword">update</span> --self --<span class="keyword">all</span>   更新所有需要更新的宏包</span><br></pre></td></tr></table></figure><h2 id="配置VSCode作为前端"><a href="#配置VSCode作为前端" class="headerlink" title="配置VSCode作为前端"></a>配置VSCode作为前端</h2><h3 id="1-环境变量确认"><a href="#1-环境变量确认" class="headerlink" title="1. 环境变量确认"></a>1. 环境变量确认</h3><p>使用brew下载BasicLaTeX的话，先把TeXLive添加到环境变量，在<code>.bash_profile</code>下新增：<code>export PATH=$PATH:/usr/local/texlive/2022basic/bin/universal-darwin</code></p><p>查看是否添加成功：<code>echo $PATH</code></p><h3 id="2-安装插件"><a href="#2-安装插件" class="headerlink" title="2. 安装插件"></a>2. 安装插件</h3><p>1、<code>James-Yu.latex-workshop</code></p><h3 id="3-安装Latexmk"><a href="#3-安装Latexmk" class="headerlink" title="3. 安装Latexmk"></a>3. 安装Latexmk</h3><p>BasicTeX不含Latexmk，但插件<code>James-Yu.latex-workshop</code>需要该Perl脚本支持。 通过<code>sudo tlmgr install latexmk</code>安装。</p><p>注意：如需使用中文需要额外配置。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]<a href="https://blog.csdn.net/tsingke/article/details/105960941">https://blog.csdn.net/tsingke/article/details/105960941</a><br>[2]<a href="https://blog.csdn.net/joey_ro/article/details/123441178">https://blog.csdn.net/joey_ro/article/details/123441178</a><br>[3]<a href="https://zhuanlan.zhihu.com/p/289417922">https://zhuanlan.zhihu.com/p/289417922</a><br>[4]<a href="https://blog.csdn.net/joey_ro/article/details/123387085">https://blog.csdn.net/joey_ro/article/details/123387085</a><br>[5]<a href="https://www.cnblogs.com/ouyangsong/p/9348175.html">https://www.cnblogs.com/ouyangsong/p/9348175.html</a><br>[6]<a href="https://mg.readthedocs.io/latexmk.html">https://mg.readthedocs.io/latexmk.html</a><br>[7]<a href="https://www.jianshu.com/p/4aee83e66ab8">https://www.jianshu.com/p/4aee83e66ab8</a></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;在MacOS上安装LaTeX环境，并利用VSCode编写LaTeX文档。&lt;br&gt;</summary>
    
    
    
    
    <category term="MacOS" scheme="http://silencezheng.top/tags/MacOS/"/>
    
    <category term="VSCode" scheme="http://silencezheng.top/tags/VSCode/"/>
    
    <category term="LaTeX" scheme="http://silencezheng.top/tags/LaTeX/"/>
    
  </entry>
  
  <entry>
    <title>tesseract安装使用</title>
    <link href="http://silencezheng.top/2022/09/13/article62/"/>
    <id>http://silencezheng.top/2022/09/13/article62/</id>
    <published>2022-09-13T04:04:16.000Z</published>
    <updated>2022-09-13T04:06:14.637Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在M1 MacBook上安装使用tesseract5。<br><span id="more"></span></p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p><code>brew install tesseract</code></p><h2 id="添加中文包"><a href="#添加中文包" class="headerlink" title="添加中文包"></a>添加中文包</h2><p>1、 到<a href="https://github.com/tesseract-ocr/tessdata_best">tessdata_best</a>中去下载中文数据集。 也可以是<a href="https://github.com/tesseract-ocr/tessdata">tessdata</a>，更快但不如best精准。</p><p>2、 放置到tesseract的<code>/share/tessdata/</code>目录下，该目录可用<code>brew list tesseract</code>查看。</p><p>3、<code>tesseract 文件名 结果名 -l chi_sim</code>即可识别中文～</p><p>4、 注意待识别图片分辨率不能太低，否则报错<code>Empty page!!</code>。</p><p>中文数据集名字解释：</p><ul><li><p>chi_sim 包含了简化的常用的汉语和英文字符。</p></li><li><p>chi_tra 包含了繁体的常用汉语和英文字符。</p></li><li><p>后带_vert的数据集表示书写方向从上到下。</p></li></ul><h2 id="自有样本训练"><a href="#自有样本训练" class="headerlink" title="自有样本训练"></a>自有样本训练</h2><p>tess2 和 tess3 都可以用<a href="https://github.com/nguyenq/jTessBoxEditor">jTessBoxEditor</a> 训练，但v4之后的LSTM训练该工具不支持了（截至22.09.13）。</p><p>关于如何训练LTSM数据集，目前找到<a href="https://blog.csdn.net/watt/article/details/124099032">一篇文章</a>，没有验证，欢迎讨论。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;在M1 MacBook上安装使用tesseract5。&lt;br&gt;</summary>
    
    
    
    
    <category term="MacOS" scheme="http://silencezheng.top/tags/MacOS/"/>
    
    <category term="Tesseract" scheme="http://silencezheng.top/tags/Tesseract/"/>
    
  </entry>
  
  <entry>
    <title>PycharmCV2无法自动提示解决</title>
    <link href="http://silencezheng.top/2022/09/12/article61/"/>
    <id>http://silencezheng.top/2022/09/12/article61/</id>
    <published>2022-09-12T13:06:15.000Z</published>
    <updated>2022-09-12T13:08:39.711Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>M1芯片 MacBook 上 <strong>Pycharm无法对cv2自动代码提示</strong> 问题解决。</p><span id="more"></span><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>1、找到环境中<code>/Users/YOURNAME/miniforge3/envs/YOURENV/lib/python3.9/site-packages/cv2</code></p><p>2、复制<code>cv2.abi3.so</code>文件，放置到上一级目录中。</p><p>3、最终效果：<code>/Users/YOURNAME/miniforge3/envs/YOURENV/lib/python3.9/site-packages/cv2.abi3.so</code></p><p>4、Boom，解决。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;M1芯片 MacBook 上 &lt;strong&gt;Pycharm无法对cv2自动代码提示&lt;/strong&gt; 问题解决。&lt;/p&gt;</summary>
    
    
    
    
    <category term="MacOS" scheme="http://silencezheng.top/tags/MacOS/"/>
    
    <category term="Pycharm" scheme="http://silencezheng.top/tags/Pycharm/"/>
    
  </entry>
  
  <entry>
    <title>普通人的反向传播理解</title>
    <link href="http://silencezheng.top/2022/09/11/article60/"/>
    <id>http://silencezheng.top/2022/09/11/article60/</id>
    <published>2022-09-11T08:14:14.000Z</published>
    <updated>2022-09-11T09:02:35.535Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>一个普通人尝试理解反向传播…从上至下阅读吧，看看有什么收获。<br><span id="more"></span> </p><p>先说结论：<strong>反向传播用于快速计算神经网络中节点的梯度</strong></p><h2 id="导数（Derivative）"><a href="#导数（Derivative）" class="headerlink" title="导数（Derivative）"></a>导数（Derivative）</h2><p>对函数$f: \mathbb{R} \rightarrow \mathbb{R}$，其输入和输出都是标量。 如果$f$的导数存在，这个极限被定义为：</p><script type="math/tex; mode=display">f'(x) = \lim_{h \rightarrow 0} \frac{f(x+h) - f(x)}{h}.</script><p>如果$f’(a)$存在，则称$f$在$a$处是<strong>可微</strong>（differentiable）的。 如果$f$在一个区间的每一个点上都是可微的，那么该函数在此区间上可微。导数$f’(x)$可以解释为$f(x)$相对于$x$的瞬时（instantaneous）变化率，该变化率基于变化$h$，$h$趋近于$0$。</p><h2 id="微分（Differential）"><a href="#微分（Differential）" class="headerlink" title="微分（Differential）"></a>微分（Differential）</h2><p>给定$y = f(x)$，其中$x$和$y$分别是函数$f$的自变量和因变量，则以下表达式等价：</p><script type="math/tex; mode=display">f'(x) = y' = \frac{dy}{dx} = \frac{df}{dx} = \frac{d}{dx} f(x) = Df(x) = D_x f(x),</script><p>其中符号$\frac{d}{dx}$和$D$是微分运算符。可以使用以下规则来对常见函数求微分：</p><ul><li>$DC = 0$（$C$为常数）</li><li>$Dx^n = nx^{n-1}$（n为任意实数）</li><li>$De^x = e^x$</li><li>$D\ln(x) = 1/x$</li></ul><p>假设函数$f$和$g$可微，$C$为常数，则有如下法则：</p><p>1、 $\frac{d}{dx} [Cf(x)] = C \frac{d}{dx} f(x)$</p><p>2、 $\frac{d}{dx} [f(x) + g(x)] = \frac{d}{dx} f(x) + \frac{d}{dx} g(x)$</p><p>3、 $\frac{d}{dx} [f(x)g(x)] = f(x) \frac{d}{dx} [g(x)] + g(x) \frac{d}{dx} [f(x)]$</p><p>4、 $\frac{d}{dx} \left[\frac{f(x)}{g(x)}\right] = \frac{g(x) \frac{d}{dx} [f(x)] - f(x) \frac{d}{dx} [g(x)]}{[g(x)]^2}$</p><p>这些法则可以便于计算由常见函数组成的函数的微分，注意，<em>复合函数无法通过这些法则微分</em>。</p><h2 id="偏导数（Partial-derivative）"><a href="#偏导数（Partial-derivative）" class="headerlink" title="偏导数（Partial derivative）"></a>偏导数（Partial derivative）</h2><p>就是将微分推广到多元函数，这个应该懂得都懂。</p><p>设$y = f(x_1, x_2, \ldots, x_n)$为具有$n$个变量的函数，$y$关于$x_i$的偏导数为：</p><script type="math/tex; mode=display">\frac{\partial y}{\partial x_i} = \lim_{h \rightarrow 0} \frac{f(x_1, \ldots, x_{i-1}, x_i+h, x_{i+1}, \ldots, x_n) - f(x_1, \ldots, x_i, \ldots, x_n)}{h}.</script><p>计算时，将$x_1, \ldots, x_{i-1}, x_{i+1}, \ldots, x_n$看作常数，直接计算$y$关于$x_i$的导数即可。</p><p>同时也有以下等价表示：</p><script type="math/tex; mode=display">\frac{\partial y}{\partial x_i} = \frac{\partial f}{\partial x_i} = f_{x_i} = f_i = D_i f = D_{x_i} f.</script><h2 id="梯度（Gradient）"><a href="#梯度（Gradient）" class="headerlink" title="梯度（Gradient）"></a>梯度（Gradient）</h2><p><strong>梯度</strong>向量是连结一个多元函数对其所有变量的偏导数后得到的。</p><p>设函数$f:\mathbb{R}^n\rightarrow\mathbb{R}$的输入是一个$n$维向量$\mathbf{x}=[x_1,x_2,\ldots,x_n]^\top$，且输出是一个标量。则函数$f(\mathbf{x})$相对于$x$的梯度为：</p><script type="math/tex; mode=display">\nabla_{\mathbf{x}} f(\mathbf{x}) = \bigg[\frac{\partial f(\mathbf{x})}{\partial x_1}, \frac{\partial f(\mathbf{x})}{\partial x_2}, \ldots, \frac{\partial f(\mathbf{x})}{\partial x_n}\bigg]^\top</script><p>$\nabla_{\mathbf{x}} f(\mathbf{x})$在没有歧义的时候可以被$\nabla f(\mathbf{x})$替换。</p><p>继续假设$x$为$n$维向量，在微分多元函数时有如下法则（注意n和m的位置）：</p><p>1、对所有$\mathbf{A} \in \mathbb{R}^{m \times n}$，有$\nabla_{\mathbf{x}} \mathbf{A} \mathbf{x} = \mathbf{A}^\top$ （用于转置矩阵）</p><p>2、 对所有$\mathbf{A} \in \mathbb{R}^{n \times m}$，有$\nabla_{\mathbf{x}} \mathbf{x}^\top \mathbf{A} = \mathbf{A}$ （抵消）</p><p>3、对所有$\mathbf{A} \in \mathbb{R}^{n \times n}$，有$\nabla_{\mathbf{x}} \mathbf{x}^\top \mathbf{A} \mathbf{x} = (\mathbf{A} + \mathbf{A}^\top)\mathbf{x}$ （方阵）</p><p>4、$\nabla_{\mathbf{x}} |\mathbf{x} |^2 = \nabla_{\mathbf{x}} \mathbf{x}^\top \mathbf{x} = 2\mathbf{x}$ （函数相对于向量$x$的梯度乘以向量$x$范数的平方等于$2x$）</p><p>接第四条，对于任何矩阵$\mathbf{X}$，都有$\nabla_{\mathbf{X}} |\mathbf{X} |_F^2 = 2\mathbf{X}$。</p><h2 id="链式法则（Chain-rule）"><a href="#链式法则（Chain-rule）" class="headerlink" title="链式法则（Chain rule）"></a>链式法则（Chain rule）</h2><p>计算梯度的关键是对多元函数求导，但深度学习中多元函数通常是<em>复合</em>（composite）的，即无法通过常见函数微分法则求导。</p><p><strong>链式法则可以帮助我们对复合多元函数求导。</strong></p><p>设可微分函数$y$有变量$u_1, u_2, \ldots, u_m$，其中每个可微分函数$u_i$都有变量$x_1, x_2, \ldots, x_n$，$y$是$x_1, x_2, \ldots, x_n$的函数。对于任意$i = 1, 2, \ldots, n$，链式法则给出：</p><script type="math/tex; mode=display">\frac{dy}{dx_i} = \frac{dy}{du_1} \frac{du_1}{dx_i} + \frac{dy}{du_2} \frac{du_2}{dx_i} + \cdots + \frac{dy}{du_m} \frac{du_m}{dx_i}</script><h2 id="反向传播（Back-Propagation）"><a href="#反向传播（Back-Propagation）" class="headerlink" title="反向传播（Back Propagation）"></a>反向传播（Back Propagation）</h2><p>现在，依据上方的基础知识，我们已经知道<strong>深度学习优化算法的关键在于梯度，梯度的关键在于求导，而这个导数通常不好求，需要链式法则的参与。</strong></p><p><strong>反向传播提供了一种基于链式法则快速计算任一偏导数的方法</strong>。在深度学习框架中，框架根据我们的模型构建<strong>计算图</strong>（computational graph），计算图用于跟踪数据、操作和组合顺序，正向传播（即由输入到输出进行计算）后框架通过<strong>自动微分</strong>（automatic differentiation）反向传播梯度，跟踪整个计算图，填充关于每个参数的偏导数。</p><p>在此我不准备罗列一堆复杂的数学公式来推导<strong>反向传播</strong>的计算细节（我也不会），而是想基于应用的角度，简单的理解一下反向传播。</p><p>设有一函数$y=2\mathbf{u} + 5$，其中$u = 3\mathbf{x}^{\top}\mathbf{x}$，$y$是关于$x$的函数。</p><p>下面我们来求$y$对$x$的梯度，求梯度本质上就是求偏导。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># requires_grad为x申请存放梯度的内存</span></span><br><span class="line">x = torch.arange(<span class="number">4.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># x的转置乘x == x和x的点积</span></span><br><span class="line">u = <span class="number">3</span> * torch.dot(x, x)</span><br><span class="line"><span class="built_in">print</span>(u)</span><br><span class="line"></span><br><span class="line">y = <span class="number">2</span> * u + <span class="number">5</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">tensor(<span class="number">42.</span>, grad_fn=&lt;MulBackward0&gt;)</span><br><span class="line">tensor(<span class="number">89.</span>, grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line"><span class="literal">None</span></span><br></pre></td></tr></table></figure><p>到这里$x$还没有梯度，因为还没有进行反向传播，下面来计算梯度。 原式$y$对$u$的导数为$2$，$u$对$x$的导数为$6x$，根据链式法则，$y$对$x$的导数为$12x$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">y.backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"><span class="built_in">print</span>(<span class="number">12</span> * x == x.grad)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([ <span class="number">0.</span>, <span class="number">12.</span>, <span class="number">24.</span>, <span class="number">36.</span>])</span><br><span class="line">tensor([<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>])</span><br></pre></td></tr></table></figure><p>验证得，梯度计算正确。 这里存在一个问题，为什么不去展示中间变量$u$的梯度呢？因为出于节省内存（显存）的考虑，pytorch在反向传播的过程中只保留了计算图中的叶子结点的梯度值，而未保留中间节点的梯度。但的确可以通过一些手段获取中间变量的梯度，只是没必要，这里也不做演示了。</p><h2 id="参考文献："><a href="#参考文献：" class="headerlink" title="参考文献："></a>参考文献：</h2><p>[1]<a href="https://zh-v2.d2l.ai/chapter_preliminaries/calculus.html">https://zh-v2.d2l.ai/chapter_preliminaries/calculus.html</a><br>[2]<a href="https://blog.csdn.net/Weary_PJ/article/details/105706318">https://blog.csdn.net/Weary_PJ/article/details/105706318</a><br>[3]<a href="https://blog.csdn.net/weixin_41799019/article/details/117353078">https://blog.csdn.net/weixin_41799019/article/details/117353078</a><br>[4]<a href="https://blog.csdn.net/xierhacker/article/details/53431207">https://blog.csdn.net/xierhacker/article/details/53431207</a></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;一个普通人尝试理解反向传播…从上至下阅读吧，看看有什么收获。&lt;br&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://silencezheng.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="机器学习" scheme="http://silencezheng.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>smartmontools安装使用</title>
    <link href="http://silencezheng.top/2022/09/08/article59/"/>
    <id>http://silencezheng.top/2022/09/08/article59/</id>
    <published>2022-09-08T10:58:22.000Z</published>
    <updated>2022-09-08T11:02:14.025Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>介绍安装和使用磁盘监控工具smartmontools，以Mac为例。<br><span id="more"></span><br>smartmontools软件包包含两个实用程序（<code>smartctl</code> 和 <code>smartd</code>），使用内置在大多数现代 ATA/SATA、SCSI/SAS 和 NVMe 磁盘中的自我监控、分析和报告技术系统 (SMART) 来控制和监控存储系统。在许多情况下，这些工具将提供磁盘降级和故障的高级警告。 Smartmontools 最初源自 Linux smartsuite 包，实际上支持 ATA/SATA、SCSI/SAS 和 NVMe 磁盘以及 SCSI/SAS 磁带设备。它应该可以在任何现代 Linux、FreeBSD、NetBSD、OpenBSD、Darwin (macOS)、Solaris、Windows、Cygwin、OS/2、eComStation 或 QNX 系统上运行。 Smartmontools 也可以从许多不同的 Live CD/DVD 之一运行。</p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p><code>brew install smartmontools</code></p><h2 id="检查硬盘"><a href="#检查硬盘" class="headerlink" title="检查硬盘"></a>检查硬盘</h2><p>方式一（通用）：<br>1、打开磁盘工具，找到设备名，如<code>disk3s1s1</code><br>2、<code>smartctl -a disk3s3s1</code></p><p>方式二（仅内置）：<br><code>smartctl -a disk0</code></p><h2 id="参数解析"><a href="#参数解析" class="headerlink" title="参数解析"></a>参数解析</h2><p><strong>ID1：Critical Warning警告状态</strong><br>RAW数值显示0为正常无警告，1为过热警告，2为闪存介质引起的内部错误导致可靠性降级，3为闪存进入只读状态，4为增强型断电保护功能失效（只针对有该特性的固态硬盘）。</p><p>正常情况下ID1的RAW属性值应为0，当显示为1时代表NVMe固态硬盘已经过热，需要改善散热条件或降低工作负载。属性值为2时应考虑返修或更换新硬盘，当属性值为3时硬盘已经进入只读状态，无法正常工作，应抓紧时间备份其中的数据。家用固态硬盘通常不会配备增强型断电保护（完整断电保护），所以通常该项目不会显示为4。</p><p><strong>ID2：Temperature当前温度（十进制显示）</strong></p><p><strong>ID3：Available Spare可用冗余空间（百分比显示）</strong><br>指示当前固态硬盘可用于替换坏块的保留备用块占出厂备用块总数量的百分比。该数值从出厂时的100%随使用过程降低，直至到零。ID3归零之前就有可能产生不可预料的故障，所以不要等到该项目彻底归零才考虑更换新硬盘。</p><p><strong>ID4：Available Spare Threshold备用空间阈值</strong><br>与ID3相关，当ID3的数值低于ID4所定义的阈值之后，固态硬盘被认为达到极限状态，此时系统可能会发出可靠性警告。该项数值由厂商定义，通常为10%或0%。</p><p><strong>ID5：Percentage Used已使用的写入耐久度（百分比显示）</strong><br>该项显示已产生的写入量占厂商定义总写入寿命的百分比。该项数值为动态显示，计算结果与写入量及固态硬盘的TBW总写入量指标有关。新盘状态下该项目为0%。</p><p><strong>ID6：Data Units Read读取扇区计数（1000）</strong><br>该项数值乘以1000后即为读取的扇区（512Byte）数量统计。</p><p><strong>ID7：Data Units Write写入扇区计数（1000）</strong><br>该项数值乘以1000后即为写入的扇区（512Byte）数量统计。</p><p><strong>ID8：Host Read Commands读取命令计数</strong><br>硬盘生命周期内累计接收到的读取命令数量统计。</p><p><strong>ID9：Host Write Commands写入命令计数</strong><br>硬盘生命周期内累计接收到的写入命令数量统计。</p><p><strong>ID10：Controller Busy Time主控繁忙时间计数</strong><br>该项统计的是主控忙于处理IO命令的时间总和（单位：分钟）。当IO队列有未完成的命令时，主控即处于“忙”的状态。</p><p><strong>ID11：Power Cycles通电次数</strong></p><p><strong>ID12：Power On Hours通电时间</strong></p><p><strong>ID13：Unsafe Shut downs不安全关机次数（异常断电计数）</strong></p><p><strong>ID14：Media and Data Integrity Errors闪存和数据完整性错误</strong></p><p>主控检测到未恢复的数据完整性错误的次数。正常情况下主控不应检测到数据完整性错误（纠错应该在此之前完成），当有不可校正的ECC、CRC校验失败或者LBA标签不匹配错误发生时，该数值会增加。正常情况下ID14应保持为零。</p><p><strong>ID15：Number of Error Information Log Entries错误日志条目计数</strong></p><p>控制器使用期限内，发生的错误信息日志条目的数量统计。正常情况该项目应为零。<br>有时该条目下会有<code>Read 1 entries from Error Information Log failed: GetLogPage failed: system=0x38, sub=0x0, code=745</code>之类的信息提示。</p><p><strong>以下项目</strong>为非标准项，并非所有NVMe SSD都支持显示。<br><strong>ID16：Warning Composite Temperature Time过热警告时间</strong><br><strong>ID17：Critical Composite Temerature Time过热临界温度时间</strong><br><strong>ID18-25：Temperature Sensor X：多个温度传感器（若存在）的读数</strong></p><p>参考：<a href="https://blog.csdn.net/qq_24343177/article/details/122521952">https://blog.csdn.net/qq_24343177/article/details/122521952</a></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;介绍安装和使用磁盘监控工具smartmontools，以Mac为例。&lt;br&gt;</summary>
    
    
    
    
    <category term="MacOS" scheme="http://silencezheng.top/tags/MacOS/"/>
    
  </entry>
  
  <entry>
    <title>现代循环神经网络--《动手学深度学习》笔记0x0A</title>
    <link href="http://silencezheng.top/2022/09/07/article58/"/>
    <id>http://silencezheng.top/2022/09/07/article58/</id>
    <published>2022-09-07T14:41:53.000Z</published>
    <updated>2022-09-07T15:20:50.097Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><p>前一章中介绍了循环神经网络的基础知识，这种网络可以更好地处理序列数据。但对于当今各种各样的序列学习问题，这些技术可能并不够用。</p><p>例如，循环神经网络在实践中一个常见问题是数值不稳定性。尽管我们已经应用了梯度裁剪等技巧来缓解这个问题，但是仍需要通过设计更复杂的序列模型可以进一步处理它。比如两个广泛使用的网络：<em>门控循环单元</em>（gated recurrent units，GRU）和<em>长短期记忆网络</em>（long short-term memory，LSTM）。然后本章将基于一个单向隐藏层来扩展循环神经网络架构，描述具有多个隐藏层的深层架构，并讨论基于前向和后向循环计算的双向设计。现代循环网络经常采用这种扩展。在解释这些循环神经网络的变体时将继续利用上一章中的语言建模问题。<br><span id="more"></span><br>事实上，语言建模只揭示了序列学习能力的冰山一角。在各种序列学习问题中，如自动语音识别、文本到语音转换和机器翻译，输入和输出都是任意长度的序列。为了阐述如何拟合这种类型的数据，我们将以机器翻译为例介绍基于循环神经网络的“编码器－解码器”架构和束搜索，并用它们来生成序列。</p><h3 id="0-1-小结"><a href="#0-1-小结" class="headerlink" title="0.1. 小结"></a>0.1. 小结</h3><ul><li>门控循环神经网络可以更好地捕获时间步距离很长的序列上的依赖关系。</li><li>重置门有助于捕获序列中的短期依赖关系。</li><li>更新门有助于捕获序列中的长期依赖关系。</li><li>重置门打开时，门控循环单元包含基本循环神经网络；更新门打开时，门控循环单元可以跳过子序列。</li><li>长短期记忆网络有三种类型的门：输入门、遗忘门和输出门。</li><li>长短期记忆网络的隐藏层输出包括“隐状态”和“记忆元”。只有隐状态会传递到输出层，而记忆元完全属于内部信息。</li><li>长短期记忆网络可以缓解梯度消失和梯度爆炸。</li><li>在深度循环神经网络中，隐状态的信息被传递到当前层的下一时间步和下一层的当前时间步。</li><li>有许多不同风格的深度循环神经网络， 如长短期记忆网络、门控循环单元、或经典循环神经网络。 这些模型在深度学习框架的高级API中都有涵盖。</li><li>总体而言，深度循环神经网络需要大量的调参（如学习率和修剪） 来确保合适的收敛，模型的初始化也需要谨慎。</li><li>在双向循环神经网络中，每个时间步的隐状态由当前时间步的前后数据同时决定。</li><li>双向循环神经网络与概率图模型中的“前向-后向”算法具有相似性。</li><li>双向循环神经网络主要用于序列编码和给定双向上下文的观测估计。</li><li>由于梯度链更长，因此双向循环神经网络的训练代价非常高</li><li>机器翻译指的是将文本序列从一种语言自动翻译成另一种语言。</li><li>使用单词级词元化时的词表大小，将明显大于使用字符级词元化时的词表大小。为了缓解这一问题，我们可以将低频词元视为相同的未知词元。</li><li>通过截断和填充文本序列，可以保证所有的文本序列都具有相同的长度，以便以小批量的方式加载。</li><li>“编码器－解码器”架构可以将长度可变的序列作为输入和输出，因此适用于机器翻译等序列转换问题。</li><li>编码器将长度可变的序列作为输入，并将其转换为具有固定形状的编码状态。</li><li>解码器将具有固定形状的编码状态映射为长度可变的序列。</li><li>根据“编码器-解码器”架构的设计， 我们可以使用两个循环神经网络来设计一个序列到序列学习的模型。</li><li>在实现编码器和解码器时，我们可以使用多层循环神经网络。</li><li>可以使用屏蔽（mask）来过滤不相关的计算，例如在计算损失时。</li><li>在“编码器－解码器”训练中，强制教学方法将原始输出序列（而非预测结果）输入解码器。</li><li>BLEU是一种常用的评估方法，它通过测量预测序列和标签序列之间的元语法的匹配度来评估预测。</li><li>序列搜索策略包括贪心搜索、穷举搜索和束搜索。</li><li>贪心搜索所选取序列的计算量最小，但精度相对较低。</li><li>穷举搜索所选取序列的精度最高，但计算量最大。</li><li>束搜索通过灵活选择束宽，在正确率和计算代价之间进行权衡。</li></ul><h2 id="1-门控循环单元（GRU）"><a href="#1-门控循环单元（GRU）" class="headerlink" title="1. 门控循环单元（GRU）"></a>1. 门控循环单元（GRU）</h2><p>上一章讨论了如何在循环神经网络中计算梯度，以及矩阵连续乘积可以导致梯度消失或梯度爆炸的问题。下面简单思考一下这种梯度异常在实践中的意义：</p><ul><li>可能会遇到这样的情况：早期观测值对预测所有未来观测值具有非常重要的意义。例如一个极端情况，其中第一个观测值包含一个校验和，目标是在序列的末尾辨别校验和是否正确。在这种情况下，第一个词元的影响至关重要。我们希望有某些机制能够在一个记忆元里存储重要的早期信息。如果没有这样的机制，我们将不得不给这个观测值指定一个非常大的梯度，因为它会影响所有后续的观测值。</li><li>可能会遇到这样的情况：一些词元没有相关的观测值。例如，在对网页内容进行情感分析时，可能有一些辅助HTML代码与网页传达的情绪无关。我们希望有一些机制来<em>跳过</em>隐状态表示中的此类词元。</li><li>可能会遇到这样的情况：序列的各个部分之间存在逻辑中断。例如，书的章节之间可能会有过渡存在，或者证券的熊市和牛市之间可能会有过渡存在。在这种情况下，最好有一种方法来<em>重置</em>内部状态表示。</li></ul><p>学术界已经提出了许多方法来解决这类问题。其中最早的方法是”长短期记忆”（long-short-term memory，LSTM）[<code>Hochreiter.Schmidhuber.1997</code>]。门控循环单元（gated recurrent unit，GRU）[<code>Cho.Van-Merrienboer.Bahdanau.ea.2014</code>]是一个稍微简化的变体，通常能够提供同等的效果，并且计算[<code>Chung.Gulcehre.Cho.ea.2014</code>]的速度明显更快。由于门控循环单元更简单，本章从它开始解读。</p><h3 id="1-1-门控隐状态"><a href="#1-1-门控隐状态" class="headerlink" title="1.1. 门控隐状态"></a>1.1. 门控隐状态</h3><p>门控循环单元与普通的循环神经网络之间的关键区别在于： 前者支持隐状态的门控。 这意味着模型有专门的机制来确定应该何时更新隐状态， 以及应该何时重置隐状态。 这些机制是可学习的，并且能够解决了上面列出的问题。 例如，如果第一个词元非常重要， 模型将学会在第一次观测之后不更新隐状态。 同样，模型也可以学会跳过不相关的临时观测。 最后，模型还将学会在需要的时候重置隐状态。 下面将详细讨论各类门控。</p><h4 id="1-1-1-重置门和更新门"><a href="#1-1-1-重置门和更新门" class="headerlink" title="1.1.1. 重置门和更新门"></a>1.1.1. 重置门和更新门</h4><p>首先介绍<em>重置门</em>（reset gate）和<em>更新门</em>（update gate）。它们被设计成$(0, 1)$区间中的向量，这样就可以进行凸组合。重置门允许我们控制“可能还想记住”的过去状态的数量；更新门将允许我们控制新状态中有多少个是旧状态的副本。</p><p>凸组合<br>: 设向量 $ { x_i }, i=1,2, \ldots, n $, 如有实数 $\lambda_i \geq 0$, 且 $\sum_{i=1}^n \lambda_i=1$, 则称 $\sum_{i=1}^n \lambda_i x_i$ 为向量 $ { x_i } $ 的一个凸组合(凸线性组合)。</p><p>下图描述了门控循环单元中的重置门和更新门的输入，输入是由当前时间步的输入和前一时间步的隐状态给出。两个门的输出是由使用sigmoid激活函数的两个全连接层给出。</p><p><img src="/assets/post_img/article58/gru-1.svg" alt="在门控循环单元模型中计算重置门和更新门"></p><p>来看一下门控循环单元的数学表达。对于给定的时间步$t$，假设输入是一个小批量$\mathbf{X}_t \in \mathbb{R}^{n \times d}$（样本个数$n$，输入个数$d$），上一个时间步的隐状态是$\mathbf{H}_{t-1} \in \mathbb{R}^{n \times h}$（隐藏单元个数$h$）。那么，重置门$\mathbf{R}_t \in \mathbb{R}^{n \times h}$和更新门$\mathbf{Z}_t \in \mathbb{R}^{n \times h}$的计算如下所示：</p><script type="math/tex; mode=display">\begin{aligned}\mathbf{R}_t = \sigma(\mathbf{X}_t \mathbf{W}_{xr} + \mathbf{H}_{t-1} \mathbf{W}_{hr} + \mathbf{b}_r),\\\mathbf{Z}_t = \sigma(\mathbf{X}_t \mathbf{W}_{xz} + \mathbf{H}_{t-1} \mathbf{W}_{hz} + \mathbf{b}_z),\end{aligned}</script><p>其中$\mathbf{W}_{xr}, \mathbf{W}_{xz} \in \mathbb{R}^{d \times h}$和$\mathbf{W}_{hr}, \mathbf{W}_{hz} \in \mathbb{R}^{h \times h}$是权重参数，$\mathbf{b}_r, \mathbf{b}_z \in \mathbb{R}^{1 \times h}$是偏置参数。注意，在求和过程中会触发广播机制。使用sigmoid函数的目的是将输入值转换到区间$(0, 1)$。</p><h4 id="1-1-2-候选隐状态"><a href="#1-1-2-候选隐状态" class="headerlink" title="1.1.2. 候选隐状态"></a>1.1.2. 候选隐状态</h4><p>接下来将重置门$\mathbf{R}_t$与前一章中的常规隐状态更新机制集成，得到在时间步$t$的<em>候选隐状态</em>（candidate hidden state）$\tilde{\mathbf{H}}_t \in \mathbb{R}^{n \times h}$。</p><script type="math/tex; mode=display">\tilde{\mathbf{H}}_t = \tanh(\mathbf{X}_t \mathbf{W}_{xh} + \left(\mathbf{R}_t \odot \mathbf{H}_{t-1}\right) \mathbf{W}_{hh} + \mathbf{b}_h),</script><p>其中$\mathbf{W}_{xh} \in \mathbb{R}^{d \times h}$和$\mathbf{W}_{hh} \in \mathbb{R}^{h \times h}$是权重参数，$\mathbf{b}_h \in \mathbb{R}^{1 \times h}$是偏置项，符号$\odot$是Hadamard积（按元素乘积）运算符。这里使用tanh非线性激活函数来确保候选隐状态中的值保持在区间$(-1, 1)$中。</p><p>与常规隐状态更新机制$\mathbf{H}_t = \phi(\mathbf{X}_t \mathbf{W}_{xh} + \mathbf{H}_{t-1} \mathbf{W}_{hh}  + \mathbf{b}_h)$相比，上式中的$\mathbf{R}_t$和$\mathbf{H}_{t-1}$的元素相乘可以减少以往状态的影响。每当重置门$\mathbf{R}_t$中的项接近$1$时，会恢复一个常规隐状态更新的普通循环神经网络。对于重置门$\mathbf{R}_t$中所有接近$0$的项，候选隐状态是以$\mathbf{X}_t$作为输入的多层感知机的结果。因此，任何预先存在的隐状态都会被<em>重置</em>为默认值。下图说明了应用重置门之后的计算流程。</p><p><img src="/assets/post_img/article58/gru-2.svg" alt="在门控循环单元模型中计算候选隐状态"></p><h4 id="1-1-3-隐状态"><a href="#1-1-3-隐状态" class="headerlink" title="1.1.3. 隐状态"></a>1.1.3. 隐状态</h4><p>上述的计算结果只是候选隐状态，之后仍然需要结合更新门$\mathbf{Z}_t$的效果。这一步确定新的隐状态$\mathbf{H}_t \in \mathbb{R}^{n \times h}$在多大程度上来自旧的状态$\mathbf{H}_{t-1}$和新的候选状态$\tilde{\mathbf{H}}_t$。更新门$\mathbf{Z}_t$仅需要在$\mathbf{H}_{t-1}$和$\tilde{\mathbf{H}}_t$之间进行按元素的凸组合就可以实现这个目标。这就得出了门控循环单元的最终更新公式：</p><script type="math/tex; mode=display">\mathbf{H}_t = \mathbf{Z}_t \odot \mathbf{H}_{t-1}  + (1 - \mathbf{Z}_t) \odot \tilde{\mathbf{H}}_t.</script><p>每当更新门$\mathbf{Z}_t$接近$1$时，模型就倾向只保留旧状态。此时，来自$\mathbf{X}_t$的信息基本上被忽略，从而有效地跳过了依赖链条中的时间步$t$。相反，当$\mathbf{Z}_t$接近$0$时，新的隐状态$\mathbf{H}_t$就会接近候选隐状态$\tilde{\mathbf{H}}_t$。这些设计可以帮助我们处理循环神经网络中的梯度消失问题，并更好地捕获时间步距离很长的序列的依赖关系。例如，如果整个子序列的所有时间步的更新门都接近于$1$，则无论序列的长度如何，在序列起始时间步的旧隐状态都将很容易保留并传递到序列结束。</p><p>下图说明了更新门起作用后的计算流。</p><p><img src="/assets/post_img/article58/gru-3.svg" alt="计算门控循环单元模型中的隐状态"></p><p>总之，门控循环单元具有以下两个显著特征：</p><ul><li>重置门有助于捕获序列中的短期依赖关系；</li><li>更新门有助于捕获序列中的长期依赖关系。</li></ul><h3 id="1-2-从零实现"><a href="#1-2-从零实现" class="headerlink" title="1.2. 从零实现"></a>1.2. 从零实现</h3><p>首先读取时间机器数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br></pre></td></tr></table></figure><h4 id="1-2-1-初始化模型参数"><a href="#1-2-1-初始化模型参数" class="headerlink" title="1.2.1. 初始化模型参数"></a>1.2.1. 初始化模型参数</h4><p>下一步是初始化模型参数。从标准差为$0.01$的高斯分布中提取权重，并将偏置项设为$0$，超参数<code>num_hiddens</code>定义隐藏单元的数量，实例化与更新门、重置门、候选隐状态和输出层相关的所有权重和偏置。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_params</span>(<span class="params">vocab_size, num_hiddens, device</span>):</span></span><br><span class="line">    num_inputs = num_outputs = vocab_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">normal</span>(<span class="params">shape</span>):</span></span><br><span class="line">        <span class="keyword">return</span> torch.randn(size=shape, device=device)*<span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">three</span>():</span></span><br><span class="line">        <span class="keyword">return</span> (normal((num_inputs, num_hiddens)),</span><br><span class="line">                normal((num_hiddens, num_hiddens)),</span><br><span class="line">                torch.zeros(num_hiddens, device=device))</span><br><span class="line"></span><br><span class="line">    W_xz, W_hz, b_z = three()  <span class="comment"># 更新门参数</span></span><br><span class="line">    W_xr, W_hr, b_r = three()  <span class="comment"># 重置门参数</span></span><br><span class="line">    W_xh, W_hh, b_h = three()  <span class="comment"># 候选隐状态参数</span></span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = normal((num_hiddens, num_outputs))</span><br><span class="line">    b_q = torch.zeros(num_outputs, device=device)</span><br><span class="line">    <span class="comment"># 附加梯度</span></span><br><span class="line">    params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q]</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> params</span><br></pre></td></tr></table></figure><h4 id="1-2-2-定义模型"><a href="#1-2-2-定义模型" class="headerlink" title="1.2.2. 定义模型"></a>1.2.2. 定义模型</h4><p>现在定义隐状态的初始化函数<code>init_gru_state</code>。与上一章中从零实现RNN中定义的<code>init_rnn_state</code>函数一样，此函数返回一个形状为（批量大小，隐藏单元个数）的张量，张量的值全部为零。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_gru_state</span>(<span class="params">batch_size, num_hiddens, device</span>):</span></span><br><span class="line">    <span class="keyword">return</span> (torch.zeros((batch_size, num_hiddens), device=device), )</span><br></pre></td></tr></table></figure><p>现在定义门控循环单元模型，模型的架构与基本的循环神经网络单元是相同的，只是权重更新公式更为复杂。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gru</span>(<span class="params">inputs, state, params</span>):</span></span><br><span class="line">    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class="line">    H, = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        Z = torch.sigmoid((X @ W_xz) + (H @ W_hz) + b_z)</span><br><span class="line">        R = torch.sigmoid((X @ W_xr) + (H @ W_hr) + b_r)</span><br><span class="line">        H_tilda = torch.tanh((X @ W_xh) + ((R * H) @ W_hh) + b_h)</span><br><span class="line">        H = Z * H + (<span class="number">1</span> - Z) * H_tilda</span><br><span class="line">        Y = H @ W_hq + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">0</span>), (H,)</span><br></pre></td></tr></table></figure><h4 id="1-2-3-训练与预测"><a href="#1-2-3-训练与预测" class="headerlink" title="1.2.3. 训练与预测"></a>1.2.3. 训练与预测</h4><p>训练和预测的工作方式与上一章完全相同。 训练结束后，分别打印输出训练集的困惑度， 以及前缀“time traveler”和“traveler”的预测序列上的困惑度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vocab_size, num_hiddens, device = <span class="built_in">len</span>(vocab), <span class="number">256</span>, d2l.try_gpu()</span><br><span class="line">num_epochs, lr = <span class="number">500</span>, <span class="number">1</span></span><br><span class="line">model = d2l.RNNModelScratch(<span class="built_in">len</span>(vocab), num_hiddens, device, get_params,</span><br><span class="line">                            init_gru_state, gru)</span><br><span class="line">d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br></pre></td></tr></table></figure><h3 id="1-3-框架实现"><a href="#1-3-框架实现" class="headerlink" title="1.3. 框架实现"></a>1.3. 框架实现</h3><p>高级API包含了前文介绍的所有配置细节， 所以可以直接实例化门控循环单元模型。 这段代码的运行速度要快得多， 因为它使用的是编译好的运算符而不是Python来处理之前阐述的许多细节。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">num_inputs = vocab_size</span><br><span class="line">gru_layer = nn.GRU(num_inputs, num_hiddens)</span><br><span class="line">model = d2l.RNNModel(gru_layer, <span class="built_in">len</span>(vocab))</span><br><span class="line">model = model.to(device)</span><br><span class="line">d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br></pre></td></tr></table></figure><h2 id="2-长短期记忆网络（LSTM）"><a href="#2-长短期记忆网络（LSTM）" class="headerlink" title="2. 长短期记忆网络（LSTM）"></a>2. 长短期记忆网络（LSTM）</h2><p>长期以来，隐变量模型存在着长期信息保存和短期输入缺失的问题。解决这一问题的最早方法之一是长短期存储器（long short-term memory，LSTM）[<code>Hochreiter.Schmidhuber.1997</code>]。它有许多与门控循环单元一样的属性。长短期记忆网络的设计比门控循环单元稍微复杂一些，却比门控循环单元早诞生了近20年。</p><h3 id="2-1-门控记忆元"><a href="#2-1-门控记忆元" class="headerlink" title="2.1. 门控记忆元"></a>2.1. 门控记忆元</h3><p>长短期记忆网络的设计灵感来自于计算机的逻辑门。长短期记忆网络引入了<em>记忆元</em>（memory cell），或简称为<em>单元</em>（cell）。有些文献认为记忆元是隐状态的一种特殊类型，它们与隐状态具有相同的形状，其设计目的是用于记录附加的信息。为了控制记忆元，我们需要许多门。其中一个门用来从单元中输出条目，称其为<em>输出门</em>（output gate）。另外一个门用来决定何时将数据读入单元，称其为<em>输入门</em>（input gate）。还需要一种机制来重置单元的内容，由<em>遗忘门</em>（forget gate）来管理，这种设计的动机与门控循环单元相同，能够通过专用机制决定什么时候记忆或忽略隐状态中的输入。下面看看这在实践中是如何运作的。</p><h4 id="2-1-1-输入门、忘记门和输出门"><a href="#2-1-1-输入门、忘记门和输出门" class="headerlink" title="2.1.1. 输入门、忘记门和输出门"></a>2.1.1. 输入门、忘记门和输出门</h4><p>就如在门控循环单元中一样，当前时间步的输入和前一个时间步的隐状态作为数据送入长短期记忆网络的门中，如下图所示。它们由三个具有sigmoid激活函数的全连接层处理，以计算输入门、遗忘门和输出门的值。因此，这三个门的值都在$(0, 1)$的范围内。</p><p><img src="/assets/post_img/article58/lstm-0.svg" alt="长短期记忆模型中的输入门、遗忘门和输出门"></p><p>详细了解一下长短期记忆网络的数学表达。假设有$h$个隐藏单元，批量大小为$n$，输入数为$d$。因此，输入为$\mathbf{X}_t \in \mathbb{R}^{n \times d}$，前一时间步的隐状态为$\mathbf{H}_{t-1} \in \mathbb{R}^{n \times h}$。相应地，时间步$t$的门被定义如下：输入门是$\mathbf{I}_t \in \mathbb{R}^{n \times h}$，遗忘门是$\mathbf{F}_t \in \mathbb{R}^{n \times h}$，输出门是$\mathbf{O}_t \in \mathbb{R}^{n \times h}$。它们的计算方法如下：</p><script type="math/tex; mode=display">\begin{aligned}\mathbf{I}_t &= \sigma(\mathbf{X}_t \mathbf{W}_{xi} + \mathbf{H}_{t-1} \mathbf{W}_{hi} + \mathbf{b}_i),\\\mathbf{F}_t &= \sigma(\mathbf{X}_t \mathbf{W}_{xf} + \mathbf{H}_{t-1} \mathbf{W}_{hf} + \mathbf{b}_f),\\\mathbf{O}_t &= \sigma(\mathbf{X}_t \mathbf{W}_{xo} + \mathbf{H}_{t-1} \mathbf{W}_{ho} + \mathbf{b}_o),\end{aligned}</script><p>其中$\mathbf{W}_{xi}, \mathbf{W}_{xf}, \mathbf{W}_{xo} \in \mathbb{R}^{d \times h}$和$\mathbf{W}_{hi}, \mathbf{W}_{hf}, \mathbf{W}_{ho} \in \mathbb{R}^{h \times h}$是权重参数，$\mathbf{b}_i, \mathbf{b}_f, \mathbf{b}_o \in \mathbb{R}^{1 \times h}$是偏置参数。</p><h4 id="2-1-2-候选记忆元"><a href="#2-1-2-候选记忆元" class="headerlink" title="2.1.2. 候选记忆元"></a>2.1.2. 候选记忆元</h4><p>由于还没有指定各种门的操作，所以先介绍<em>候选记忆元</em>（candidate memory cell）$\tilde{\mathbf{C}}_t \in \mathbb{R}^{n \times h}$。它的计算与上面描述的三个门的计算类似，但是使用$\tanh$函数作为激活函数，函数的值范围为$(-1, 1)$。下面导出在时间步$t$处的方程：</p><script type="math/tex; mode=display">\tilde{\mathbf{C}}_t = \text{tanh}(\mathbf{X}_t \mathbf{W}_{xc} + \mathbf{H}_{t-1} \mathbf{W}_{hc} + \mathbf{b}_c),</script><p>其中$\mathbf{W}_{xc} \in \mathbb{R}^{d \times h}$和<br>$\mathbf{W}_{hc} \in \mathbb{R}^{h \times h}$是权重参数，$\mathbf{b}_c \in \mathbb{R}^{1 \times h}$是偏置参数。如下图所示：</p><p><img src="/assets/post_img/article58/lstm-1.svg" alt="长短期记忆模型中的候选记忆元"></p><h4 id="2-1-3-记忆元"><a href="#2-1-3-记忆元" class="headerlink" title="2.1.3. 记忆元"></a>2.1.3. 记忆元</h4><p>在门控循环单元中，有一种机制来控制输入和遗忘（或称跳过）。类似地，在长短期记忆网络中，也有两个门用于这样的目的：输入门$\mathbf{I}_t$控制采用多少来自$\tilde{\mathbf{C}}_t$的新数据，而遗忘门$\mathbf{F}_t$控制保留多少过去的记忆元$\mathbf{C}_{t-1} \in \mathbb{R}^{n \times h}$的内容。使用按元素乘法，得出：</p><script type="math/tex; mode=display">\mathbf{C}_t = \mathbf{F}_t \odot \mathbf{C}_{t-1} + \mathbf{I}_t \odot \tilde{\mathbf{C}}_t.</script><p>如果遗忘门始终为$1$且输入门始终为$0$，则过去的记忆元$\mathbf{C}_{t-1}$将随时间被保存并传递到当前时间步。引入这种设计是为了缓解梯度消失问题，并更好地捕获序列中的长距离依赖关系。<br>这样就得到了计算记忆元的流程图，如下图。</p><p><img src="/assets/post_img/article58/lstm-2.svg" alt="在长短期记忆网络模型中计算记忆元"></p><h4 id="2-1-4-隐状态"><a href="#2-1-4-隐状态" class="headerlink" title="2.1.4. 隐状态"></a>2.1.4. 隐状态</h4><p>最后需要定义如何计算隐状态$\mathbf{H}_t \in \mathbb{R}^{n \times h}$，这就是输出门发挥作用的地方。在长短期记忆网络中，它仅仅是记忆元的$\tanh$的门控版本。这就确保了$\mathbf{H}_t$的值始终在区间$(-1, 1)$内（因为输出门在0和1之间）：</p><script type="math/tex; mode=display">\mathbf{H}_t = \mathbf{O}_t \odot \tanh(\mathbf{C}_t).</script><p>只要输出门接近$1$，就能够有效地将所有记忆信息传递给预测部分，而对于输出门接近$0$，则只保留记忆元内的所有信息，而不需要更新隐状态。<br>下图提供了数据流的图形化演示。</p><p><img src="/assets/post_img/article58/lstm-3.svg" alt="在长短期记忆模型中计算隐状态"></p><h3 id="2-2-从零实现"><a href="#2-2-从零实现" class="headerlink" title="2.2. 从零实现"></a>2.2. 从零实现</h3><p>首先加载时光机器数据集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br></pre></td></tr></table></figure><h4 id="2-2-1-初始化模型参数"><a href="#2-2-1-初始化模型参数" class="headerlink" title="2.2.1. 初始化模型参数"></a>2.2.1. 初始化模型参数</h4><p>接下来需要定义和初始化模型参数。如前所述，超参数<code>num_hiddens</code>定义隐藏单元的数量。按照标准差$0.01$的高斯分布初始化权重，并将偏置项设为$0$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_lstm_params</span>(<span class="params">vocab_size, num_hiddens, device</span>):</span></span><br><span class="line">    num_inputs = num_outputs = vocab_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">normal</span>(<span class="params">shape</span>):</span></span><br><span class="line">        <span class="keyword">return</span> torch.randn(size=shape, device=device)*<span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">three</span>():</span></span><br><span class="line">        <span class="keyword">return</span> (normal((num_inputs, num_hiddens)),</span><br><span class="line">                normal((num_hiddens, num_hiddens)),</span><br><span class="line">                torch.zeros(num_hiddens, device=device))</span><br><span class="line"></span><br><span class="line">    W_xi, W_hi, b_i = three()  <span class="comment"># 输入门参数</span></span><br><span class="line">    W_xf, W_hf, b_f = three()  <span class="comment"># 遗忘门参数</span></span><br><span class="line">    W_xo, W_ho, b_o = three()  <span class="comment"># 输出门参数</span></span><br><span class="line">    W_xc, W_hc, b_c = three()  <span class="comment"># 候选记忆元参数</span></span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = normal((num_hiddens, num_outputs))</span><br><span class="line">    b_q = torch.zeros(num_outputs, device=device)</span><br><span class="line">    <span class="comment"># 附加梯度</span></span><br><span class="line">    params = [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc,</span><br><span class="line">              b_c, W_hq, b_q]</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> params</span><br></pre></td></tr></table></figure><h4 id="2-2-2-定义模型"><a href="#2-2-2-定义模型" class="headerlink" title="2.2.2. 定义模型"></a>2.2.2. 定义模型</h4><p>在初始化函数中，长短期记忆网络的隐状态需要返回一个<em>额外</em>的记忆元，单元的值为0，形状为（批量大小，隐藏单元数）。因此得到以下的状态初始化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_lstm_state</span>(<span class="params">batch_size, num_hiddens, device</span>):</span></span><br><span class="line">    <span class="keyword">return</span> (torch.zeros((batch_size, num_hiddens), device=device),</span><br><span class="line">            torch.zeros((batch_size, num_hiddens), device=device))</span><br></pre></td></tr></table></figure><p>实际模型的定义与前面讨论的一样：提供三个门和一个额外的记忆元。注意只有隐状态才会传递到输出层，而记忆元$\mathbf{C}_t$不直接参与输出计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm</span>(<span class="params">inputs, state, params</span>):</span></span><br><span class="line">    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c,</span><br><span class="line">     W_hq, b_q] = params</span><br><span class="line">    (H, C) = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        I = torch.sigmoid((X @ W_xi) + (H @ W_hi) + b_i)</span><br><span class="line">        F = torch.sigmoid((X @ W_xf) + (H @ W_hf) + b_f)</span><br><span class="line">        O = torch.sigmoid((X @ W_xo) + (H @ W_ho) + b_o)</span><br><span class="line">        C_tilda = torch.tanh((X @ W_xc) + (H @ W_hc) + b_c)</span><br><span class="line">        C = F * C + I * C_tilda</span><br><span class="line">        H = O * torch.tanh(C)</span><br><span class="line">        Y = (H @ W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">0</span>), (H, C)</span><br></pre></td></tr></table></figure><h4 id="2-2-3-训练和预测"><a href="#2-2-3-训练和预测" class="headerlink" title="2.2.3. 训练和预测"></a>2.2.3. 训练和预测</h4><p>通过实例化上一章引入的RNNModelScratch类来训练一个长短期记忆网络，就如在上一节中所做的一样。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vocab_size, num_hiddens, device = <span class="built_in">len</span>(vocab), <span class="number">256</span>, d2l.try_gpu()</span><br><span class="line">num_epochs, lr = <span class="number">500</span>, <span class="number">1</span></span><br><span class="line">model = d2l.RNNModelScratch(<span class="built_in">len</span>(vocab), num_hiddens, device, get_lstm_params,</span><br><span class="line">                            init_lstm_state, lstm)</span><br><span class="line">d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br></pre></td></tr></table></figure><h3 id="2-3-框架实现"><a href="#2-3-框架实现" class="headerlink" title="2.3. 框架实现"></a>2.3. 框架实现</h3><p>使用高级API可以直接实例化LSTM模型。 高级API封装了前文介绍的所有配置细节。 这段代码的运行速度要快得多， 因为它使用的是编译好的运算符而不是Python来处理之前阐述的许多细节。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">num_inputs = vocab_size</span><br><span class="line">lstm_layer = nn.LSTM(num_inputs, num_hiddens)</span><br><span class="line">model = d2l.RNNModel(lstm_layer, <span class="built_in">len</span>(vocab))</span><br><span class="line">model = model.to(device)</span><br><span class="line">d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br></pre></td></tr></table></figure><p>长短期记忆网络是典型的具有重要状态控制的隐变量自回归模型。 多年来已经提出了其许多变体，例如，多层、残差连接、不同类型的正则化。 然而，由于序列的长距离依赖性，训练长短期记忆网络和其他序列模型（例如门控循环单元）的成本是相当高的。 后面的内容中将讲述更高级的替代模型，如transformer。</p><h2 id="3-深度循环神经网络"><a href="#3-深度循环神经网络" class="headerlink" title="3. 深度循环神经网络"></a>3. 深度循环神经网络</h2><p>到目前为止，我们一直专注于定义由序列输入、单个隐藏 RNN 层和输出层组成的网络。尽管在任何时间步长的输入和相应的输出之间只有一个隐藏层，但这些网络在某种意义上是很深的。第一个时间步的输入可以影响最后一个时间步$T$的输出 （通常是 100 或 1000 步之后）。这些输入通过长度为$T$的时间距离，在产生最终输出之前一直应用了<em>循环层</em>。但通常人们也希望保留在某时间步长上的输入与输出之间表达复杂关系的能力。因此，我们经常构建不仅在时间方向上而且在输入到输出方向上都很深的 RNN。这正是在开发 MLP 和深度 CNN 时已经遇到的深度概念。</p><p>构建这种深度 RNN 的标准方法非常简单：可以将多层循环神经网络堆叠在一起，给定一个长度序列$T$，第一个 RNN 产生一系列输出，长度也一样为$T$. 这些又构成了下一个 RNN 层的输入。通过对几个简单层的组合，产生了一个灵活的机制。特别的是数据可能与不同层的堆叠有关。例如人们可能希望保持有关金融市场状况（熊市或牛市）的宏观数据可用，而微观数据只记录较短期的时间动态。</p><p>下图描述了一个具有$L$个隐藏层的深度循环神经网络，每个隐状态都连续地传递到当前层的下一个时间步和下一层的当前时间步。</p><p><img src="/assets/post_img/article58/deep-rnn.svg" alt="深度循环神经网络结构"></p><h3 id="3-1-函数依赖关系"><a href="#3-1-函数依赖关系" class="headerlink" title="3.1. 函数依赖关系"></a>3.1. 函数依赖关系</h3><p>可以将深度架构中的函数依赖关系形式化，这个架构是由上图中描述的$L$个隐藏层构成。后续的讨论主要集中在经典的循环神经网络模型上，但是这些讨论也适应于其他序列模型。</p><p>假设在时间步$t$有一个小批量的输入数据$\mathbf{X}_t \in \mathbb{R}^{n \times d}$（样本数：$n$，每个样本中的输入数：$d$）。同时，将$l^\mathrm{th}$隐藏层（$l=1,\ldots,L$）的隐状态设为$\mathbf{H}_t^{(l)}  \in \mathbb{R}^{n \times h}$（隐藏单元数：$h$），输出层变量设为$\mathbf{O}_t \in \mathbb{R}^{n \times q}$（输出数：$q$）。设置$\mathbf{H}_t^{(0)} = \mathbf{X}_t$，第$l$个隐藏层的隐状态使用激活函数$\phi_l$，则：</p><script type="math/tex; mode=display">\mathbf{H}_t^{(l)} = \phi_l(\mathbf{H}_t^{(l-1)} \mathbf{W}_{xh}^{(l)} + \mathbf{H}_{t-1}^{(l)} \mathbf{W}_{hh}^{(l)}  + \mathbf{b}_h^{(l)}),</script><p>其中，权重$\mathbf{W}_{xh}^{(l)} \in \mathbb{R}^{h \times h}$，$\mathbf{W}_{hh}^{(l)} \in \mathbb{R}^{h \times h}$和偏置$\mathbf{b}_h^{(l)} \in \mathbb{R}^{1 \times h}$都是第$l$个隐藏层的模型参数。</p><p>最后，输出层的计算仅基于第$l$个隐藏层最终的隐状态：</p><script type="math/tex; mode=display">\mathbf{O}_t = \mathbf{H}_t^{(L)} \mathbf{W}_{hq} + \mathbf{b}_q,</script><p>其中，权重$\mathbf{W}_{hq} \in \mathbb{R}^{h \times q}$和偏置$\mathbf{b}_q \in \mathbb{R}^{1 \times q}$都是输出层的模型参数。</p><p>与多层感知机一样，隐藏层数目$L$和隐藏单元数目$h$都是超参数，它们可以被人为调整。另外，用门控循环单元或长短期记忆网络的隐状态来代替深度循环网络中的隐状态进行计算，可以得到深度门控循环神经网络或深度长短期记忆神经网络。</p><h3 id="3-2-框架实现"><a href="#3-2-框架实现" class="headerlink" title="3.2. 框架实现"></a>3.2. 框架实现</h3><p>实现多层循环神经网络所需的许多逻辑细节在高级API中都是现成的。 简单起见这里仅示范使用此类内置函数的使用方式。 以长短期记忆网络模型为例， 该代码与上节中使用的代码非常相似，唯一的区别是我们指定了层的数量，而不是使用单一层这个默认值。从加载数据集开始。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br></pre></td></tr></table></figure><p>像选择超参数这类架构决策也跟上节中的决策非常相似。因为我们有不同的词元，所以输入和输出都选择相同数量（why？），即<code>vocab_size</code>。隐藏单元的数量仍然是$256$。唯一的区别是现在通过<code>num_layers</code>的值来设定隐藏层数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vocab_size, num_hiddens, num_layers = <span class="built_in">len</span>(vocab), <span class="number">256</span>, <span class="number">2</span></span><br><span class="line">num_inputs = vocab_size</span><br><span class="line">device = d2l.try_gpu()</span><br><span class="line">lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers)</span><br><span class="line">model = d2l.RNNModel(lstm_layer, <span class="built_in">len</span>(vocab))</span><br><span class="line">model = model.to(device)</span><br></pre></td></tr></table></figure><h3 id="3-3-训练与预测"><a href="#3-3-训练与预测" class="headerlink" title="3.3. 训练与预测"></a>3.3. 训练与预测</h3><p>由于使用了长短期记忆网络模型来实例化两个层，因此训练速度被大大降低了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, lr = <span class="number">500</span>, <span class="number">2</span></span><br><span class="line">d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br></pre></td></tr></table></figure><h2 id="4-双向循环神经网络（Bidirectional-RNN）"><a href="#4-双向循环神经网络（Bidirectional-RNN）" class="headerlink" title="4. 双向循环神经网络（Bidirectional RNN）"></a>4. 双向循环神经网络（Bidirectional RNN）</h2><p>在序列学习中，以往假设的目标是：在给定观测的情况下（例如，在时间序列的上下文中或在语言模型的上下文中），对下一个输出进行建模。虽然这是一个典型情景，但不是唯一的。还可能发生什么其它的情况呢？考虑以下三个在文本序列中填空的任务。</p><ul><li>我<code>___</code>。</li><li>我<code>___</code>饿了。</li><li>我<code>___</code>饿了，我可以吃半头猪。</li></ul><p>根据可获得的信息量，可以用不同的词填空，如“很高兴”（”happy”）、“不”（”not”）和“非常”（”very”）。很明显，每个短语的“下文”传达了重要信息（如果有的话），而这些信息关乎到选择哪个词来填空，所以无法利用这一点的序列模型将在相关任务上表现不佳。例如，如果要做好命名实体识别（例如，识别“Green”指的是“格林先生”还是绿色），不同长度的上下文范围重要性是相同的。为了获得一些解决问题的灵感，让我们先迂回到概率图模型。</p><h3 id="4-1-隐马尔可夫模型中的动态规划"><a href="#4-1-隐马尔可夫模型中的动态规划" class="headerlink" title="4.1. 隐马尔可夫模型中的动态规划"></a>4.1. 隐马尔可夫模型中的动态规划</h3><p>这一小节是用来说明动态规划问题的，具体的技术细节对于理解深度学习模型并不重要，但有助于思考为什么要使用深度学习，以及为什么要选择特定的架构。</p><p>如果想用概率图模型来解决这个问题，可以设计一个隐变量模型：在任意时间步$t$，假设存在某个隐变量$h_t$，通过概率$P(x_t \mid h_t)$控制我们观测到的$x_t$。此外，任何$h_t \to h_{t+1}$转移都是由一些状态转移概率$P(h_{t+1} \mid h_{t})$给出。这个概率图模型就是一个<em>隐马尔可夫模型</em>（hidden Markov model，HMM），如下图所示。</p><p><img src="/assets/post_img/article58/hmm.svg" alt="隐马尔可夫模型"></p><p>因此，对于有$T$个观测值的序列，在观测状态和隐状态上具有以下联合概率分布：</p><script type="math/tex; mode=display">P(x_1, \ldots, x_T, h_1, \ldots, h_T) = \prod_{t=1}^T P(h_t \mid h_{t-1}) P(x_t \mid h_t), \text{ where } P(h_1 \mid h_0) = P(h_1).</script><p>现在假设观测到了所有的$x_i$，除了$x_j$，并且我们的目标是计算$P(x_j \mid x_{-j})$，其中$x_{-j} = (x_1, \ldots, x_{j-1}, x_{j+1}, \ldots, x_{T})$。由于$P(x_j \mid x_{-j})$中没有隐变量，因此我们考虑对$h_1, \ldots, h_T$选择构成的所有可能的组合进行求和。如果任何$h_i$可以接受$k$个不同的值（有限的状态数），则意味着需要对$k^T$个项求和，这个任务显然难于登天。幸运的是，有个巧妙的解决方案：<em>动态规划</em>（dynamic programming）。</p><p>为了解动态规划的工作方式，考虑对隐变量$h_1, \ldots, h_T$的依次求和。根据上式，将得出：</p><script type="math/tex; mode=display">\begin{aligned}    &P(x_1, \ldots, x_T) \\    =& \sum_{h_1, \ldots, h_T} P(x_1, \ldots, x_T, h_1, \ldots, h_T) \\    =& \sum_{h_1, \ldots, h_T} \prod_{t=1}^T P(h_t \mid h_{t-1}) P(x_t \mid h_t) \\    =& \sum_{h_2, \ldots, h_T} \underbrace{\left[\sum_{h_1} P(h_1) P(x_1 \mid h_1) P(h_2 \mid h_1)\right]}_{\pi_2(h_2) \stackrel{\mathrm{def}}{=}}    P(x_2 \mid h_2) \prod_{t=3}^T P(h_t \mid h_{t-1}) P(x_t \mid h_t) \\    =& \sum_{h_3, \ldots, h_T} \underbrace{\left[\sum_{h_2} \pi_2(h_2) P(x_2 \mid h_2) P(h_3 \mid h_2)\right]}_{\pi_3(h_3)\stackrel{\mathrm{def}}{=}}    P(x_3 \mid h_3) \prod_{t=4}^T P(h_t \mid h_{t-1}) P(x_t \mid h_t)\\    =& \dots \\    =& \sum_{h_T} \pi_T(h_T) P(x_T \mid h_T).\end{aligned}</script><p>通常我们将<em>前向递归</em>（forward recursion）写为：</p><script type="math/tex; mode=display">\pi_{t+1}(h_{t+1}) = \sum_{h_t} \pi_t(h_t) P(x_t \mid h_t) P(h_{t+1} \mid h_t).</script><p>递归被初始化为$\pi_1(h_1) = P(h_1)$。符号简化，也可以写成$\pi_{t+1} = f(\pi_t, x_t)$，其中$f$是一些可学习的函数。这看起来就像循环神经网络中讨论的隐变量模型中的更新方程。</p><p>与前向递归（或称正向）一样，也可以使用后向递归对同一组隐变量求和。这将得到：</p><script type="math/tex; mode=display">\begin{aligned}    & P(x_1, \ldots, x_T) \\     =& \sum_{h_1, \ldots, h_T} P(x_1, \ldots, x_T, h_1, \ldots, h_T) \\    =& \sum_{h_1, \ldots, h_T} \prod_{t=1}^{T-1} P(h_t \mid h_{t-1}) P(x_t \mid h_t) \cdot P(h_T \mid h_{T-1}) P(x_T \mid h_T) \\    =& \sum_{h_1, \ldots, h_{T-1}} \prod_{t=1}^{T-1} P(h_t \mid h_{t-1}) P(x_t \mid h_t) \cdot    \underbrace{\left[\sum_{h_T} P(h_T \mid h_{T-1}) P(x_T \mid h_T)\right]}_{\rho_{T-1}(h_{T-1})\stackrel{\mathrm{def}}{=}} \\    =& \sum_{h_1, \ldots, h_{T-2}} \prod_{t=1}^{T-2} P(h_t \mid h_{t-1}) P(x_t \mid h_t) \cdot    \underbrace{\left[\sum_{h_{T-1}} P(h_{T-1} \mid h_{T-2}) P(x_{T-1} \mid h_{T-1}) \rho_{T-1}(h_{T-1}) \right]}_{\rho_{T-2}(h_{T-2})\stackrel{\mathrm{def}}{=}} \\    =& \ldots \\    =& \sum_{h_1} P(h_1) P(x_1 \mid h_1)\rho_{1}(h_{1}).\end{aligned}</script><p>因此可以将<em>后向递归</em>（backward recursion）写为：</p><script type="math/tex; mode=display">\rho_{t-1}(h_{t-1})= \sum_{h_{t}} P(h_{t} \mid h_{t-1}) P(x_{t} \mid h_{t}) \rho_{t}(h_{t}),</script><p>初始化$\rho_T(h_T) = 1$。前向和后向递归都允许我们对$T$个隐变量在$\mathcal{O}(kT)$（线性而不是指数）时间内对$(h_1, \ldots, h_T)$的所有值求和。这是使用图模型进行概率推理的巨大好处之一。它也是通用消息传递算法[<code>Aji.McEliece.2000</code>]的一个非常特殊的例子。结合前向和后向递归，我们能够计算</p><script type="math/tex; mode=display">P(x_j \mid x_{-j}) \propto \sum_{h_j} \pi_j(h_j) \rho_j(h_j) P(x_j \mid h_j).</script><blockquote><p>∝，数学符号，表示与某个量成正比例。A∝B也可表示有一个从 𝐴 到 𝐵 的多项式变换，当A、B为集合或表示$\text { set }{(a, b) \in A \times B: a=k b} \text { for some constant } k$</p></blockquote><p>因为符号简化的需要，后向递归也可以写为$\rho_{t-1} = g(\rho_t, x_t)$，其中$g$是一个可以学习的函数。这同样看起来像一个更新方程，只是不像在循环神经网络中看到的那样前向运算，而是后向计算。事实上，知道未来数据何时可用对隐马尔可夫模型是有益的。信号处理学家将是否知道未来观测这两种情况区分为内插和外推，有关更多详细信息，请参阅 [<code>Doucet.De-Freitas.Gordon.2001</code>]。</p><h3 id="4-2-双向模型"><a href="#4-2-双向模型" class="headerlink" title="4.2. 双向模型"></a>4.2. 双向模型</h3><p>如果我们希望在循环神经网络中拥有一种机制，使之能够提供与隐马尔可夫模型类似的前瞻能力，就需要修改循环神经网络的设计。这在概念上很容易，只需要增加一个“从最后一个词元开始从后向前运行”的循环神经网络，而不是只有一个在前向模式下“从第一个词元开始运行”的循环神经网络。<em>双向循环神经网络</em>（bidirectional RNNs）添加了反向传递信息的隐藏层，以便更灵活地处理此类信息。下图描述了具有单个隐藏层的双向循环神经网络的架构。</p><p><img src="/assets/post_img/article58/birnn.svg" alt="双向循环神经网络架构"></p><p>事实上这与隐马尔可夫模型中的动态规划的前向和后向递归没有太大区别。主要区别是在隐马尔可夫模型中的方程具有特定的统计意义。双向循环神经网络没有这样容易理解的解释，我们只能把它们当作通用的、可学习的函数。这种转变集中体现了现代深度网络的设计原则：首先使用经典统计模型的函数依赖类型，然后将其参数化为通用形式。（就是模仿结构但是没有数学理论支持？）</p><h4 id="4-2-1-定义"><a href="#4-2-1-定义" class="headerlink" title="4.2.1. 定义"></a>4.2.1. 定义</h4><p>双向循环神经网络是由[<code>Schuster.Paliwal.1997</code>]提出的，关于各种架构的详细讨论请参阅[<code>Graves.Schmidhuber.2005</code>]。来看看这样一个网络的细节。</p><p>对于任意时间步$t$，给定一个小批量的输入数据$\mathbf{X}_t \in \mathbb{R}^{n \times d}$（样本数$n$，每个示例中的输入数$d$），并且令隐藏层激活函数为$\phi$。在双向架构中，设该时间步的前向和反向隐状态分别为$\overrightarrow{\mathbf{H}}_t  \in \mathbb{R}^{n \times h}$和$\overleftarrow{\mathbf{H}}_t  \in \mathbb{R}^{n \times h}$，其中$h$是隐藏单元的数目。前向和反向隐状态的更新如下：</p><script type="math/tex; mode=display">\begin{aligned}\overrightarrow{\mathbf{H}}_t &= \phi(\mathbf{X}_t \mathbf{W}_{xh}^{(f)} + \overrightarrow{\mathbf{H}}_{t-1} \mathbf{W}_{hh}^{(f)}  + \mathbf{b}_h^{(f)}),\\\overleftarrow{\mathbf{H}}_t &= \phi(\mathbf{X}_t \mathbf{W}_{xh}^{(b)} + \overleftarrow{\mathbf{H}}_{t+1} \mathbf{W}_{hh}^{(b)}  + \mathbf{b}_h^{(b)}),\end{aligned}</script><p>其中，权重$\mathbf{W}_{xh}^{(f)} \in \mathbb{R}^{d \times h}, \mathbf{W}_{hh}^{(f)} \in \mathbb{R}^{h \times h}, \mathbf{W}_{xh}^{(b)} \in \mathbb{R}^{d \times h}, \mathbf{W}_{hh}^{(b)} \in \mathbb{R}^{h \times h}$和偏置$\mathbf{b}_h^{(f)} \in \mathbb{R}^{1 \times h}, \mathbf{b}_h^{(b)} \in \mathbb{R}^{1 \times h}$都是模型参数。</p><p>接下来，将前向隐状态$\overrightarrow{\mathbf{H}}_t$和反向隐状态$\overleftarrow{\mathbf{H}}_t$连接起来，获得需要送入输出层的隐状态$\mathbf{H}_t \in \mathbb{R}^{n \times 2h}$。在具有多个隐藏层的深度双向循环神经网络中，该信息作为输入传递到下一个双向层。最后，输出层计算得到的输出为$\mathbf{O}_t \in \mathbb{R}^{n \times q}$（$q$是输出单元的数目）：</p><script type="math/tex; mode=display">\mathbf{O}_t = \mathbf{H}_t \mathbf{W}_{hq} + \mathbf{b}_q.</script><p>这里，权重矩阵$\mathbf{W}_{hq} \in \mathbb{R}^{2h \times q}$和偏置$\mathbf{b}_q \in \mathbb{R}^{1 \times q}$是输出层的模型参数。事实上，这两个方向可以拥有不同数量的隐藏单元。</p><h4 id="4-2-2-模型的计算代价及其应用"><a href="#4-2-2-模型的计算代价及其应用" class="headerlink" title="4.2.2. 模型的计算代价及其应用"></a>4.2.2. 模型的计算代价及其应用</h4><p>双向循环神经网络的一个关键特性是：使用来自序列两端的信息来估计输出。也就是说，我们使用来自过去和未来的观测信息来预测当前的观测。但是在对下一个词元进行预测的情况中，这样的模型并不是我们所需的。因为在预测下一个词元时我们无法知道下一个词元的下文是什么，所以将不会得到很好的精度。具体地说，在训练期间，我们能够利用过去和未来的数据来估计现在空缺的词；而在测试期间，我们只有过去的数据，因此精度将会很差。下面的实验将说明这一点。</p><p>另一个严重问题是，双向循环神经网络的计算速度非常慢。其主要原因是网络的前向传播需要在双向层中进行前向和后向递归，并且网络的反向传播还依赖于前向传播的结果。因此，梯度求解将有一个非常长的链。</p><p>双向层的使用在实践中非常少，并且仅仅应用于部分场合。例如，填充缺失的单词、词元注释（例如，用于命名实体识别）以及作为序列处理流水线中的一个步骤对序列进行编码（例如，用于机器翻译）。本书未来将介绍如何使用双向循环神经网络编码文本序列。</p><h3 id="4-3-双向循环神经网络的错误应用"><a href="#4-3-双向循环神经网络的错误应用" class="headerlink" title="4.3. 双向循环神经网络的错误应用"></a>4.3. 双向循环神经网络的错误应用</h3><p>由于双向循环神经网络使用了过去的和未来的数据，所以不能盲目地将这一语言模型应用于任何预测任务。 尽管模型产出的困惑度是合理的，该模型预测未来词元的能力却可能存在严重缺陷。 这里用下面的示例代码引以为戒，以防在错误的环境中使用它们。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line">batch_size, num_steps, device = <span class="number">32</span>, <span class="number">35</span>, d2l.try_gpu()</span><br><span class="line">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br><span class="line"><span class="comment"># 通过设置“bidirective=True”来定义双向LSTM模型</span></span><br><span class="line">vocab_size, num_hiddens, num_layers = <span class="built_in">len</span>(vocab), <span class="number">256</span>, <span class="number">2</span></span><br><span class="line">num_inputs = vocab_size</span><br><span class="line">lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers, bidirectional=<span class="literal">True</span>)</span><br><span class="line">model = d2l.RNNModel(lstm_layer, <span class="built_in">len</span>(vocab))</span><br><span class="line">model = model.to(device)</span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">num_epochs, lr = <span class="number">500</span>, <span class="number">1</span></span><br><span class="line">d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">perplexity <span class="number">1.1</span>, <span class="number">130005.5</span> tokens/sec on cuda:<span class="number">0</span></span><br><span class="line">time travellerererererererererererererererererererererererererer</span><br><span class="line">travellerererererererererererererererererererererererererer</span><br></pre></td></tr></table></figure><p>最终结果虽然困惑度降低了，但预测效果非常差。关于如何更有效地使用双向循环神经网络的讨论，请参阅情感分类应用。</p><h2 id="5-机器翻译与数据集"><a href="#5-机器翻译与数据集" class="headerlink" title="5. 机器翻译与数据集"></a>5. 机器翻译与数据集</h2><p>语言模型是自然语言处理的关键，而<em>机器翻译</em>是语言模型最成功的基准测试。因为机器翻译正是将输入序列转换成输出序列的<em>序列转换模型</em>（sequence transduction）的核心问题。序列转换模型在各类现代人工智能应用中发挥着至关重要的作用，因此将其做为本章剩余部分和下一章的重点。本节将介绍机器翻译问题及其后文需要使用的数据集。</p><p><em>机器翻译</em>（machine translation）指的是将序列从一种语言自动翻译成另一种语言。这个研究领域可以追溯到数字计算机发明后不久的20世纪40年代，特别是在第二次世界大战中使用计算机破解语言编码。几十年来，在使用神经网络进行端到端学习的兴起之前，统计学方法在这一领域一直占据主导地位。因为<em>统计机器翻译</em>（statisticalmachine translation）涉及了翻译模型和语言模型等组成部分的统计分析，因此基于神经网络的方法通常被称为<em>神经机器翻译</em>（neuralmachine translation），用于将两种翻译模型区分开来。</p><p>本书的关注点是神经网络机器翻译方法，强调的是端到端的学习。与上一章中语料库是单一语言的语言模型问题存在不同，机器翻译的数据集是由源语言和目标语言的文本序列对组成的。因此需要一种完全不同的方法来预处理机器翻译数据集，而不是复用语言模型的预处理程序。下面看一下如何将预处理后的数据加载到小批量中用于训练。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><h3 id="5-1-下载和预处理数据集"><a href="#5-1-下载和预处理数据集" class="headerlink" title="5.1. 下载和预处理数据集"></a>5.1. 下载和预处理数据集</h3><p>首先，下载一个由<a href="http://www.manythings.org/anki/">Tatoeba项目的双语句子对</a>组成的“英－法”数据集，数据集中的每一行都是制表符分隔的文本序列对，序列对由英文文本序列和翻译后的法语文本序列组成。请注意，每个文本序列可以是一个句子，也可以是包含多个句子的一个段落。在这个将英语翻译成法语的机器翻译问题中，英语是<em>源语言</em>（source language），法语是<em>目标语言</em>（target language）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">d2l.DATA_HUB[<span class="string">&#x27;fra-eng&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;fra-eng.zip&#x27;</span>,</span><br><span class="line">                           <span class="string">&#x27;94646ad1522d915e7b0f9296181140edcf86a4f5&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_data_nmt</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;载入“英语－法语”数据集&quot;&quot;&quot;</span></span><br><span class="line">    data_dir = d2l.download_extract(<span class="string">&#x27;fra-eng&#x27;</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(data_dir, <span class="string">&#x27;fra.txt&#x27;</span>), <span class="string">&#x27;r&#x27;</span>,</span><br><span class="line">             encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">return</span> f.read()</span><br><span class="line"></span><br><span class="line">raw_text = read_data_nmt()</span><br><span class="line"><span class="built_in">print</span>(raw_text[:<span class="number">75</span>])</span><br></pre></td></tr></table></figure><p>下载数据集后，原始文本数据需要经过几个预处理步骤。例如:用空格代替<em>不间断空格</em>（non-breaking space），使用小写字母替换大写字母，并在单词和标点符号之间插入空格。</p><p>关于空格的种类（带u的是unicode编码）：<br>1、半角空格：<code>\0x20</code>，占位符为一个半角字符，日常英文数学和代码编写使用。<br>2、全角空格：<code>\u3000</code>，中文输入空格，两个半角空格。<br>3、不间断空格：<code>\u00A0</code>或<code>\xa0</code>，在word、html等中大量使用。<br>4、零宽度空格：<code>\u200B</code>，不可见非打印字符，可以替换html中的<code>&lt;wbr/&gt;</code>标签。<br>5、零宽度非中断空格：<code>\u2060</code>，结合了 non-breaking space 和 零宽度空格的特点。既会自动换行，宽度又是0。<br>6、还有一些html宽度度量下的其他空格字符，如<code>\u202f</code>就是一种窄不间断空格，不同语种中不太一样。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_nmt</span>(<span class="params">text</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;预处理“英语－法语”数据集&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">no_space</span>(<span class="params">char, prev_char</span>):</span></span><br><span class="line">        <span class="keyword">return</span> char <span class="keyword">in</span> <span class="built_in">set</span>(<span class="string">&#x27;,.!?&#x27;</span>) <span class="keyword">and</span> prev_char != <span class="string">&#x27; &#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用空格替换不间断空格</span></span><br><span class="line">    <span class="comment"># 使用小写字母替换大写字母</span></span><br><span class="line">    text = text.replace(<span class="string">&#x27;\u202f&#x27;</span>, <span class="string">&#x27; &#x27;</span>).replace(<span class="string">&#x27;\xa0&#x27;</span>, <span class="string">&#x27; &#x27;</span>).lower()</span><br><span class="line">    <span class="comment"># 在单词和标点符号之间插入空格</span></span><br><span class="line">    out = [<span class="string">&#x27; &#x27;</span> + char <span class="keyword">if</span> i &gt; <span class="number">0</span> <span class="keyword">and</span> no_space(char, text[i - <span class="number">1</span>]) <span class="keyword">else</span> char</span><br><span class="line">           <span class="keyword">for</span> i, char <span class="keyword">in</span> <span class="built_in">enumerate</span>(text)]</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;&#x27;</span>.join(out)</span><br><span class="line"></span><br><span class="line">text = preprocess_nmt(raw_text)</span><br><span class="line"><span class="built_in">print</span>(text[:<span class="number">80</span>])</span><br></pre></td></tr></table></figure><h3 id="5-2-词元化"><a href="#5-2-词元化" class="headerlink" title="5.2. 词元化"></a>5.2. 词元化</h3><p>与上一章中的字符级词元化不同，在机器翻译中，更喜欢做单词级词元化（最先进的模型可能使用更高级的词元化技术）。下面的<code>tokenize_nmt</code>函数对前<code>num_examples</code>个文本序列对进行词元，其中每个词元要么是一个词，要么是一个标点符号。此函数返回两个词元列表：<code>source</code>和<code>target</code>：<code>source[i]</code>是源语言（这里是英语）第$i$个文本序列的词元列表，<code>target[i]</code>是目标语言（这里是法语）第$i$个文本序列的词元列表。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize_nmt</span>(<span class="params">text, num_examples=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;词元化“英语－法语”数据数据集&quot;&quot;&quot;</span></span><br><span class="line">    source, target = [], []</span><br><span class="line">    <span class="keyword">for</span> i, line <span class="keyword">in</span> <span class="built_in">enumerate</span>(text.split(<span class="string">&#x27;\n&#x27;</span>)):</span><br><span class="line">        <span class="keyword">if</span> num_examples <span class="keyword">and</span> i &gt; num_examples:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        parts = line.split(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(parts) == <span class="number">2</span>:</span><br><span class="line">            source.append(parts[<span class="number">0</span>].split(<span class="string">&#x27; &#x27;</span>))</span><br><span class="line">            target.append(parts[<span class="number">1</span>].split(<span class="string">&#x27; &#x27;</span>))</span><br><span class="line">    <span class="keyword">return</span> source, target</span><br><span class="line"></span><br><span class="line">source, target = tokenize_nmt(text)</span><br></pre></td></tr></table></figure><p>绘制每个文本序列所包含的词元数量的直方图。在这个简单的“英－法”数据集中，大多数文本序列的词元数量少于$20$个。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_list_len_pair_hist</span>(<span class="params">legend, xlabel, ylabel, xlist, ylist</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;绘制列表长度对的直方图&quot;&quot;&quot;</span></span><br><span class="line">    d2l.set_figsize()</span><br><span class="line">    _, _, patches = d2l.plt.hist(</span><br><span class="line">        [[<span class="built_in">len</span>(l) <span class="keyword">for</span> l <span class="keyword">in</span> xlist], [<span class="built_in">len</span>(l) <span class="keyword">for</span> l <span class="keyword">in</span> ylist]])</span><br><span class="line">    d2l.plt.xlabel(xlabel)</span><br><span class="line">    d2l.plt.ylabel(ylabel)</span><br><span class="line">    <span class="keyword">for</span> patch <span class="keyword">in</span> patches[<span class="number">1</span>].patches:</span><br><span class="line">        patch.set_hatch(<span class="string">&#x27;/&#x27;</span>)</span><br><span class="line">    d2l.plt.legend(legend)</span><br><span class="line"></span><br><span class="line">show_list_len_pair_hist([<span class="string">&#x27;source&#x27;</span>, <span class="string">&#x27;target&#x27;</span>], <span class="string">&#x27;# tokens per sequence&#x27;</span>,</span><br><span class="line">                        <span class="string">&#x27;count&#x27;</span>, source, target);</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article58/output_machine-translation-and-dataset_887557_54_0.svg" alt="tokens per sequence"></p><h3 id="5-3-词表"><a href="#5-3-词表" class="headerlink" title="5.3. 词表"></a>5.3. 词表</h3><p>由于机器翻译数据集由语言对组成，则可以分别为源语言和目标语言构建两个词表。使用单词级词元化时，词表大小将明显大于使用字符级词元化时的词表大小。为了缓解这一问题，这里将出现次数少于2次的低频率词元视为相同的未知（“&lt;unk&gt;”）词元。除此之外，还指定了额外的特定词元，例如在小批量时用于将序列填充到相同长度的填充词元（“&lt;pad&gt;”），以及序列的开始词元（“&lt;bos&gt;”）和结束词元（“&lt;eos&gt;”）。这些特殊词元在自然语言处理任务中比较常用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">src_vocab = d2l.Vocab(source, min_freq=<span class="number">2</span>,</span><br><span class="line">                      reserved_tokens=[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>, <span class="string">&#x27;&lt;bos&gt;&#x27;</span>, <span class="string">&#x27;&lt;eos&gt;&#x27;</span>])</span><br><span class="line"><span class="built_in">len</span>(src_vocab)</span><br></pre></td></tr></table></figure><h3 id="5-4-加载数据集"><a href="#5-4-加载数据集" class="headerlink" title="5.4. 加载数据集"></a>5.4. 加载数据集</h3><p>语言模型中的序列样本都有一个固定的长度，无论这个样本是一个句子的一部分还是跨越了多个句子的一个片断。这个固定长度是上一章中第3节中的<code>num_steps</code>（时间步数或词元数量）参数指定的。在机器翻译中，每个样本都是由源和目标组成的文本序列对，其中的每个文本序列可能具有不同的长度。</p><p>为了提高计算效率，我们仍然可以通过<em>截断</em>（truncation）和<em>填充</em>（padding）方式实现一次只处理一个小批量的文本序列。假设同一个小批量中的每个序列都应该具有相同的长度<code>num_steps</code>，那么如果文本序列的词元数目少于<code>num_steps</code>时，我们将继续在其末尾添加特定的“&lt;pad&gt;”词元，直到其长度达到<code>num_steps</code>；反之，我们将截断文本序列时，只取其前<code>num_steps</code> 个词元，并且丢弃剩余的词元。这样，每个文本序列将具有相同的长度，以便以相同形状的小批量进行加载。</p><p>下面的<code>truncate_pad</code>函数将截断或填充文本序列。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">truncate_pad</span>(<span class="params">line, num_steps, padding_token</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;截断或填充文本序列&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(line) &gt; num_steps:</span><br><span class="line">        <span class="keyword">return</span> line[:num_steps]  <span class="comment"># 截断</span></span><br><span class="line">    <span class="keyword">return</span> line + [padding_token] * (num_steps - <span class="built_in">len</span>(line))  <span class="comment"># 填充</span></span><br><span class="line"></span><br><span class="line">truncate_pad(src_vocab[source[<span class="number">0</span>]], <span class="number">10</span>, src_vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>])</span><br></pre></td></tr></table></figure><p>现在定义一个函数，可以将文本序列转换成小批量数据集用于训练。我们将特定的“&lt;eos&gt;”词元添加到所有序列的末尾，用于表示序列的结束。当模型通过一个词元接一个词元地生成序列进行预测时，生成的“&lt;eos&gt;”词元说明完成了序列输出工作。此外，我们还记录了每个文本序列的长度，统计长度时排除了填充词元，之后介绍的一些模型会需要这个长度信息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_array_nmt</span>(<span class="params">lines, vocab, num_steps</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;将机器翻译的文本序列转换成小批量&quot;&quot;&quot;</span></span><br><span class="line">    lines = [vocab[l] <span class="keyword">for</span> l <span class="keyword">in</span> lines]</span><br><span class="line">    lines = [l + [vocab[<span class="string">&#x27;&lt;eos&gt;&#x27;</span>]] <span class="keyword">for</span> l <span class="keyword">in</span> lines]</span><br><span class="line">    array = torch.tensor([truncate_pad(</span><br><span class="line">        l, num_steps, vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]) <span class="keyword">for</span> l <span class="keyword">in</span> lines])</span><br><span class="line">    valid_len = (array != vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]).<span class="built_in">type</span>(torch.int32).<span class="built_in">sum</span>(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> array, valid_len</span><br></pre></td></tr></table></figure><h3 id="5-5-训练模型"><a href="#5-5-训练模型" class="headerlink" title="5.5. 训练模型"></a>5.5. 训练模型</h3><p>最后定义<code>load_data_nmt</code>函数来返回数据迭代器，以及源语言和目标语言的两种词表。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_nmt</span>(<span class="params">batch_size, num_steps, num_examples=<span class="number">600</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回翻译数据集的迭代器和词表&quot;&quot;&quot;</span></span><br><span class="line">    text = preprocess_nmt(read_data_nmt())</span><br><span class="line">    source, target = tokenize_nmt(text, num_examples)</span><br><span class="line">    src_vocab = d2l.Vocab(source, min_freq=<span class="number">2</span>,</span><br><span class="line">                          reserved_tokens=[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>, <span class="string">&#x27;&lt;bos&gt;&#x27;</span>, <span class="string">&#x27;&lt;eos&gt;&#x27;</span>])</span><br><span class="line">    tgt_vocab = d2l.Vocab(target, min_freq=<span class="number">2</span>,</span><br><span class="line">                          reserved_tokens=[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>, <span class="string">&#x27;&lt;bos&gt;&#x27;</span>, <span class="string">&#x27;&lt;eos&gt;&#x27;</span>])</span><br><span class="line">    src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)</span><br><span class="line">    tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)</span><br><span class="line">    data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)</span><br><span class="line">    data_iter = d2l.load_array(data_arrays, batch_size)</span><br><span class="line">    <span class="keyword">return</span> data_iter, src_vocab, tgt_vocab</span><br></pre></td></tr></table></figure><p>下面读出“英语－法语”数据集中的第一个小批量数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">train_iter, src_vocab, tgt_vocab = load_data_nmt(batch_size=<span class="number">2</span>, num_steps=<span class="number">8</span>)</span><br><span class="line"><span class="keyword">for</span> X, X_valid_len, Y, Y_valid_len <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;X:&#x27;</span>, X.<span class="built_in">type</span>(torch.int32))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;X的有效长度:&#x27;</span>, X_valid_len)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Y:&#x27;</span>, Y.<span class="built_in">type</span>(torch.int32))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Y的有效长度:&#x27;</span>, Y_valid_len)</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">X: tensor([[ <span class="number">9</span>,  <span class="number">4</span>,  <span class="number">3</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>],</span><br><span class="line">        [<span class="number">87</span>, <span class="number">22</span>,  <span class="number">4</span>,  <span class="number">3</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>]], dtype=torch.int32)</span><br><span class="line">X的有效长度: tensor([<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">Y: tensor([[ <span class="number">16</span>,   <span class="number">5</span>,   <span class="number">3</span>,   <span class="number">1</span>,   <span class="number">1</span>,   <span class="number">1</span>,   <span class="number">1</span>,   <span class="number">1</span>],</span><br><span class="line">        [<span class="number">175</span>, <span class="number">176</span>,   <span class="number">4</span>,   <span class="number">3</span>,   <span class="number">1</span>,   <span class="number">1</span>,   <span class="number">1</span>,   <span class="number">1</span>]], dtype=torch.int32)</span><br><span class="line">Y的有效长度: tensor([<span class="number">3</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure><h2 id="6-编码器-解码器架构"><a href="#6-编码器-解码器架构" class="headerlink" title="6. 编码器-解码器架构"></a>6. 编码器-解码器架构</h2><p>机器翻译是序列转换模型的一个核心问题，其输入和输出都是长度可变的序列。为了处理这种类型的输入和输出，可以设计一个包含两个主要组件的架构：第一个组件是一个<em>编码器</em>（encoder）：它接受一个长度可变的序列作为输入，并将其转换为具有固定形状的编码状态。第二个组件是<em>解码器</em>（decoder）：它将固定形状的编码状态映射到长度可变的序列。这被称为<em>编码器-解码器</em>（encoder-decoder）架构，如下图所示。</p><p><img src="/assets/post_img/article58/encoder-decoder.svg" alt="编码器-解码器架构"></p><p>以英语到法语的机器翻译为例：给定一个英文的输入序列：“They”“are”“watching”“.”。首先，这种“编码器－解码器”架构将长度可变的输入序列编码成一个“状态”，然后对该状态进行解码，一个词元接着一个词元地生成翻译后的序列作为输出：“Ils”“regordent”“.”。由于“编码器－解码器”架构是形成后续章节中不同序列转换模型的基础，因此本节将把这个架构转换为接口方便后面的代码实现。</p><h3 id="6-1-编码器"><a href="#6-1-编码器" class="headerlink" title="6.1. 编码器"></a>6.1. 编码器</h3><p>在编码器接口中，我们只指定长度可变的序列作为编码器的输入<code>X</code>。<br>任何继承这个<code>Encoder</code>基类的模型将完成代码实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;编码器-解码器架构的基本编码器接口&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, *args</span>):</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure><p>这里解析一下<code>super(Encoder, self).__init__()</code>这个写法。self指的是实例（instance）本身，Python的类中规定：<strong>类中的方法的第一个参数一定要是self，而且不能省略。</strong> 所以构造函数也就是<code>__init__ ()</code>方法必须包含一个self参数，而且要是第一个参数。</p><p><code>super()</code>是Python的内置函数，用于调用父类。在Python3中我们通常使用<code>super().xxx</code>代替<code>super(Class, self).xxx</code>。<code>super(Encoder, self).__init__()</code>的工作原理是首先找到Encoder的父类nn.Module，然后把实例self转化为父类的对象，再去调用该实例的构造方法，实际上也就是调用了父类的构造方法。</p><h3 id="6-2-解码器"><a href="#6-2-解码器" class="headerlink" title="6.2. 解码器"></a>6.2. 解码器</h3><p>在下面的解码器接口中，新增一个<code>init_state</code>函数，用于将编码器的输出（<code>enc_outputs</code>）转换为编码后的状态。注意，此步骤可能需要额外的输入，例如：输入序列的有效长度。为了逐个地生成长度可变的词元序列，解码器在每个时间步都会将输入（例如：在前一时间步生成的词元）和编码后的状态映射成当前时间步的输出词元。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;编码器-解码器架构的基本解码器接口&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_state</span>(<span class="params">self, enc_outputs, *args</span>):</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, state</span>):</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure><h3 id="6-3-合并编码器和解码器"><a href="#6-3-合并编码器和解码器" class="headerlink" title="6.3. 合并编码器和解码器"></a>6.3. 合并编码器和解码器</h3><p>总而言之，“编码器-解码器”架构包含了一个编码器和一个解码器， 并且还拥有可选的额外的参数。 在前向传播中，编码器的输出用于生成编码状态， 这个状态又被解码器作为其输入的一部分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderDecoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;编码器-解码器架构的基类&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, encoder, decoder, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(EncoderDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, enc_X, dec_X, *args</span>):</span></span><br><span class="line">        enc_outputs = self.encoder(enc_X, *args)</span><br><span class="line">        dec_state = self.decoder.init_state(enc_outputs, *args)</span><br><span class="line">        <span class="keyword">return</span> self.decoder(dec_X, dec_state)</span><br></pre></td></tr></table></figure><p>“编码器－解码器”体系架构中的术语<em>状态</em>会启发人们使用具有状态的神经网络来实现该架构。下一节将学习如何应用循环神经网络，来设计基于“编码器－解码器”架构的序列转换模型。</p><h2 id="7-序列到序列学习（seq2seq）"><a href="#7-序列到序列学习（seq2seq）" class="headerlink" title="7. 序列到序列学习（seq2seq）"></a>7. 序列到序列学习（seq2seq）</h2><p>机器翻译中的输入序列和输出序列都是长度可变的。为了解决这类问题，我们设计了一个通用的”编码器－解码器“架构。本节将使用两个循环神经网络的编码器和解码器，并将其应用于<em>序列到序列</em>（sequence to sequence，seq2seq）类的学习任务[<code>Sutskever.Vinyals.Le.2014,Cho.Van-Merrienboer.Gulcehre.ea.2014</code>]。</p><p>遵循编码器－解码器架构的设计原则，循环神经网络编码器使用长度可变的序列作为输入，将其转换为固定形状的隐状态。换言之，输入序列的信息被<em>编码</em>到循环神经网络编码器的隐状态中。为了连续生成输出序列的词元，独立的循环神经网络解码器是基于输入序列的编码信息和输出序列已经看见的或者生成的词元来预测下一个词元。下图演示了如何在机器翻译中使用两个循环神经网络进行序列到序列学习。</p><p><img src="/assets/post_img/article58/seq2seq.svg" alt="使用循环神经网络编码器和循环神经网络解码器的序列到序列学习"></p><p>图中，特定的“&lt;eos&gt;”表示序列结束词元。一旦输出序列生成此词元，模型就会停止预测。在循环神经网络解码器的初始化时间步，有两个特殊的设计：第一，特定的“&lt;bos&gt;”表示序列开始词元，它是解码器的输入序列的第一个词元。第二，使用循环神经网络编码器最终的隐状态来初始化解码器的隐状态。例如，在[<code>Sutskever.Vinyals.Le.2014</code>]的设计中，正是基于这种设计将输入序列的编码信息送入到解码器中来生成输出序列的。在其他一些设计中[<code>Cho.Van-Merrienboer.Gulcehre.ea.2014</code>]，如上图所示，<em>编码器最终的隐状态在每一个时间步都作为解码器的输入序列的一部分</em>。类似于上一章中语言模型的训练，可以允许标签成为原始的输出序列，从源序列词元“&lt;bos&gt;”“Ils”“regardent”“.”到新序列词元“Ils”“regardent”“.”“&lt;eos&gt;”来移动预测的位置。</p><p>下面来动手构建以上的设计，并将基于“英－法”数据集来训练这个机器翻译模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><h3 id="7-1-编码器"><a href="#7-1-编码器" class="headerlink" title="7.1. 编码器"></a>7.1. 编码器</h3><p>从技术上讲，编码器将长度可变的输入序列转换成形状固定的上下文变量$\mathbf{c}$，并且将输入序列的信息在该上下文变量中进行编码。如前图所示，可以使用循环神经网络来设计编码器。</p><p>对于由一个序列组成的样本（批量大小是$1$）。假设输入序列是$x_1, \ldots, x_T$，其中$x_t$是输入文本序列中的第$t$个词元。在时间步$t$，循环神经网络将词元$x_t$的输入特征向量$\mathbf{x}_t$和$\mathbf{h} _{t-1}$（即上一时间步的隐状态）转换为$\mathbf{h}_t$（即当前步的隐状态）。使用一个函数$f$来描述循环神经网络的循环层所做的变换：</p><script type="math/tex; mode=display">\mathbf{h}_t = f(\mathbf{x}_t, \mathbf{h}_{t-1}).</script><p>而编码器通过选定的函数$q$，将所有时间步的隐状态转换为上下文变量：</p><script type="math/tex; mode=display">\mathbf{c} =  q(\mathbf{h}_1, \ldots, \mathbf{h}_T).</script><p>比如在上面的图中，指定$q(\mathbf{h}_1, \ldots, \mathbf{h}_T) = \mathbf{h}_T$后，上下文变量则仅是输入序列在最后时间步的隐状态$\mathbf{h}_T$。</p><p>目前为止，我们使用的是一个单向循环神经网络来设计编码器，其中隐状态只依赖于输入子序列，这个子序列是由输入序列的开始位置到隐状态所在的时间步的位置（包括隐状态所在的时间步）组成。当然也可以使用双向循环神经网络构造编码器，其中隐状态依赖于两个输入子序列，两个子序列是由隐状态所在的时间步的位置之前的序列 和 之后的序列（包括隐状态所在的时间步），因此隐状态对整个序列的信息都进行了编码。</p><p>现在来实现循环神经网络编码器。注意这里使用了<em>嵌入层</em>（embedding layer）来获得输入序列中每个词元的特征向量。嵌入层的权重是一个矩阵，其行数等于输入词表的大小（<code>vocab_size</code>），其列数等于特征向量的维度（<code>embed_size</code>）。对于任意输入词元的索引$i$，嵌入层获取权重矩阵的第$i$行（从$0$开始）以返回其特征向量。另外，本文选择了一个多层门控循环单元来实现编码器。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Seq2SeqEncoder</span>(<span class="params">Encoder</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;用于序列到序列学习的循环神经网络编码器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embed_size, num_hiddens, num_layers,</span></span></span><br><span class="line"><span class="params"><span class="function">                 dropout=<span class="number">0</span>, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Seq2SeqEncoder, self).__init__(**kwargs)</span><br><span class="line">        <span class="comment"># 嵌入层</span></span><br><span class="line">        <span class="comment"># Embedding的作用简单来说就是为单词编码，将单词编码成为向量。</span></span><br><span class="line">        <span class="comment"># 嵌入层比独热编码更节约空间，能方便运算。</span></span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        <span class="comment"># 嵌入层size等于特征向量维数作为input_size</span></span><br><span class="line">        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers,</span><br><span class="line">                          dropout=dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, *args</span>):</span></span><br><span class="line">        <span class="comment"># 输出&#x27;X&#x27;的形状：(batch_size,num_steps,embed_size)</span></span><br><span class="line">        <span class="comment"># batch_size,num_steps来自输入，经嵌入层后会增加一维，因为原本的元素（单词）被向量化。</span></span><br><span class="line">        X = self.embedding(X)</span><br><span class="line">        <span class="comment"># 在循环神经网络模型中，第一个轴对应于时间步</span></span><br><span class="line">        <span class="comment"># permute就是维度变换，其中0、1、2指第一维、第二维、第三维</span></span><br><span class="line">        <span class="comment"># 这里就是调换两个维度，把形状(batch_size,num_steps,embed_size)</span></span><br><span class="line">        <span class="comment"># 转换为(num_steps,batch_size,embed_size)</span></span><br><span class="line">        X = X.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 如果未提及状态，则默认为0</span></span><br><span class="line">        output, state = self.rnn(X)</span><br><span class="line">        <span class="comment"># output的形状:(num_steps,batch_size,num_hiddens)</span></span><br><span class="line">        <span class="comment"># state的形状:(num_layers,batch_size,num_hiddens)</span></span><br><span class="line">        <span class="keyword">return</span> output, state</span><br></pre></td></tr></table></figure><p>循环层返回变量的说明可以参考上一章“循环神经网络的框架实现”。</p><p>下面实例化上述编码器的实现：使用一个两层门控循环单元编码器，其隐藏单元数为$16$。给定一小批量的输入序列<code>X</code>（批量大小为$4$，时间步为$7$）。在完成所有时间步后，最后一层的隐状态的输出是一个张量（<code>output</code>由编码器的循环层返回），其形状为（时间步数，批量大小，隐藏单元数）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">encoder = Seq2SeqEncoder(vocab_size=<span class="number">10</span>, embed_size=<span class="number">8</span>, num_hiddens=<span class="number">16</span>,</span><br><span class="line">                         num_layers=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 评估模式，测试时一定要使用，对Dropout和BatchNorm等有作用。</span></span><br><span class="line">encoder.<span class="built_in">eval</span>()</span><br><span class="line">X = torch.zeros((<span class="number">4</span>, <span class="number">7</span>), dtype=torch.long)</span><br><span class="line">output, state = encoder(X)</span><br><span class="line">output.shape, state.shape</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">(torch.Size([<span class="number">7</span>, <span class="number">4</span>, <span class="number">16</span>]), torch.Size([<span class="number">2</span>, <span class="number">4</span>, <span class="number">16</span>]))</span><br></pre></td></tr></table></figure><p>由于这里使用的是门控循环单元，所以在最后一个时间步的多层隐状态的形状是 （隐藏层的数量，批量大小，隐藏单元的数量）。 如果使用长短期记忆网络，state中还将包含记忆单元信息。</p><h3 id="7-2-解码器"><a href="#7-2-解码器" class="headerlink" title="7.2. 解码器"></a>7.2. 解码器</h3><p>如上文所说，编码器输出的上下文变量$\mathbf{c}$对整个输入序列$x_1, \ldots, x_T$进行编码。来自训练数据集的输出序列$y_1, y_2, \ldots, y_{T’}$，对于每个时间步$t’$（与输入序列或编码器的时间步$t$不同），解码器输出$y_{t’}$的概率取决于先前的输出子序列$y_1, \ldots, y_{t’-1}$和上下文变量$\mathbf{c}$，即$P(y_{t’} \mid y_1, \ldots, y_{t’-1}, \mathbf{c})$。</p><p>为了在序列上模型化这种条件概率，可以使用另一个循环神经网络作为解码器。在输出序列上的任意时间步$t^\prime$，循环神经网络将来自上一时间步的输出$y_{t^\prime-1}$和上下文变量$\mathbf{c}$作为其输入，然后在当前时间步将它们和上一隐状态$\mathbf{s}_{t^\prime-1}$转换为隐状态$\mathbf{s}_{t^\prime}$。可以使用函数$g$来表示解码器的隐藏层的变换：</p><script type="math/tex; mode=display">\mathbf{s}_{t^\prime} = g(y_{t^\prime-1}, \mathbf{c}, \mathbf{s}_{t^\prime-1}).</script><p>在获得解码器的隐状态之后，我们可以使用输出层和softmax操作来计算在时间步$t^\prime$时输出$y_{t^\prime}$的条件概率分布$P(y_{t^\prime} \mid y_1, \ldots, y_{t^\prime-1}, \mathbf{c})$。</p><p>根据一开始给出的图，当实现解码器时，我们直接使用编码器最后一个时间步的隐状态来初始化解码器的隐状态。这就要求使用循环神经网络实现的编码器和解码器具有相同数量的层和隐藏单元（批量大小是一样的）。为了进一步包含经过编码的输入序列的信息，上下文变量在所有的时间步与解码器的输入进行拼接（concatenate）。为了预测输出词元的概率分布，在循环神经网络解码器的最后一层使用全连接层来变换隐状态。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Seq2SeqDecoder</span>(<span class="params">Decoder</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;用于序列到序列学习的循环神经网络解码器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embed_size, num_hiddens, num_layers,</span></span></span><br><span class="line"><span class="params"><span class="function">                 dropout=<span class="number">0</span>, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Seq2SeqDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers,</span><br><span class="line">                          dropout=dropout)</span><br><span class="line">        self.dense = nn.Linear(num_hiddens, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_state</span>(<span class="params">self, enc_outputs, *args</span>):</span></span><br><span class="line">        <span class="keyword">return</span> enc_outputs[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, state</span>):</span></span><br><span class="line">        <span class="comment"># 输出&#x27;X&#x27;的形状：(batch_size,num_steps,embed_size)</span></span><br><span class="line">        X = self.embedding(X).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 广播context，使其具有与X相同的num_steps</span></span><br><span class="line">        <span class="comment"># PyTorch中的repeat()函数可以对张量进行重复扩充。</span></span><br><span class="line">        <span class="comment"># 三个参数分别是：通道数的重复倍数，列的重复倍数，行的重复倍数。</span></span><br><span class="line">        <span class="comment"># 这里表示第三维扩充batch_size倍，其他不变</span></span><br><span class="line">        context = state[-<span class="number">1</span>].repeat(X.shape[<span class="number">0</span>], <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        X_and_context = torch.cat((X, context), <span class="number">2</span>)</span><br><span class="line">        output, state = self.rnn(X_and_context, state)</span><br><span class="line">        output = self.dense(output).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># output的形状:(batch_size,num_steps,vocab_size)</span></span><br><span class="line">        <span class="comment"># state的形状:(num_layers,batch_size,num_hiddens)</span></span><br><span class="line">        <span class="keyword">return</span> output, state</span><br></pre></td></tr></table></figure><p>下面用与前面提到的编码器中相同的超参数来实例化解码器。解码器的输出形状变为（批量大小，时间步数，词表大小）， 其中张量的最后一个维度存储预测的词元分布。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">decoder = Seq2SeqDecoder(vocab_size=<span class="number">10</span>, embed_size=<span class="number">8</span>, num_hiddens=<span class="number">16</span>,</span><br><span class="line">                         num_layers=<span class="number">2</span>)</span><br><span class="line">decoder.<span class="built_in">eval</span>()</span><br><span class="line">state = decoder.init_state(encoder(X))</span><br><span class="line">output, state = decoder(X, state)</span><br><span class="line">output.shape, state.shape</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">(torch.Size([<span class="number">4</span>, <span class="number">7</span>, <span class="number">10</span>]), torch.Size([<span class="number">2</span>, <span class="number">4</span>, <span class="number">16</span>]))</span><br></pre></td></tr></table></figure><p>上述循环神经网络“编码器－解码器”模型中的各层如下图所示：</p><p><img src="/assets/post_img/article58/seq2seq-details.svg" alt="循环神经网络编码器-解码器模型中的层"></p><h3 id="7-3-损失函数"><a href="#7-3-损失函数" class="headerlink" title="7.3. 损失函数"></a>7.3. 损失函数</h3><p>在每个时间步，解码器预测了输出词元的概率分布。类似于语言模型，可以使用softmax来获得分布，并通过计算交叉熵损失函数来进行优化。<a href="#5-机器翻译与数据集">第五节</a>中，特定的填充词元被添加到序列的末尾，因此不同长度的序列可以以相同形状的小批量加载。但是，我们应该将填充词元的预测排除在损失函数的计算之外。</p><p>为此可以使用下面的<code>sequence_mask</code>函数通过零值化屏蔽不相关的项，以便后面任何不相关预测的计算都是与零的乘积，结果都等于零。例如，如果两个序列的有效长度（不包括填充词元）分别为$1$和$2$，则第一个序列的第一项和第二个序列的前两项之后的剩余项将被清除为零。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sequence_mask</span>(<span class="params">X, valid_len, value=<span class="number">0</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;在序列中屏蔽不相关的项&quot;&quot;&quot;</span></span><br><span class="line">    maxlen = X.size(<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 在[]中加入None表示维度扩充，第二维用:表示在第二维放入全部元素</span></span><br><span class="line">    <span class="comment"># 例如原本的张量为：tensor([0., 1., 2.])</span></span><br><span class="line">    <span class="comment"># 则tensor([0., 1., 2.])[None, :] = tensor([[0., 1., 2.]])</span></span><br><span class="line">    <span class="comment"># 即第一维扩充了，第二维放入原本张量的所有元素</span></span><br><span class="line">    <span class="comment"># [:, None]则会使torch.tensor([1, 2]) 变为 tensor([[1], [2]])</span></span><br><span class="line">    <span class="comment"># 后面的 &lt; 比较符号应用广播机制，使得mask数组变为布尔值</span></span><br><span class="line">    mask = torch.arange((maxlen), dtype=torch.float32,</span><br><span class="line">                        device=X.device)[<span class="literal">None</span>, :] &lt; valid_len[:, <span class="literal">None</span>]</span><br><span class="line">    <span class="comment"># ～表示按位取反，对于布尔值就是T变F，F变T</span></span><br><span class="line">    <span class="comment"># 在一个张量的索引中放入另一个布尔值张量，会进行按位比对，将True对应的元素提取出来，对这些元素所做的更改会体现在原张量中。</span></span><br><span class="line">    X[~mask] = value</span><br><span class="line">    <span class="keyword">return</span> X</span><br><span class="line"></span><br><span class="line">X = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">sequence_mask(X, torch.tensor([<span class="number">1</span>, <span class="number">2</span>]))</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>, <span class="number">0</span>]])</span><br></pre></td></tr></table></figure><p>可以使用此函数屏蔽最后几个轴上的所有项。也可以通过指定<code>value</code>参数使用非零值来替换这些项。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">X = torch.ones(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">sequence_mask(X, torch.tensor([<span class="number">1</span>, <span class="number">2</span>]), value=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">输出:</span><br><span class="line">tensor([[[ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">         [-<span class="number">1.</span>, -<span class="number">1.</span>, -<span class="number">1.</span>, -<span class="number">1.</span>],</span><br><span class="line">         [-<span class="number">1.</span>, -<span class="number">1.</span>, -<span class="number">1.</span>, -<span class="number">1.</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">         [ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">         [-<span class="number">1.</span>, -<span class="number">1.</span>, -<span class="number">1.</span>, -<span class="number">1.</span>]]])</span><br></pre></td></tr></table></figure><p>现在可以通过扩展softmax交叉熵损失函数来遮蔽不相关的预测。最初，所有预测词元的掩码都设置为1。一旦给定了有效长度，与填充词元对应的掩码将被设置为0。最后，将所有词元的损失乘以掩码，以过滤掉损失中填充词元产生的不相关预测。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MaskedSoftmaxCELoss</span>(<span class="params">nn.CrossEntropyLoss</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;带屏蔽的softmax交叉熵损失函数&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># pred的形状：(batch_size,num_steps,vocab_size)</span></span><br><span class="line">    <span class="comment"># label的形状：(batch_size,num_steps)</span></span><br><span class="line">    <span class="comment"># valid_len的形状：(batch_size,)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, pred, label, valid_len</span>):</span></span><br><span class="line">        weights = torch.ones_like(label)</span><br><span class="line">        weights = sequence_mask(weights, valid_len)</span><br><span class="line">        self.reduction=<span class="string">&#x27;none&#x27;</span></span><br><span class="line">        <span class="comment"># 原本的交叉熵</span></span><br><span class="line">        unweighted_loss = <span class="built_in">super</span>(MaskedSoftmaxCELoss, self).forward(</span><br><span class="line">            pred.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>), label)</span><br><span class="line">        <span class="comment"># 除去屏蔽掉的填充词元</span></span><br><span class="line">        weighted_loss = (unweighted_loss * weights).mean(dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> weighted_loss</span><br></pre></td></tr></table></figure><p>可以创建三个相同的序列来进行代码健全性检查，然后分别指定这些序列的有效长度为$4$、$2$和$0$。结果就是，第一个序列的损失应为第二个序列的两倍，而第三个序列的损失应为零。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">loss = MaskedSoftmaxCELoss()</span><br><span class="line">loss(torch.ones(<span class="number">3</span>, <span class="number">4</span>, <span class="number">10</span>), torch.ones((<span class="number">3</span>, <span class="number">4</span>), dtype=torch.long),</span><br><span class="line">     torch.tensor([<span class="number">4</span>, <span class="number">2</span>, <span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([<span class="number">2.3026</span>, <span class="number">1.1513</span>, <span class="number">0.0000</span>])</span><br></pre></td></tr></table></figure><h3 id="7-4-训练"><a href="#7-4-训练" class="headerlink" title="7.4. 训练"></a>7.4. 训练</h3><p>在下面的循环训练过程中，如<a href="#7-序列到序列学习seq2seq">前图</a>所示，特定的序列开始词元（“&lt;bos&gt;”）和原始的输出序列（不包括序列结束词元“&lt;eos&gt;”）拼接在一起作为解码器的输入。这被称为<em>强制教学</em>（teacher forcing），因为原始的输出序列（词元的标签）被送入解码器。或者，将来自上一个时间步的<em>预测</em>得到的词元作为解码器的当前输入。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_seq2seq</span>(<span class="params">net, data_iter, lr, num_epochs, tgt_vocab, device</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练序列到序列模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">xavier_init_weights</span>(<span class="params">m</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">            nn.init.xavier_uniform_(m.weight)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.GRU:</span><br><span class="line">            <span class="keyword">for</span> param <span class="keyword">in</span> m._flat_weights_names:</span><br><span class="line">                <span class="keyword">if</span> <span class="string">&quot;weight&quot;</span> <span class="keyword">in</span> param:</span><br><span class="line">                    nn.init.xavier_uniform_(m._parameters[param])</span><br><span class="line"></span><br><span class="line">    net.apply(xavier_init_weights)</span><br><span class="line">    net.to(device)</span><br><span class="line">    optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">    loss = MaskedSoftmaxCELoss()</span><br><span class="line">    net.train()</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>,</span><br><span class="line">                     xlim=[<span class="number">10</span>, num_epochs])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        timer = d2l.Timer()</span><br><span class="line">        metric = d2l.Accumulator(<span class="number">2</span>)  <span class="comment"># 训练损失总和，词元数量</span></span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> data_iter:</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            X, X_valid_len, Y, Y_valid_len = [x.to(device) <span class="keyword">for</span> x <span class="keyword">in</span> batch]</span><br><span class="line">            bos = torch.tensor([tgt_vocab[<span class="string">&#x27;&lt;bos&gt;&#x27;</span>]] * Y.shape[<span class="number">0</span>],</span><br><span class="line">                          device=device).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            dec_input = torch.cat([bos, Y[:, :-<span class="number">1</span>]], <span class="number">1</span>)  <span class="comment"># 强制教学</span></span><br><span class="line">            Y_hat, _ = net(X, dec_input, X_valid_len)</span><br><span class="line">            l = loss(Y_hat, Y, Y_valid_len)</span><br><span class="line">            l.<span class="built_in">sum</span>().backward()      <span class="comment"># 损失函数的标量进行“反向传播”</span></span><br><span class="line">            d2l.grad_clipping(net, <span class="number">1</span>) <span class="comment"># 梯度裁剪</span></span><br><span class="line">            num_tokens = Y_valid_len.<span class="built_in">sum</span>()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                metric.add(l.<span class="built_in">sum</span>(), num_tokens)</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>, (metric[<span class="number">0</span>] / metric[<span class="number">1</span>],))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;loss <span class="subst">&#123;metric[<span class="number">0</span>] / metric[<span class="number">1</span>]:<span class="number">.3</span>f&#125;</span>, <span class="subst">&#123;metric[<span class="number">1</span>] / timer.stop():<span class="number">.1</span>f&#125;</span> &#x27;</span></span><br><span class="line">        <span class="string">f&#x27;tokens/sec on <span class="subst">&#123;<span class="built_in">str</span>(device)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>现在可以在 机器翻译数据集 上创建和训练一个循环神经网络“编码器－解码器”模型用于序列到序列的学习。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">embed_size, num_hiddens, num_layers, dropout = <span class="number">32</span>, <span class="number">32</span>, <span class="number">2</span>, <span class="number">0.1</span></span><br><span class="line">batch_size, num_steps = <span class="number">64</span>, <span class="number">10</span></span><br><span class="line">lr, num_epochs, device = <span class="number">0.005</span>, <span class="number">300</span>, d2l.try_gpu()</span><br><span class="line"></span><br><span class="line">train_iter, src_vocab, tgt_vocab = load_data_nmt(batch_size, num_steps)</span><br><span class="line">encoder = Seq2SeqEncoder(<span class="built_in">len</span>(src_vocab), embed_size, num_hiddens, num_layers,</span><br><span class="line">                        dropout)</span><br><span class="line">decoder = Seq2SeqDecoder(<span class="built_in">len</span>(tgt_vocab), embed_size, num_hiddens, num_layers,</span><br><span class="line">                        dropout)</span><br><span class="line">net = EncoderDecoder(encoder, decoder)</span><br><span class="line">train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)</span><br></pre></td></tr></table></figure><h3 id="7-5-预测"><a href="#7-5-预测" class="headerlink" title="7.5. 预测"></a>7.5. 预测</h3><p>为了采用一个接着一个词元的方式预测输出序列，每个解码器当前时间步的输入都将来自于前一时间步的预测词元。与训练类似，序列开始词元（“&lt;bos&gt;”）在初始时间步被输入到解码器中。该预测过程如下图所示，当输出序列的预测遇到序列结束词元（“&lt;eos&gt;”）时，预测就结束了。</p><p><img src="/assets/post_img/article58/seq2seq-predict.svg" alt="使用循环神经网络编码器-解码器逐词元地预测输出序列。"></p><p>下一节中将介绍不同的序列生成策略。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_seq2seq</span>(<span class="params">net, src_sentence, src_vocab, tgt_vocab, num_steps,</span></span></span><br><span class="line"><span class="params"><span class="function">                    device, save_attention_weights=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;序列到序列模型的预测&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 在预测时将net设置为评估模式</span></span><br><span class="line">    net.<span class="built_in">eval</span>()</span><br><span class="line">    src_tokens = src_vocab[src_sentence.lower().split(<span class="string">&#x27; &#x27;</span>)] + [src_vocab[<span class="string">&#x27;&lt;eos&gt;&#x27;</span>]]</span><br><span class="line">    enc_valid_len = torch.tensor([<span class="built_in">len</span>(src_tokens)], device=device)</span><br><span class="line">    src_tokens = truncate_pad(src_tokens, num_steps, src_vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>])</span><br><span class="line">    <span class="comment"># 添加批量轴</span></span><br><span class="line">    <span class="comment"># unsqueeze()函数起升维的作用,参数表示在哪个地方加一个维度。</span></span><br><span class="line">    enc_X = torch.unsqueeze(torch.tensor(src_tokens, dtype=torch.long, device=device), dim=<span class="number">0</span>)</span><br><span class="line">    enc_outputs = net.encoder(enc_X, enc_valid_len)</span><br><span class="line">    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)</span><br><span class="line">    <span class="comment"># 添加批量轴</span></span><br><span class="line">    dec_X = torch.unsqueeze(torch.tensor([tgt_vocab[<span class="string">&#x27;&lt;bos&gt;&#x27;</span>]], dtype=torch.long, device=device), dim=<span class="number">0</span>)</span><br><span class="line">    output_seq, attention_weight_seq = [], []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_steps):</span><br><span class="line">        Y, dec_state = net.decoder(dec_X, dec_state)</span><br><span class="line">        <span class="comment"># 使用具有预测最高可能性的词元，作为解码器在下一时间步的输入</span></span><br><span class="line">        dec_X = Y.argmax(dim=<span class="number">2</span>)</span><br><span class="line">        pred = dec_X.squeeze(dim=<span class="number">0</span>).<span class="built_in">type</span>(torch.int32).item()</span><br><span class="line">        <span class="comment"># 保存注意力权重（稍后讨论）</span></span><br><span class="line">        <span class="keyword">if</span> save_attention_weights:</span><br><span class="line">            attention_weight_seq.append(net.decoder.attention_weights)</span><br><span class="line">        <span class="comment"># 一旦序列结束词元被预测，输出序列的生成就完成了</span></span><br><span class="line">        <span class="keyword">if</span> pred == tgt_vocab[<span class="string">&#x27;&lt;eos&gt;&#x27;</span>]:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        output_seq.append(pred)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27; &#x27;</span>.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq</span><br></pre></td></tr></table></figure><h3 id="7-6-预测序列的评估"><a href="#7-6-预测序列的评估" class="headerlink" title="7.6. 预测序列的评估"></a>7.6. 预测序列的评估</h3><p>我们可以通过与真实的标签序列进行比较来评估预测序列。虽然[<code>Papineni.Roukos.Ward.ea.2002</code>]提出的BLEU（bilingual evaluation understudy）最先是用于评估机器翻译的结果，但现在它已经被广泛用于测量许多应用的输出序列的质量。原则上说，对于预测序列中的任意$n$元语法（n-grams），BLEU的评估都是这个$n$元语法是否出现在标签序列中。</p><p>我们将BLEU定义为：</p><script type="math/tex; mode=display">\exp\left(\min\left(0, 1 - \frac{\mathrm{len}_{\text{label}}}{\mathrm{len}_{\text{pred}}}\right)\right) \prod_{n=1}^k p_n^{1/2^n},</script><p>其中$\mathrm{len}_{\text{label}}$表示标签序列中的词元数和$\mathrm{len}_{\text{pred}}$表示预测序列中的词元数，$k$是用于匹配的最长的$n$元语法。另外，用$p_n$表示$n$元语法的精确度，它是两个数量的比值：第一个是预测序列与标签序列中匹配的$n$元语法的数量，第二个是预测序列中$n$元语法的数量的比率。具体地说，给定标签序列$A$、$B$、$C$、$D$、$E$、$F$和预测序列$A$、$B$、$B$、$C$、$D$，我们有$p_1 = 4/5$、$p_2 = 3/4$、$p_3 = 1/3$和$p_4 = 0$。</p><p>这里解释一下，先说$p_1$，首先明确一点，序列是有方向有顺序的，我们把序列都从左向右看，那么预测序列中的1元语法分别为：$P(A)$、$P(B)$、$P(B)$、$P(C)$、$P(D)$，标签序列同理。可知预测标签与标签序列中匹配的1元语法数量为4，分别为：$P(A)$、$P(B)$、$P(C)$、$P(D)$，注意这里是一一对应，所以要去重。而预测序列中1元语法的数量为5，故有$p_1 = 4/5$。对于$p_2$，预测序列中的2元语法分别为：$P(B \mid A)$、$P(B \mid B)$、$P(C \mid B)$、$P(D \mid C)$，后面同理。</p><p>根据上述BLEU的定义，当预测序列与标签序列完全相同时，BLEU为$1$。此外，由于$n$元语法越长则匹配难度越大，所以BLEU为更长的$n$元语法的精确度分配更大的权重。具体来说，当$p_n$固定时，$p_n^{1/2^n}$会随着$n$的增长而增加（原始论文使用$p_n^{1/n}$）。而且由于预测的序列越短获得的$p_n$值越高，所以上式中乘法项之前的系数 $\exp\left(\min\left(0, 1 - \frac{\mathrm{len}_{\text{label}}}{\mathrm{len}_{\text{pred}}}\right)\right)$ 用于惩罚较短的预测序列。例如，当$k=2$时，给定标签序列$A$、$B$、$C$、$D$、$E$、$F$和预测序列$A$、$B$，尽管$p_1 = p_2 = 1$，惩罚因子$\exp(1-6/2) \approx 0.14$会降低BLEU。BLEU的代码实现如下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bleu</span>(<span class="params">pred_seq, label_seq, k</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算BLEU&quot;&quot;&quot;</span></span><br><span class="line">    pred_tokens, label_tokens = pred_seq.split(<span class="string">&#x27; &#x27;</span>), label_seq.split(<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">    len_pred, len_label = <span class="built_in">len</span>(pred_tokens), <span class="built_in">len</span>(label_tokens)</span><br><span class="line">    score = math.exp(<span class="built_in">min</span>(<span class="number">0</span>, <span class="number">1</span> - len_label / len_pred))</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, k + <span class="number">1</span>):</span><br><span class="line">        num_matches, label_subs = <span class="number">0</span>, collections.defaultdict(<span class="built_in">int</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(len_label - n + <span class="number">1</span>):</span><br><span class="line">            label_subs[<span class="string">&#x27; &#x27;</span>.join(label_tokens[i: i + n])] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(len_pred - n + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> label_subs[<span class="string">&#x27; &#x27;</span>.join(pred_tokens[i: i + n])] &gt; <span class="number">0</span>:</span><br><span class="line">                num_matches += <span class="number">1</span></span><br><span class="line">                label_subs[<span class="string">&#x27; &#x27;</span>.join(pred_tokens[i: i + n])] -= <span class="number">1</span></span><br><span class="line">        score *= math.<span class="built_in">pow</span>(num_matches / (len_pred - n + <span class="number">1</span>), math.<span class="built_in">pow</span>(<span class="number">0.5</span>, n))</span><br><span class="line">    <span class="keyword">return</span> score</span><br></pre></td></tr></table></figure><p>最后，利用训练好的循环神经网络“编码器－解码器”模型， 将几个英语句子翻译成法语，并计算BLEU的最终结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">engs = [<span class="string">&#x27;go .&#x27;</span>, <span class="string">&quot;i lost .&quot;</span>, <span class="string">&#x27;he\&#x27;s calm .&#x27;</span>, <span class="string">&#x27;i\&#x27;m home .&#x27;</span>]</span><br><span class="line">fras = [<span class="string">&#x27;va !&#x27;</span>, <span class="string">&#x27;j\&#x27;ai perdu .&#x27;</span>, <span class="string">&#x27;il est calme .&#x27;</span>, <span class="string">&#x27;je suis chez moi .&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> eng, fra <span class="keyword">in</span> <span class="built_in">zip</span>(engs, fras):</span><br><span class="line">    translation, attention_weight_seq = predict_seq2seq(</span><br><span class="line">        net, eng, src_vocab, tgt_vocab, num_steps, device)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;eng&#125;</span> =&gt; <span class="subst">&#123;translation&#125;</span>, bleu <span class="subst">&#123;bleu(translation, fra, k=<span class="number">2</span>):<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">go . =&gt; va !, bleu <span class="number">1.000</span></span><br><span class="line">i lost . =&gt; j<span class="string">&#x27;ai perdu ?, bleu 0.687</span></span><br><span class="line"><span class="string">he&#x27;</span>s calm . =&gt; soyez &lt;unk&gt; !, bleu <span class="number">0.000</span></span><br><span class="line">i<span class="string">&#x27;m home . =&gt; je suis chez moi qui l&#x27;</span>ai vu ., bleu <span class="number">0.640</span></span><br></pre></td></tr></table></figure><p><strong>第一次阅读，只对我注意到的细节部分做一些解释，实际上在更大的层面上，我完全是一知半解，甚至一窍不通，由于没有决定是否选择NLP方向，所以并没有追求一次性完全弄懂训练、预测过程中的所有细节，留待以后回看吧。 — SilenceZheng于22.09.07</strong></p><h2 id="8-束搜索"><a href="#8-束搜索" class="headerlink" title="8. 束搜索"></a>8. 束搜索</h2><p>上节中，我们逐个预测输出序列，直到预测序列中出现特定的序列结束词元“&lt;eos&gt;”。本节将首先介绍<em>贪心搜索</em>（greedy search）策略，并探讨其存在的问题，然后对比其他替代策略：<em>穷举搜索</em>（exhaustive search）和<em>束搜索</em>（beam search）。</p><p>在正式介绍贪心搜索之前，使用与上节中相同的数学符号定义搜索问题。在任意时间步$t’$，解码器输出$y_{t’}$的概率取决于时间步$t’$之前的输出子序列$y_1, \ldots, y_{t’-1}$和对输入序列的信息进行编码得到的上下文变量$\mathbf{c}$。为了量化计算代价，用$\mathcal{Y}$表示输出词表，其中包含“&lt;eos&gt;”，所以这个词汇集合的基数$\left|\mathcal{Y}\right|$就是词表的大小。再将输出序列的最大词元数指定为$T’$。则我们的目标是从所有$\mathcal{O}(\left|\mathcal{Y}\right|^{T’})$个可能的输出序列中寻找理想的输出。这种计算方式略微高估了可能输出的数量，因为对于所有输出序列，在“&lt;eos&gt;”之后的部分将在实际输出中丢弃。但大体上这个数字反应了搜索空间的大小。</p><h3 id="8-1-贪心搜索"><a href="#8-1-贪心搜索" class="headerlink" title="8.1. 贪心搜索"></a>8.1. 贪心搜索</h3><p>首先看一个简单的策略：<em>贪心搜索</em>，该策略已用于<a href="#76-预测序列的评估">上节</a>的序列预测。对于输出序列的每一时间步$t’$，我们都将基于贪心搜索从$\mathcal{Y}$中找到具有最高条件概率的词元，即：</p><script type="math/tex; mode=display">y_{t'} = \operatorname*{argmax}_{y \in \mathcal{Y}} P(y \mid y_1, \ldots, y_{t'-1}, \mathbf{c})</script><p>一旦输出序列包含了“&lt;eos&gt;”或者达到其最大长度$T’$，则输出完成。</p><p><img src="/assets/post_img/article58/s2s-prob1.svg" alt="在每个时间步，贪心搜索选择具有最高条件概率的词元"></p><p>如图，假设输出中有四个词元“A”“B”“C”和“&lt;eos&gt;”。每个时间步下的四个数字分别表示在该时间步生成“A”“B”“C”和“&lt;eos&gt;”的条件概率。在每个时间步，贪心搜索选择具有最高条件概率的词元。因此图中预测输出序列为“A”“B”“C”和“&lt;eos&gt;”。这个输出序列的条件概率是$0.5\times0.4\times0.4\times0.6 = 0.048$。</p><p>那么贪心搜索存在的问题是什么呢？现实中，<em>最优序列</em>（optimal sequence）应该是最大化$\prod_{t’=1}^{T’} P(y_{t’} \mid y_1, \ldots, y_{t’-1}, \mathbf{c})$值的输出序列，这是基于输入序列生成输出序列的条件概率。贪心搜索无法保证得到最优序列。</p><p><img src="/assets/post_img/article58/s2s-prob2.svg" alt="在时间步2，选择具有第二高条件概率的词元“C”（而非最高条件概率的词元）"></p><p>上图中的另一个例子阐述了这个问题。与第一种情况不同，在时间步$2$中，我们选择词元“C”，它具有<em>第二</em>高的条件概率。由于时间步$3$所基于的时间步$1$和$2$处的输出子序列已从 第一种情况中的“A”和“B”改变为上图中的“A”和“C”，因此时间步$3$处的每个词元的条件概率也在上图中改变。假设我们在时间步$3$选择词元“B”，于是当前的时间步$4$基于前三个时间步的输出子序列“A”“C”和“B”为条件，这与第一种情况中的“A”“B”和“C”不同。此时上图中的时间步$4$生成每个词元的条件概率也不同于第一种情况中的条件概率。结果，上图中的输出序列“A”“C”“B”和“&lt;eos&gt;”的条件概率为$0.5\times0.3 \times0.6\times0.6=0.054$，大于第一种情况中的贪心搜索的条件概率。这个例子说明：贪心搜索获得的输出序列“A”“B”“C”和“&lt;eos&gt;”不一定是最佳序列。</p><h3 id="8-2-穷举搜索"><a href="#8-2-穷举搜索" class="headerlink" title="8.2. 穷举搜索"></a>8.2. 穷举搜索</h3><p>如果目标是获得最优序列，可以考虑使用<em>穷举搜索</em>（exhaustive search）：穷举地列举所有可能的输出序列及其条件概率，然后计算输出条件概率最高的一个。</p><p>虽然我们可以使用穷举搜索来获得最优序列，但其计算量$\mathcal{O}(\left|\mathcal{Y}\right|^{T’})$高的惊人。例如，当$|\mathcal{Y}|=10000$和$T’=10$时，我们需要评估$10000^{10} = 10^{40}$序列，这是一个极大的数，现有的计算机几乎不可能计算它。然而贪心搜索的计算量$\mathcal{O}(\left|\mathcal{Y}\right|T’)$要显著地小于穷举搜索。例如，当$|\mathcal{Y}|=10000$和$T’=10$时，我们只需要评估$10000\times10=10^5$个序列。</p><h3 id="8-3-束搜索"><a href="#8-3-束搜索" class="headerlink" title="8.3. 束搜索"></a>8.3. 束搜索</h3><p>那么该选取哪种序列搜索策略呢？如果精度最重要，则显然是穷举搜索。如果计算成本最重要，则显然是贪心搜索。而束搜索的实际应用则介于这两个极端之间。</p><p><em>束搜索</em>（beam search）是贪心搜索的一个改进版本。它有一个超参数，名为<em>束宽</em>（beam size）$k$。在时间步$1$，我们选择具有最高条件概率的$k$个词元。这$k$个词元将分别是$k$个候选输出序列的第一个词元。在随后的每个时间步，基于上一时间步的$k$个候选输出序列，我们将继续从$k\left|\mathcal{Y}\right|$个可能的选择中挑出具有最高条件概率的$k$个候选输出序列。</p><p><img src="/assets/post_img/article58/beam-search.svg" alt="束搜索过程（束宽：2，输出序列的最大长度：3）。候选输出序列是$A$、$C$、$AB$、$CE$、$ABD$和$CED$"></p><p>上图演示了束搜索的过程。假设输出的词表只包含五个元素：$\mathcal{Y} = {A, B, C, D, E}$，其中有一个是“&lt;eos&gt;”。设置束宽为$2$，输出序列的最大长度为$3$。在时间步$1$，假设具有最高条件概率$P(y_1 \mid \mathbf{c})$的词元是$A$和$C$。在时间步$2$，我们计算所有$y_2 \in \mathcal{Y}$为：</p><script type="math/tex; mode=display">\begin{aligned}P(A, y_2 \mid \mathbf{c}) = P(A \mid \mathbf{c})P(y_2 \mid A, \mathbf{c}),\\ P(C, y_2 \mid \mathbf{c}) = P(C \mid \mathbf{c})P(y_2 \mid C, \mathbf{c}),\end{aligned}</script><p>从这十个值中选择最大的两个，比如$P(A, B \mid \mathbf{c})$和$P(C, E \mid \mathbf{c})$。然后在时间步$3$，我们计算所有$y_3 \in \mathcal{Y}$为：</p><script type="math/tex; mode=display">\begin{aligned}P(A, B, y_3 \mid \mathbf{c}) = P(A, B \mid \mathbf{c})P(y_3 \mid A, B, \mathbf{c}),\\P(C, E, y_3 \mid \mathbf{c}) = P(C, E \mid \mathbf{c})P(y_3 \mid C, E, \mathbf{c}),\end{aligned}</script><p>从这十个值中选择最大的两个，即$P(A, B, D \mid \mathbf{c})$和$P(C, E, D \mid  \mathbf{c})$，我们会得到六个候选输出序列：</p><p>（1）$A$；（2）$C$；（3）$A,B$；（4）$C,E$；（5）$A,B,D$；（6）$C,E,D$。</p><p>最后，基于这六个序列（例如，丢弃包括“&lt;eos&gt;”和之后的部分），我们获得最终候选输出序列集合。然后我们选择其中条件概率乘积最高的序列作为输出序列：</p><script type="math/tex; mode=display">\frac{1}{L^\alpha} \log P(y_1, \ldots, y_{L}\mid \mathbf{c}) = \frac{1}{L^\alpha} \sum_{t'=1}^L \log P(y_{t'} \mid y_1, \ldots, y_{t'-1}, \mathbf{c}),</script><p>其中$L$是最终候选序列的长度，$\alpha$通常设置为$0.75$。因为一个较长的序列在上式的求和中会有更多的对数项，因此分母中的$L^\alpha$用于惩罚长序列。</p><p>束搜索的计算量为$\mathcal{O}(k\left|\mathcal{Y}\right|T’)$，这个结果介于贪心搜索和穷举搜索之间。实际上，贪心搜索可以看作一种束宽为$1$的特殊类型的束搜索。通过灵活地选择束宽，束搜索可以在正确率和计算代价之间进行权衡。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;p&gt;前一章中介绍了循环神经网络的基础知识，这种网络可以更好地处理序列数据。但对于当今各种各样的序列学习问题，这些技术可能并不够用。&lt;/p&gt;
&lt;p&gt;例如，循环神经网络在实践中一个常见问题是数值不稳定性。尽管我们已经应用了梯度裁剪等技巧来缓解这个问题，但是仍需要通过设计更复杂的序列模型可以进一步处理它。比如两个广泛使用的网络：&lt;em&gt;门控循环单元&lt;/em&gt;（gated recurrent units，GRU）和&lt;em&gt;长短期记忆网络&lt;/em&gt;（long short-term memory，LSTM）。然后本章将基于一个单向隐藏层来扩展循环神经网络架构，描述具有多个隐藏层的深层架构，并讨论基于前向和后向循环计算的双向设计。现代循环网络经常采用这种扩展。在解释这些循环神经网络的变体时将继续利用上一章中的语言建模问题。&lt;br&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://silencezheng.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
