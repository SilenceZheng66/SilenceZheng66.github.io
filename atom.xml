<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>silenceZheng66-BLOG</title>
  
  <subtitle>log</subtitle>
  <link href="http://silencezheng.top/atom.xml" rel="self"/>
  
  <link href="http://silencezheng.top/"/>
  <updated>2022-06-18T15:35:33.561Z</updated>
  <id>http://silencezheng.top/</id>
  
  <author>
    <name>silenceZheng66</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>.gitignore文件使用</title>
    <link href="http://silencezheng.top/2022/06/18/article45/"/>
    <id>http://silencezheng.top/2022/06/18/article45/</id>
    <published>2022-06-18T15:21:59.000Z</published>
    <updated>2022-06-18T15:35:33.561Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>记录一下如何编写git忽略文件，文件的格式，以及使用pycharm插件帮助生成。<br><span id="more"></span></p><h2 id="Git-ignore文件"><a href="#Git-ignore文件" class="headerlink" title="Git ignore文件"></a>Git ignore文件</h2><p>Git的忽略文件名为 <code>.gitignore</code>，在这个文件中列出那些不希望添加到git中的文件名后，当使用<code>git add .</code>时这些文件就会被自动忽略掉。</p><p>忽视文件的格式很简单，同时也支持格式匹配（包括文件和目录），用 <code>*</code> 表示省略，用<code>#</code>表示注释，如：<br><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># Java class <span class="keyword">Files</span></span><br><span class="line"><span class="comment">*.class</span></span><br><span class="line"></span><br><span class="line"># Package <span class="keyword">Files</span></span><br><span class="line"><span class="comment">*.jar</span></span><br><span class="line"><span class="comment">*.war</span></span><br><span class="line"><span class="comment">*.ear</span></span><br><span class="line"></span><br><span class="line"># 忽略名称中末尾为bin的目录</span><br><span class="line"><span class="comment">*bin/</span></span><br><span class="line"></span><br><span class="line"># 忽略名称中间包含bin的目录</span><br><span class="line"><span class="comment">*bin*/</span></span><br></pre></td></tr></table></figure></p><p>GitHub也给出了各种各样项目的忽视文件模版，见<a href="https://github.com/github/gitignore">这里</a>。</p><h2 id="忽视文件的一般原则"><a href="#忽视文件的一般原则" class="headerlink" title="忽视文件的一般原则"></a>忽视文件的一般原则</h2><p>忽略操作系统自动生成的文件，比如缩略图等。</p><p>忽略编译生成的中间文件、可执行文件等，也就是如果一个文件是通过另一个文件自动生成的，那自动生成的文件就没必要放进版本库，比如Java编译产生的.class文件。</p><p>忽略自己带有敏感信息的配置文件，比如存放口令的配置文件。</p><h2 id="Pycharm添加git忽视文件"><a href="#Pycharm添加git忽视文件" class="headerlink" title="Pycharm添加git忽视文件"></a>Pycharm添加git忽视文件</h2><p>下载.ignore插件：<br><img src="/assets/post_img/article45/plugin.png" alt=".ignore"></p><p>然后在项目根目录上右键移到New，就可以看到.ignore的选项。从中选择gitignore文件，还可以选择一些模版创建，也可以直接建立空的忽视文件。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;记录一下如何编写git忽略文件，文件的格式，以及使用pycharm插件帮助生成。&lt;br&gt;</summary>
    
    
    
    
    <category term="Git" scheme="http://silencezheng.top/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>多层感知机--《动手学深度学习》笔记0x04</title>
    <link href="http://silencezheng.top/2022/06/16/article44/"/>
    <id>http://silencezheng.top/2022/06/16/article44/</id>
    <published>2022-06-16T07:03:39.000Z</published>
    <updated>2022-06-18T07:00:39.691Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><p>这一章开始学习真正的深度网络。<br>最简单的深度网络称为多层感知机。多层感知机由多层神经元组成， 每一层与它的上一层相连，从中接收输入； 同时每一层也与它的下一层相连，影响当前层的神经元。<br>这一章从基本的概念介绍开始讲起，包括过拟合、欠拟合和模型选择。 为了解决这些问题，本章将介绍权重衰减和暂退法等正则化技术，以及将讨论数值稳定性和参数初始化相关的问题。最后应用一个真实的案例：房价预测。</p><p>对应实践：<a href="https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x04.ipynb">https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x04.ipynb</a><br><span id="more"></span></p><h3 id="0-1-结论"><a href="#0-1-结论" class="headerlink" title="0.1. 结论"></a>0.1. 结论</h3><ul><li>多层感知机通过激活函数+隐藏层摆脱线性模型的限制。</li><li>欠拟合是指模型无法继续减少训练误差。过拟合是指训练误差远小于验证误差。</li><li>由于不能基于训练误差来估计泛化误差，因此简单地最小化训练误差并不一定意味着泛化误差的减小。机器学习模型需要注意防止过拟合，即防止泛化误差过大。</li><li>验证集可以用于模型选择，但不能过于随意地使用它。</li><li>应该选择一个复杂度适当的模型，避免使用数量不足的训练样本。简单模型导致欠拟合，复杂模型导致过拟合。</li><li>正则化是处理过拟合的常用方法：在训练集的损失函数中加入惩罚项，以降低学习到的模型的复杂度。</li><li>保持模型简单的一个特别的选择是使用$L_2$惩罚的权重衰减。这会导致学习算法在更新步骤中递减权重。权重衰减功能在深度学习框架的优化器中提供。</li><li>在同一训练代码实现中，不同的参数集可以有不同的更新行为。</li><li>暂退法在前向传播过程中，计算每一内部层的同时丢弃一些神经元。</li><li>暂退法可以避免过拟合，它通常与控制权重向量的维数和大小结合使用的。</li><li>暂退法将活性值$h$替换为具有期望值$h$的随机变量。</li><li>暂退法仅在训练期间使用。</li><li>前向传播在神经网络定义的计算图中按顺序计算和存储中间变量，它的顺序是从输入层到输出层。</li><li>反向传播按相反的顺序（从输出层到输入层）计算和存储神经网络的中间变量和参数的梯度。</li><li>在训练深度学习模型时，前向传播和反向传播是相互依赖的。</li><li>训练比预测需要更多的内存。</li><li>梯度消失和梯度爆炸是深度网络中常见的问题。在参数初始化时需要非常小心，以确保梯度和参数可以得到很好的控制。</li><li>需要用启发式的初始化方法来确保初始梯度既不太大也不太小</li><li>ReLU激活函数缓解了梯度消失问题，这样可以加速收敛。</li><li>随机初始化是保证在进行优化前打破对称性的关键。</li><li>Xavier初始化表明，对于每一层，输出的方差不受输入数量的影响，任何梯度的方差不受输出数量的影响。</li><li>在许多情况下，训练集和测试集并不来自同一个分布。这就是所谓的分布偏移。</li><li>真实风险是从真实分布中抽取的所有数据的总体损失的预期。然而，这个数据总体通常是无法获得的。经验风险是训练数据的平均损失，用于近似真实风险。在实践中，我们进行经验风险最小化。</li><li>在相应的假设条件下，可以在测试时检测并纠正协变量偏移和标签偏移。在测试时，不考虑这种偏移可能会成为问题。</li></ul><h2 id="1-多层感知机"><a href="#1-多层感知机" class="headerlink" title="1. 多层感知机"></a>1. 多层感知机</h2><p>开始对深度神经网络的探索！</p><h3 id="1-1-隐藏层"><a href="#1-1-隐藏层" class="headerlink" title="1.1. 隐藏层"></a>1.1. 隐藏层</h3><p>在线性神经网络章节描述了仿射变换， 它是一种带有偏置项的线性变换。<br>softmax回归模型通过单个仿射变换将输入直接映射到输出，然后进行softmax操作。<br>如果我们的标签通过仿射变换后确实与我们的输入数据相关，那么这种方法确实足够了。 但是，仿射变换中的线性是一个很强的假设。</p><h4 id="1-1-1-线性模型可能会出错"><a href="#1-1-1-线性模型可能会出错" class="headerlink" title="1.1.1. 线性模型可能会出错"></a>1.1.1. 线性模型可能会出错</h4><p>线性意味着单调假设： 任何特征的增大都会导致模型输出的增大（如果对应的权重为正）， 或者导致模型输出的减小（如果对应的权重为负）。<br>有时这是有道理的。 例如，如果我们试图预测一个人是否会偿还贷款。 我们可以认为，在其他条件不变的情况下， 收入较高的申请人比收入较低的申请人更有可能偿还贷款。<br>但是，虽然收入与还款概率存在单调性，但它们不是线性相关的。 收入从0增加到5万，可能比从100万增加到105万带来更大的还款可能性。 处理这一问题的一种方法是对我们的数据进行预处理， 使线性变得更合理，如使用收入的对数作为我们的特征。</p><p>然而我们可以很容易找出违反单调性的例子。 例如，我们想要根据体温预测死亡率。 对于体温高于37摄氏度的人来说，温度越高风险越大。 然而，对于体温低于37摄氏度的人来说，温度越高风险就越低。 在这种情况下，我们也可以通过一些巧妙的预处理来解决问题。 例如，我们可以使用与37摄氏度的距离作为特征。</p><p>但如果是对于图像分类问题，单个像素的强度大小对预测整张图片为某个类别的作用并非简单的线性关系。任何像素的重要性都以复杂的方式取决于该像素的上下文（周围像素的值）。 </p><p>然而数据可能会有一种考虑到特征之间的相关交互作用的表示，在这个表示的基础上可以使用线性模型。对于深度神经网络，我们使用观测数据来联合学习<em>隐藏层</em>表示和应用于该表示的线性预测器。</p><h4 id="1-1-2-在网络中加入隐藏层"><a href="#1-1-2-在网络中加入隐藏层" class="headerlink" title="1.1.2. 在网络中加入隐藏层"></a>1.1.2. 在网络中加入隐藏层</h4><p>隐藏层也称隐含层。<br>可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制， 使其能处理更普遍的函数关系类型。 要做到这一点，最简单的方法是将许多<em>全连接层</em>堆叠在一起。 每一层都输出到上面的层，直到生成最后的输出。<br>我们可以把前L-1层看作表示，把最后一层(输出）看作线性预测器。 这种架构通常称为多层感知机（multilayer perceptron），通常缩写为MLP。 下面，我们以图的方式描述了多层感知机（单隐藏层的多层感知机）。<br><img src="/assets/post_img/article44/隐藏层.svg" alt="mlp"></p><p>这个多层感知机有4个输入，3个输出，其隐藏层包含5个隐藏单元。 输入层不涉及任何计算，因此使用此网络产生输出只需要实现隐藏层和输出层的计算。 因此，这个多层感知机中的层数为2。 注意，这两个层都是全连接的。 每个输入都会影响隐藏层中的每个神经元， 而隐藏层中的每个神经元又会影响输出层中的每个神经元。</p><p>但全连接层的多层感知机的参数开销可能很高，在不改变输入或输出大小的情况下，需要在参数节约和模型有效性之间进行权衡。</p><h4 id="1-1-3-从线性到非线性，激活函数！"><a href="#1-1-3-从线性到非线性，激活函数！" class="headerlink" title="1.1.3. 从线性到非线性，激活函数！"></a>1.1.3. 从线性到非线性，激活函数！</h4><p>我们通过矩阵 $\mathbf{X} \in \mathbb{R}^{n \times d}$ 来表示 $n$ 个样本的小批量，其中每个样本具有$d$个输入特征。对于具有$h$个隐藏单元的单隐藏层多层感知机， 用 $\mathbf{H} \in \mathbb{R}^{n \times h}$ 表示隐藏层的输出，称为隐藏表示（hidden representations）。 在数学或代码中，$\mathbf{H}$ 也被称为隐藏层变量（hidden-layer variable）或隐藏变量（hidden variable）。</p><p>因为隐藏层和输出层都是全连接的， 所以我们有隐藏层权重 $\mathbf{W}^{(1)} \in \mathbb{R}^{d \times h}$ 和隐藏层偏置 $\mathbf{b}^{(1)} \in \mathbb{R}^{1 \times h}$ 以及输出层权重 $\mathbf{W}^{(2)} \in \mathbb{R}^{h \times q}$ 和输出层偏置 $\mathbf{b}^{(2)} \in \mathbb{R}^{1 \times q}$。 形式上，我们按如下方式计算单隐藏层多层感知机的输出 $\mathbf{O} \in \mathbb{R}^{n \times q}$：</p><script type="math/tex; mode=display">\begin{split}\begin{aligned} \mathbf{H} & = \mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}, \\ \mathbf{O} & = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}. \end{aligned}\end{split}</script><p>首先回顾一下什么是仿射函数：<br>仿射函数即由由1阶多项式构成的函数，一般形式为 f (x) = A x + b，这里，A 是一个 m×k 矩阵，x 是一个 k 向量,b是一个m向量，实际上反映了一种从 k 维到 m 维的空间映射关系。</p><p>添加隐藏层之后，模型现在需要跟踪和更新额外的参数（权重和偏移），但却仍然是线性模型！因为隐藏单元由输入的仿射函数给出，而输出（在softmax操作前）只是隐藏单元的仿射函数。仿射的仿射依然是仿射函数（线性模型可以表示任意一个仿射函数）。<br>下面证明这一等价性，即对于任意权重值，只需合并隐藏层，便可产生具有参数 $\mathbf{W} = \mathbf{W}^{(1)}\mathbf{W}^{(2)}$ 和  $\mathbf{b} = \mathbf{b}^{(1)} \mathbf{W}^{(2)} + \mathbf{b}^{(2)}$ 的等价单层模型（即同样是线性模型）：</p><script type="math/tex; mode=display">\mathbf{O} = (\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)})\mathbf{W}^{(2)} + \mathbf{b}^{(2)} = \mathbf{X} \mathbf{W}^{(1)}\mathbf{W}^{(2)} + \mathbf{b}^{(1)} \mathbf{W}^{(2)} + \mathbf{b}^{(2)} = \mathbf{X} \mathbf{W} + \mathbf{b}</script><p>因此为了克服线性模型的限制，还需要一个额外的关键要素：在仿射变换之后对每个隐藏单元应用非线性的<strong>激活函数</strong>（activation function）   $\sigma$。 <strong>激活函数</strong>的输出（例如，$\sigma(\cdot)$）被称为活性值（activations）。<br>有了激活函数，就不可能再将我们的多层感知机退化成线性模型，如下：</p><script type="math/tex; mode=display">\begin{split}\begin{aligned} \mathbf{H} & = \sigma(\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}), \\ \mathbf{O} & = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}.\\ \end{aligned}\end{split}</script><p>由于 $\mathbf{X}$ 中的每一行对应于小批量中的一个样本， 出于记号习惯的考量， 我们定义非线性函数 $\sigma$ 也以按行的方式作用于其输入， 即一次计算一个样本（与之前的softmax函数相同）。<br>这里我们应用于隐藏层的激活函数通常不仅按行操作，也按元素操作。 这意味着在计算每一层的线性部分之后，我们可以计算每个活性值，而不需要查看其他隐藏单元所取的值。</p><p>为了构建更通用的多层感知机， 我们可以继续堆叠这样的隐藏层， 例如$\mathbf{H}^{(1)} = \sigma_1(\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}$ 和 $\mathbf{H}^{(2)} = \sigma_2(\mathbf{H}^{(1)} \mathbf{W}^{(2)} + \mathbf{b}^{(2)})$， 一层叠一层，从而产生更有表达能力的模型。</p><h4 id="1-1-4-通用近似定理（Universal-Approximation-Theorem）"><a href="#1-1-4-通用近似定理（Universal-Approximation-Theorem）" class="headerlink" title="1.1.4. 通用近似定理（Universal Approximation Theorem）"></a>1.1.4. 通用近似定理（Universal Approximation Theorem）</h4><p>人工神经网络最有价值的地方可能就在于，它可以在理论上证明：“一个包含足够多隐含层神经元的多层前馈网络，能以任意精度逼近任意预定的连续函数”。这个定理即为通用近似定理，这里的“Universal”，也有人将其翻译成“万能的”，也有译为“万能逼近定理”。</p><p>也就是说，通用近似定理告诉我们，不管连续函数$f(x)$在形式上有多复杂，我们总能确保找到一个神经网络，对任何可能的输入$x$，以任意高的精度（通过增加隐含层神元的个数来提升近似的精度）近似输出$f(x)$（即使函数有多个输入和输出）。</p><p>多层感知机可以通过隐藏神经元，捕捉到输入之间复杂的相互作用， 这些神经元依赖于每个输入的值。 我们可以很容易地设计隐藏节点来执行任意计算。即使是网络只有一个隐藏层，给定足够的神经元和正确的权重， 我们可以对任意函数建模，尽管实际中学习该函数是很困难的。</p><p>虽然一个单隐层网络能学习任何函数， 但我们不应该尝试使用单隐藏层网络来解决所有问题，因为这种网络结构可能会格外庞大，进而无法正确地学习和泛化。通过使用更深（而不是更广）的网络，可以更容易地逼近许多函数。 </p><p>参考：<br><a href="https://blog.csdn.net/qq_41554005/article/details/110821533">通用近似定理（学习笔记）</a></p><h3 id="1-2-激活函数（activation-function）"><a href="#1-2-激活函数（activation-function）" class="headerlink" title="1.2. 激活函数（activation function）"></a>1.2. 激活函数（activation function）</h3><p>激活函数通过计算加权和并加上偏置来确定神经元是否应该被激活，它们将输入信号转换为输出的可微运算。 大多数激活函数都是<em>非线性</em>的。激活函数是深度学习的基础，下面简要介绍一些常见的激活函数。</p><h4 id="1-2-1-ReLU函数"><a href="#1-2-1-ReLU函数" class="headerlink" title="1.2.1. ReLU函数"></a>1.2.1. ReLU函数</h4><p>最受欢迎的激活函数是修正线性单元（Rectified linear unit，ReLU），因为它实现简单，同时在各种预测任务中表现良好。<br>ReLU提供了一种非常简单的非线性变换。 给定元素$x$，ReLU函数被定义为该元素与0的最大值：</p><script type="math/tex; mode=display">\operatorname{ReLU}(x) = \max(x, 0)</script><p>图像：<br><img src="/assets/post_img/article44/relu.png" alt="relu"><br>通俗地说，ReLU函数通过将相应的活性值设为0，仅保留正元素并丢弃所有负元素。ReLU激活函数是分段线性的。</p><p>ReLU导数图像：<br><img src="/assets/post_img/article44/grelu.png" alt="relu"><br>当输入为负时，ReLU函数的导数为0，而当输入为正时，ReLU函数的导数为1。 注意，当输入值精确等于0时，ReLU函数不可导。此时默认使用左侧的导数，即当输入为0时导数为0。可以忽略这种情况，因为输入可能永远都不会是0。<br>“如果微妙的边界条件很重要，我们很可能是在研究数学而非工程”。</p><p>使用ReLU的原因是，它求导表现得特别好：要么让参数消失，要么让参数通过。 这使得优化表现得更好，并且ReLU减轻了困扰以往神经网络的梯度消失问题（稍后详细介绍）。</p><p>ReLU函数有许多变体，包括参数化ReLU函数（pReLU）。该变体为ReLU添加了一个线性项，因此即使参数是负的，某些信息仍然可以通过：</p><script type="math/tex; mode=display">\operatorname{pReLU}(x) = \max(0, x) + \alpha \min(0, x)</script><h4 id="1-2-2-sigmoid函数"><a href="#1-2-2-sigmoid函数" class="headerlink" title="1.2.2. sigmoid函数"></a>1.2.2. sigmoid函数</h4><p>对于一个定义域在$\mathbb{R}$中的输入， sigmoid函数将输入变换为区间(0, 1)上的输出。 因此，sigmoid通常称为挤压函数（squashing function）： 它将范围（-inf, inf）中的任意输入压缩到区间（0, 1）中的某个值：</p><script type="math/tex; mode=display">\operatorname{sigmoid}(x) = \frac{1}{1 + \exp(-x)}</script><p>图像：<br><img src="/assets/post_img/article44/sigmoid.png" alt="relu"><br>当输入接近0时，sigmoid函数接近线性变换。</p><p>在最早的神经网络中，科学家们专注于阈值单元。阈值单元在其输入低于某个阈值时取值0，当输入超过阈值时取值1。<br>当人们逐渐关注到到基于梯度的学习时， sigmoid函数是一个自然的选择，因为它是一个平滑的、可微的阈值单元近似。 当我们想要将输出视作二元分类问题的概率时， sigmoid仍然被广泛用作输出单元上的激活函数 （可以将sigmoid视为softmax的特例）。</p><p>sigmoid函数的导数为下面的公式：</p><script type="math/tex; mode=display">\frac{d}{dx} \operatorname{sigmoid}(x) = \frac{\exp(-x)}{(1 + \exp(-x))^2} = \operatorname{sigmoid}(x)\left(1-\operatorname{sigmoid}(x)\right)</script><p>sigmoid导数图像：<br><img src="/assets/post_img/article44/gsigmoid.png" alt="relu"><br>当输入为0时，sigmoid函数的导数达到最大值0.25； 而输入在任一方向上越远离0点时，导数越接近0。</p><p>然而，sigmoid在隐藏层中已经较少使用，它在大部分时候被更简单、更容易训练的ReLU所取代。 在后面关于循环神经网络的章节中将描述利用sigmoid单元来控制时序信息流的架构。</p><h4 id="1-2-3-tanh函数"><a href="#1-2-3-tanh函数" class="headerlink" title="1.2.3. tanh函数"></a>1.2.3. tanh函数</h4><p>tanh(双曲正切)函数也能将其输入压缩转换到区间(-1, 1)上。 tanh函数的公式如下：</p><script type="math/tex; mode=display">\operatorname{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}</script><p>图像：<br><img src="/assets/post_img/article44/tanh.png" alt="relu"><br>当输入在0附近时，tanh函数接近线性变换。 函数的形状类似于sigmoid函数，不同的是tanh函数关于坐标系原点中心对称。</p><p>tanh函数的导数是：</p><script type="math/tex; mode=display">\frac{d}{dx} \operatorname{tanh}(x) = 1 - \operatorname{tanh}^2(x)</script><p>tanh导数图像：<br><img src="/assets/post_img/article44/gtanh.png" alt="relu"><br>当输入接近0时，tanh函数的导数接近最大值1。与sigmoid函数导数图像中看到的类似，输入在任一方向上越远离0点，导数越接近0。</p><h4 id="1-2-4-关于激活函数的小结"><a href="#1-2-4-关于激活函数的小结" class="headerlink" title="1.2.4. 关于激活函数的小结"></a>1.2.4. 关于激活函数的小结</h4><p>这些知识只是掌握了一个类似于1990年左右深度学习从业者的工具。<br>使用深度学习框架只需几行代码就可以快速构建模型，而以前训练这些网络需要研究人员编写数千行的C或Fortran代码。<br>这些激活函数在框架中可以直接调用，如<code>y = torch.relu(x)</code>。</p><h3 id="1-3-实现多层感知机"><a href="#1-3-实现多层感知机" class="headerlink" title="1.3. 实现多层感知机"></a>1.3. 实现多层感知机</h3><p><strong>从零实现</strong>：<br>继续使用Fashion-MNIST图像分类数据集，并与之前softmax回归的结果进行比较。将每个图像视为具有784个输入特征（使用reshape将每个二维图像转换为一个长度为784的向量）和10个类的简单分类数据集，实现一个具有单隐藏层的多层感知机，包含256个隐藏单元。选择2的若干次幂作为层的宽度（隐含层单元的个数）是因为考虑到内存在硬件中的分配和寻址方式，这么做往往可以在计算上更高效。<br>对于每一层都要记录一个权重矩阵和一个偏置向量，并为损失关于这些参数的梯度分配内存。<br>手动实现一个简单的多层感知机是很容易的。然而如果有大量的层，从零开始实现多层感知机会变得很麻烦（例如，要命名和记录大量参数）。<br><strong>框架实现</strong>：<br>使用高级API更简洁地实现多层感知机。<br>对于相同的分类问题，多层感知机的实现与softmax回归的实现相同，只是多层感知机的实现里增加了带有激活函数的隐藏层。</p><h2 id="2-模型选择、欠拟合和过拟合"><a href="#2-模型选择、欠拟合和过拟合" class="headerlink" title="2. 模型选择、欠拟合和过拟合"></a>2. 模型选择、欠拟合和过拟合</h2><p>机器学习问题中，我们的目标是发现模式（pattern），确定模型真正发现了一种可泛化的模式而不是简单的记住了所有数据。<br>我们的目标是发现某些模式， 这些模式捕捉到了我们训练集潜在总体的规律。 如果成功做到了这点，即使是对以前从未遇到过的个体， 模型也可以成功地评估风险。 如何发现可以泛化的模式是机器学习的根本问题。</p><p>困难在于，在训练模型时，我们只能访问数据中的小部分样本。 最大的公开图像数据集包含大约一百万张图像。 而在大部分时候，我们只能从数千或数万个数据样本中学习。 在大型医院系统中，我们可能会访问数十万份医疗记录。 当我们使用有限的样本时，可能会遇到这样的问题： 当收集到更多的数据时，会发现之前找到的明显关系并不成立。</p><p>模型在训练数据上拟合的比在潜在分布中更接近的现象称为过拟合（overfitting）， 用于对抗过拟合的技术称为正则化（regularization）。<br>比如在使用Fashion-MNIST数据集时如果有足够多的神经元、层数和训练迭代周期， 模型最终可以在训练集上达到完美的精度，但此时在测试集上的准确性却下降了。</p><h3 id="2-1-训练误差和泛化误差"><a href="#2-1-训练误差和泛化误差" class="headerlink" title="2.1. 训练误差和泛化误差"></a>2.1. 训练误差和泛化误差</h3><p>误差越小，模型越有效。<br><strong>训练误差</strong>（training error）是指， 模型在训练数据集上计算得到的误差。<br><strong>泛化误差</strong>（generalization error）是指， 模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。</p><p>泛化误差永远不能被准确地计算出。 这是因为无限多的数据样本是一个虚构的对象。 实际中只能通过将模型应用于一个独立的测试集来估计泛化误差， 该测试集由随机选取的、未曾在训练集中出现的数据样本构成。</p><p>对于之前提到的记住所有数据的模式举一个例子，应用查表法来进行预测，对于$28\times28$的灰度图像，如果每个像素可以取$256$个灰度值中的一个， 则有$256^{784}$个可能的图像。 这意味着指甲大小的低分辨率灰度图像的数量比宇宙中的原子要多得多。 即使我们可能遇到这样的数据，我们也不可能存储整个查找表。</p><h4 id="2-1-1-独立同分布假设"><a href="#2-1-1-独立同分布假设" class="headerlink" title="2.1.1. 独立同分布假设"></a>2.1.1. 独立同分布假设</h4><p>监督学习情景中， 通常会假设训练数据和测试数据都是从相同的分布中独立提取的（独立同分布）。 这意味着对数据进行采样的过程没有进行“记忆”。 换句话说，抽取的第2个样本和第3个样本的相关性， 并不比抽取的第2个样本和第200万个样本的相关性更强。</p><p>但这个假设很多时候是有漏洞的，例如一个模型应用在多个数据集时，不同数据集的分布可能都是不同的。其次，抽样过程可能与时间有关，从而违反独立性。有时轻微违法独立同分布假设时模型依然可以运行良好，但另一些时候在违背独立同分布的数据上应用模型会导致错误。</p><p>当训练模型时，我们试图找到一个能够尽可能拟合训练数据的函数。 但是如果它执行地“太好了”，而不能对看不见的数据做到很好泛化，就会导致过拟合。 这种情况正是我们想要避免或控制的。 深度学习中有许多启发式的技术旨在防止过拟合。</p><h4 id="2-1-2-模型复杂性"><a href="#2-1-2-模型复杂性" class="headerlink" title="2.1.2. 模型复杂性"></a>2.1.2. 模型复杂性</h4><p>模型复杂性由什么构成是一个复杂的问题。 一个模型是否能很好地泛化取决于很多因素。 例如，具有更多参数的模型可能被认为更复杂， 参数有更大取值范围的模型可能更为复杂。 通常对于神经网络，我们认为需要更多训练迭代的模型比较复杂， 而需要“早停”（early stopping）的模型（即较少训练迭代周期）就不那么复杂。</p><p>很难比较本质上不同大类的模型之间（例如，决策树与神经网络）的复杂性。统计学家认为，能够轻松解释任意事实的模型是复杂的， 而表达能力有限但仍能很好地解释数据的模型可能更有现实用途。<br>几个影响模型泛化的因素：</p><ol><li>可调整参数的数量。当可调整参数的数量（有时称为自由度）很大时，模型往往更容易过拟合。</li><li>参数采用的值。当权重的取值范围较大时，模型可能更容易过拟合。</li><li>训练样本的数量。即使你的模型很简单，也很容易过拟合只包含一两个样本的训练集。而过拟合一个有数百万个样本的数据集则需要一个极其灵活的模型。<h3 id="2-2-模型选择"><a href="#2-2-模型选择" class="headerlink" title="2.2. 模型选择"></a>2.2. 模型选择</h3>在机器学习中，我们通常在评估几个候选模型后选择最终的模型。 这个过程叫做模型选择。<br>有时进行比较的模型在本质上是完全不同的（比如，决策树与线性模型）。 又有时我们需要比较不同的超参数设置下的同一类模型。</li></ol><p>例如，训练多层感知机模型时，我们可能希望比较具有不同数量的隐藏层、不同数量的隐藏单元以及不同的的激活函数组合的模型。 为了确定候选模型中的最佳模型，我们通常会使用验证集。</p><h4 id="2-2-1-验证集"><a href="#2-2-1-验证集" class="headerlink" title="2.2.1. 验证集"></a>2.2.1. 验证集</h4><p>原则上，在确定所有的超参数之前，我们不希望用到测试集。 如果在模型选择过程中使用测试数据，可能会有过拟合测试数据的风险。过拟合了训练数据，还可以在测试数据上的评估来判断过拟合。 但我们无法知晓模型是否过拟合了测试数据。<br>因此决不能依靠测试数据进行模型选择，但也不能仅仅依靠训练数据来选择模型，因为我们无法估计训练数据的泛化误差。</p><p>同时虽然理想情况下我们只会使用测试数据一次，以评估最好的模型或比较一些模型效果，但现实是测试数据很少在使用一次后被丢弃。我们很少能有充足的数据来对每一轮实验采用全新测试集。</p><p>解决此问题的常见做法是将数据分成三份， 除了训练和测试数据集之外，还增加一个验证数据集（validation dataset）， 也叫验证集（validation set）。<br>但现实是验证数据和测试数据之间的边界也十分模糊。d2l书中每次实验都在使用训练集和验证集，而没有真正的测试集，实验报告的准确度都是验证集准确度。</p><h4 id="2-2-2-K-折交叉验证"><a href="#2-2-2-K-折交叉验证" class="headerlink" title="2.2.2.  $K$折交叉验证"></a>2.2.2.  $K$折交叉验证</h4><p>当训练数据稀缺时，可能无法提供足够的数据来构成一个合适的验证集。 一个解决方案是采用$K$折交叉验证。<br>原始训练数据被分成$K$个不重叠的子集。 然后执行$K$次模型训练和验证，每次在$K-1$个子集上进行训练， 并在剩余的一个子集（在该轮中没有用于训练的子集）上进行验证。 最后，通过对$K$次实验的结果取平均来估计训练和验证误差。</p><h3 id="2-3-欠拟合，过拟合"><a href="#2-3-欠拟合，过拟合" class="headerlink" title="2.3. 欠拟合，过拟合"></a>2.3. 欠拟合，过拟合</h3><p>比较训练和验证误差时要注意两种常见的情况： 欠拟合，过拟合。<br><strong>欠拟合</strong>：训练误差和验证误差都很严重，但它们之间仅有一点差距。如果模型不能降低训练误差，这可能意味着模型过于简单（即表达能力不足），无法捕获试图学习的模式。 又由于训练和验证误差之间的泛化误差很小， 我们有理由相信可以用一个更复杂的模型降低训练误差。 这种现象被称为欠拟合（underfitting）。</p><p><strong>过拟合</strong>：当我们的训练误差明显低于验证误差时要小心， 这表明严重的过拟合（overfitting）。过拟合并不总是一件坏事，特别是在深度学习领域，最好的预测模型在训练数据上的表现往往比在保留（验证）数据上好得多。最终通常更关心验证误差，而不是训练误差和验证误差之间的差距。</p><p>是否过拟合或欠拟合可能取决于模型复杂性和可用训练数据集的大小。</p><h4 id="2-3-1-模型复杂性"><a href="#2-3-1-模型复杂性" class="headerlink" title="2.3.1. 模型复杂性"></a>2.3.1. 模型复杂性</h4><p>关于过拟合和模型复杂性的经典直觉：给定由单个特征$x$和对应实数标签$y$组成的训练数据， 我们试图找到下面的$d$阶多项式来估计标签$y$。</p><script type="math/tex; mode=display">\hat{y}= \sum_{i=0}^d x^i w_i</script><p>这只是一个线性回归问题，特征是$x$的幂给出的， 模型的权重是$w_i$给出的，偏置是$w_0$给出的 （因为对于所有的$x$都有$x^0$ = 1）。线性回归问题可以使用平方误差作为损失函数。</p><p>高阶多项式函数比低阶多项式函数复杂得多。高阶多项式的参数较多，模型函数的选择范围较广。 因此在固定训练数据集的情况下， 高阶多项式函数相对于低阶多项式的训练误差应该始终更低（最坏也是相等）。 其实当数据样本包含了$x$的不同值时， 函数阶数等于数据样本数量的多项式函数可以完美拟合训练集（因为y就是一个多项式，通过优化参数总可以表示出来y）。下图直观地描述了多项式的阶数和欠拟合与过拟合之间的关系。<br><img src="/assets/post_img/article44/模型复杂度对拟合的影响.svg" alt="fit"></p><h4 id="2-3-2-数据集大小"><a href="#2-3-2-数据集大小" class="headerlink" title="2.3.2. 数据集大小"></a>2.3.2. 数据集大小</h4><p>训练数据集中的样本越少，我们就越有可能（且更严重地）过拟合。 随着训练数据量的增加，泛化误差通常会减小。一般来说，更多的数据不会有什么坏处。<br>对于固定的任务和数据分布，模型复杂性和数据集大小之间通常存在关系。 给出更多的数据，我们可能会尝试拟合一个更复杂的模型。 能够拟合更复杂的模型可能是有益的。 如果没有足够的数据，简单的模型可能更有用。 对于许多任务，深度学习只有在有数千个训练样本时才优于线性模型。 从一定程度上来说，深度学习目前的生机要归功于廉价存储、互联设备以及数字化经济带来的海量数据集。</p><h3 id="2-4-多项式回归"><a href="#2-4-多项式回归" class="headerlink" title="2.4. 多项式回归"></a>2.4. 多项式回归</h3><p>通过多项式拟合来探索上述概念。 一样放到notebook中，话说如果都放到notebook中我为什么不把博客格式都做成notebook…<br>给定$x$，使用以下三阶多项式来生成训练和测试数据的标签：</p><script type="math/tex; mode=display">y = 5 + 1.2x - 3.4\frac{x^2}{2!} + 5.6 \frac{x^3}{3!} + \epsilon \text{ where } \epsilon \sim \mathcal{N}(0, 0.1^2)</script><p>噪声项$\epsilon$服从均值为0且标准差为0.1的正态分布。 在优化的过程中，我们通常希望避免非常大的梯度值或损失值，所以将特征从$x^i$调整为$\frac{x^i}{i!}$，这样可以避免很大的$i$带来的特别大的指数值。训练集和测试集各生成100个样本。</p><p>分别对3阶、2阶（线性函数）和高阶多项式训练并观察结果，可以看出3阶是正常的，线性函数会产生欠拟合问题，而高阶（20）则会产生过拟合问题。<br>结果说明2阶多项式模型过于简单。而在20阶多项式模型的实验中，实际上只有前5阶有非零的权值，其余阶均为0，这种情况下，高阶多项式模型的参数过多，但训练样本只有100个，数量较少，模型无法从中学习到高阶权值应为0的特点，即模型被数据中的噪声轻易的影响了。虽然高阶模型能够得到更小的训练误差，但在测试集上的表现会比在训练集上要差，故过拟合。<br>但对于这个实验，使用高阶多项式模型的损失仍然小于线性模型（在同样的迭代周期下）。</p><h2 id="3-权重衰减"><a href="#3-权重衰减" class="headerlink" title="3. 权重衰减"></a>3. 权重衰减</h2><p>正则化就是把额外的约束或者惩罚项加到已有模型（损失函数）上，以防止过拟合并提高泛化能力。假设我们已经拥有尽可能多的高质量数据，我们便可以将重点放在正则化技术上。</p><p>在多项式回归的例子中， 我们可以通过调整拟合多项式的阶数来限制模型的容量。事实上这种限制特征的数量是缓解过拟合的一种常用技术，但这种直接丢弃特征的做法可能过于生硬。 多项式对多变量数据的自然扩展称为单项式（monomials），也就是多变量幂的乘积。单项式的阶数是幂的和。 例如，$x_1^2 x_2$和$x_3 x_5^2$都是3次单项式。<br>随着阶数$d$的增长，带有阶数$d$的项数迅速增加。 给定$k$个变量，阶数为$d$的项的个数为：（这个式子的含义就是从k-1+d个中取k-1个，即$C^{k-1}_{k-1+d}$）</p><script type="math/tex; mode=display">{k - 1 + d} \choose {k - 1}</script><p>例如对于3个变量（x1，x2，x3）的多项式，阶数为1的项有3个（x1，x2，x3），对应从3个（3-1+1）中选2个（3-1），也就是$C^2_3 = C^1_3 = 3$。具体计算方式如下（就是排列组合）：<br><img src="/assets/post_img/article44/排列组合.jpeg" alt="排列组合"><br>那么对于2阶项就对应从4个（3-1+2）中选2个，即$C^2_4 = 6种$，分别是：x1x2，x1x3，x2x3和三个变量单独的2次项。<br>因此即使是阶数上的微小变化，也会显著增加模型的复杂性。仅仅通过简单的限制特征数量（在多项式回归中体现为限制阶数）可能仍然使模型在过简单和过复杂中徘徊，我们需要一个更细粒度的工具来调整函数的复杂性，使其达到一个合适的平衡位置。</p><h3 id="3-1-范数与权重衰减"><a href="#3-1-范数与权重衰减" class="headerlink" title="3.1. 范数与权重衰减"></a>3.1. 范数与权重衰减</h3><p>回顾范数的三个性质，以及$L_1$范数：向量元素的绝对值之和 和 $L_2$范数：向量元素平方和的平方根。</p><p>在训练参数化机器学习模型时， 权重衰减（weight decay）是最广泛使用的正则化的技术之一， 它通常也被称为$L_2$正则化。 这项技术通过函数与零的距离来衡量函数的复杂度， 因为在所有函数$f$中，函数$f = 0$（所有输入都得到值$0$） 在某种意义上是最简单的。 但是应该如何精确地测量一个函数和零之间的距离并没有标准答案。</p><p>一种简单的方法是通过线性函数 $f(\mathbf{x}) = \mathbf{w}^\top \mathbf{x}$ 中的权重向量的某个范数来度量其复杂性（这是一种正则化线性模型）， 例如$| \mathbf{w} |^2$。 要保证权重向量比较小， 最常用方法是将其范数作为惩罚项加到最小化损失的问题中。 （为什么权重大会导致模型复杂度提高？）</p><p>这样一来训练目标就由最小化训练标签上的预测损失变为最小化预测损失和惩罚项之和。 此时如果权重向量增长的太大， 我们的学习算法可能会更集中于最小化权重范数$| \mathbf{w} |^2$（简而言之我们想起到的效果就是在最小化损失函数的同时保证权重不太大，至于为什么这样做，不太懂）。现在回顾一下之前的线性回归例子。其中损失由下式给出：</p><script type="math/tex; mode=display">L(\mathbf{w}, b) = \frac{1}{n}\sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2</script><p>$\mathbf{x}^{(i)}$是样本$i$的特征， $y^{(i)}$是样本$i$的标签， $(\mathbf{w}, b)$是权重和偏置参数。 为了惩罚权重向量的大小， 我们必须以某种方式在损失函数中添加$| \mathbf{w} |^2$， 但是模型应该如何平衡这个新的额外惩罚的损失？通常通过正则化常数 $\lambda$ 来描述这种权衡， 这是一个非负超参数，我们使用验证数据拟合：</p><script type="math/tex; mode=display">L(\mathbf{w}, b) + \frac{\lambda}{2} \|\mathbf{w}\|^2</script><p>这样操作后对于$\lambda = 0$，保持了原来的损失函数。 对于$\lambda &gt; 0$，则可以通过这一惩罚项来增大损失函数，从而使参数向$| \mathbf{w} |$更小的方向进行优化（我的理解是这样的）。 这里仍然除以$2$是因为当求导时， $2$和$1/2$会抵消，以确保更新表达式看起来既漂亮又简单。 关于为什么这里使用范数的平方而不是标准范数（即欧几里得距离）则是为了便于计算，通过使用$L_2$范数的平方（之前也提到过机器学习经常使用该范数的平方而不是它本身），我们去掉平方根，留下权重向量每个分量的平方和。 这使得惩罚的导数很容易计算，即惩罚项导数的和等于和的导数。</p><p>此外，关于为什么我们首先使用$L_2$范数，而不是$L_1$范数。这是因为这个选择在整个统计领域中都是有效的和受欢迎的。 $L_2$正则化线性模型构成经典的岭回归（ridge regression）算法， $L_1$正则化线性回归是统计学中类似的基本模型， 通常被称为套索回归（lasso regression）。 </p><p>使用$L_2$范数的一个原因是它对权重向量的大分量施加了巨大的惩罚。 这使得我们的学习算法偏向于在大量特征上均匀分布权重的模型。 在实践中，这可能使它们对单个变量中的观测误差更为稳定。 相比之下，$L_1$惩罚会导致模型将权重集中在一小部分特征上， 而将其他权重清除为零。 这称为<strong>特征选择</strong>（feature selection），这可能是其他场景下需要的。<br>$L_2$正则化回归的小批量随机梯度下降更新如下式，其中$\mathcal{B}$为小批量，$\eta$为预先确定的正数，$\lambda$是正则化常数 ：</p><script type="math/tex; mode=display">\begin{aligned} \mathbf{w} & \leftarrow \left(1- \eta\lambda \right) \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right). \end{aligned}</script><p>我们依然是根据估计值与观测值之间的差异来更新$\mathbf{w}$，但同时也在试图将$\mathbf{w}$的大小缩小到零（有点没看懂）。 这就是为什么这种方法有时被称为权重衰减。 我们仅考虑惩罚项，优化算法在训练的每一步衰减权重。 与特征选择相比，权重衰减为我们提供了一种连续的机制来调整函数的复杂度。 较小的$\lambda$值对应较少约束的$\mathbf{w}$， 而较大的$\lambda$值对$\mathbf{w}$的约束更大（这里约束就是使值更小的意思吧）。</p><p>是否对相应的偏置$b^2$（为什么是b方？可能也是范数平方吧）进行惩罚在不同的实践中会有所不同， 在神经网络的不同层中也会有所不同。 通常，网络输出层的偏置项不会被正则化。</p><h3 id="3-2-权重衰减演示的从零实现"><a href="#3-2-权重衰减演示的从零实现" class="headerlink" title="3.2. 权重衰减演示的从零实现"></a>3.2. 权重衰减演示的从零实现</h3><p>通过高维线性回归演示如何实现权重衰减。（见实践）<br>简单说就是先生成了一些高维（200个特征）的数据，用如下公式：</p><script type="math/tex; mode=display">y = 0.05 + \sum_{i = 1}^d 0.01 x_i + \epsilon \text{ where } \epsilon \sim \mathcal{N}(0, 0.01^2)</script><p>标签是关于输入的线性函数。 标签同时被均值为0，标准差为0.01高斯噪声破坏。</p><p>然后将$L_2$的平方惩罚添加到原始目标函数中。最终通过对比使用正则化和不使用正则化的过拟合优化情况可以看到正则化后测试误差减小。</p><h3 id="3-3-权重衰减演示的框架实现"><a href="#3-3-权重衰减演示的框架实现" class="headerlink" title="3.3. 权重衰减演示的框架实现"></a>3.3. 权重衰减演示的框架实现</h3><p>由于权重衰减在神经网络优化中很常用， 深度学习框架为了便于我们使用权重衰减， 将权重衰减集成到优化算法中，以便与任何损失函数结合使用。 此外，这种集成还有计算上的好处， 允许在不增加任何额外的计算开销的情况下向算法中添加权重衰减。 由于更新的权重衰减部分仅依赖于每个参数的当前值， 因此优化器必须至少接触每个参数一次。</p><h2 id="4-暂退法（Dropout）"><a href="#4-暂退法（Dropout）" class="headerlink" title="4. 暂退法（Dropout）"></a>4. 暂退法（Dropout）</h2><p>在概率角度看，可以通过以下论证来证明这一技术的合理性： 我们已经假设了一个先验，即权重的值取自均值为0的正态分布。我们希望模型深度挖掘特征，即将其权重分散到许多特征中， 而不是过于依赖少数潜在的虚假关联。</p><h3 id="4-1-重新审视过拟合"><a href="#4-1-重新审视过拟合" class="headerlink" title="4.1. 重新审视过拟合"></a>4.1. 重新审视过拟合</h3><p>当面对更多的特征而样本不足时，线性模型往往会过拟合。 相反，当给出更多样本而不是特征，通常线性模型不会过拟合。<br>但线性模型泛化的可靠性是有代价的，线性模型不会考虑到特征之间的交互作用。对于每个特征，线性模型必须指定正的或负的权重，而忽略其他特征。</p><p>泛化性和灵活性之间的这种基本权衡被描述为偏差-方差权衡（bias-variance tradeoff）。 线性模型有很高的偏差：它们只能表示一小类函数。 然而，这些模型的方差很低（泛化性好）：它们在不同的随机数据样本上可以得出相似的结果。</p><p>深度神经网络位于偏差-方差谱的另一端。 与线性模型不同，神经网络并不局限于单独查看每个特征，而是学习特征之间的交互。 例如，神经网络可能推断“尼日利亚”和“西联汇款”一起出现在电子邮件中表示垃圾邮件， 但单独出现则不表示垃圾邮件。</p><p>即使我们有比特征多得多的样本，深度神经网络也有可能过拟合。深度网络的泛化性质令人费解，而这种泛化性质的数学基础仍然是悬而未决的研究问题。</p><h3 id="4-2-扰动的稳健性"><a href="#4-2-扰动的稳健性" class="headerlink" title="4.2. 扰动的稳健性"></a>4.2. 扰动的稳健性</h3><p>在探究泛化性之前，我们先来定义一下什么是一个“好”的预测模型？ 我们期待“好”的预测模型能在未知的数据上有很好的表现： 经典泛化理论认为，为了缩小训练和测试性能之间的差距，应该以简单的模型为目标。 简单性以较小维度的形式展现，此外，参数的范数也代表了一种有用的简单性度量。</p><p>简单性的另一个角度是平滑性，即函数不应该对其输入的微小变化敏感。 例如，当我们对图像进行分类时，我们预计向像素添加一些随机噪声应该是基本无影响的。1995年，克里斯托弗·毕晓普用数学证实了“要求函数光滑”和“要求函数对输入的随机噪声具有适应性”之间的联系。</p><p>在2014年，斯里瓦斯塔瓦等人就如何将毕晓普的想法应用于网络的内部层提出了一个想法： 在训练过程中，他们建议在计算后续层之前向网络的每一层注入噪声。 因为当训练一个有多层的深层网络时，注入噪声只会在输入-输出映射上增强平滑性。</p><p>这个想法被称为<strong>暂退法（dropout）</strong>。 暂退法在<strong>前向传播</strong>过程中，计算每一内部层的同时注入噪声，这已经成为训练神经网络的常用技术。 这种方法之所以被称为暂退法，因为我们从表面上看是在训练过程中丢弃（drop out）一些神经元。 在整个训练过程的每一次迭代中，标准暂退法包括在计算下一层之前将当前层中的一些节点置零。<br>暂退法的原始论文提到了一个关于有性繁殖的类比： 神经网络过拟合与每一层都依赖于前一层激活值相关，称这种情况为“共适应性”。 作者认为，暂退法会破坏共适应性，就像有性生殖会破坏共适应的基因一样。</p><p>那么关键的挑战就是如何注入这种噪声。 一种想法是以一种无偏向（unbiased）的方式注入噪声。 这样在固定住其他层时，每一层的期望值等于没有噪音时的值。</p><p>在毕晓普的工作中，他将正态分布的噪声添加到线性模型的输入中。 在每次训练迭代中，他将从均值为零的分布$\epsilon \sim \mathcal{N}(0,\sigma^2)$ 采样噪声添加到输入$\mathbf{x}$， 从而产生扰动点$\mathbf{x}’ = \mathbf{x} + \epsilon$， 预期是$E[\mathbf{x}’] = \mathbf{x}$。</p><p>在标准暂退法正则化中，通过按保留（未丢弃）的节点的分数进行规范化来消除每一层的偏差。 换言之，每个中间活性值$h$以暂退概率$p$由随机变量$h’$替换，如下所示：</p><script type="math/tex; mode=display">\begin{split}\begin{aligned} h' = \begin{cases} 0 & \text{ 概率为 } p \\ \frac{h}{1-p} & \text{ 其他情况} \end{cases} \end{aligned}\end{split}</script><p>根据此模型的设计，其期望值保持不变，即$E[h’] = h$。</p><h3 id="4-3-暂退法实践"><a href="#4-3-暂退法实践" class="headerlink" title="4.3. 暂退法实践"></a>4.3. 暂退法实践</h3><p>下图为dropout前后的多层感知机，在左图中，删除了$h_2$和$h_5$， 因此输出的计算不再依赖于$h_2$或$h_5$，并且它们各自的梯度在执行反向传播时也会消失。 这样，输出层的计算不能过度依赖于$h_1, \ldots, h_5$的任何一个元素。<br><img src="/assets/post_img/article44/dropout.svg" alt="暂退法"><br>通常在测试时不使用暂退法（训练时用）。 给定一个训练好的模型和一个新的样本，我们不会丢弃任何节点，因此不需要标准化。 然而也有一些例外：一些研究人员在测试时使用暂退法， 用于估计神经网络预测的“不确定性”： 如果通过许多不同的暂退法遮盖后得到的预测结果都是一致的，那么我们可以说网络发挥更稳定。</p><h3 id="4-4-暂退法的实现"><a href="#4-4-暂退法的实现" class="headerlink" title="4.4. 暂退法的实现"></a>4.4. 暂退法的实现</h3><p>从零实现和框架实现具体见<a href="https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x04.ipynb">对应实践</a>,几个可以注意的点如下：</p><ul><li>在靠近输入层的地方设置较低的暂退概率。</li><li>框架中，在训练时，Dropout层将根据指定的暂退概率随机丢弃上一层的输出（相当于下一层的输入）。 在测试时，Dropout层仅传递数据。</li></ul><h2 id="5-前向传播与反向传播"><a href="#5-前向传播与反向传播" class="headerlink" title="5. 前向传播与反向传播"></a>5. 前向传播与反向传播</h2><p>前向传播（forward propagation或forward pass） 指的是：按顺序（从输入层到输出层）计算和存储神经网络中每层的结果。</p><p>反向传播（backward propagation或backpropagation）指的是计算神经网络参数梯度的方法。 简言之，该方法根据微积分中的链式规则，按相反的顺序从输出层到输入层遍历网络。 该算法存储了计算某些参数梯度时所需的任何中间变量（偏导数）。假设有函数 $Y=f(\mathrm{X})$ 和 $\mathrm{Z}=g(\mathrm{Y})$, 其中输入 和输出 $X, Y, Z$ 是任意形状的张量。利用链式法则, 我们可以计算Z关于 $X$ 的导数</p><script type="math/tex; mode=display">\frac{\partial \mathrm{Z}}{\partial \mathrm{X}}=\operatorname{prod}\left(\frac{\partial \mathrm{Z}}{\partial \mathrm{Y}}, \frac{\partial \mathrm{Y}}{\partial \mathrm{X}}\right)</script><p>这里使用prod运算符在执行必要的操作 (如换位和交换输入位置) 后将其参数相乘。对于向量只是矩阵矩阵乘法。对于高维张量则使用适当的对应项。运算符prod指代了所有的这些符号。</p><p>在训练神经网络时，在初始化模型参数后， 我们交替使用前向传播和反向传播，利用反向传播给出的梯度来更新模型参数。 注意，反向传播重复利用前向传播中存储的中间值，以避免重复计算。 带来的影响之一是我们需要保留中间值，直到反向传播完成。 这也是训练比单纯的预测需要更多的内存（显存）的原因之一。 此外，这些中间值的大小与网络层的数量和批量的大小大致成正比。 因此，使用更大的批量来训练更深层次的网络更容易导致内存不足（out of memory）错误。</p><h2 id="6-数值稳定性和模型初始化"><a href="#6-数值稳定性和模型初始化" class="headerlink" title="6. 数值稳定性和模型初始化"></a>6. 数值稳定性和模型初始化</h2><p>到目前为止学习的每个模型都是根据某个预先指定的分布来初始化模型的参数。但事实上这种初始化方案并不是理所当然的，初始化方案的选择在神经网络学习中起着举足轻重的作用，它对保持数值稳定性至关重要。 此外，这些初始化方案的选择可以与非线性激活函数的选择有趣的结合在一起。 我们选择哪个函数以及如何初始化参数可以决定优化算法收敛的速度有多快。 糟糕选择可能会导致我们在训练时遇到<strong>梯度爆炸</strong>或<strong>梯度消失</strong>。<br>一些有用的启发式方法在整个深度学习生涯中都很有用。</p><h3 id="6-1-梯度消失和梯度爆炸"><a href="#6-1-梯度消失和梯度爆炸" class="headerlink" title="6.1. 梯度消失和梯度爆炸"></a>6.1. 梯度消失和梯度爆炸</h3><p>在深度神经网络中输出关于任何一组参数（权重）的梯度都可以表示n个矩阵与梯度向量的乘积。其中n个矩阵和梯度向量取决于关于哪组参数求梯度，设当前深度神经网络为L层，当关于 $l$ 层参数时，n为 $L-l$ ，梯度向量为$\partial_{\mathbf{W}}(l) \boldsymbol{h}^{(l)}$。<br>注：每一层 $l$ 由变换 $f_{l}$ 定义, 该变换的参数为权重 $\mathbf{W}^{(l)}$, 其隐藏变量是 $\mathbf{h}^{(l)}$ (令 $\mathbf{h}^{(0)}=输入\mathbf{x}$ )。</p><p>这样不稳定的梯度（太多概率相乘）造成两个问题：<br>一是容易受到数值下溢影响（概率乘积过小），即便对概率取对数，将数值表示的压力从尾数转移到指数，也有可能出现数值溢出问题（当概率趋近零，取对数后对数值趋近负无穷）。</p><p>二是威胁到优化算法的稳定性。 此时面临的问题要么是<strong>梯度爆炸</strong>（gradient exploding）问题： 参数更新过大，破坏了模型的稳定收敛； 要么是<strong>梯度消失</strong>（gradient vanishing）问题： 参数更新过小，在每次更新时几乎不会移动，导致模型无法学习。</p><h4 id="6-1-1-梯度消失（gradient-vanishing）"><a href="#6-1-1-梯度消失（gradient-vanishing）" class="headerlink" title="6.1.1. 梯度消失（gradient vanishing）"></a>6.1.1. 梯度消失（gradient vanishing）</h4><p>sigmoid函数曾经是导致梯度消失问题的一个常见的原因。<br><img src="/assets/post_img/article44/sigmoid_gradient.svg" alt="sg"><br>当sigmoid函数的输入很大或是很小时，它的梯度都会消失。 此外，当反向传播通过许多层时，除非我们在刚刚好的地方， 这些地方sigmoid函数的输入接近于零，否则整个乘积的梯度可能会消失。 当我们的网络有很多层时，除非我们很小心，否则在某一层可能会切断梯度。 这个问题曾经困扰着深度网络的训练，因此，更稳定的ReLU系列函数已经成为从业者的默认选择（虽然在神经科学的角度看起来不太合理）。</p><h4 id="6-1-2-梯度爆炸（gradient-exploding）"><a href="#6-1-2-梯度爆炸（gradient-exploding）" class="headerlink" title="6.1.2. 梯度爆炸（gradient exploding）"></a>6.1.2. 梯度爆炸（gradient exploding）</h4><p>当梯度爆炸是由于深度网络的初始化所导致时，我们没有机会让梯度下降优化器收敛。</p><h4 id="6-1-3-打破对称性"><a href="#6-1-3-打破对称性" class="headerlink" title="6.1.3. 打破对称性"></a>6.1.3. 打破对称性</h4><p>神经网络设计中的另一个问题是其参数化所固有的对称性。例如对于简单的多层感知机，每一层的隐藏单元之间具有排列对称性。</p><p>例如对于一个含两个隐藏单元隐藏层的多层感知机，输出层将两个隐藏单元的多层感知机转换为仅一个输出单元。如果将隐藏层的所有参数初始化为 $\mathbf{W}^{(1)}=c ， c$ 为常量。在这种情况下, 在前向传播期间, 两个隐藏单元采用相同的输入和参数, 产生相同的激活, 该激活被送到输出单元。在反向传播期间, 根据参数 $\mathbf{W}^{(1)}$ 对输出单元进行微分, 得到一个梯度, 其元素都取相同的值。因此，在基于梯度的迭代 (例如, 小批量随机梯度下降）之后, $\mathbf{W}^{(1)}$ 的所有元素仍然采用相同的值。这样的迭代永远不会打破对称性, 则可能永远也无法实现网络的表达能力。隐藏层的行为就好像只有一个单元。 小批量随机梯度下降不会打破这种对称性，但暂退法正则化可以。</p><h3 id="6-2-参数初始化"><a href="#6-2-参数初始化" class="headerlink" title="6.2. 参数初始化"></a>6.2. 参数初始化</h3><p>减缓上述三个问题的一种办法是在参数初始化方面进行改良，在优化期间做一些工作和适当的正则化也可以进一步提高稳定性。</p><h4 id="6-2-1-默认初始化"><a href="#6-2-1-默认初始化" class="headerlink" title="6.2.1. 默认初始化"></a>6.2.1. 默认初始化</h4><p>如果不人为指定初始化方法，框架将使用默认的随机初始化方法，对于中等难度的问题，这种方法通常很有效。</p><h4 id="6-2-2-Xavier初始化"><a href="#6-2-2-Xavier初始化" class="headerlink" title="6.2.2. Xavier初始化"></a>6.2.2. Xavier初始化</h4><p>对于只有线性计算的全连接层的输出$o_{i}$，设有$n_{\mathrm{in}}$个输入 $x_{j}$ 及对应的相关权重 $w_{i j}$，则输出可由下式表示：</p><script type="math/tex; mode=display">o_{i}=\sum_{j=1}^{n_{\text {in }}} w_{i j} x_{j}</script><p>权重 $w_{i j}$ 都是从同一分布中独立抽取的。假设该分布（并不一定是正态分布，只是需要均值和方差存在）具有零均值和方差 $\sigma^{2}$，同时假设层 $x_{j}$ 的输入也具有零均值和方差 $\gamma^{2}$, 它们独立于 $w_{i j}$ 并且彼此独立。在这种假设下就可以按如下方式计算 $o_{i}$ 的期望和方差：</p><script type="math/tex; mode=display">\begin{aligned}E\left[o_{i}\right] &=\sum_{j=1}^{n_{\text {in }}} E\left[w_{i j} x_{j}\right] \\&=\sum_{j=1}^{n_{\text {in }}} E\left[w_{i j}\right] E\left[x_{j}\right] \\&=0, \\\operatorname{Var}\left[o_{i}\right] &=E\left[o_{i}^{2}\right]-\left(E\left[o_{i}\right]\right)^{2} \\&=\sum_{j=1}^{n_{\text {in }}} E\left[w_{i j}^{2} x_{j}^{2}\right]-0 \\&=\sum_{j=1}^{n_{\text {in }}} E\left[w_{i j}^{2}\right] E\left[x_{j}^{2}\right] \\&=n_{\text {in }} \sigma^{2} \gamma^{2}\end{aligned}</script><p>所以保持输出与输入的方差不变的一种方法是设置 $n_{\text {in }} \sigma^{2}=1$ 。 在反向传播过程中也是类似的问题, 梯度从更靠近输出的层传播的。使用与正向传播相同的推断, 可以得出除非 $n_{\text {out }} \sigma^{2}=1$, 否则梯度的方差可能会增大（$n_{\text {out }}$ 是该层的输出的数量）。但很明显不可能同时满足这两个条件（$n_{\text {in}}$和$n_{\text {out}}$相等且与$\sigma^{2}$乘积为1）。则只需满足：</p><script type="math/tex; mode=display">\frac{1}{2}\left(n_{\text {in }}+n_{\text {out }}\right) \sigma^{2}=1 \text { 或等价于 } \sigma=\sqrt{\frac{2}{n_{\text {in }}+n_{\mathrm{out}}}}</script><p>这就是现在标准且实用的Xavier初始化的基础，它以其提出者第一作者的名字命名。</p><p>通常, Xavier初始化从均值为零, 方差 $\sigma^{2}=\frac{2}{n_{\text {in }}+n_{\text {aut }}}$ 的正态分布中采样权重。我们也可以利用 Xavier的直觉来选择从均匀分布中抽取权重时的方差。注意均匀分布 $U(-a, a)$ 的方差为 $\frac{a^{2}}{3}$ 。将 $\frac{a^{2}}{3}$ 代 入到 $\sigma^{2}$ 的条件中，将得到初始化值域：</p><script type="math/tex; mode=display">U\left(-\sqrt{\frac{6}{n_{\text {in }}+n_{\text {out }}}}, \sqrt{\frac{6}{n_{\text {in }}+n_{\text {out }}}}\right) .</script><p>虽然上述的“不存在非线性计算”的假设在神经网络中很容易被违反, 但Xavier初始化方法在实践中被证明是有效的。</p><h4 id="6-2-3-更多初始化方法"><a href="#6-2-3-更多初始化方法" class="headerlink" title="6.2.3. 更多初始化方法"></a>6.2.3. 更多初始化方法</h4><p>上面的推理仅仅触及了现代参数初始化方法的皮毛。 深度学习框架通常实现十几种不同的启发式方法。 参数初始化一直是深度学习基础研究的热点领域。 其中包括专门用于参数绑定（共享）、超分辨率、序列模型和其他情况的启发式算法。</p><h2 id="7-环境和分布偏移"><a href="#7-环境和分布偏移" class="headerlink" title="7. 环境和分布偏移"></a>7. 环境和分布偏移</h2><p>许多机器学习应用中存在的问题之一就是在将基于模型的决策引入环境时可能会破坏模型。比如用户根据模型的某些特点，做出对应更改从而获取非法利益。</p><h3 id="7-1-分布偏移的类型"><a href="#7-1-分布偏移的类型" class="headerlink" title="7.1. 分布偏移的类型"></a>7.1. 分布偏移的类型</h3><p>分布偏移，在我理解就是两个（或多个）数据集间分布不同的情况，比如在训练集和测试集之间，如果没有一个关于两者间相互关系的预估，那将不能学习到有用的模型。<br>基于对未来数据可能发生变化的一些限制性假设，有些算法可以检测这种偏移，甚至可以动态调整，提高原始分类器的精度。</p><h4 id="7-1-1-协变量偏移（covariate-shift）"><a href="#7-1-1-协变量偏移（covariate-shift）" class="headerlink" title="7.1.1. 协变量偏移（covariate shift）"></a>7.1.1. 协变量偏移（covariate shift）</h4><p>在不同分布偏移中, 协变量偏移可能是最为广泛研究的。 协变量偏移是指：输入的分布可能随时间而改变, 但标签函数（即条件分布 $P(y \mid \mathbf{x})$ ）没有改变。<br>统计学家称之为协变量偏移是因为这个问题是由于协变量（特征）分布的变化而产生的。具体的例子比如在训练分类器时，训练集为真实图片，而测试集为卡通图片。<br>当认为 $\mathbf{x}$ 导致 $y$ 时, 标签偏移是一个合理的假设。</p><h4 id="7-1-2-标签偏移（label-shift）"><a href="#7-1-2-标签偏移（label-shift）" class="headerlink" title="7.1.2. 标签偏移（label shift）"></a>7.1.2. 标签偏移（label shift）</h4><p>标签偏移（label shift）描述了与协变量偏移相反的问题。它假设标签边缘概率 $P(y)$ 可以改变, 但是类别条件分布 $P(\mathbf{x} \mid y)$ 在不同的领域之间保持不变。<br>当认为 $y$ 导致 $\mathbf{x}$ 时, 标签偏移是一个合理的假设。例如预测患者的疾病, 我们可能根据症状来判断, 即使疾病的相对流行率随着时间的推移而变化。标签偏移在这里是恰当的假设, 因为疾病会引起症状，输入为症状（不变），输出为可能的疾病（随时间而分布变化）。</p><p>有时标签偏移和协变量偏移假设可以同时成立。在这些情况下，使用基于标签偏移假设的方法通常是有利的。这是因为这些方法倾向于包含看起来像标签 (低维) 的对象, 而不是像输入（高维）的对象。</p><h4 id="7-1-3-概念偏移（concept-shift）"><a href="#7-1-3-概念偏移（concept-shift）" class="headerlink" title="7.1.3. 概念偏移（concept shift）"></a>7.1.3. 概念偏移（concept shift）</h4><p>当标签的定义发生变化时，就会出现这种问题。精神疾病的诊断标准、所谓的时髦、以及工作头衔等等，都是概念偏移的日常映射。</p><h3 id="7-2-分布偏移纠正"><a href="#7-2-分布偏移纠正" class="headerlink" title="7.2. 分布偏移纠正"></a>7.2. 分布偏移纠正</h3><p>许多情况下训练分布与测试分布是不同的，有时这对模型没什么影响，有时则需要运用一些手段去应对这种偏移。</p><h4 id="7-2-1-经验风险与实际风险"><a href="#7-2-1-经验风险与实际风险" class="headerlink" title="7.2.1. 经验风险与实际风险"></a>7.2.1. 经验风险与实际风险</h4><p>在模型的训练期间，训练数据 ${(\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_n, y_n)}$  的特征和相关的标签经过迭代，在每一个小批量之后更新模型 $f$ 的参数。为了简单起见，我们先不考虑正则化，此时极大地降低了训练损失（即经验风险最小化）：</p><script type="math/tex; mode=display">\underset{f}{\operatorname{minimize}} \frac{1}{n} \sum_{i=1}^{n} l\left(f\left(\mathbf{x}_{i}\right), y_{i}\right),</script><p>其中 $l$ 是损失函数, 用来度量给定标签 $y_{i}$, 预测 $f\left(\mathbf{x}_{i}\right)$ 的 “糟糕程度”。 统计学家称上式中的这一项 $l\left(f\left(\mathbf{x}_{i}\right), y_{i}\right)$ 为经验风险。 经验风险 (empirical risk) 是为了近似真实风险（true risk）。</p><p>真实风险是整个训练数据上的平均损失, 即从其真实分布 $p(\mathbf{x}, y)$ 中抽取的所有数据的总体损失的期望值：</p><script type="math/tex; mode=display">E_{p(\mathbf{x}, y)}[l(f(\mathbf{x}), y)]=\iint l(f(\mathbf{x}), y) p(\mathbf{x}, y) d \mathbf{x} d y</script><p>然而在实践中通常无法获得总体数据。因此, 经验风险最小化是一种实用的机器学习策略，希望能近似最小化真实风险。</p><h4 id="7-2-2-协变量偏移纠正"><a href="#7-2-2-协变量偏移纠正" class="headerlink" title="7.2.2. 协变量偏移纠正"></a>7.2.2. 协变量偏移纠正</h4><p><a href="https://zh-v2.d2l.ai/chapter_multilayer-perceptrons/environment.html#subsec-covariate-shift-correction">下面三种纠正原理</a>暂时放一下，详细看还是得多读几遍书。<br>给出完整的协变量偏移纠正算法，假设我们有一个训练集 ${(\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_n, y_n)}$ 和一个未标记的测试集 ${\mathbf{u}_1, \ldots, \mathbf{u}_m}$ 对于协变量偏移，我们假设 $1 \leq i \leq n$ 的 $\mathbf{x}_{i}$ 来自某个源分布 $q(\mathbf{x})$, $\mathbf{u}_{i}$ 来自目标分布 $p(\mathbf{x})$。以下是纠正协变量偏移的典型算法：</p><ol><li>生成一个二元分类训练集： ${(\mathbf{x}_1, -1), \ldots, (\mathbf{x}_n, -1), (\mathbf{u}_1, 1), \ldots, (\mathbf{u}_m, 1)}$ 。</li><li>用对数几率回归训练二元分类器得到函数 $h_{\circ}$</li><li>使用 $\beta_{i}=\exp \left(h\left(\mathbf{x}_{i}\right)\right)$ 或更好的 $\beta_{i}=\min \left(\exp \left(h\left(\mathbf{x}_{i}\right)\right), c\right)$ (c为常量）对训练数据进行加权。</li><li>使用权重 $\beta_{i}$ 进行”加权经验风险最小化“中 ${(\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_n, y_n)}$ 的训练。</li></ol><p>上述算法依赖于一个重要的假设：需要目标分布(例如, 测试分布)中的每个数据样本在训练时出现的概率非零。如果找到 $p(\mathbf{x})&gt;0$ 但 $q(\mathbf{x})=0$ 的点，那么相应的重要性权重会是无穷大（因为 $\beta_{i} \stackrel{\operatorname{def}}{=} \frac{p\left(\mathbf{x}_{i}\right)}{q\left(\mathbf{x}_{i}\right)}$）。</p><p>加权经验风险最小化:</p><script type="math/tex; mode=display">\underset{f}{\operatorname{minimize}} \frac{1}{n} \sum_{i=1}^{n} \beta_{i} l\left(f\left(\mathbf{x}_{i}\right), y_{i}\right) .</script><h4 id="7-2-3-标签偏移纠正"><a href="#7-2-3-标签偏移纠正" class="headerlink" title="7.2.3. 标签偏移纠正"></a>7.2.3. 标签偏移纠正</h4><p>标签偏移的一个好处是，如果我们在源分布上有一个相当好的模型，那么我们可以得到对这些权重的一致估计，而不需要处理周边的其他维度。 在深度学习中，输入往往是高维对象（如图像），而标签通常是低维（如类别）。</p><h4 id="7-2-4-概念偏移纠正"><a href="#7-2-4-概念偏移纠正" class="headerlink" title="7.2.4. 概念偏移纠正"></a>7.2.4. 概念偏移纠正</h4><p>概念偏移很难用原则性的方式解决。 例如，在一个问题突然从“区分猫和狗”偏移为“区分白色和黑色动物”的情况下， 除了从零开始收集新标签和训练没有其他的办法。 但在实践中这种极端的偏移是罕见的，通常情况下，概念的变化总是缓慢的。 在这种情况下，我们可以使用与训练网络相同的方法，使其适应数据的变化。 换言之，我们使用新数据更新现有的网络权重，而不是从头开始训练。</p><h3 id="7-3-学习问题的分类方法"><a href="#7-3-学习问题的分类方法" class="headerlink" title="7.3. 学习问题的分类方法"></a>7.3. 学习问题的分类方法</h3><p>机器学习研究的是计算机怎样模拟人类的学习行为，以获取新的知识或技能，并重新组织已有的知识结构使之不断改善自身。<br>机器学习能解决的问题包括以下几种（<a href="https://blog.csdn.net/lovenankai/article/details/99965501">参考</a>）：</p><ol><li>分类问题：根据数据样本上抽取出的特征，判定其属于有限个类别中的哪一个。</li><li>回归问题：根据数据样本上抽取出的特征，预测一个连续值的结果。</li><li>聚类问题：根据数据样本上抽取出的特征，让样本抱抱团(相近/相关的样本在一团内)。</li></ol><p>在机器学习模拟学习的方法上，有许多分类，如批量学习、强化学习等等，目前就是在讨论这方面（这些学习问题）的类别有哪些。</p><h4 id="7-3-1-批量学习（batch-learning）"><a href="#7-3-1-批量学习（batch-learning）" class="headerlink" title="7.3.1. 批量学习（batch learning）"></a>7.3.1. 批量学习（batch learning）</h4><p>在批量学习中, 我们可以访问一组训练特征和标签 ${(\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_n, y_n)}$, 使用这些特性和标签训练 $f(\mathbf{x})$ 。 然后部署此模型来对来自同一分布的新数据 $(\mathbf{x}, y)$ 进行评分。 例如, 我们可以根据猫和狗的大量图片训练一个猫检测器。 一旦我们训练了它, 就把它作为智能猫门计算视觉系统的一部分, 来控制只允许猫进入该门。 然后这个系统会被安装在客户家中, 基本再也不会更新。</p><h4 id="7-3-2-在线学习（online-learning）"><a href="#7-3-2-在线学习（online-learning）" class="headerlink" title="7.3.2. 在线学习（online learning）"></a>7.3.2. 在线学习（online learning）</h4><p>“在线”逐个学习数据$\left(\mathbf{x}_{i}, y_{i}\right)$。我们首先观测到$\mathbf{x}_{i}$，然后得出一个估计值 $f\left(\mathbf{x}_{i}\right)$, 当完成估计后，我们才观测到 $y_{i}$。然后根据模型的决定（估计）, 给予模型奖励或惩罚。<br>例如, 我们预测明天的股票价格来根据这个预测进行交易。在一天结束时, 我们会评估模型的预测是否盈利。<br>在线学习的循环如下，这样看起来就清晰易懂了：</p><script type="math/tex; mode=display">\text { model } f_{t} \longrightarrow \text { data } \mathbf{x}_{t} \longrightarrow \text { estimate } f_{t}\left(\mathbf{x}_{t}\right) \longrightarrow \text { observation } y_{t} \longrightarrow \operatorname{loss} l\left(y_{t}, f_{t}\left(\mathbf{x}_{t}\right)\right) \longrightarrow \operatorname{model} f_{t+1}</script><h4 id="7-3-3-老虎机（bandits）"><a href="#7-3-3-老虎机（bandits）" class="headerlink" title="7.3.3. 老虎机（bandits）"></a>7.3.3. 老虎机（bandits）</h4><p>在一个老虎机问题中，只有有限数量的手臂可以拉动。 也就是说我们可以采取的行动是有限的。 老虎机问题是一个相较于上述问题更简单的情景，可以获得更强的最优性理论保证。 这个问题经常被视为一个单独的学习问题的情景。</p><h4 id="7-3-4-控制"><a href="#7-3-4-控制" class="headerlink" title="7.3.4. 控制"></a>7.3.4. 控制</h4><p>在很多情况下，环境（许多算法形成的环境模型）会记住我们所做的事，虽然不一定是以对抗的方式，并且环境会根据记忆做出相应的反应。 例如，咖啡锅炉控制器将根据之前是否加热锅炉来观测到不同的温度。 在这种情况下，PID（比例—积分—微分）控制器算法是一个流行的选择，温度PID控制器的原理是将温度偏差的比例、积分和微分通过线性组合构成控制量，对控制对象进行控制。</p><blockquote><p>这里还是有点没看明白，英文翻译的不好。 22.06.16</p></blockquote><p>近年来，控制理论（如PID的变体）也被用于自动调整超参数， 以获得更好的解构和重建质量，提高生成文本的多样性和生成图像的重建质量。</p><h4 id="7-3-5-强化学习（reinforcement-learning）"><a href="#7-3-5-强化学习（reinforcement-learning）" class="headerlink" title="7.3.5. 强化学习（reinforcement learning）"></a>7.3.5. 强化学习（reinforcement learning）</h4><p>强化学习强调如何基于环境而行动，以取得最大化的预期利益。 国际象棋、围棋、西洋双陆棋或星际争霸都是强化学习的应用实例，如果在游戏中采取某种策略可以取得较高的得分，那么就进一步「强化」这种策略，以期继续取得较好的结果。</p><h4 id="7-3-6-基于应用环境选择学习方法"><a href="#7-3-6-基于应用环境选择学习方法" class="headerlink" title="7.3.6. 基于应用环境选择学习方法"></a>7.3.6. 基于应用环境选择学习方法</h4><p>上述不同情况之间的一个关键区别是： 在静止环境中可能一直有效的相同策略，在环境能够改变的情况下可能不会始终有效。<br>环境变化的速度和方式在很大程度上决定了我们可以采用的算法类型。 例如，如果我们知道事情只会缓慢地变化，就可以迫使任何估计也只能缓慢地发生改变。 如果我们知道环境可能会瞬间发生变化，但这种变化非常罕见，我们就可以在使用算法时考虑到这一点。<br>当一个数据科学家试图解决的问题会随着时间的推移而发生变化时这些类型的知识至关重要。</p><h3 id="7-4-机器学习中的公平、责任和透明度"><a href="#7-4-机器学习中的公平、责任和透明度" class="headerlink" title="7.4. 机器学习中的公平、责任和透明度"></a>7.4. 机器学习中的公平、责任和透明度</h3><p>当部署机器学习系统时，你不仅仅是在优化一个预测模型，通常是在提供一个会被用来（部分或完全）进行自动化决策的工具。 这些技术系统可能会通过其进行的决定而影响到每个人的生活。<br>从 预测 到 决策 的飞跃不仅提出了新的技术问题， 而且还提出了一系列必须仔细考虑的伦理问题。</p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>Kaggle房价预测比赛的实践，放到下一篇笔记中吧。<br>还遇到了kramed渲染不了公式中大括号的问题，先将就，等我学了JS回来再修复。<br>还发现了链接图片的原理，前面加斜杠是表示在域名下一级。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;p&gt;这一章开始学习真正的深度网络。&lt;br&gt;最简单的深度网络称为多层感知机。多层感知机由多层神经元组成， 每一层与它的上一层相连，从中接收输入； 同时每一层也与它的下一层相连，影响当前层的神经元。&lt;br&gt;这一章从基本的概念介绍开始讲起，包括过拟合、欠拟合和模型选择。 为了解决这些问题，本章将介绍权重衰减和暂退法等正则化技术，以及将讨论数值稳定性和参数初始化相关的问题。最后应用一个真实的案例：房价预测。&lt;/p&gt;
&lt;p&gt;对应实践：&lt;a href=&quot;https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x04.ipynb&quot;&gt;https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x04.ipynb&lt;/a&gt;&lt;br&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://silencezheng.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>回调函数</title>
    <link href="http://silencezheng.top/2022/06/07/article43/"/>
    <id>http://silencezheng.top/2022/06/07/article43/</id>
    <published>2022-06-07T08:20:56.000Z</published>
    <updated>2022-06-07T08:43:00.930Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>回调函数（callback）是一个常用的概念，最近在写代码的过程中遇到了相关问题，写个文梳理一下。以下都是个人的浅显理解，如有错误请指正。<br><span id="more"></span></p><h2 id="什么是回调函数？"><a href="#什么是回调函数？" class="headerlink" title="什么是回调函数？"></a>什么是回调函数？</h2><p>回调函数，本身实际上就是普通的功能函数，当功能函数以参数的方式被传入到其他函数中时，它便成为了回调函数。</p><p>下面我们将回调过程中的函数分为两部分，调用者称为调用函数，被调用者称为回调函数。</p><h2 id="为什么要使用回调函数？"><a href="#为什么要使用回调函数？" class="headerlink" title="为什么要使用回调函数？"></a>为什么要使用回调函数？</h2><p>回调函数首先的作用就是解耦，调用函数不用关心回调函数的具体实现，只是按照其格式拿来即用就可以了。<br>回调函数的一个直观作用就是可以将事件与函数绑定，当事件发生时触发回调函数。（也是目前我遇到的场景）<br>关于其他的作用，我认为<a href="https://blog.csdn.net/qiuhuanghe/article/details/109245579">这篇文章</a>讲的还不错，可以看一下学习学习。<br>关于同步回调和异步回调，<a href="https://cloud.tencent.com/developer/article/1373683">这篇文章</a>可以看看。</p><h2 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h2><p>应用场景是这样的：在编写一个游戏时，需要通过鼠标点击组件来使用人物背包中的物品。</p><p>其中背包组件类的实现大致如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">InventoryView</span>(<span class="params">tk.Frame</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, master: <span class="type">Union</span>[tk.Tk, tk.Frame], **kwargs</span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(master)</span><br><span class="line">        self.callback = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_click_callback</span>(<span class="params">self, callback: <span class="type">Callable</span>[[<span class="built_in">str</span>], <span class="literal">None</span>]</span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Sets the function to be called when an item is clicked.</span></span><br><span class="line"><span class="string">        The provided callback function should take one argument: the string name of the item.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.callback = callback</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_draw_item</span>(<span class="params">self, name: <span class="built_in">str</span>, num: <span class="built_in">int</span>, colour: <span class="built_in">str</span></span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Creates and binds (if a callback exists) a single tk.Label in the InventoryView frame.</span></span><br><span class="line"><span class="string">        name is the name of the item, num is the quantity currently in the users inventory,</span></span><br><span class="line"><span class="string">        and colour is the background colour for this item label (determined by the type of item).</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        label = tk.Label(self, text=<span class="string">f&quot;<span class="subst">&#123;name&#125;</span>: <span class="subst">&#123;<span class="built_in">str</span>(num)&#125;</span>&quot;</span>, background=colour)</span><br><span class="line">        label.bind(<span class="string">&quot;&lt;Button-1&gt;&quot;</span>, self.callback)</span><br><span class="line">        label.pack(side=<span class="string">&#x27;top&#x27;</span>, fill=tk.X)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">draw_inventory</span>(<span class="params">self, inventory: Inventory</span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        ...</span><br><span class="line"></span><br></pre></td></tr></table></figure><br>类中的callback为回调函数（这里也就是”物品使用函数“），通过与label组件上的鼠标左键点击事件绑定，实现点击调用“物品使用函数”。 可以看出，这里背包并不需要关心物品是如何被使用的，他只是接收一个回调函数，并将这个函数与背包中的每一个物品（label）绑定。</p><p>但这里其实是简化过的版本，真正在使用时，由于“物品使用函数”需要通过接收一个str类型的参数来分辨被使用的物品是何类型，所以不能直接绑定（tkinter的bind方法会将事件作为参数传入到绑定的回调函数中，这个参数表与我们规定的“物品使用函数”参数表不同）。</p><p>所以对于想要使用Event的同时绑定带参函数的情况，需要中间函数来匹配两边的规则（“物品使用函数”和bind函数）。如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_draw_item</span>(<span class="params">self, name: <span class="built_in">str</span>, num: <span class="built_in">int</span>, colour: <span class="built_in">str</span></span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Creates and binds (if a callback exists) a single tk.Label in the InventoryView frame.</span></span><br><span class="line"><span class="string">    name is the name of the item, num is the quantity currently in the users inventory,</span></span><br><span class="line"><span class="string">    and colour is the background colour for this item label (determined by the type of item).</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">handlerAdaptor</span>(<span class="params">function, **kwds</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">lambda</span> event, fun = function, kwds = kwds: fun(event, **kwds)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">handler</span>(<span class="params">event, item_name</span>):</span></span><br><span class="line">        self.callback(item_name)</span><br><span class="line"></span><br><span class="line">    label = tk.Label(self, text=<span class="string">f&quot;<span class="subst">&#123;name&#125;</span>: <span class="subst">&#123;<span class="built_in">str</span>(num)&#125;</span>&quot;</span>, background=colour)</span><br><span class="line">    label.bind(<span class="string">&quot;&lt;Button-1&gt;&quot;</span>, handlerAdaptor(handler, item_name=name))</span><br><span class="line">    label.pack(side=<span class="string">&#x27;top&#x27;</span>, fill=tk.X)</span><br></pre></td></tr></table></figure><br>这里我们在函数内部又定义了两个中间函数用来实现目的，handler增加了event参数来接收bind函数交给回调函数的Event，但此时不能直接绑定，因为绑定时无处获取Event作为实参传入。</p><p>所以又定义了handlerAdaptor来实现一个不含event在参数表中的函数，使用lambda表达式来解决这一问题，虽然参数表中不包含event，但在实际调用的过程中，bind函数依然会传入event参数（我猜想bind传入的参数的名字恰好就是event，所以这个匿名函数才能成立），使得lambda表达式的参数表匹配，返回调用handler的结果。</p><p>写两个中间函数的好处是（其实也比较牵强）：handler函数可以规范一下回调函数的参数表命名，我们可以不去管self.callback中的参数实际名字是如何，直接用item_name这一参数名去传入即可。</p><p>但实际上，如果self.callback的参数表是已知的，我们只需要一个中间函数即可实现“使用Event的同时绑定带参函数”，如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_draw_item</span>(<span class="params">self, name: <span class="built_in">str</span>, num: <span class="built_in">int</span>, colour: <span class="built_in">str</span></span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">middle</span>(<span class="params">function, **kwds</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">lambda</span> event, fun = function, kwds = kwds: fun(**kwds)</span><br><span class="line"></span><br><span class="line">    label = tk.Label(self, text=<span class="string">f&quot;<span class="subst">&#123;name&#125;</span>: <span class="subst">&#123;<span class="built_in">str</span>(num)&#125;</span>&quot;</span>, background=colour)</span><br><span class="line">    <span class="comment"># 这里已知回调函数中的参数名为item_name</span></span><br><span class="line">    label.bind(<span class="string">&quot;&lt;Button-1&gt;&quot;</span>, middle(self.callback, item_name=name))</span><br><span class="line">    label.pack(side=<span class="string">&#x27;top&#x27;</span>, fill=tk.X)</span><br></pre></td></tr></table></figure><br>总的来说，使用lambda表达式就可以解决这个问题～<br>参考：<a href="https://blog.csdn.net/tinym87/article/details/6957438">https://blog.csdn.net/tinym87/article/details/6957438</a></p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>理解的还是很基础，只是知道个大概，以后用到了再总结补充吧。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;回调函数（callback）是一个常用的概念，最近在写代码的过程中遇到了相关问题，写个文梳理一下。以下都是个人的浅显理解，如有错误请指正。&lt;br&gt;</summary>
    
    
    
    
    <category term="Python" scheme="http://silencezheng.top/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch适配M1芯片测试</title>
    <link href="http://silencezheng.top/2022/06/01/article42/"/>
    <id>http://silencezheng.top/2022/06/01/article42/</id>
    <published>2022-05-31T17:03:49.000Z</published>
    <updated>2022-05-31T17:08:06.334Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近用mac学习深度学习时越来越感觉硬件的重要…随便跑个小demo都温度90+，听闻最近Pytorch适配了M1芯片，正好试试能否有所提升！<br><span id="more"></span></p><h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><p>miniforge是要安的，<code>bash Miniforge3-MacOSX-arm64.sh</code>。</p><p>然后我们上Pytorch官网看一下情况，可以看到有稳定版和预览版两种，其中预览版是支持GPU加速的。<br>稳定版：<br><img src="/assets/post_img/article42/stable.jpg" alt="stable"><br>预览版：<br><img src="/assets/post_img/article42/preview.png" alt="preview"><br>这里提到的MPS加速是指用苹果的Metal Performance Shaders (MPS)作为Pytorch后端启用GPU加速训练。</p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>我的conda环境之前安装过稳定版：<br><code>conda install pytorch torchvision -c pytorch</code></p><p>这里的 -c 参数是指定channel，即软件下载的渠道。所以conda本身是没有这个包的，要从pytorch的channel下载。</p><p>这时在终端中进入安装稳定版的环境，启用python，输入<code>torch.__version__</code>可以查看到当前的版本是1.8.0（这里挺奇怪的，官网说MacOS不支持1.8的…）:<br><img src="/assets/post_img/article42/原版本.jpg" alt="1.8.0"></p><p>下面我们安装预览版试一下：<br><code>conda install pytorch torchvision -c pytorch-nightly</code></p><p>可以看到下载的已经是1.13版本了：<br><img src="/assets/post_img/article42/1.13版本.jpg" alt="下载提示"></p><p><img src="/assets/post_img/article42/环境中加载成功.jpg" alt="完成"></p><p>安完之后有个无语的问题，<code>conda help</code>不能用了，可能是安装了4.13版本的问题..用-h就好了。</p><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>有一个现成的<a href="https://github.com/rasbt/machine-learning-notes/tree/main/benchmark/pytorch-m1-gpu">测试</a></p><p>但是发现mps和cpu没有区别呀，如图：<br><img src="/assets/post_img/article42/test1.jpg" alt="benchmark1"></p><p>倒是发现了一个BUG，标准化地方要改成如下，否则报错：<br><code>transforms.Normalize((0.5,),(0.5,)),</code></p><p>然后又用我自己的代码小测一下，发现好像…还是没区别：<br><img src="/assets/post_img/article42/test2.jpg" alt="benchmark2"></p><p>结论：Mac目前的GPU加速就是个笑话😅</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;最近用mac学习深度学习时越来越感觉硬件的重要…随便跑个小demo都温度90+，听闻最近Pytorch适配了M1芯片，正好试试能否有所提升！&lt;br&gt;</summary>
    
    
    
    
    <category term="MacOS" scheme="http://silencezheng.top/tags/MacOS/"/>
    
  </entry>
  
  <entry>
    <title>TLS过程简单了解</title>
    <link href="http://silencezheng.top/2022/05/26/article41/"/>
    <id>http://silencezheng.top/2022/05/26/article41/</id>
    <published>2022-05-26T15:12:50.000Z</published>
    <updated>2022-05-26T15:15:03.601Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><p>答辩前夕，补习一下计算机网络知识，因为在部署ELK的过程中频繁的使用到了私钥、证书、签名等等，有必要梳理一下TLS的相关知识。本文参考了谢希仁第八版计算机网络教材，只做简单理解，深入学习还是要仔细啃书。<br>场景有如下几个：</p><ol><li>Elasticsearch节点间通信加密</li><li>Elasticsearch的HTTP通信加密</li><li>Kibana与浏览器间HTTP通信加密<span id="more"></span></li></ol><h2 id="1-什么是TLS？"><a href="#1-什么是TLS？" class="headerlink" title="1. 什么是TLS？"></a>1. 什么是TLS？</h2><p>TLS即Transport Layer Security，传输层安全协议。学过计算机网络的同学都知道，我们常用的网络模型有五层，自底向上依次为物理层、数据链路层、网络层、传输层、应用层。 而TLS事实上是属于传输层与应用层之间的一个安全协议。</p><p>TLS由SSL发展而来，但并不兼容，其优势在于与高层的应用层协议（如HTTP、FTP等）无耦合，应用层协议能透明地运行在TLS协议之上，由TLS协议进行 创建加密通道所需的 协商和认证。应用层协议传送的数据在通过TLS协议时都会被加密，从而保证通信的私密性</p><p>TLS是针对TCP进行加密的，所以任何在TCP基础上运行的应用程序程序都可以使用TLS进行加密。最常用的就是HTTP协议，HTTP在不使用传输层安全协议的时候就直接使用TCP连接，而使用了传输层安全协议后，链接前方就会显示<code>https://</code>的字样。</p><p>目前TLS的最新版本为TLS1.3，在2020年1.0和1.1均已被废弃掉了，现在能够使用的传输层安全协议只有TLS1.2和TLS1.3。</p><h2 id="2-那么TLS是如何工作的"><a href="#2-那么TLS是如何工作的" class="headerlink" title="2. 那么TLS是如何工作的"></a>2. 那么TLS是如何工作的</h2><p>谈论这个之前，我们需要了解一些基本的概念。</p><h3 id="2-1-鉴别、密钥、密码体制"><a href="#2-1-鉴别、密钥、密码体制" class="headerlink" title="2.1. 鉴别、密钥、密码体制"></a>2.1. 鉴别、密钥、密码体制</h3><p>对于安全的计算机网络，<strong>端点鉴别</strong>是一个必不可少的特性，即网络能够鉴别信息的发送方和接收方的真实身份。TLS具有双向鉴别的功能，即客户端可以鉴别服务器，服务器也可以鉴别客户端，但大多数情况下只使用前者（在HTTP中体现为浏览器鉴别服务器）。</p><p><strong>密钥</strong>是数据加密模型中的重要组成部分，通常密钥的形式为一个字符串（比特串），一般来说，一段信息（明文）需要通过加密算法和加密密钥进行加密操作，变成一段密文，接受方通过解密算法和解密密钥求解出原本的信息。</p><p>在这个过程中，产生了两种密码体制，即<strong>对称密码体制</strong>和<strong>公钥密码体制</strong>，这个概念很好理解，对称密码体制中加密密钥==解密密钥，而公钥密码体制中两者不同。本文主要研究公钥密码体制。</p><h3 id="2-2-公钥密码体制"><a href="#2-2-公钥密码体制" class="headerlink" title="2.2. 公钥密码体制"></a>2.2. 公钥密码体制</h3><p>公钥密码体制源于两个需求，一是对称密码体制中密钥分配不便，二是应用中对电子信息的 <strong>数字签名</strong> 需求。数字签名可以绑定信息与某个特定的人。</p><p>公钥密码体制中，存在两种密钥，即<strong>公钥（Public Key）</strong>和<strong>私钥（Secret Key）</strong>。其中公钥为加密密钥，对大众公开；而私钥则为解密密钥，需要保密，公钥与私钥是成对出现的。另外，加密算法与解密算法也都是公开的。</p><p>这种密码体制的通信可以是多对一的<strong>单向保密通信</strong>，即不同人持有公钥对信息加密后发送给同一接收方，但只有持有对应私钥的接收方才能获取其中的信息。</p><h3 id="2-3-用数字签名进行鉴别"><a href="#2-3-用数字签名进行鉴别" class="headerlink" title="2.3. 用数字签名进行鉴别"></a>2.3. 用数字签名进行鉴别</h3><p>数字签名的原理可以用公钥密码体制原理的逆向过程来思考，但该方式并不准确，如下：<br>当某人需要发送进行数字签名的信息时，使用自己的私钥对该信息进行了D运算，得到了一个不可读的密文，并将密文发送给接收方。接收方在收到密文后，使用对应的公钥进行E运算（核实签名），解出了原本的信息，同时证明了该信息来自特定的发送方，因为私钥只有该发送方持有。<br>注意在这个过程中，任何人都可以通过公钥解密出此人的信息，因此数字签名只对信息进行了签名，并没有加密。实现数字签名安全性的关键在于确定没有其他人能够持有发送方的私钥。</p><h3 id="2-4-那公钥是如何分配的？什么是证书签名？"><a href="#2-4-那公钥是如何分配的？什么是证书签名？" class="headerlink" title="2.4. 那公钥是如何分配的？什么是证书签名？"></a>2.4. 那公钥是如何分配的？什么是证书签名？</h3><p>说了这么多，密钥如何分配的问题还没有解决，这在实现公钥密码体制中十分重要，有必要详细的说一说。</p><p>公钥的分配过程中，最最关键的名词就是<strong>CA</strong>和<strong>公钥证书</strong>了。首先，当前流行的办法是找一个通信双方都信任的第三方机构来给拥有公钥的实体颁发一个具有数字签名的数字证书，该证书是公钥与其对应实体进行绑定的证明。这个颁发公钥证书的机构即CA（Certification Authority），通常由政府或知名公司出资建立。</p><p>每个公钥证书中都写有公钥及其拥有者的标识信息，同时包含颁发的CA自己的数字签名。但这个最终的公钥证书在形成前有一个被签名的过程，即公钥的拥有方首先将公钥与自己进行绑定，形成一个未认证的证书，CA将该证书与自己签名放在一起（中间进行一些运算）最终形成公钥证书。这个过程即为证书“签名”的过程。</p><p>当然，任何东西想要扩散起来，离不开标准化，数字证书的格式也是如此。ITU-T制定了<strong>X.509</strong>协议标准，现在的版本是 X.509V3，该标准又称为互联网<strong>公钥基础结构PKI</strong>。具体来说，该标准提出了把多级认证中心连接起来构成树状的认证系统，最高一级的认证中心为<strong>Root CA</strong>，根CA向下的所有链接称为<strong>信任链</strong>，表示这些CA都是可信的。</p><p>现代的操作系统厂商已经把许多主流CA的根证书内置到了操作系统中，这些根证书有CA们的公钥和并且使用CA的私钥进行了自签名。用户只要打开浏览器就可以查到这些根证书，并利用公钥对各网站服务器的证书进行验证。</p><h3 id="2-5-行了，TLS是啥来着？"><a href="#2-5-行了，TLS是啥来着？" class="headerlink" title="2.5. 行了，TLS是啥来着？"></a>2.5. 行了，TLS是啥来着？</h3><p>扯了一圈别的概念，这一节主要还是为了解释TLS是如何工作的…下面来看。<br>前面说到了，TLS的单向鉴别是客户端（浏览器）鉴别服务器，也就是浏览器A要确定服务器B是可信的，这就需要两个前提，一是服务器B需要有一个证书证明自己，二是浏览器A能有办法通过证书证明B是安全的。下面我们假设B已经拥有了由可靠CA颁发的证书。</p><p>首先，执行TLS的前提是TCP连接建立完成，我们假设A和B的连接已经建立。<br>TLS主要分为两个阶段，<strong>握手阶段</strong>和<strong>会话阶段</strong>。在不同的阶段，TLS使用不同的协议：握手协议和记录协议。本文中主要关注握手阶段，会话阶段读者可以自行查阅。</p><p>协商加密算法：<br>1、A向B发送自己选定的加密算法以及密钥交换算法<br>2、B从中确认自己支持的<strong>算法C</strong>，并把证书发送给A<br>服务器鉴别：<br>3、A用B证书中CA的对应公钥（这里的公钥是A本身带有的，不是B证书中包含的）对证书进行鉴别<br>生成主密钥：<br>4、A按照C生成主密钥<br>5、A用B的公钥对主密钥加密并发送给B<br>6、B用私钥解密出相同的主密钥</p><p>上述步骤完成后，A和B拥有了相同的主密钥，可以进行数据会话。但这样会有些不安全，故A和B会各自将主密钥分割成4个不同的密钥：<br>7、A生成对话密钥<br>8、B生成对话密钥</p><p>这样以后，双方各自持有4个密钥。注意这4个密钥也是相同的，分别为A发送数据使用的会话密钥、A发送数据使用的MAC密钥、B发送数据使用的会话密钥和B的MAC密钥。会话密钥也采用对称的形式是因为对称密钥的运行速度快得多。</p><h2 id="3-验证"><a href="#3-验证" class="headerlink" title="3. 验证"></a>3. 验证</h2><p>回到开头所提到的三个场景，都是我在部署ELK过程中使用加密的案例，我们来一一验证，看他们是否是TLS。首先要明确Elasticsearch节点间通信与HTTP通信使用不同的端口，故事实上属于应用层的两个应用。</p><h3 id="3-1-Elasticsearch节点间通信加密"><a href="#3-1-Elasticsearch节点间通信加密" class="headerlink" title="3.1. Elasticsearch节点间通信加密"></a>3.1. Elasticsearch节点间通信加密</h3><p>在这一过程中，大致步骤如下：<br>1、使用工具生成”elastic-stack-ca.p12”，这个文件包含生成CA的公共证书和它为每个节点签名的私钥<br>2、使用刚才生成的CA为集群上的节点们生成一个证书和一把私钥，得到“elastic-certificates.p12”，这个文件包含了节点证书、节点私钥和CA证书。然后在所有节点上放入“elastic-certificates.p12”。</p><p>让我们把这个过程与TLS对应起来，第一步事实上是主节点形成CA的过程，第二步使用CA颁发了证书，该证书存在于所有非主节点上。这样以来，主节点相当于浏览器A，而非主节点相当于服务器B，在通信时，主节点会验证其余节点的证书是否是由自己的CA签名的，这样一来就完成了鉴别，所以是TLS。</p><h3 id="3-2-Elasticsearch的HTTP通信加密（对Kibana和Logstash）"><a href="#3-2-Elasticsearch的HTTP通信加密（对Kibana和Logstash）" class="headerlink" title="3.2. Elasticsearch的HTTP通信加密（对Kibana和Logstash）"></a>3.2. Elasticsearch的HTTP通信加密（对Kibana和Logstash）</h3><p>大致流程如下：<br>1、运行命令利用”elastic-stack-ca.p12”生成一个zip压缩文件，其中包含用于 Elasticsearch 和 Kibana 的证书和私钥。“http.p12”文件为Elasticsearch使用的证书及密钥，“elasticsearch-ca.pem”为Kibana使用的。<br>2、将“http.p12”部署到所有ELasticsearch节点中。<br>3、将“elasticsearch-ca.pem”部署到Kibana中，并修改指向链接为<code>https://</code>格式。<br>4、使用工具利用”elastic-stack-ca.p12”为Logstash节点生成证书，部署到Logstash节点中。</p><p>不难看出，这部分使用的是TLS加密HTTP，从<code>https://</code>就可以看出了，依然是利用第一步产生的CA为各E、L、K节点签名证书，发放私钥。</p><h3 id="3-3-Kibana与浏览器间HTTP通信加密"><a href="#3-3-Kibana与浏览器间HTTP通信加密" class="headerlink" title="3.3. Kibana与浏览器间HTTP通信加密"></a>3.3. Kibana与浏览器间HTTP通信加密</h3><p>不用说就知道了，HTTP加密！标准的TLS！但这里存在另一个有趣的问题，Elasticsearch产生的CA固然可以为其他需要与之通信的程序们签名证书，但浏览器并不认识他，所以，该找谁签名呢？</p><p>先说大致过程：<br>1、使用工具为Kibana创建服务器证书和私钥，产生了两个文件：“kibana-server.csr”和私钥“kibana-server.key”，前者的后缀即证书签名请求，可以知道该文件需要CA签名后才能够使用。<br>2、使用OpenSSL为证书签名，得到签名后的公钥证书。部署公钥证书和私钥到Kibana。</p><p>可以看出，在这一部分中，Kibana作为服务器B的角色出现，他需要寻找一个浏览器A可信任的CA为其签名后才能够与浏览器A（其实是用户们）进行安全通信，这里我因为方便使用到了OpenSSL，但其实这个签名在浏览器眼中是不安全的。</p><p>OpenSSL是一个强大的安全套接字层密码库，Apache使用它加密HTTPS，OpenSSH使用它加密SSH，但它同时还是一个多用途的、跨平台的密码工具。</p><h2 id="4-结语"><a href="#4-结语" class="headerlink" title="4. 结语"></a>4. 结语</h2><p>从头梳理了一下TLS的流程，感觉理解加深了许多，文章写了有两个多小时了吧，希望对想了解TLS的人有一些帮助，文中部分参考了百度百科，嗯就这样。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;p&gt;答辩前夕，补习一下计算机网络知识，因为在部署ELK的过程中频繁的使用到了私钥、证书、签名等等，有必要梳理一下TLS的相关知识。本文参考了谢希仁第八版计算机网络教材，只做简单理解，深入学习还是要仔细啃书。&lt;br&gt;场景有如下几个：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Elasticsearch节点间通信加密&lt;/li&gt;
&lt;li&gt;Elasticsearch的HTTP通信加密&lt;/li&gt;
&lt;li&gt;Kibana与浏览器间HTTP通信加密</summary>
    
    
    
    
    <category term="计算机网络" scheme="http://silencezheng.top/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>JavaFX环境配置--MacOS</title>
    <link href="http://silencezheng.top/2022/05/26/article40/"/>
    <id>http://silencezheng.top/2022/05/26/article40/</id>
    <published>2022-05-26T05:47:36.000Z</published>
    <updated>2022-05-26T05:52:47.996Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>换了MacBook后想打开一下以前写的JavaFX程序，发现我的jdk1.8没有自带这个SDK（据说有打包的jdk），所以需要重新下载并在IDEA里配置，记录一下。<br><span id="more"></span></p><h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><p>这里有ARM架构的SDK（Software Development Kit）<br><a href="https://gluonhq.com/products/javafx/">https://gluonhq.com/products/javafx/</a><br>我下载了18版本。</p><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p>到IDEA项目结构-&gt;Library中添加解压出来的sdk中的lib文件夹。</p><p>然后到运行配置里添加VM Options：<br><code>--module-path &quot;/xxx/javafx-sdk-18.0.1/lib&quot; --add-modules javafx.controls,javafx.fxml</code></p><p>理论上就可以运行了，但是因为我的Java版本是8，不知道是否因为版本不适配，会报错版本问题。所以我又配置了OpenJDK17（已经适配Arm平台了）。然后就可以运行了～</p><p><a href="https://www.oracle.com/java/technologies/downloads/#java17">OpenJDK17</a></p><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>但是我运行成功后，原本程序的功能还是实现不了，Debug了一下发现定位在<code>System.getenv(&quot;COMPUTERNAME&quot;)</code>，原本的程序是在Windows平台写的，是不是MacOS没有这个环境变量呢？</p><p>在Terminal中输入<code>export</code>查看所有的环境变量，发现似乎应该在MacOS中可以使用<strong>HOME</strong>，为当前用户的根目录。修改后解决～</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;换了MacBook后想打开一下以前写的JavaFX程序，发现我的jdk1.8没有自带这个SDK（据说有打包的jdk），所以需要重新下载并在IDEA里配置，记录一下。&lt;br&gt;</summary>
    
    
    
    
    <category term="Java" scheme="http://silencezheng.top/tags/Java/"/>
    
    <category term="MacOS" scheme="http://silencezheng.top/tags/MacOS/"/>
    
  </entry>
  
  <entry>
    <title>Markdown博客编写工具选择</title>
    <link href="http://silencezheng.top/2022/05/22/article39/"/>
    <id>http://silencezheng.top/2022/05/22/article39/</id>
    <published>2022-05-22T13:34:31.000Z</published>
    <updated>2022-05-22T13:37:48.246Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>关于我编写Markdown格式博客的工具选择。先说设备，我现在使用的是M1的MacBook Air。<br>最初使用的是Sublime Text，一个很经典的开源文本编辑器，缺点就是如果不订阅的话保存几次就会弹出广告，故舍弃。<br>然后使用的就是Effie，这是一个国产的笔记软件，十分的简洁，用于编写普通的文本和代码是很好的，直到我需要在笔记中加入公式…遂也舍弃了。<br>然后就是VSCode了，只需要使用几个简单的插件就可以满足我当前的博客编辑需求，公式、无限插图、多层文件夹（当然了）等等，给大家简单分享一下。<br><span id="more"></span></p><h2 id="工具及插件"><a href="#工具及插件" class="headerlink" title="工具及插件"></a>工具及插件</h2><p>工具：Visual Studio Code（神器！）<br>插件：</p><ol><li>yzhang.markdown-all-in-one</li><li>shd101wyy.markdown-preview-enhanced</li></ol><h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p>我的博客是用Hexo搭建的，解释器换用了包含LaTex的公式的Kramed。<br>首先展示一下编辑时的效果：<br><img src="/assets/post_img/article39/总览.jpg" alt="all"></p><p>左侧就是工作区了，这里是Markdown的真实样貌，右边就是预览插件的效果，由于使用了插件2所以可以使用目录进行快速跳转，对于写长文的帮助很大（Effie缺少这个功能）。如图：<br><img src="/assets/post_img/article39/toc.jpg" alt="toc"></p><p>那么插件1能为我们提供什么帮助呢？对我来说，是一个Effie的”低配版“。<br>最常用的应该就是<strong>粗体</strong>、<em>斜体</em>的快捷键，分别是<code>cmd+b</code>和<code>cmd+i</code>，与Effie相同。</p><p>但是对于代码行和代码块似乎没有对应的快捷键，这一点对我来说是不方便的，以后可能会设置一下。</p><p>数学公式方面，插件提供<code>cmd+m</code>可以直接打两个dollar号出来，比较方便。</p><p>预览方面，通常在点开一个Markdown文件时会自动在右侧开启预览，但如果不小心关闭掉了，可以通过<code>cmd+k + v</code>开启，注意这里要先输入<code>cmd+k</code>～</p><p>再下面就是一些高级的功能了，比如：自动生成目录。</p><ul><li><a href="#前言">前言</a></li><li><a href="#工具及插件">工具及插件</a></li><li><a href="#使用">使用</a></li></ul><p>这个似乎没有默认的快捷键，需要使用<code>shift+cmd+p</code>调出命令面板，然后搜索目录就可以找到了，对了，我的VSCode安装了中文插件，所以大多事情可以用中文完成，我认为VSCode的中文插件比其他我用过的IDE都要好，至少用起来不别扭。</p><p>关于插件1就说这么多，再说说插件2，预览时默认使用的是白色背景，对于我这种夜猫子，简直太不友好了。所以需要更换一个深色的。具体就是在插件的设置面板更换Preview Theme为一个你喜欢的主题，我使用的是atom dark。</p><p>以上就是关于我如何编辑Markdown格式博客的全部内容了，如果以后有其他更有用的插件，再继续分享～。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;关于我编写Markdown格式博客的工具选择。先说设备，我现在使用的是M1的MacBook Air。&lt;br&gt;最初使用的是Sublime Text，一个很经典的开源文本编辑器，缺点就是如果不订阅的话保存几次就会弹出广告，故舍弃。&lt;br&gt;然后使用的就是Effie，这是一个国产的笔记软件，十分的简洁，用于编写普通的文本和代码是很好的，直到我需要在笔记中加入公式…遂也舍弃了。&lt;br&gt;然后就是VSCode了，只需要使用几个简单的插件就可以满足我当前的博客编辑需求，公式、无限插图、多层文件夹（当然了）等等，给大家简单分享一下。&lt;br&gt;</summary>
    
    
    
    
    <category term="Hexo" scheme="http://silencezheng.top/tags/Hexo/"/>
    
  </entry>
  
  <entry>
    <title>Hexo博客更换Markdown解释器</title>
    <link href="http://silencezheng.top/2022/05/17/article38/"/>
    <id>http://silencezheng.top/2022/05/17/article38/</id>
    <published>2022-05-17T05:04:36.000Z</published>
    <updated>2022-05-17T05:11:56.050Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>最近在学习深度学习，总是需要在博客中添加一些公式，就选择使用LaTex来添加，结果发现使用hexo默认的hexo-renderer-marked解释器问题很多，还没有办法修改（网上没找到），索性更换一个解释器，记录一下。<br><span id="more"></span></p><h1 id="选择解释器"><a href="#选择解释器" class="headerlink" title="选择解释器"></a>选择解释器</h1><p>选择 hexo-renderer-kramed，原因无他，只是找到了一篇通过修改 hexo-renderer-kramed使hexo对公式的支持达到不错效果的文章，照葫芦画瓢抄起来～<br>参考：<a href="https://blog.csdn.net/weixin_44441126/article/details/119745642">https://blog.csdn.net/weixin_44441126/article/details/119745642</a></p><h1 id="卸载、安装"><a href="#卸载、安装" class="headerlink" title="卸载、安装"></a>卸载、安装</h1><p><code>npm uninstall hexo-renderer-marked --save</code> 卸载默认解释器<br><code>npm install hexo-renderer-kramed --save</code> 安装新的～</p><h1 id="解决问题"><a href="#解决问题" class="headerlink" title="解决问题"></a>解决问题</h1><h2 id="问题1：下划线-被转义为斜体而非LaTeX下标"><a href="#问题1：下划线-被转义为斜体而非LaTeX下标" class="headerlink" title="问题1：下划线_被转义为斜体而非LaTeX下标"></a>问题1：下划线_被转义为斜体而非LaTeX下标</h2><p>如图：<br><img src="/assets/post_img/article38/下划线错误.jpg" alt=""><br>Markdown本身的语法是支持*和_都被转义为斜体的，所以我们需要取消掉kramed对_的转义。</p><p>打开本地hexo文件夹下的/node_modules/kramed/lib/rules/inline.js，找到第20行如下代码：<br><code>em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</code></p><p>把正则中对下划线匹配的部分去掉，修改后如下：<br><code>em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</code></p><p>这样修改以后斜体就只能用*了，不过足够的～</p><h2 id="问题2-反斜杠-被转义为-而非LaTeX换行"><a href="#问题2-反斜杠-被转义为-而非LaTeX换行" class="headerlink" title="问题2:反斜杠\\被转义为\而非LaTeX换行"></a>问题2:反斜杠\\被转义为\而非LaTeX换行</h2><p>如图：<br><img src="/assets/post_img/article38/双反斜杠换行错误.jpg" alt=""></p><p>当公式中出现\\表示换行时，会被kramed渲染为\，导致公式显示异常。<br>找到inline.js中第11行如下代码：<br><figure class="highlight taggerscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">escape: /^<span class="symbol">\\</span>([<span class="symbol">\\</span>`*<span class="symbol">\[</span><span class="symbol">\]</span>()#$+<span class="symbol">\-</span>.!_&gt;])/,</span><br></pre></td></tr></table></figure><br>修改如下：<br><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">escape: /^\\([`*\[\]()#$+\-.!_&gt;])/,`</span><br></pre></td></tr></table></figure></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;最近在学习深度学习，总是需要在博客中添加一些公式，就选择使用LaTex来添加，结果发现使用hexo默认的hexo-renderer-marked解释器问题很多，还没有办法修改（网上没找到），索性更换一个解释器，记录一下。&lt;br&gt;</summary>
    
    
    
    
    <category term="Hexo" scheme="http://silencezheng.top/tags/Hexo/"/>
    
  </entry>
  
  <entry>
    <title>线性神经网络--《动手学深度学习》笔记0x03</title>
    <link href="http://silencezheng.top/2022/05/16/article37/"/>
    <id>http://silencezheng.top/2022/05/16/article37/</id>
    <published>2022-05-16T15:55:05.000Z</published>
    <updated>2022-05-27T14:14:41.763Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><p>从经典算法————<em>线性</em>神经网络开始，介绍神经网络的基础知识。 经典统计学习技术中的线性回归和softmax回归可以视为线性神经网络。</p><p>对应实践：<a href="https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x03.ipynb">https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x03.ipynb</a></p><span id="more"></span><h3 id="0-1-各章总结"><a href="#0-1-各章总结" class="headerlink" title="0.1. 各章总结"></a>0.1. 各章总结</h3><ul><li>机器学习模型中的关键要素是训练数据、损失函数、优化算法，还有模型本身。</li><li>矢量化使数学表达上更简洁，同时运行的更快。</li><li>最小化目标函数和执行极大似然估计等价。</li><li>线性回归模型也是一个简单的神经网络。</li><li>我们可以使用PyTorch的高级API更简洁地实现模型。在PyTorch中，data模块提供了数据处理工具，nn模块定义了大量的神经网络层和常见损失函数。可以通过<code>_</code>结尾的方法将参数替换，从而初始化参数。</li><li>softmax运算获取一个向量并将其映射为概率。</li><li>softmax回归适用于分类问题，它使用了softmax运算中输出类别的概率分布。</li><li>交叉熵是一个衡量两个概率分布之间差异的很好的度量，它测量给定模型编码数据所需的比特数。</li><li>数据迭代器是获得更高性能的关键组件。依靠实现良好的数据迭代器，利用高性能计算来避免减慢训练过程。</li><li>借助softmax回归，我们可以训练多分类的模型。</li><li>训练softmax回归循环模型与训练线性回归模型非常相似：先读取数据，再定义模型和损失函数，然后使用优化算法训练模型。大多数常见的深度学习模型都有类似的训练过程。</li><li>使用深度学习框架的高级API，我们可以更简洁地实现softmax回归。</li><li>从计算的角度来看，实现softmax回归比较复杂。在许多情况下，深度学习框架在这些著名的技巧之外采取了额外的预防措施，来确保数值的稳定性。这使我们避免了在实践中从零开始编写模型时可能遇到的陷阱。</li></ul><h2 id="1-线性回归"><a href="#1-线性回归" class="headerlink" title="1. 线性回归"></a>1. 线性回归</h2><p><em>回归</em>（regression）是能为一个或多个自变量与因变量之间关系建模的一类方法。 在自然科学和社会科学领域，回归经常用来表示输入和输出之间的关系。</p><p>在机器学习领域中的大多数任务通常都与<em>预测</em>（prediction）有关。 当我们想预测一个数值时，就会涉及到回归问题。 常见的例子包括：预测价格（房屋、股票等）、预测住院时间（针对住院病人等）、 预测需求（零售销量等）。 但不是所有的<em>预测</em>都是回归问题，比如分类问题也属于<em>预测</em>，目标是预测数据属于一组类别中的哪一个。</p><h3 id="1-1-线性回归基本元素"><a href="#1-1-线性回归基本元素" class="headerlink" title="1.1. 线性回归基本元素"></a>1.1. 线性回归基本元素</h3><p><em>线性回归</em>（linear regression）基于几个简单的假设： 首先，假设自变量x和因变量y之间的关系是线性的， 即y可以表示为x中元素的加权和，这里通常允许包含观测值的一些噪声； 其次，我们假设任何噪声都比较正常，如噪声遵循正态分布。</p><p>为了解释<em>线性回归</em>，我们举一个实际的例子： 我们希望根据房屋的面积（平方英尺）和房龄（年）来估算房屋价格（美元）。 为了开发一个能预测房价的模型，我们需要收集一个真实的数据集。 这个数据集包括了房屋的销售价格、面积和房龄。 在机器学习的术语中，该数据集称为<em>训练数据集</em>（training data set） 或<em>训练集</em>（training set）。 每行数据（比如一次房屋交易相对应的数据）称为<em>样本</em>（sample）， 也可以称为<em>数据点</em>（data point）或<em>数据样本</em>（data instance）。 我们把试图预测的目标（比如预测房屋价格）称为<em>标签</em>（label）或<em>目标</em>（target）。 预测所依据的自变量（面积和房龄）称为<em>特征</em>（feature）或<em>协变量</em>（covariate）。</p><h4 id="1-1-1-线性模型"><a href="#1-1-1-线性模型" class="headerlink" title="1.1.1. 线性模型"></a>1.1.1. 线性模型</h4><p>线性假设是指目标（房屋价格）可以表示为特征（面积和房龄）的加权和，如下面的式子：</p><script type="math/tex; mode=display">\text { price }=w_{\text {1}} \cdot \text { area }+w_{\text {2}} \cdot \text { age }+b</script><p>其中的w1和w2称为<em>权重</em>（weight），分别对应面积（area）和房龄（age），权重决定了每个特征对我们预测值的影响。b称为<em>偏置</em>（bias）、<em>偏移量</em>（offset）或<em>截距</em>（intercept）。 偏置是指当所有特征都取值为0时，预测值应该为多少。 即使现实中不会有任何房子的面积是0或房龄正好是0年，我们仍然需要偏置项。 如果没有偏置项，我们模型的表达能力将受到限制。 严格来说，上式是输入特征的一个<strong>仿射变换（affine transformation）</strong>。 仿射变换的特点是通过加权和对特征进行<em>线性变换（linear transformation）</em>， 并通过偏置项来进行<em>平移（translation）</em>。</p><p>给定一个数据集，我们的目标是寻找模型的权重w和偏置b， 使得根据模型做出的预测大体符合数据里的真实价格。 输出的预测值由输入特征通过<em>线性模型</em>的仿射变换决定，仿射变换由所选权重和偏置确定。</p><p>而在机器学习领域，我们通常使用的是高维数据集，建模时采用线性代数表示法会比较方便。 当我们的输入包含d个特征时，我们将预测结果$\hat{y}$（通常使用“尖角”符号表示y的估计值）表示为：$\hat{y}=w_{1} x_{1}+\ldots+w_{d} x_{d}+b$.</p><p>将所有特征放到向量x∈Rd中， 并将所有权重放到向量w∈Rd中， 我们可以用点积形式来简洁地表达模型：$\hat{y}=\mathbf{w}^{\top} \mathbf{x}+b$，其中向量x对应于单个数据样本的特征。 用符号表示的矩阵X∈Rn×d可以很方便地引用我们整个数据集的n个样本。 其中，X的每一行是一个样本，每一列是一种特征。</p><p>对于特征集合X，预测值y^∈Rn可以通过矩阵-向量乘法表示为：$\hat{\mathbf{y}}=\mathbf{X} \mathbf{w}+b$。</p><p>这个过程中的求和将使用广播机制。给定训练数据特征X和对应的已知标签y， 线性回归的目标是找到一组<strong>权重向量w和偏置b</strong>：当给定从X的同分布中取样的新样本特征时， 这组权重向量和偏置能够使得新样本预测标签的误差尽可能小。</p><p>虽然我们相信给定x预测y的最佳模型会是线性的， 但我们很难找到一个有n个样本的真实数据集，其中对于所有的1≤i≤n，$\boldsymbol{y}^{(i)}$完全等于$\mathbf{w}^{\top} \mathbf{x}^{(i)}+b$。 无论我们使用什么手段来观察特征X和标签y， 都可能会出现少量的观测误差。 因此，即使确信特征与标签的潜在关系是线性的， 我们也会加入一个噪声项来考虑观测误差带来的影响。</p><p>在开始寻找最好的<em>模型参数</em>（model parameters）w和b之前， 我们还需要两个东西： （1）一种模型质量的度量方式（损失函数）； （2）一种能够更新模型以提高模型预测质量的方法（比如随机梯度下降）。</p><h4 id="1-1-2-损失函数（loss-function）！"><a href="#1-1-2-损失函数（loss-function）！" class="headerlink" title="1.1.2. 损失函数（loss function）！"></a>1.1.2. 损失函数（loss function）！</h4><p>在开始考虑如何用模型<em>拟合</em>（fit）数据之前，我们需要确定一个拟合程度的度量。<strong>损失函数（loss function）</strong>能够量化目标的<em>实际</em>值与<em>预测</em>值之间的差距。 通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。 </p><p>回归问题中最常用的损失函数是平方误差函数。当样本i的预测值为y，其相应的真实标签为ty时， 平方误差可以定义为以下公式：<code>l(w,b)=0.5(y−ty)^2</code>其中常数0.5不会带来本质的差别，但这样在形式上稍微简单一些 （因为当我们对损失函数求导后常数系数为1）。由于训练数据集并不受我们控制，所以经验误差只是关于模型参数的函数。经验误差即l，w和b是线性模型参数。<br>由于平方误差函数中的二次方项， 估计值和观测值之间较大的差异将导致更大的损失。为了度量模型在整个数据集上的质量，我们需计算在训练集n个样本上的<em>损失均值</em>，也就是对每一个样本计算损失然后求和再除样本数。</p><script type="math/tex; mode=display">L(\mathbf{w}, b)=\frac{1}{n} \sum_{i=1}^{n} l^{(i)}(\mathbf{w}, b)=\frac{1}{n} \sum_{i=1}^{n} \frac{1}{2}\left(\mathbf{w}^{\top} \mathbf{x}^{(i)}+b-y^{(i)}\right)^{2}</script><p>在训练模型时，我们希望寻找一组参数（w∗,b∗）， 这组参数能最小化在所有训练样本上的总损失。</p><h4 id="1-1-3-解析解"><a href="#1-1-3-解析解" class="headerlink" title="1.1.3. 解析解"></a>1.1.3. 解析解</h4><p>线性回归刚好是一个很简单的优化问题。 与在d2l中所讲到的其他大部分模型不同，线性回归的解可以用一个公式简单地表达出来， 这类解叫作解析解（analytical solution）。 首先，我们将偏置b合并到参数w中，合并方法是在包含所有参数的矩阵中附加一列。 我们的预测问题是最小化$|\mathbf{y}-\mathbf{X} \mathbf{w}|^{2}$。 这在损失平面上只有一个临界点，这个临界点对应于整个区域的损失极小点。 将损失关于w的导数设为0，得到解析解：</p><script type="math/tex; mode=display">\mathbf{w}^{*}=\left(\mathbf{X}^{\top} \mathbf{X}\right)^{-1} \mathbf{X}^{\top} \mathbf{y}</script><p>像线性回归这样的简单问题存在解析解，但并不是所有的问题都存在解析解。 解析解可以进行很好的数学分析，但解析解对问题的限制很严格，导致它无法广泛应用在深度学习里。</p><h4 id="1-1-4-随机梯度下降（Stochastic-Gradient-Descent）！"><a href="#1-1-4-随机梯度下降（Stochastic-Gradient-Descent）！" class="headerlink" title="1.1.4. 随机梯度下降（Stochastic Gradient Descent）！"></a>1.1.4. 随机梯度下降（Stochastic Gradient Descent）！</h4><p>即使在我们无法得到解析解的情况下，我们仍然可以有效地训练模型。 在许多任务上，那些难以优化的模型效果要更好。 因此，弄清楚如何训练这些难以优化的模型是非常重要的。</p><p>本书中我们用到一种名为<strong>梯度下降</strong>的方法， 这种方法几乎可以优化所有深度学习模型。<strong>它通过不断地在损失函数递减的方向上更新参数来降低误差。</strong></p><p>梯度下降最简单的用法是计算损失函数（数据集中所有样本的损失均值） 关于模型参数的导数（在这里也可以称为梯度）。 但实际中的执行可能会非常慢：因为在每一次更新参数之前，我们必须遍历整个数据集。 因此，我们通常会在每次需要计算更新的时候随机抽取一小批样本， 这种变体叫做<strong>小批量随机梯度下降（minibatch stochastic gradient descent）</strong>。</p><p>在每次迭代中，我们首先随机抽样一个小批量B， 它是由固定数量的训练样本组成的。 然后，我们计算小批量的平均损失关于模型参数的导数（也可以称为梯度）。 最后，我们将梯度乘以一个预先确定的正数η，并从当前参数的值中减掉。</p><p>总结一下，算法的步骤如下： （1）初始化模型参数的值，如随机初始化； （2）从数据集中随机抽取小批量样本且在负梯度的方向上更新参数，并不断迭代这一步骤。</p><p>|B|表示每个小批量中的样本数，这也称为<strong>批量大小（batch size）</strong>。η表示<strong>学习率（learning rate）</strong>。 批量大小和学习率的值通常是手动预先指定，而不是通过模型训练得到的。 这些可以调整但不在训练过程中更新的参数称为<strong>超参数（hyperparameter）</strong>。<strong>调参（hyperparameter tuning</strong>是选择超参数的过程。 超参数通常是我们根据训练迭代结果来调整的， 而训练迭代结果是在独立的<em>验证数据集</em>（validation dataset）上评估得到的。</p><p>在训练了预先确定的若干迭代次数后（或者直到满足某些其他停止条件后）， 我们记录下模型参数的估计值，表示为$\hat{\mathbf{w}}, \hat{b}$。 但是，即使我们的函数确实是线性的且无<strong>噪声</strong>，这些估计值也不会使损失函数真正地达到最小值。 因为算法会使得损失向最小值缓慢收敛，但却不能在有限的步数内非常精确地达到最小值。</p><p>关于<strong>噪声</strong>的解释：深度神经网络的成功依赖于高质量标记的训练数据。训练数据中存在标记错误（标记噪声，即Noisy Labels）会大大降低模型在干净测试数据上的准确性。大型数据集几乎总是包含带有不正确或不准确的标签。这导致了一个悖论：一方面，大型数据集对于深度网络的训练是非常必要的，而另一方面，深度网络往往会记住训练标签噪声，从而在实践中导致较差的模型性能。</p><p>线性回归恰好是一个在整个域中只有一个最小值的学习问题。 但是对于像深度神经网络这样复杂的模型来说，损失平面上通常包含多个最小值。 深度学习实践者很少会去花费大力气寻找这样一组参数，使得在<em>训练集</em>上的损失达到最小。 事实上，更难做到的是找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失， 这一挑战被称为<em>泛化</em>（generalization）。</p><h4 id="1-1-5-用模型进行预测"><a href="#1-1-5-用模型进行预测" class="headerlink" title="1.1.5. 用模型进行预测"></a>1.1.5. 用模型进行预测</h4><p>给定“已学习”的线性回归模型$\hat{\mathbf{w}}^{\top} \mathbf{x}+\hat{b}$， 现在我们可以通过房屋面积和房龄来估计一个（未包含在训练数据中的）新房屋价格。 给定特征估计目标的过程通常称为<em>预测</em>（prediction）或<em>推断</em>（inference）。</p><h3 id="1-2-矢量化加速"><a href="#1-2-矢量化加速" class="headerlink" title="1.2. 矢量化加速"></a>1.2. 矢量化加速</h3><p>在训练我们的模型时，我们经常希望能够同时处理整个小批量的样本。 为了实现这一点，需要我们对计算进行矢量化， 从而利用线性代数库，而不是在Python中编写开销高昂的for循环。矢量化代码通常会带来数量级的加速。而且将更多的数学运算放到库中，而无须自己编写那么多的计算，从而减少了出错的可能性。</p><h3 id="1-3-正态分布与平方损失"><a href="#1-3-正态分布与平方损失" class="headerlink" title="1.3. 正态分布与平方损失"></a>1.3. 正态分布与平方损失</h3><p>这一部分通过对噪声分布的假设（假设符合正态分布）来解读平方损失目标函数（1.1.2决定的损失函数），也就是解释为什么均方误差可以用来作为线性回归的损失函数。<br>回顾一下，最大似然估计，就是利用已知的样本结果信息，反推具有最大概率导致这些样本结果出现的模型参数值。<br>正态分布和线性回归之间的关系很密切。 正态分布（normal distribution），也称为<em>高斯分布</em>（Gaussian distribution）。设随机变量x具有均值μ和方差 $\sigma^{2}$（标准差σ），概率密度函数如下。</p><script type="math/tex; mode=display">p(x)=\frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left(-\frac{1}{2 \sigma^{2}}(x-\mu)^{2}\right)</script><p>从正态分布的图像上看，改变均值会产生沿x轴的偏移，增加方差将会分散分布、降低其峰值。<br>均方误差损失函数（简称均方损失）可以用于线性回归的一个原因是： 我们假设了观测中包含噪声，其中噪声服从正态分布。噪声正态分布如：$y=\mathbf{w}^{\top} \mathbf{x}+b+\epsilon$,其中，$\epsilon \sim \mathcal{N}\left(0, \sigma^{2}\right)$。</p><p>通过把 $\epsilon$ 以外的项移到式子右侧可以得出 $y-\mathbf{w}^{\top} \mathbf{x}-b=\epsilon$，所以可以写出通过给定的x观测到特定y的<em>似然</em>（likelihood）：</p><script type="math/tex; mode=display">P(y \mid \mathbf{x})=\frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left(-\frac{1}{2 \sigma^{2}}\left(y-\mathbf{w}^{\top} \mathbf{x}-b\right)^{2}\right)</script><p>然后根据极大似然估计法，参数w和b的最优值是使整个数据集的<em>似然</em>最大的值：</p><script type="math/tex; mode=display">P(\mathbf{y} \mid \mathbf{X})=\prod_{i=1}^{n} p\left(y^{(i)} \mid \mathbf{x}^{(i)}\right)</script><p>根据极大似然估计法选择的估计量称为<em>极大似然估计量</em>。 通过最大化似然对数来简化使许多指数函数的乘积最大化的问题。 优化通常是说最小化而不是最大化，我们可以改为<em>最小化负对数似然</em>$-\log P(\mathbf{y} \mid \mathbf{X})$，由此得到：</p><script type="math/tex; mode=display">-\log P(\mathbf{y} \mid \mathbf{X})=\sum_{i=1}^{n} \frac{1}{2} \log \left(2 \pi \sigma^{2}\right)+\frac{1}{2 \sigma^{2}}\left(y^{(i)}-\mathbf{w}^{\top} \mathbf{x}^{(i)}-b\right)^{2}</script><p>只需要假设 $\sigma$ 是某个固定常数就可以忽略第一项， 因为第一项不依赖于w和b，改变参数不会对负对数似然的大小造成影响。 现在第二项除了常数 $\frac{1}{\sigma^{2}}$ 外，其余部分和前面介绍的均方误差是一样的。 可以看到标准差作为系数也不影响结果的大小，也就是说上面式子的解并不依赖于 $\sigma$。 因此，在高斯噪声的假设下，最小化均方误差等价于对线性模型的极大似然估计。</p><h3 id="1-4-从线性回归到深度网络"><a href="#1-4-从线性回归到深度网络" class="headerlink" title="1.4. 从线性回归到深度网络"></a>1.4. 从线性回归到深度网络</h3><p>用描述神经网络的方式来描述线性模型， 从而把线性模型看作一个神经网络。 首先，我们用“层”符号来重写这个模型。线性回归是单层神经网络（通常我们在计算层数时不考虑输入层），只有输入层和输出层构成，其中输入层有d个节点，输出层有1个节点。</p><p>如输入为x1,…,xd， 因此输入层中的<em>输入数</em>（或称为<em>特征维度</em>，feature dimensionality）为d。 网络的输出为o1，因此输出层中的<em>输出数</em>是1。 需要注意的是，输入值都是已经给定的，并且只有一个<em>计算</em>神经元。 我们可以将线性回归模型视为仅由单个人工神经元（输出层的一个）组成的神经网络，或称为单层神经网络。<br><img src="/assets/post_img/article37/线性回归神经网络.svg" alt="0x03pic1"><br>对于线性回归，每个输入都与每个输出（在本例中只有一个输出）相连， 我们将这种变换（图中的输出层） 称为<em>全连接层</em>（fully-connected layer）或称为<em>稠密层</em>（dense layer）。</p><h3 id="1-5-线性回归的从零实现与框架实现"><a href="#1-5-线性回归的从零实现与框架实现" class="headerlink" title="1.5. 线性回归的从零实现与框架实现"></a>1.5. 线性回归的从零实现与框架实现</h3><p><strong>从零实现</strong>：从零开始实现包括数据流水线、模型、损失函数和小批量随机梯度下降优化器。 虽然现代的深度学习框架几乎可以自动化地进行所有这些工作，但从零开始实现可以确保你真正知道自己在做什么。 同时，了解更细致的工作原理将方便我们自定义模型、自定义层或自定义损失函数。 在这一部分中将只使用张量和自动求导。</p><p><strong>框架实现</strong>：成熟的开源框架可以自动化基于梯度的学习算法中重复性的工作。除了张量和自动求导外，数据迭代器、损失函数、优化器和神经网络层很常用， 现代深度学习库为我们实现了这些组件。</p><p>细节代码放在实践notebook中，关于框架实现中采用求和方式后如何修改学习率的问题还是没有答案，希望以后可以搞明白。</p><h2 id="2-softmax回归"><a href="#2-softmax回归" class="headerlink" title="2. softmax回归"></a>2. softmax回归</h2><p>回归可以用于预测<em>多少</em>的问题。 比如预测房屋被售出价格，或者棒球队可能获得的胜场数，又或者患者住院的天数。<br>事实上，我们也对<em>分类</em>问题感兴趣：不是问“多少”，而是问“哪一个”：</p><ul><li>某个电子邮件是否属于垃圾邮件文件夹？</li><li>某个图像描绘的是驴、狗、猫、还是鸡？</li><li>某人接下来最有可能看哪部电影？</li></ul><p>通常，机器学习实践者用<em>分类</em>这个词来描述两个有微妙差别的问题： 1. 我们只对样本的“硬性”类别感兴趣，即属于哪个类别； 2. 我们希望得到“软性”类别，即得到属于每个类别的概率。 这两者的界限往往很模糊。其中的一个原因是：即使我们只关心硬类别，我们仍然使用软类别的模型。</p><h3 id="2-1-分类问题"><a href="#2-1-分类问题" class="headerlink" title="2.1. 分类问题"></a>2.1. 分类问题</h3><p>从一个图像分类问题开始。 假设每次输入是一个的2X2灰度图像。灰度图像是每个像素只有一个采样颜色的图像。这类图像通常显示为从最暗黑色到最亮的白色的灰度，尽管理论上这个采样可以任何颜色的不同深浅，甚至可以是不同亮度上的不同颜色。（注意：与黑白图像不同）<br>我们可以用一个标量表示每个像素值，每个图像对应四个特征x1,x2,x3,x4。此外，假设每个图像属于类别“猫”，“鸡”和“狗”中的一个。</p><p>接下来，我们要选择如何表示标签。 我们有两个明显的选择：最直接的想法是选择y∈{1,2,3}， 其中整数分别代表猫、鸡、狗。 这是在计算机上存储此类信息的有效方法。 如果类别间有一些自然顺序， 比如说我们试图预测{婴儿,儿童,青少年,青年人,中年人,老年人}， 那么将这个问题转变为回归问题，并且保留这种格式是有意义的。一般的分类问题并不与类别之间的自然顺序有关。</p><p>表示分类数据的简单方法：<em>独热编码（one-hot encoding）</em>。<br>独热编码是一个向量，它的分量和类别一样多。 类别对应的分量设置为1，其他所有分量设置为0。 在我们的例子中，标签y将是一个三维向量， 其中(1,0,0)对应于“猫”、(0,1,0)对应于“鸡”、(0,0,1)对应于“狗”：</p><script type="math/tex; mode=display">y \in\{(1,0,0),(0,1,0),(0,0,1)\}</script><h3 id="2-2-网络架构"><a href="#2-2-网络架构" class="headerlink" title="2.2. 网络架构"></a>2.2. 网络架构</h3><p>为了估计所有可能类别的条件概率，我们需要一个有多个输出的模型，每个类别对应一个输出。 为了解决线性模型的分类问题，我们需要和输出一样多的<strong>仿射函数（affine function）</strong>。 每个输出对应于它自己的仿射函数。 在我们的例子中，由于我们有4个特征和3个可能的输出类别， 我们将需要12个标量来表示权重（带下标的）， 3个标量来表示偏置（带下标的）。 下面我们为每个输入计算三个未规范化的预测（logit）：o1,o2和o3。</p><script type="math/tex; mode=display">\begin{aligned}&o_{1}=x_{1} w_{11}+x_{2} w_{12}+x_{3} w_{13}+x_{4} w_{14}+b_{1} \\&o_{2}=x_{1} w_{21}+x_{2} w_{22}+x_{3} w_{23}+x_{4} w_{24}+b_{2} \\&o_{3}=x_{1} w_{31}+x_{2} w_{32}+x_{3} w_{33}+x_{4} w_{34}+b_{3}\end{aligned}</script><p>可以用下面的单层神经网络图来描述计算过程，与线性回归一样，softmax回归也是一个单层神经网络。 由于计算每个输出(o1,o2,o3)取决于所有输入(x1,x2,x3,x4)，所以softmax回归的输出层也是全连接层。<br><img src="/assets/post_img/article37/softmax网络.png" alt="0x03pic2"></p><p>模型通过向量形式表达为：$\mathbf{o}=\mathbf{W} \mathbf{x}+\mathbf{b}$<br>这是一种更适合数学和编写代码的形式。 到此，我们已经将所有权重放到一个3X4矩阵中。 对于给定数据样本的特征x， 我们的输出是由权重与输入特征进行矩阵-向量乘法再加上偏置b得到的。</p><h3 id="2-3-全连接层的参数开销"><a href="#2-3-全连接层的参数开销" class="headerlink" title="2.3. 全连接层的参数开销"></a>2.3. 全连接层的参数开销</h3><p>深度学习中，全连接层无处不在。全连接层是“完全”连接的，可能有很多可学习的参数。 具体来说，对于任何具有d个输入和q个输出的全连接层， 参数开销为O(dq)，这个数字在实践中可能高得令人望而却步。 幸运的是，将d个输入转换为q个输出的成本可以减少到O$\left(\frac{d q}{n}\right)$， 其中超参数n可以由我们灵活指定，以在实际应用中平衡参数节约和模型有效性。</p><h3 id="2-4-softmax运算"><a href="#2-4-softmax运算" class="headerlink" title="2.4. softmax运算"></a>2.4. softmax运算</h3><p>现在我们将优化参数以最大化观测数据的概率。 为了得到预测结果，我们将设置一个阈值，如选择具有最大概率的标签。<br>我们希望模型的输出 $\hat{y}_{j}$ 可以视为属于类 $j$ 的概率， 然后选择具有最大输出值的类别argmax $\boldsymbol{}_{j} y_{j}$ 作为我们的预测。例如, 如果 $\hat{y}_{1}$ 、 $\hat{y}_{2}$ 和 $\hat{y}_{3}$ 分别为 $0.1$ 、 $0.8$ 和 $0.1$, 那么我们预测的类别是2，在我们的例子中代表“鸡”。</p><p>然而我们不能将未规范化的预测o直接视作我们感兴趣的输出。因为将线性层的输出直接视为概率时存在一些问题： 一方面，我们没有限制这些输出数字的总和为1。 另一方面，根据输入的不同，它们可以为负值。<br>要将输出视为概率，我们必须保证在任何数据上的输出都是非负的且总和为1。 此外，我们需要一个训练目标，来鼓励模型精准地估计概率。 在分类器输出0.5的所有样本中，我们希望这些样本有一半实际上属于预测的类。 这个属性叫做校准（calibration）。</p><p>softmax函数正是这样做的： softmax函数将未规范化的预测变换为非负并且总和为1，同时要求模型保持可导。 我们首先对每个未规范化的预测求幂（就是把预测作为e的指数），这样可以确保输出非负。 为了确保最终输出的总和为1，我们再对每个求幂后的结果除以它们的总和。如下式：</p><script type="math/tex; mode=display">\hat{\mathbf{y}}=\operatorname{softmax}(\mathbf{o}) \quad \text { 其中 } \quad \hat{y}_{j}=\frac{\exp \left(o_{j}\right)}{\sum_{k} \exp \left(o_{k}\right)}</script><p>这里, 对于所有的 $j$ 总有 $0 \leq \hat{y}_{j} \leq 1_{\circ}$ 因此, $\hat{\mathbf{y}}$ 可以视为一个正确的概率分布。 softmax运算不会改变末规范化的预测o之间的顺序, 只会确定分配给每个类别的概率。因此，在预测过程中, 我们仍然可以用下式来选择最有可能的类别。</p><script type="math/tex; mode=display">\underset{j}{\operatorname{argmax}} \hat{y}_{j}=\underset{j}{\operatorname{argmax}} o_{j}</script><p>softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。 因此，softmax回归是一个线性模型（linear model）。</p><h3 id="2-5-小批量样本的矢量化"><a href="#2-5-小批量样本的矢量化" class="headerlink" title="2.5. 小批量样本的矢量化"></a>2.5. 小批量样本的矢量化</h3><p>为了提高计算效率并且充分利用GPU，我们通常会针对小批量数据执行矢量计算。假设我们读取了一个批量的样本X, 其中特征维度 (输入数量) 为 $d$, 批量大小为 $n$。 此外, 假设我们在输出中有 $q$ 个类别。那么小批量特征为 $\mathrm{X} \in \mathbb{R}^{n \times d}$ ， 权重为 $\mathbf{W} \in \mathbb{R}^{d \times q}$ ，偏置为 $\mathbf{b} \in \mathbb{R}^{1 \times q}$ 。 softmax回归的矢量计算表达式为：</p><script type="math/tex; mode=display">\begin{aligned}&\mathbf{O}=\mathbf{X W}+\mathbf{b}, \\&\hat{\mathbf{Y}}=\operatorname{softmax}(\mathbf{O}) .\end{aligned}</script><p>相对于一次处理一个样本, 小批量样本的矢量化加快了 $\mathbf{X}$ 和 $\mathbf{W}$ 的矩阵-向量乘法。由于 $\mathbf{X}$ 中的每一行代表一个数据样本, 那么softmax运算可以按行 (rowwise) 执行：对于 $\mathrm{O}$ 的每一行, 我们先对所有项进行幂运算, 然后通过求和对它们进行标准化（规范化，对应线性代数中的正交规范化比较好理解）。在上式中, $\mathbf{X W}+\mathbf{b}$ 的求和会使用<em>广播</em>（每行都加）, 小批量的末规范化预测 $\mathbf{O}$ 和输出概率 $\hat{\mathbf{Y}}$ 都是形状为 $n \times q$ 的矩阵。</p><h3 id="2-6-损失函数"><a href="#2-6-损失函数" class="headerlink" title="2.6. 损失函数"></a>2.6. 损失函数</h3><p>损失函数用来度量预测的效果。使用与线性回归中相同的最大似然估计。</p><h4 id="2-6-1-对数似然"><a href="#2-6-1-对数似然" class="headerlink" title="2.6.1. 对数似然"></a>2.6.1. 对数似然</h4><p>softmax函数给出了一个向量 $\hat{\mathbf{y}}$, 我们可以将其视为“对给定任意输入 $\mathbf{x}$ 的每个类的<strong>条件概率</strong>”。例如, $\hat{y}_{1}=$ $P(y=$ 猫 $\mid \mathbf{x})$ ，因为猫1鸡2狗3嘛。 假设整个数据集 ${\mathbf{X}, \mathbf{Y}}$ （也就是n个（特征，标签）对）具有 $n$ 个样本, 其中索引 $i$ 的样本由特征向量 $\mathbf{x}^{(i)}$ 和独热标签向量 $\mathbf{y}^{(i)}$ 组成。 我们可以将估计值与实际值进行比较：</p><script type="math/tex; mode=display">P(\mathbf{Y} \mid \mathbf{X})=\prod_{i=1}^{n} P\left(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)}\right)</script><p>根据最大似然估计，我们最大化 $P(\mathbf{Y} \mid \mathbf{X})$ ，相当于最小化负对数似然：</p><script type="math/tex; mode=display">-\log P(\mathbf{Y} \mid \mathbf{X})=\sum_{i=1}^{n}-\log P\left(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)}\right)=\sum_{i=1}^{n} l\left(\mathbf{y}^{(i)}, \hat{\mathbf{y}}^{(i)}\right)</script><p>其中, 对于任何标签 $\mathbf{y}$ 和模型预测 $\hat{\mathbf{y}}$ ，损失函数为：</p><script type="math/tex; mode=display">l(\mathbf{y}, \hat{\mathbf{y}})=-\sum_{j=1}^{q} \boldsymbol{y}_{j} \log \hat{y}_{j}</script><p>上面式子中的损失函数 $l$ 被称为交叉熵损失（cross-entropy loss）。由于y是一个长度为q的独热编码向量， 所以除了一个项以外的所有项j都消失了。 由于所有 $\hat{y}_{j}$ 都是预测的概率，所以它们的对数永远不会大于0（因为预测概率小于1）。 因此，如果正确地预测实际标签，即如果实际标签 $P(\mathbf{y} \mid \mathbf{x})=1$， 则损失函数不能进一步最小化，虽然这往往是不可能的，因为数据集中可能存在标签噪声（比如某些样本可能被误标）， 或输入特征没有足够的信息来完美地对每一个样本分类。</p><h4 id="2-6-2-softmax及其导数"><a href="#2-6-2-softmax及其导数" class="headerlink" title="2.6.2. softmax及其导数"></a>2.6.2. softmax及其导数</h4><p>将softmax函数代入到损失函数中：</p><script type="math/tex; mode=display">\begin{aligned}l(\mathbf{y}, \hat{\mathbf{y}}) &=-\sum_{j=1}^{q} y_{j} \log \frac{\exp \left(o_{j}\right)}{\sum_{k=1}^{q} \exp \left(o_{k}\right)} \\&=\sum_{j=1}^{q} y_{j} \log \sum_{k=1}^{q} \exp \left(o_{k}\right)-\sum_{j=1}^{q} y_{j} o_{j} \\&=\log \sum_{k=1}^{q} \exp \left(o_{k}\right)-\sum_{j=1}^{q} y_{j} o_{j}\end{aligned}</script><p>代入后的损失函数相对于任何末规范化的预测 $o_{j}$ 求导：</p><script type="math/tex; mode=display">\partial_{o_{j}} l(\mathbf{y}, \hat{\mathbf{y}})=\frac{\exp \left(o_{j}\right)}{\sum_{k=1}^{q} \exp \left(o_{k}\right)}-y_{j}=\operatorname{softmax}(\mathbf{o})_{j}-y_{j}</script><p>可以看出，该导数是我们softmax模型分配的概率与实际发生的情况（由独热标签向量表示）之间的差异。 从这个意义上讲，这与我们在回归中看到的非常相似（哪里？）， 其中梯度是观测值y和估计值 $\hat{\mathbf{y}}$ 之间的差异。<br>这不是巧合，在任何指数族分布模型中，对数似然的梯度正是由此得出的。这使梯度计算在实践中变得容易很多。</p><h4 id="2-6-3-交叉熵损失"><a href="#2-6-3-交叉熵损失" class="headerlink" title="2.6.3. 交叉熵损失"></a>2.6.3. 交叉熵损失</h4><p>如果考虑整个结果分布的情况，即观察到的不仅仅是一个结果。 对于标签y，我们可以使用与以前相同的表示形式。 唯一的区别是，我们现在用一个概率向量表示，如（0.1，0.2，0.7）， 而不是独热编码的向量，如（0，0，1）。<br>使用 $l(\mathbf{y}, \hat{\mathbf{y}})=-\sum_{j=1}^{q} y_{j} \log \hat{y}_{j}$来定义损失$l$，它是所有标签分布的预期损失值。 此损失称为交叉熵损失（cross-entropy loss），是分类问题最常用的损失之一。</p><h3 id="2-7-信息论基础"><a href="#2-7-信息论基础" class="headerlink" title="2.7. 信息论基础"></a>2.7. 信息论基础</h3><p>信息论（information theory）涉及编码、解码、发送以及尽可能简洁地处理信息或数据。</p><h4 id="2-7-1-熵"><a href="#2-7-1-熵" class="headerlink" title="2.7.1. 熵"></a>2.7.1. 熵</h4><p>信息论的核心思想是量化数据中的信息内容。 在信息论中，该数值被称为分布的熵（entropy）。可以通过以下方程得到：</p><script type="math/tex; mode=display">H[P]=\sum_{j}-P(j) \log P(j)</script><p>信息论的基本定理之一指出，为了对从分布 p 中随机抽取的数据进行编码， 我们至少需要 $H[P]$ “纳特（nat）”对其进行编码。 “纳特”相当于比特（bit），但是对数底为 $e$ 而不是2。因此，一个纳特约为1.44比特。</p><h4 id="2-7-2-信息量"><a href="#2-7-2-信息量" class="headerlink" title="2.7.2. 信息量"></a>2.7.2. 信息量</h4><p>信息量是指信息多少的量度。信息量用 $\log \frac{1}{P(j)}即-\log P(j)$ 表示，在观察一个事件 $j$ 时，并赋予它（主观）概率 $P(j)$。<br>当我们赋予一个事件较低的概率时，该事件的信息量就更大。熵就是当分配的概率真正匹配数据生成过程时的信息量的期望。（这个地方比较晦涩）</p><h4 id="2-7-3-重新审视交叉熵"><a href="#2-7-3-重新审视交叉熵" class="headerlink" title="2.7.3. 重新审视交叉熵"></a>2.7.3. 重新审视交叉熵</h4><p>熵 $H[P]$，交叉熵从P到Q，记为 $H(P,Q)$。可以把交叉熵想象为“主观概率为Q的观察者在看到根据概率P生成的数据时的预期‘惊讶’程度”。 当P=Q时，交叉熵达到最低。 在这种情况下，从P到Q的交叉熵是 $H(P, P)=H(P)$。<br>可以从两方面来考虑交叉熵分类目标： </p><ol><li>最大化观测数据的似然</li><li>最小化传达标签所需的信息量。</li></ol><h3 id="2-8-模型预测和评估"><a href="#2-8-模型预测和评估" class="headerlink" title="2.8. 模型预测和评估"></a>2.8. 模型预测和评估</h3><p>在训练softmax回归模型后，给出任何样本特征，我们可以预测每个输出类别的概率。 通常我们使用预测概率最高的类别作为输出类别。 如果预测与实际类别（标签）一致，则预测是正确的。<br>在后续的实验中将使用<strong>精度（accuracy）</strong>来评估模型的性能。 精度等于正确预测数与预测总数之间的比率。</p><h3 id="2-9-图像分类数据集"><a href="#2-9-图像分类数据集" class="headerlink" title="2.9. 图像分类数据集"></a>2.9. 图像分类数据集</h3><p>MNIST数据集（一个手写数字集）是图像分类中广泛使用的数据集之一，但作为基准数据集过于简单。因为很多算法在测试集上的性能已经达到 99.6%！一个算法即便在这个数据集上work在其他数据集上也很可能不怎么样。<br>Fashion-MNIST 是一个替代 MNIST 手写数字集的图像数据集。 它是由 Zalando（一家德国的时尚科技公司）旗下的研究部门提供。其涵盖了来自 10 种类别的共 7 万个不同商品的正面图片。Fashion-MNIST 的大小、格式和训练集/测试集划分与原始的 MNIST 完全一致。60000/10000 的训练测试数据划分，<em>通道</em>数为1，28x28（28像素）的灰度图片。<br>关于<em>通道（channels）</em>：描述一个像素点，如果是灰度，那么只需要一个数值来描述它，就是单通道。如果一个像素点，有RGB三种颜色来描述它，就是三通道。常用图片形式还有四通道的，即再加一个透明度。<br>该数据集样子大致如下（每个类别占三行）：<br><img src="/assets/post_img/article37/FasionMNIST.jpeg" alt="0x03pic3"><br>数据集的存储方式如下：<br><img src="/assets/post_img/article37/FasionMNIST存储方式.jpeg" alt="0x03pic4"><br>其中样本标注如下，十个类别：<br><img src="/assets/post_img/article37/FasionMNIST样本标注方式.jpeg" alt="0x03pic5"></p><p>深度学习框架中的内置函数可以将Fashion-MNIST数据集下载并读取到内存中。具体操作在本笔记对应实践中。</p><h3 id="2-10-softmax回归的从零实现与框架实现"><a href="#2-10-softmax回归的从零实现与框架实现" class="headerlink" title="2.10. softmax回归的从零实现与框架实现"></a>2.10. softmax回归的从零实现与框架实现</h3><p><strong>从零实现</strong>：softmax回归也是重要的基础，因此应该知道实现softmax回归的细节，从零实现他。每个样本都将用固定长度的向量表示。 原始数据集中的每个样本都是28x28的图像。 在这里通过展平每个图像，把它们看作长度为784的向量，即暂时只把每个像素位置看作一个特征。引入的交叉熵损失函数可能是深度学习中最常见的损失函数，因为目前分类问题的数量远远超过回归问题的数量。</p><p><strong>框架实现</strong>：通过深度学习框架的高级API也能更方便地实现softmax回归模型。 框架实现中继续使用Fashion-MNIST数据集，并保持批量大小为256。同时在softmax函数的实现上进行了优化（具体如何优化翻书看），解决了数值可能上、下溢出的问题。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;p&gt;从经典算法————&lt;em&gt;线性&lt;/em&gt;神经网络开始，介绍神经网络的基础知识。 经典统计学习技术中的线性回归和softmax回归可以视为线性神经网络。&lt;/p&gt;
&lt;p&gt;对应实践：&lt;a href=&quot;https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x03.ipynb&quot;&gt;https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x03.ipynb&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://silencezheng.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>开源协议简单了解</title>
    <link href="http://silencezheng.top/2022/05/14/article36/"/>
    <id>http://silencezheng.top/2022/05/14/article36/</id>
    <published>2022-05-14T14:55:13.000Z</published>
    <updated>2022-05-27T14:14:58.426Z</updated>
    
    <content type="html"><![CDATA[<p>最近看到B站有人用pyqt5被发律师函，于是对开源协议产生了一些兴趣，对网络相关热门帖子结合协议官网整理归纳一下，简单了解一下开源协议，如有错误请评论指正，感谢。<br>提及的协议有：GPL、LGPL、BSD、MIT、Apache License 2.0。<br><span id="more"></span></p><h2 id="0-基本概念"><a href="#0-基本概念" class="headerlink" title="0. 基本概念"></a>0. 基本概念</h2><h3 id="0-1-Copyleft和Copyright"><a href="#0-1-Copyleft和Copyright" class="headerlink" title="0.1. Copyleft和Copyright"></a>0.1. Copyleft和Copyright</h3><p>Copyleft和Copyright是两种截然相反的版权所有方式。copyright的英文本意是版权©️。copyleft现在被译为公共版权或非盈利版权。<br><img src="/assets/post_img/article36/clcr.jpg" alt="pic1"></p><p><strong>Copyright</strong>：版权所有，即软件的一切权利归软件作者私有。<br>软件的版权和其它一切权利归软件作者所私有。用户只有使用权，没有其它权利，包括没有复制软件的权利。<br><strong>Copyleft</strong>：是一种让程序或其它作品保持自由的通用方法，它要求所有对 Copyleft 程序的修改和扩展都保持自由。但该规则仅与GNU相关。<br>Copyleft 是指任何人都可以重新分发软件，不管有没有进行修改，但必须同时保留软件所具有的自由特性。Copyleft是为了保证所有用户都拥有自由的权利。<br>一个程序遵循 Copyleft，我们首先声明它是有版权的；然后，我们给它加上发布的规则，这个规则就是一个法律声明，它赋予所有人有使用、修改和重新发布程序的代码 及其衍生作品 的权利，但要求在这个过程中发布规则是不可以改变的。这样的话，代码和自由权利在法律上就不可分割了。</p><h3 id="0-2-其他"><a href="#0-2-其他" class="headerlink" title="0.2. 其他"></a>0.2. 其他</h3><p><strong>复制（Copy）</strong>：将软件复制到你的电脑，你客户的电脑，或者任何地方。<br><strong>分发（Distribution）</strong>：在网站供他人下载，拷贝到U盘送人。<br><strong>商用（Commercial use）</strong>：盈利，在分发软件的时候收费。<br><strong>修改（Modification）</strong>：添加或删除某个功能，在别的项目中使用部分代码等。<br><strong>专利使用（Patent use）</strong>：用于申请专利。<br><strong>个人使用（Private use）</strong>：供私人使用，基本都支持。<br>下面的介绍中权利部分严格按照以上概念进行，但通常带“自由”字眼的并非完全自由，有相应的规则加以限制，本文也会略微提到。<br><strong>开源</strong>：开源软件最大的特点是开放而不是免费，也就是任何人都可以得到软件的源代码，加以修改学习，甚至重新发放，但需要在版权限制范围之内。</p><h2 id="1-GPL（GNU-General-Public-License）"><a href="#1-GPL（GNU-General-Public-License）" class="headerlink" title="1. GPL（GNU General Public License）"></a>1. GPL（GNU General Public License）</h2><p>GNU 通用公共许可证 (GPL)，目前共有3个版本，分别为GPLv1、GPLv2、GPLv3（最新）。<br>GNU 通用公共许可证有以下几种格式：HTML、纯文本、ODF、Docbook v4或者v5、Texinfo、LaTeX、Markdown和RTF。这些文档的格式不是为了单独发布使用，它们的目的是嵌入其他文档。</p><p>GPL提供的权利主要有：自由复制、自由分发、“自由”修改、盈利、专利使用。<br>注意：</p><ol><li>盈利收费前向你的客户提供该软件的GPL许可协议，以便让他们知道，他们可以从别的渠道免费得到这份软件，以及你收费的理由。</li><li>修改时，使用了这段代码的项目也必须使用 GPL 协议。</li><li>分发时，软件自始至终都以开放源代码形式发布，无论软件以何种形式发布，都必须同时附上源代码。</li><li>GPLv3允许专利使用。</li></ol><p>总结：<br>GPL 大致就是一个Copyleft的体现。你可以去掉所有原作的版权信息，只要你保持开源，并且随源代码、二进制版附上GPL的许可证就行。开发或维护遵循 GPL 协议开发的软件的公司或个人，可以对使用者收取一定的服务费用。但必须无偿提供软件的完整源代码，不得将源代码与服务做捆绑或任何变相捆绑销售。</p><h2 id="2-LGPL（GNU-Lesser-General-Public-License）"><a href="#2-LGPL（GNU-Lesser-General-Public-License）" class="headerlink" title="2. LGPL（GNU Lesser General Public License）"></a>2. LGPL（GNU Lesser General Public License）</h2><p>GNU宽通用公共许可证 (GNU LGPL)，其最新版本号是3。它对产品所保留的权利比 GPL 少。<br>GNU 宽通用公共许可证文本有以下格式：HTML、纯文本、Docbook、Texinfo、Markdown、ODF和RTF。这些文档的格式不是为了单独发布使用，它们的目的是嵌入其他文档。</p><p>LGPL提供的权利：自由复制、自由分发、自由修改、盈利。<br>与GPL大致相同，但条件上有差异：</p><ol><li>GNU不建议使用LGPL，因为LGPL许可证允许专有软件使用该函数库；而普通的GNU GPL只允许在自由软件中使用该函数库。</li><li>LGPL不要求其它使用LGPL授权代码的软件以LGPL方式发布。</li><li>LGPL软件可以被转换成GPL，这种特性对于在GPL库或应用程序中直接使用LGPL程序有一定程度的帮助。</li></ol><p>总结：<br>使用普通GPL的函数库给予自由软件开发者一个超越专有软件开发者的优势：能够使用按照普通GPL发布的函数库，而专有软件的开发者不能使用这些库。<br>使用LGPL的常见情况是当专有软件可以通过其他函数库来实现使用自由软件函数库的功能时，该函数库便对自由软件没有任何独有优势，此时采用LGPL。</p><h2 id="3-BSD-Berkeley-Software-Distribution"><a href="#3-BSD-Berkeley-Software-Distribution" class="headerlink" title="3. BSD (Berkeley Software Distribution)"></a>3. BSD (Berkeley Software Distribution)</h2><p>BSD，即伯克利软件分发许可协议。BSD协议的出发点是鼓励代码重用，可以将公开的资源纳入私人软件并使用闭源的形式进行出售，但是需要尊重代码作者的著作权。通常有BSD 3-Clause （又称”New BSD License”或”Modified BSD License”）, BSD 2-Clause（又称”Simplified BSD License”或”FreeBSD License”。<br>新 BSD 协议在软件分发方面，除需要包含一份版权提示和免责声明之外，没有任何限制。另外，该协议还禁止拿开发者的名义为衍生产品背书，但简单 BSD 协议删除了这一条款。</p><p>BSD提供的权利：自由复制、自由分发、自由修改、盈利。<br>注意：</p><ol><li>BSD不允许专利使用。</li><li>如果再发布的产品中包含源代码，则在源代码中必须带有原来代码中的 BSD 协议。如果再发布的只是二进制类库 / 软件，则需要在类库 / 软件的文档和版权声明中包含原来代码中的 BSD 协议。</li><li>BSD 3-Clause不可以用开源代码的作者 / 机构名字和原来产品的名字做市场推广。</li></ol><p>总结：<br>BSD 代码鼓励代码共享，但需要尊重代码作者的著作权。BSD由于允许使用者修改和重新发布代码，也允许使用或在 BSD 代码上开发商业软件发布和销售，因此是对商业集成很友好的协议。很多的公司企业在选用开源产品的时候都首选 BSD 协议，因为可以完全控制这些第三方的代码，在必要的时候可以修改或者二次开发。</p><h2 id="4-MIT-（Massachusetts-Institute-of-Technology）"><a href="#4-MIT-（Massachusetts-Institute-of-Technology）" class="headerlink" title="4. MIT （Massachusetts Institute of Technology）"></a>4. MIT （Massachusetts Institute of Technology）</h2><p>MIT 许可证之名源自麻省理工学院（Massachusetts Institute of Technology, MIT），又称「X 条款」（X License）或「X11 条款」（X11 License）<br>MIT 内容与三条款 BSD 许可证（3-clause BSD license）内容颇为近似，但是赋予软体被授权人更大的权利与更少的限制。<br>MIT 条款可与其他授权条款并存。另外，MIT 条款也是自由软体基金会（FSF）所认可的自由软体授权条款，与 GPL 相容。</p><p>MIT提供的权利：自由复制、自由分发、自由修改、盈利。<br>注意：</p><ol><li>该软件及其相关文档对所有人免费，可以任意处置，包括使用，复制，修改，合并，发表，分发，再授权，或者销售。唯一的限制是，软件中必须包含上述版 权和许可提示。</li><li>此授权条款并非属 copyleft 的自由软体授权条款，允许在自由 / 开放源码软件或非自由软件（proprietary software）所使用。</li><li>不允许专利使用。</li></ol><p>总结：<br>MIT 协议是所有开源许可中最宽松的一个，除了必须包含许可声明外，再无任何限制。</p><h2 id="5-Apache-License-2-0"><a href="#5-Apache-License-2-0" class="headerlink" title="5. Apache License 2.0"></a>5. Apache License 2.0</h2><p>Apache License 2.0是对商业应用友好的许可。使用者也可以在需要的时候修改代码来满足需要并作为开源或商业产品发布/销售。</p><p>Apache License 2.0提供的权利：自由复制、自由分发、自由修改、盈利、专利使用。<br>注意：</p><ol><li>一旦被授权，永久拥有。</li><li>在一个国家获得授权，适用于所有国家。</li><li>授权免费，且无版税。</li><li>任何人都可以获得授权。</li><li>一旦获得授权，没有任何人可以取消。比如，你基于该产品代码开发了衍生产品，你不用担心会在某一天被禁止使用该代码。</li><li>分发代码方面包含一些要求，主要是，要在声明中对参与开发的人给予认可并包含一份许可协议原文。</li><li>如果修改了代码，需要在被修改的文件中说明。并且在延伸的代码中（修改和有源代码衍生的代码中）需要带有原来代码中的协议，商标，专利声明和其他原来作者规定需要包含的说明。</li><li>如果再发布的产品中包含一个 Notice 文件，则在 Notice 文件中需要带有 Apache Licence。你可以在 Notice 中增加自己的许可，但不可以表现为对 Apache Licence 构成更改。</li></ol><p>总结：<br>Apache 协议 2.0 和除了为用户提供版权许可之外，还有专利许可，对于那些涉及专利内容的开发者而言，该协议最适合。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近看到B站有人用pyqt5被发律师函，于是对开源协议产生了一些兴趣，对网络相关热门帖子结合协议官网整理归纳一下，简单了解一下开源协议，如有错误请评论指正，感谢。&lt;br&gt;提及的协议有：GPL、LGPL、BSD、MIT、Apache License 2.0。&lt;br&gt;</summary>
    
    
    
    
    <category term="开源协议" scheme="http://silencezheng.top/tags/%E5%BC%80%E6%BA%90%E5%8D%8F%E8%AE%AE/"/>
    
  </entry>
  
  <entry>
    <title>Spring boot学习笔记0x01</title>
    <link href="http://silencezheng.top/2022/05/10/article35/"/>
    <id>http://silencezheng.top/2022/05/10/article35/</id>
    <published>2022-05-10T04:16:39.000Z</published>
    <updated>2022-05-27T14:15:07.615Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-前言："><a href="#1-前言：" class="headerlink" title="1. 前言："></a>1. 前言：</h2><p>继续学习Spring boot～<br><span id="more"></span></p><h2 id="2-概念"><a href="#2-概念" class="headerlink" title="2. 概念"></a>2. 概念</h2><p>对Spring boot的宏观理解，它的诞生是为了简化 Spring 应用的搭建和开发过程，具有 Spring 一切优秀特性，而且使用更加简单，功能更加丰富，性能更加稳定而健壮。在Spring boot中我们使用基于注解和JAVA的配置方式，抛弃XML！</p><p>学习Spring boot最好还是对Spring、Servlet有基本的理解。</p><h3 id="2-1-三层架构"><a href="#2-1-三层架构" class="headerlink" title="2.1. 三层架构"></a>2.1. 三层架构</h3><p>注意区别MVC架构即可，MVC是表现层（Web层）的一个设计模式，真正意义的三层架构如下：</p><ul><li>A 表现层—Web层</li><li>B 业务层—Service层</li><li>C 持久层—Dao层</li></ul><h3 id="2-2-控制反转—IoC"><a href="#2-2-控制反转—IoC" class="headerlink" title="2.2. 控制反转—IoC"></a>2.2. 控制反转—IoC</h3><p>IoC——Inversion of Control，指的是将对象的创建权交给 Spring 去创建。使用 Spring 之前，对象的创建都是由我们自己在代码中new创建。而使用 Spring 之后。对象的创建都是给了 Spring 框架。控制反转可以用许多方式表达，依赖注入是其中一种方式。</p><h3 id="2-3-依赖注入—DI"><a href="#2-3-依赖注入—DI" class="headerlink" title="2.3. 依赖注入—DI"></a>2.3. 依赖注入—DI</h3><p>Dependency Injection，当某个角色(可能是一个Java实例，调用者)需要另一个角色(另一个Java实例，被调用者)的协助时，在传统的程序设计过程中，通常由调用者来创建被调用者的实例。但在Spring里，创建被调用者的工作不再由调用者来完成，因此称为控制反转;创建被调用者实例的工作通常由Spring容器来完成，然后注入调用者。</p><p>Spring框架的核心功能之一就是通过依赖注入的方式来管理Bean之间的依赖关系。DI 主要有两种变体，即通过构造函数参数形式和通过setter方法形式：</p><ol><li>Constructor-based dependency injection，当容器调用带有多个参数的构造函数类时，实现基于构造函数的 DI，每个代表在其他类中的一个依赖关系。</li><li>Setter-based dependency injection，基于 setter 方法的 DI 是通过在调用无参数的构造函数或无参数的静态工厂方法实例化 bean 之后容器调用 beans 的 setter 方法来实现的。</li></ol><h3 id="2-4-面向切面编程—AOP"><a href="#2-4-面向切面编程—AOP" class="headerlink" title="2.4.  面向切面编程—AOP"></a>2.4.  面向切面编程—AOP</h3><p>先理解什么是切面。用刀把一个西瓜分成两瓣，切开的切口就是切面；炒菜，锅与炉子共同来完成炒菜，锅与炉子就是切面。web层级设计中，web层->网关层->服务层->数据层，每一层之间也是一个切面。编程中，对象与对象之间，方法与方法之间，模块与模块之间都是一个个切面。</p><p>其他的先忽略，需要仔细研究。</p><p>Spring 框架的 AOP 模块提供了面向方面的程序设计实现，可以定义诸如方法拦截器和切入点等，从而使实现功能的代码彻底的解耦出来。</p><h3 id="2-5-容器—Container"><a href="#2-5-容器—Container" class="headerlink" title="2.5.  容器—Container"></a>2.5.  容器—Container</h3><p>Spring 是一个容器，因为它包含并且管理应用对象的生命周期。Spring 容器是 Spring 框架的核心。容器将创建对象，把它们连接在一起，配置它们，并管理他们的整个生命周期从创建到销毁。Spring 容器使用依赖注入（DI）来管理组成一个应用程序的组件。这些对象被称为 Spring Beans。在Spring中BeanFactory是IOC容器的实际代表者。</p><h3 id="2-6-“对象”—Bean"><a href="#2-6-“对象”—Bean" class="headerlink" title="2.6.  “对象”—Bean"></a>2.6.  “对象”—Bean</h3><p>Bean是一个被实例化，组装，并通过 Spring IoC 容器所管理的对象。这些 bean 是由用容器提供的配置元数据创建的。把配置元数据提供给 Spring 容器有基于XML、注解和JAVA三种方式。Bean的重要属性有作用域（scope）、初始化与销毁和与依赖注入相关的constructor-arg、properties、autowiring mode等。</p><h3 id="2-7-自动装配—Autowire"><a href="#2-7-自动装配—Autowire" class="headerlink" title="2.7.  自动装配—Autowire"></a>2.7.  自动装配—Autowire</h3><p>Spring 容器可以在不使用<code>&lt;constructor-arg&gt;</code>和<code>&lt;property&gt;</code>元素的情况下<strong>自动装配</strong>相互协作的 bean 之间的关系。Spring中可以使用<code>&lt;bean&gt;</code>元素的<strong>autowire</strong>属性为一个 bean 定义指定自动装配模式，选择由属性名或由属性数据类型自动装配。</p><p>以由属性名自动装配举例，在 XML 配置文件中 beans 的<em>auto-wire</em>属性设置为<em>byName</em>。然后尝试将它的属性与配置文件中定义为相同名称的 beans 进行匹配和连接。如果找到匹配项，它将注入这些 beans，否则，它将抛出异常。总的来说，如果在一个类A中使用了另一个类B作为属性，若不想显式的对B进行绑定，就可以使用自动装配。</p><p>Spring采用基于注解的方式配置时，@Autowired是最常用的注解之一，这个注解的功能就是为我们注入一个定义好的 bean。</p><h3 id="2-8-Spring-boot-starter"><a href="#2-8-Spring-boot-starter" class="headerlink" title="2.8. Spring boot starter"></a>2.8. Spring boot starter</h3><p>Spring Boot 将日常企业应用研发中的各种场景都抽取出来，做成一个个的 starter（启动器），starter 中整合了该场景下各种可能用到的依赖，用户只需要在 Maven 中引入 starter 依赖，SpringBoot 就能自动扫描到要加载的信息并启动相应的默认配置。starter 提供了大量的自动配置，让用户摆脱了处理各种依赖和配置的困扰。所有这些 starter 都遵循着约定成俗的默认配置，并允许用户调整这些配置，即遵循“约定大于配置”的原则。</p><p>以 spring-boot-starter-web 为例，它能够为提供 Web 开发场景所需要的几乎所有依赖，因此在使用 Spring Boot 开发 Web 项目时，只需要引入该 Starter 即可，而不需要额外导入 Web 服务器和其他的 Web 依赖。</p><p>spring-boot-starter-parent 是所有 Spring Boot 项目的父级依赖，它被称为 Spring Boot 的版本仲裁中心，可以对项目内的部分常用依赖进行统一管理。Spring Boot 项目可以通过继承 spring-boot-starter-parent 来获得一些合理的默认配置如默认 JDK 版本、默认字符集、依赖管理功能、资源过滤、默认插件配置、识别 application.properties（或yml）类型的配置文件。</p><h3 id="2-9-配置文件—Spring-boot-profile"><a href="#2-9-配置文件—Spring-boot-profile" class="headerlink" title="2.9. 配置文件—Spring boot profile"></a>2.9. 配置文件—Spring boot profile</h3><p>在实际的项目开发中，一个项目通常会存在多个环境，例如，开发环境、测试环境和生产环境等，不同环境的配置也不尽相同。</p><p>Spring Boot 的配置文件共有两种形式：.properties  文件和 .yml 文件，不管哪种形式，它们都能通过文件名的命名形式区分出不同的环境的配置，文件命名格式为：</p><p>application-{profile}.properties/yml</p><p>其中，{profile} 一般为各个环境的名称或简称，例如 dev、test 和 prod 等等。</p><h4 id="2-9-1-默认配置文件"><a href="#2-9-1-默认配置文件" class="headerlink" title="2.9.1. 默认配置文件"></a>2.9.1. 默认配置文件</h4><p>通常情况下，Spring Boot 在启动时会将 resources 目录下的 application.properties 或 apllication.yml 作为其默认配置文件，我们可以在该配置文件中对项目进行配置，但这并不意味着 Spring Boot 项目中只能存在一个 application.properties 或 application.yml。</p><h4 id="2-9-2-外部配置文件"><a href="#2-9-2-外部配置文件" class="headerlink" title="2.9.2. 外部配置文件"></a>2.9.2. 外部配置文件</h4><p>除了默认配置文件，Spring Boot 还可以加载一些位于项目外部的配置文件。我们可以通过如下 2 个参数，指定外部配置文件的路径：</p><ul><li>spring.config.location</li><li>spring.config.additional-location</li></ul><p>我们可以先将 Spring Boot 项目打包成 JAR 文件，然后在命令行启动命令中，使用命令行参数 —spring.config.location，指定外部配置文件的路径。</p><p><code>java -jar &#123;JAR&#125;  --spring.config.location=&#123;外部配置文件全路径&#125;</code></p><p>需要注意的是，使用该参数指定配置文件后，会使项目默认配置文件（application.properties 或 application.yml ）失效，Spring Boot 将只加载指定的外部配置文件。</p><h3 id="2-10-Spring-boot-配置"><a href="#2-10-Spring-boot-配置" class="headerlink" title="2.10. Spring boot 配置"></a>2.10. Spring boot 配置</h3><p>Spring Boot 不仅可以通过配置文件进行配置，还可以通过环境变量、命令行参数等多种形式进行配置。这些配置都可以让开发人员在不修改任何代码的前提下，直接将一套 Spring Boot  应用程序在不同的环境中运行。</p><h4 id="2-10-1-配置加载顺序"><a href="#2-10-1-配置加载顺序" class="headerlink" title="2.10.1. 配置加载顺序"></a>2.10.1. 配置加载顺序</h4><ol><li>命令行参数</li><li>来自 java:comp/env 的 JNDI 属性</li><li>Java 系统属性（System.getProperties()）</li><li>操作系统环境变量</li><li>RandomValuePropertySource 配置的 random.* 属性值</li><li>配置文件（YAML 文件、Properties 文件）</li><li>@Configuration 注解类上的 @PropertySource 指定的配置文件</li><li>通过 SpringApplication.setDefaultProperties 指定的默认属性</li></ol><p>以上所有形式的配置都会被加载，当存在相同配置内容时，高优先级的配置会覆盖低优先级的配置；存在不同的配置内容时，高优先级和低优先级的配置内容取并集，共同生效，形成互补配置。同一位置下，Properties 文件优先级高于 YAML 文件。</p><h4 id="2-10-2-自动配置"><a href="#2-10-2-自动配置" class="headerlink" title="2.10.2. 自动配置"></a>2.10.2. 自动配置</h4><p>Spring Boot 的<strong>自动配置</strong>是基于 Spring Factories 机制实现的。</p><p>Spring Factories 机制是 Spring Boot 中的一种服务发现机制，这种扩展机制与 Java SPI 机制十分相似。Spring Boot 会自动扫描所有 Jar 包类路径下 META-INF/spring.factories 文件，并读取其中的内容，进行实例化，这种机制也是 Spring Boot Starter 的基础。具体来说，spring-core 包里定义了 SpringFactoriesLoader 类，这个类会扫描所有 Jar 包类路径下的 META-INF/spring.factories 文件，并获取指定接口的配置。</p><p>spring.factories 文件本质上与 properties 文件相似，其中包含一组或多组键值对（key=vlaue），其中，key 的取值为接口的完全限定名；value 的取值为接口实现类的完全限定名，一个接口可以设置多个实现类，不同实现类之间使用“，”隔开。</p><p>基于以上，Spring boot的<strong>自动配置</strong>也是通过同样方式实现的，在 spring-boot-autoconfigure-xxx.jar 类路径下的 META-INF/spring.factories 中设置了 Spring Boot 自动配置的内容。</p><h3 id="2-11-拦截器"><a href="#2-11-拦截器" class="headerlink" title="2.11. 拦截器"></a>2.11. 拦截器</h3><p>拦截器可以根据 URL 对请求进行拦截，主要应用于登陆校验、权限验证、乱码解决、性能监控和异常处理等功能上。</p><p>在 Spring Boot 项目中，使用拦截器功能通常需要以下 3 步：</p><ol><li>定义拦截器；</li><li>注册拦截器；</li><li>指定拦截规则（如果是拦截所有，静态资源也会被拦截）。</li></ol><h4 id="2-11-1-定义拦截器"><a href="#2-11-1-定义拦截器" class="headerlink" title="2.11.1. 定义拦截器"></a>2.11.1. 定义拦截器</h4><p>只需要创建一个拦截器类，并实现 HandlerInterceptor 接口即可，该接口中定义以下 3 个方法（按需重写即可）：</p><ol><li>boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler)，该方法在控制器处理请求方法前执行，其返回值表示是否中断后续操作，返回 true 表示继续向下执行，返回 false 表示中断后续操作。</li><li>void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView)，该方法在控制器处理请求方法调用之后、解析视图之前执行，可以通过此方法对请求域中的模型和视图做进一步修改。</li><li>void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex)，该方法在视图渲染结束后执行，可以通过此方法实现资源清理、记录日志信息等工作。</li></ol><h4 id="2-11-2-注册拦截器"><a href="#2-11-2-注册拦截器" class="headerlink" title="2.11.2. 注册拦截器"></a>2.11.2. 注册拦截器</h4><p>创建一个实现了 WebMvcConfigurer 接口的配置类（使用了 @Configuration 注解的类），重写 addInterceptors() 方法，并在该方法中调用 registry.addInterceptor() 方法将自定义的拦截器注册到容器中。示例：</p><pre><code>@ConfigurationpublicclassMyMvcConfigimplementsWebMvcConfigurer&#123;    ......    @Override    public void addInterceptors(InterceptorRegistryregistry)&#123;      registry.addInterceptor(newLoginInterceptor());    &#125;&#125;</code></pre><h4 id="2-11-3-指定拦截规则"><a href="#2-11-3-指定拦截规则" class="headerlink" title="2.11.3. 指定拦截规则"></a>2.11.3. 指定拦截规则</h4><p>在指定拦截器拦截规则时，可以调用两个方法，说明如下：</p><ul><li>addPathPatterns：该方法用于指定拦截路径，例如拦截路径为<code>/**</code>，表示拦截所有请求，包括对静态资源的请求。</li><li>excludePathPatterns：该方法用于排除拦截路径，即指定不需要被拦截器拦截的请求。</li></ul><p>示例：</p><p><code>registry.addInterceptor(newLoginInterceptor()).addPathPatterns(&quot;/**&quot;)</code></p><p>可以链式调用。</p><h2 id="3-常见注解介绍"><a href="#3-常见注解介绍" class="headerlink" title="3. 常见注解介绍"></a>3. 常见注解介绍</h2><h3 id="3-1-Autowired"><a href="#3-1-Autowired" class="headerlink" title="3.1. @Autowired"></a>3.1. @Autowired</h3><p>这个注解是属于 Spring 的容器配置的一个注解，与它同属容器配置的注解还有：@Required,@Primary, @Qualifier 等等。</p><p>在 Spring 的世界当中，自动装配指的就是使用将 Spring 容器中的 bean 自动的和我们需要这个 bean 的类组装在一起，注入一个定义好的 bean。</p><p>用法：</p><ol><li>应用于字段</li><li>应用于构造函数</li><li>应用于 setter 方法</li><li>应用于具有任意名称和多个参数的方法</li><li>添加到需要该类型数组（或容器）的字段或方法，则 Spring 会从 ApplicationContext 中搜寻符合指定类型的所有 bean</li></ol><h3 id="3-2-RestController"><a href="#3-2-RestController" class="headerlink" title="3.2. @RestController"></a>3.2. @RestController</h3><p>用于标注控制层组件。在 Spring Boot 中，@Controller 注解是专门用于处理 Http 请求处理的，是以 MVC 为核心的设计思想的控制层。@RestController 则是 @Controller 的衍生注解，都是用来表示Spring某个类的是否可以接收HTTP请求。</p><p>@RestController是@Controller和@ResponseBody的结合体，两个标注合并起来的作用。@Controller类中的方法可以直接通过返回String跳转到jsp、ftl、html等模版页面。在方法上加@ResponseBody注解，也可以返回实体对象。@RestController类中的所有方法只能返回String、Object、Json等实体对象，不能跳转到模版页面。</p><p>用法（感觉有点问题）：</p><p>@Controller: 一般应用在有返回界面的应用场景下.例如，管理后台使用了 thymeleaf 作为模板开发，需要从后台直接返回 Model 对象到前台，那么这时候就需要使用 @Controller 来注解。</p><p>@RestController: 如果只是接口，那么就用 RestController 来注解.如前端页面全部使用了 Html、Jquery来开发，通过 Ajax 请求服务端接口，那么接口就使用 @RestController 统一注解。</p><h3 id="3-3-…Mapping"><a href="#3-3-…Mapping" class="headerlink" title="3.3. @…Mapping"></a>3.3. @…Mapping</h3><p>表示路由请求，可以设置各种操作方法。最基本的是@RequestMapping，@GetMapping、@PostMapping、@PutMapping、@DeleteMapping 是 @RequestMapping 的子集，分别表示用不同的请求方式的对应路由。</p><p>举例：</p><p><code>@RequestMapping(value=&quot;/add&quot;,method = RequestMethod.POST),params=&quot;myParam=xyz&quot;</code> 表示路由为 /add 的 POST 请求，但仅仅处理头部包括 myParam=xyz 的请求。</p><p><code>@GetMapping(&quot;/add&quot;)等价于@RequestMapping(method = RequestMethod.GET,value = &quot;/add&quot;)</code></p><h3 id="3-4-Configuration-和-Bean"><a href="#3-4-Configuration-和-Bean" class="headerlink" title="3.4. @Configuration 和 @Bean"></a>3.4. @Configuration 和 @Bean</h3><p>带有<strong>@Configuration</strong>的注解类表示这个类可以使用 Spring IoC 容器作为 bean 定义的来源。<strong>@Bean</strong>注解告诉 Spring，一个带有 @Bean 的注解方法将返回一个对象，该对象应该被注册为在 Spring 应用程序上下文中的 bean。例如：</p><pre><code>@Configurationpublic class HelloWorldConfig&#123;    @Bean     public HelloWorld helloWorld()&#123;        return new HelloWorld();       &#125;&#125;</code></pre><h3 id="3-5-SpringBootApplication"><a href="#3-5-SpringBootApplication" class="headerlink" title="3.5. @SpringBootApplication"></a>3.5. @SpringBootApplication</h3><p>所有 Spring Boot 项目的主启动程序类上都使用了一个 @SpringBootApplication 注解，该注解是 Spring Boot 中最重要的注解之一 ，也是 Spring Boot 实现自动化配置的关键。</p><p>@SpringBootApplication 是一个组合元注解，其主要包含两个注解：<strong>@SpringBootConfiguration</strong> 和 <strong>@EnableAutoConfiguration</strong>，其中 @EnableAutoConfiguration 注解是 SpringBoot 自动化配置的核心所在。</p><h4 id="3-5-1-EnableAutoConfiguration"><a href="#3-5-1-EnableAutoConfiguration" class="headerlink" title="3.5.1. @EnableAutoConfiguration"></a>3.5.1. @EnableAutoConfiguration</h4><p>@EnableAutoConfiguration 注解用于开启 Spring Boot 的自动配置功能， 它使用 Spring 框架提供的 @Import 注解通过 AutoConfigurationImportSelector类（选择器）给容器中导入自动配置组件。</p><h4 id="3-5-2-SpringBootConfiguration"><a href="#3-5-2-SpringBootConfiguration" class="headerlink" title="3.5.2. @SpringBootConfiguration"></a>3.5.2. @SpringBootConfiguration</h4><p>@SpringBootConfiguration继承自@Configuration，二者功能也一致，标注当前类是配置类，并会将当前类内声明的一个或多个以@Bean注解标记的方法的实例纳入到spring容器中，并且实例名就是方法名。</p><h3 id="3-6-RequestBody"><a href="#3-6-RequestBody" class="headerlink" title="3.6. @RequestBody"></a>3.6. @RequestBody</h3><p>@RequestBody主要用来接收前端传递给后端的json字符串中的数据(请求体中的数据)；而最常用的使用请求体传参的无疑是POST请求了，所以使用@RequestBody接收数据时，一般都用POST方式进行提交。<br>在后端的同一个接收方法里，@RequestBody与@RequestParam()可以同时使用，@RequestBody最多只能有一个，而@RequestParam()可以有多个。</p><h3 id="3-7-Service"><a href="#3-7-Service" class="headerlink" title="3.7. @Service"></a>3.7. @Service</h3><p>对于业务层（service层）的类，在类上用 @Service 注解声明，表示是业务层组件。SpringBoot会将标注类自动注册到 Spring 容器中，可以通过指定value参数更改名称。<br>SpringBoot的企业级开发中经常采用Service+ServiceImpl的结构，一开始大多数项目都是直接在业务处理层的Service类中嵌入JDBC代码，这就使得这个Service类与数据库紧耦合，在换一种数据库后，就要修改Service类中的sql。于是就有了Controller+Service+ServiceImpl，Service类设计成一个接口，使控制层只依赖这个接口，这样，当某天这个应用要跑在其它数据库上时，就而只需要增加一个serviceImpl类。</p><h3 id="3-8-CrossOrigin"><a href="#3-8-CrossOrigin" class="headerlink" title="3.8. @CrossOrigin"></a>3.8. @CrossOrigin</h3><p>该注解用于解决跨域问题，可以有以下使用方法：</p><ol><li>对@Controller中的方法使用</li><li>对@Controller使用</li><li>同时对Controller和其中的方法使用</li></ol><p>@CrossOrigin中的2个参数：</p><ul><li>origins： 允许可访问的域列表</li><li>maxAge:准备响应前的缓存持续的最大时间（以秒为单位）</li></ul><p>除了这种细粒度的注解配置方式外，SpringBoot还提供全局配置的方式。<br><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">@Configuration</span><br><span class="line">public <span class="keyword">class</span> CorsConfig &#123;</span><br><span class="line">    @Bean</span><br><span class="line">    public CorsFilter cors<span class="constructor">Filter()</span> &#123;</span><br><span class="line">        UrlBasedCorsConfigurationSource source = <span class="keyword">new</span> <span class="constructor">UrlBasedCorsConfigurationSource()</span>;</span><br><span class="line">        CorsConfiguration corsConfiguration = <span class="keyword">new</span> <span class="constructor">CorsConfiguration()</span>;</span><br><span class="line">        <span class="comment">//允许所有源</span></span><br><span class="line">        corsConfiguration.add<span class="constructor">AllowedOrigin(<span class="string">&quot;*&quot;</span>)</span>;</span><br><span class="line">        <span class="comment">//允许所有请求头</span></span><br><span class="line">        corsConfiguration.add<span class="constructor">AllowedHeader(<span class="string">&quot;*&quot;</span>)</span>;</span><br><span class="line">        <span class="comment">//允许所有方法</span></span><br><span class="line">        corsConfiguration.add<span class="constructor">AllowedMethod(<span class="string">&quot;*&quot;</span>)</span>;</span><br><span class="line">        <span class="comment">//允许跨域cookies</span></span><br><span class="line">        corsConfiguration.set<span class="constructor">AllowCredentials(<span class="params">true</span>)</span>;</span><br><span class="line">        source.register<span class="constructor">CorsConfiguration(<span class="string">&quot;/**&quot;</span>, <span class="params">corsConfiguration</span>)</span>;</span><br><span class="line">        return <span class="keyword">new</span> <span class="constructor">CorsFilter(<span class="params">source</span>)</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-前言：&quot;&gt;&lt;a href=&quot;#1-前言：&quot; class=&quot;headerlink&quot; title=&quot;1. 前言：&quot;&gt;&lt;/a&gt;1. 前言：&lt;/h2&gt;&lt;p&gt;继续学习Spring boot～&lt;br&gt;</summary>
    
    
    
    
    <category term="Java" scheme="http://silencezheng.top/tags/Java/"/>
    
    <category term="Spring" scheme="http://silencezheng.top/tags/Spring/"/>
    
  </entry>
  
  <entry>
    <title>数据操作及数学基础--《动手学深度学习》笔记0x02</title>
    <link href="http://silencezheng.top/2022/05/05/article34/"/>
    <id>http://silencezheng.top/2022/05/05/article34/</id>
    <published>2022-05-04T16:49:29.000Z</published>
    <updated>2022-05-27T14:15:16.255Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><p>这一笔记对应预备知识章节，包括数据操作、数据预处理、线性代数、微积分、概率论等。其中很多知识在学数一的时候都更深入理解过了，但是现在发现忘的差不多了，哎～</p><p>对应实践：<a href="https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x02.ipynb">https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x02.ipynb</a></p><p>在M1芯片的设备上使用miniforge安装pytorch：<code>conda install -c pytorch pytorch</code><br><span id="more"></span></p><h2 id="1-数据操作"><a href="#1-数据操作" class="headerlink" title="1. 数据操作"></a>1. 数据操作</h2><h3 id="1-1-概念"><a href="#1-1-概念" class="headerlink" title="1.1. 概念"></a>1.1. 概念</h3><h4 id="1-1-1-张量（tensor）"><a href="#1-1-1-张量（tensor）" class="headerlink" title="1.1.1 张量（tensor）"></a>1.1.1 张量（tensor）</h4><p>即n维数组，无论使用哪个深度学习框架，它的<em>张量类</em>（在MXNet中为<code>ndarray</code>， 在PyTorch和TensorFlow中为<code>Tensor</code>）都与Numpy的<code>ndarray</code>类似。 但深度学习框架又比Numpy的<code>ndarray</code>多一些重要功能： 首先，GPU很好地支持加速计算，而NumPy仅支持CPU计算； 其次，张量类支持自动微分。 这些功能使得张量类更适合深度学习。</p><p>张量表示由一个数值组成的数组，这个数组可能有多个维度。 具有一个轴的张量对应数学上的<em>向量</em>（vector）； 具有两个轴的张量对应数学上的<em>矩阵</em>（matrix）； 具有两个轴以上的张量没有特殊的数学名称。张量中的每个值都称为张量的<em>元素</em>（element）。</p><h4 id="1-1-2-运算符"><a href="#1-1-2-运算符" class="headerlink" title="1.1.2. 运算符"></a>1.1.2. 运算符</h4><p>我们的兴趣不仅限于读取数据和写入数据。 我们想在这些数据上执行数学运算，其中最简单且最有用的操作是<em>按元素</em>（elementwise）运算。 它们将标准标量运算符应用于数组的<strong>每个元素</strong>。 对于将两个数组作为输入的函数，按元素运算将二元运算符应用于两个数组中的<strong>每对位置对应的元素</strong>。 我们可以基于任何从标量到标量的函数来创建按元素函数。</p><p>对于任意具有相同形状的张量， 常见的标准算术运算符（<code>+</code>、<code>-</code>、<code>*</code>、<code>/</code>和<code>**</code>）都可以被升级为按元素运算。 我们可以在同一形状的任意两个张量上调用按元素操作。</p><h4 id="1-1-3-广播机制"><a href="#1-1-3-广播机制" class="headerlink" title="1.1.3. 广播机制"></a>1.1.3. 广播机制</h4><p>在上面的部分中，我们看到了如何在相同形状的两个张量上执行按元素操作。 在某些情况下，即使形状不同，我们仍然可以通过调用<em>广播机制</em>（broadcasting mechanism）来执行按元素操作。 这种机制的工作方式如下：首先，通过适当复制元素来扩展一个或两个数组， 以便在转换之后，两个张量具有相同的形状。 其次，对生成的数组执行按元素操作。</p><h4 id="1-1-4-索引和切片"><a href="#1-1-4-索引和切片" class="headerlink" title="1.1.4. 索引和切片"></a>1.1.4. 索引和切片</h4><p>就像在任何其他Python数组中一样，张量中的元素可以通过索引访问。 与任何Python数组一样：第一个元素的索引是0，最后一个元素索引是-1； 可以指定范围以包含第一个元素和最后一个之前的元素。例如：我们可以用[-1]选择最后一个元素，可以用[1:3]选择第二个和第三个元素。</p><h4 id="1-1-5-内存变动"><a href="#1-1-5-内存变动" class="headerlink" title="1.1.5. 内存变动"></a>1.1.5. 内存变动</h4><p>运行一些操作可能会导致为新结果分配内存。 例如，如果我们用Y=X+Y，我们将取消引用Y指向的张量，而是指向新分配的内存处的张量。Python的id()函数给我们提供了内存中引用对象的确切地址。 运行Y=Y+X后，我们会发现id(Y)指向另一个位置。 这是因为Python首先计算Y+X，为结果分配新的内存，然后使Y指向内存中的这个新位置。</p><p>我们不希望内存在不必要时发生重新分配的情况，原因有两个：首先，我们不想总是不必要地分配内存。 在机器学习中，我们可能有数百兆的参数，并且在一秒内多次更新所有参数。 通常情况下，我们希望原地执行这些更新。 其次，如果我们不原地更新，其他引用仍然会指向旧的内存位置， 这样我们的某些代码可能会无意中引用旧的参数。</p><p>在这种情况下，我们可以使用切片表示法将操作的结果分配给先前分配的数组，例如<code>Y[:]=&lt;expression&gt;</code>。</p><p>在节省内存开销方面，如果在后续计算中没有重复使用X， 我们也可以使用<code>X[:]=X+Y</code>或<code>X+=Y</code>来减少操作的内存开销。</p><h4 id="1-1-6-对象转换"><a href="#1-1-6-对象转换" class="headerlink" title="1.1.6. 对象转换"></a>1.1.6. 对象转换</h4><p>将深度学习框架定义的张量转换为NumPy张量（ndarray）很容易，反之也同样容易。 注意torch张量和numpy数组将共享它们的底层内存，就地操作更改一个张量也会同时更改另一个张量。</p><p>要将大小为1的张量转换为Python标量，可以调用item函数或Python的内置函数。</p><h3 id="1-2-以PyTorch为例，列举常见操作"><a href="#1-2-以PyTorch为例，列举常见操作" class="headerlink" title="1.2. 以PyTorch为例，列举常见操作"></a>1.2. 以PyTorch为例，列举常见操作</h3><p>虽然它被称为PyTorch，但是代码中使用torch而不是pytorch。</p><p><code>import torch</code></p><p>首先，我们可以使用arange创建一个行向量x。这个行向量包含以0开始的前12个整数，它们默认创建为整数。也可指定创建类型为浮点数。例如，张量x中有 12 个元素。除非额外指定，新的张量将存储在内存中，并采用基于CPU的计算。</p><p><code>x=torch.arange(12)</code></p><p>可以通过张量的shape属性来访问张量（沿每个轴的长度）的<em>形状</em>。</p><p><code>x.shape</code></p><p>如果只想知道张量中元素的总数，即形状的所有元素乘积，可以检查它的大小（size）。 因为这里在处理的是一个向量，所以它的shape与它的size相同。</p><p><code>x.numel()</code></p><p>要想改变一个张量的形状而不改变元素数量和元素值，可以调用reshape函数。 例如，可以把张量x从形状为（12,）的行向量转换为形状为（3,4）的矩阵。 这个新的张量包含与转换前相同的值，但是它被看成一个3行4列的矩阵。 要重点说明一下，虽然张量的形状发生了改变，但其元素值并没有变。 注意，通过改变张量的形状，张量的大小（size）不会改变。我们可以通过-1来调用此自动计算出维度的功能。 即我们可以用<code>x.reshape(-1,4)</code>或<code>x.reshape(3,-1)</code>来取代<code>x.reshape(3,4)</code>。</p><p><code>X=x.reshape(3,4)</code></p><p>有时，我们希望使用全0、全1、其他常量，或者从特定分布中随机采样的数字来初始化矩阵。 我们可以创建一个形状为（2,3,4）的张量，其中所有元素都设置为0。<code>torch.ones((2,3,4))</code>则创建元素全为1的张量，但默认数据类型为float。</p><pre><code>torch.zeros((2,3,4))#结果：tensor([[[0.,0.,0.,0.],         [0.,0.,0.,0.],         [0.,0.,0.,0.]],        [[0.,0.,0.,0.],         [0.,0.,0.,0.],         [0.,0.,0.,0.]]])</code></pre><p>有时我们想通过从某个特定的概率分布中随机采样来得到张量中每个元素的值。 例如，当我们构造数组来作为神经网络中的参数时，我们通常会随机初始化参数的值。 以下代码创建一个形状为（3,4）的张量。 其中的每个元素都从均值为0、标准差为1的标准高斯分布（正态分布）中随机采样。</p><p><code>torch.randn(3,4)</code></p><p>我们还可以通过提供包含数值的Python列表（或嵌套列表），来为所需张量中的每个元素赋予确定值。 在这里，最外层的列表对应于轴0，内层的列表对应于轴1。</p><p><code>torch.tensor([[2,1,4,3],[1,2,3,4],[4,3,2,1]])</code></p><p>在下面的例子中，我们使用逗号来表示一个具有5个元素的元组，其中每个元素都是按元素操作的结果。</p><pre><code>x=torch.tensor([1.0,2,4,8])y=torch.tensor([2,2,2,2])x+y,x-y,x*y,x/y,x**y# **运算符是求幂运算#结果：(tensor([3.,4.,6.,10.]),tensor([-1.,0.,2.,6.]),tensor([2.,4.,8.,16.]),tensor([0.5000,1.0000,2.0000,4.0000]),tensor([1.,4.,16.,64.]))</code></pre><p>“按元素”方式可以应用更多的计算，包括像求幂这样的一元运算符。</p><p><code>torch.exp(x)</code></p><p>我们也可以把多个张量<em>连结</em>（concatenate）在一起， 把它们端对端地叠起来形成一个更大的张量。 我们只需要提供张量列表，并给出沿哪个轴连结。 下面的例子分别演示了当我们沿行（轴-0，形状的第一个元素） 和按列（轴-1，形状的第二个元素）连结两个矩阵时，会发生什么情况。 我们可以看到，第一个输出张量的轴-0长度（6）是两个输入张量轴-0长度的总和（3+3）； 第二个输出张量的轴-1长度（8）是两个输入张量轴-1长度的总和（4+4）。</p><pre><code>X=torch.arange(12,dtype=torch.float32).reshape((3,4))Y=torch.tensor([[2.0,1,4,3],[1,2,3,4],[4,3,2,1]])torch.cat((X,Y),dim=0),torch.cat((X,Y),dim=1)#结果：(tensor([[0.,1.,2.,3.],[4.,5.,6.,7.],[8.,9.,10.,11.],[2.,1.,4.,3.],[1.,2.,3.,4.],[4.,3.,2.,1.]]),tensor([[0.,1.,2.,3.,2.,1.,4.,3.],[4.,5.,6.,7.,1.,2.,3.,4.],[8.,9.,10.,11.,4.,3.,2.,1.]]))</code></pre><p>有时，我们想通过<em>逻辑运算符</em>构建二元张量。 以<code>X==Y</code>为例： 对于每个位置，如果X和Y在该位置相等，则新张量中相应项的值为1。 这意味着逻辑语句X==Y在该位置处为真，否则该位置为0。</p><pre><code>X==Y#结果：tensor([[False,True,False,True],[False,False,False,False],[False,False,False,False]])</code></pre><p>对张量中的所有元素进行求和，会产生一个单元素张量。</p><pre><code>X.sum()#结果：tensor(66.)</code></pre><p>在大多数情况下，我们将沿着数组中长度为1的轴进行广播，如下例子：</p><pre><code>a=torch.arange(3).reshape((3,1))b=torch.arange(2).reshape((1,2))a,b#结果：(tensor([[0],[1],[2]]),tensor([[0,1]]))</code></pre><p>由于a和b分别是3×1和1×2矩阵，如果让它们相加，它们的形状不匹配。 我们将两个矩阵<em>广播</em>为一个更大的3×2矩阵，如下所示：矩阵a将复制列， 矩阵b将复制行，然后再按元素相加。</p><pre><code>a+b#结果：tensor([[0,1],[1,2],[2,3]])</code></pre><p>如下所示，我们可以用[-1]选择最后一个元素，可以用[1:3]选择第二个和第三个元素：</p><p><code>X[-1],X[1:3]</code></p><p>除读取外，我们还可以通过指定索引来将元素写入矩阵。</p><p><code>X[1,2]=9</code></p><p>如果我们想为多个元素赋值相同的值，我们只需要索引所有元素，然后为它们赋值。 例如，[0:2,:]访问第1行和第2行，其中“:”代表沿轴1（列）的所有元素。虽然我们讨论的是矩阵的索引，但这也适用于向量和超过2个维度的张量。</p><p><code>X[0:2,:]=12</code></p><p>将深度学习框架定义的张量转换为NumPy张量（ndarray）很容易，反之也同样容易。 torch张量和numpy数组将共享它们的底层内存，就地操作更改一个张量也会同时更改另一个张量。</p><pre><code>A=X.numpy()B=torch.tensor(A)type(A),type(B)#结果：(numpy.ndarray,torch.Tensor)</code></pre><p>要将大小为1的张量转换为Python标量，我们可以调用<code>item</code>函数或Python的内置函数。</p><pre><code>a=torch.tensor([3.5])a,a.item(),float(a),int(a)#结果：(tensor([3.5000]),3.5,3.5,3)</code></pre><h2 id="2-数据预处理"><a href="#2-数据预处理" class="headerlink" title="2. 数据预处理"></a>2. 数据预处理</h2><p>在Python中常用的数据分析工具中，我们通常使用pandas软件包。 像庞大的Python生态系统中的许多其他扩展包一样，pandas可以与张量兼容。 本节我们将简要介绍使用pandas预处理原始数据，并将原始数据转换为张量格式的步骤。</p><h3 id="2-1-读取数据集"><a href="#2-1-读取数据集" class="headerlink" title="2.1. 读取数据集"></a>2.1. 读取数据集</h3><p>要从创建的CSV文件中加载原始数据集，我们导入pandas包并调用read_csv函数。</p><pre><code>data=pd.read_csv(data_file)print(data)#结果：   NumRooms  Alley0   NaN      Pave1   2.0      NaN2   4.0      NaN3   NaN      NaN</code></pre><h3 id="2-2-处理缺失值"><a href="#2-2-处理缺失值" class="headerlink" title="2.2. 处理缺失值"></a>2.2. 处理缺失值</h3><p>注意，“NaN”项代表缺失值。 为了处理缺失的数据，典型的方法包括<em>插值法</em>和<em>删除法</em>， 其中插值法用一个替代值弥补缺失值，而删除法则直接忽略缺失值。 </p><p>在对上面例子的处理中，我们将考虑插值法。通过位置索引iloc，我们将data分成inputs和outputs， 其中前者为data的前两列，而后者为data的最后一列。 对于inputs中缺少的数值，我们用同一列的均值替换“NaN”项。</p><pre><code>inputs,outputs=data.iloc[:,0:2],data.iloc[:,2]inputs=inputs.fillna(inputs.mean())print(inputs)#结果：  NumRooms  Alley0   3.0      Pave1   2.0      NaN2   4.0      NaN3   3.0      NaN</code></pre><p>对于inputs中的类别值或离散值，我们将“NaN”视为一个类别。 由于“巷子类型”（“Alley”）列只接受两种类型的类别值“Pave”和“NaN”，pandas可以自动将此列转换为两列“Alley_Pave”和“Alley_nan”。 巷子类型为“Pave”的行会将“Alley_Pave”的值设置为1，“Alley_nan”的值设置为0。 缺少巷子类型的行会将“Alley_Pave”和“Alley_nan”分别设置为0和1。Pandas中的get_dummies方法主要用于对类别型特征做<strong>One-Hot</strong>编码。dummy_na参数默认为False，增加一列表示空缺值，如果为False就忽略空缺值。</p><pre><code>inputs=pd.get_dummies(inputs,dummy_na=True)print(inputs)#结果：  NumRooms  Alley_Pave  Alley_nan0   3.0              1          01   2.0              0          12   4.0              0          13   3.0              0          1</code></pre><h3 id="2-3-转换为张量"><a href="#2-3-转换为张量" class="headerlink" title="2.3. 转换为张量"></a>2.3. 转换为张量</h3><p>现在inputs和outputs中的所有条目都是数值类型，它们需要转换为张量格式进行下一步操作。</p><pre><code>import torchX,y=torch.tensor(inputs.values),torch.tensor(outputs.values)</code></pre><h2 id="3-线性代数"><a href="#3-线性代数" class="headerlink" title="3. 线性代数"></a>3. 线性代数</h2><h3 id="3-1-基础概念"><a href="#3-1-基础概念" class="headerlink" title="3.1. 基础概念"></a>3.1. 基础概念</h3><p><strong>标量（scalar）</strong>：称仅包含一个数值的叫<em>标量</em>。标量由只有一个元素的张量表示时如<code>x=torch.tensor(3.0)</code>。</p><p><strong>变量（variable）</strong>：符号（x、y等）称为<em>变量</em>，它们表示未知的标量值。</p><p><strong>向量（vector）</strong>：可以将向量视为标量值组成的列表。 我们将这些标量值称为向量的<em>元素</em>（element）或<em>分量</em>（component）。向量的长度通常称为向量的<em>维度</em>。 当向量表示数据集中的样本时，它们的值具有一定的现实意义。例如，如果我们正在研究医院患者可能面临的心脏病发作风险，我们可能会用一个向量来表示每个患者， 其分量为最近的生命体征、胆固醇水平、每天运动时间等。我们通过一维张量处理向量，其长度任意，通过张量的索引来访问向量中任一元素。</p><p><strong>维度（dimension）</strong>：<em>向量</em>或<em>轴</em>的维度被用来表示<em>向量</em>或<em>轴</em>的长度，即向量或轴的元素数量。 然而，张量的维度用来表示张量具有的轴数。 在这个意义上，张量的某个轴的维数就是这个轴的长度。</p><p><strong>矩阵（matrix）</strong>：正如向量将标量从零阶推广到一阶，矩阵将向量从一阶推广到二阶，在代码中表示为具有两个轴的张量。当调用函数来实例化张量时， 我们可以通过指定两个分量m和n来创建一个形状为m×n的矩阵，如<code>A=torch.arange(20).reshape(5,4)</code>。在代码中访问矩阵的转置<code>A.T</code>。</p><p><strong>张量（tensor）</strong>：在线性代数中指代数对象，为我们提供了描述具有任意数量轴的n维数组的通用方法。向量是一阶张量，矩阵是二阶张量。它们的索引机制与矩阵类似。</p><p><strong>点积（dot product）</strong>：也就是内积，给定两个向量x,y∈Rd， 它们的<em>点积</em>（dot product）x⊤y（或⟨x,y⟩） 是相同位置的按元素乘积的和。点积在很多场合都很有用。 例如，给定一组由向量x∈Rd表示的值， 和一组由w∈Rd表示的权重。x中的值根据权重w的加权和， 可以表示为点积x⊤w。 当权重为非负数且和为1（即(∑i=1dwi=1)）时， 点积表示<em>加权平均</em>（weighted average）。 将两个向量规范化得到单位长度后，点积表示它们夹角的余弦。 在代码中使用<code>torch.dot(x,y)</code>表示，注意点积中只接收向量（1维）。</p><p><strong>矩阵-向量积（matrix-vector product）</strong>：将矩阵看作一个列向量，其中每一个元素为一个行向量，则矩阵-向量积为将该列向量的每个元素替换为对应行向量与向量的点积。在代码中使用张量表示矩阵-向量积，我们使用mv函数，接收一个矩阵（2维）和一个向量（1维）。 当我们为矩阵<code>A</code>和向量<code>x</code>调用<code>torch.mv(A,x)</code>时，会执行矩阵-向量积。 注意，<code>A</code>的列维数（沿轴1的长度）必须与<code>x</code>的维数（其长度）相同。</p><p><strong>矩阵乘法（matrix-matrix multiplication）</strong>：设A为n×k矩阵、B为k×m矩阵，可以将矩阵-矩阵乘法AB看作是简单地执行m次矩阵-向量积，并将结果拼接在一起，形成一个n×m矩阵。在代码中：<code>torch.mm(A,B)</code></p><p><strong>表示法</strong>：在《动手学深度学习》中，标量变量由普通小写字母表示（例如，x、y和z），用R表示所有（连续）<em>实数</em>标量的空间，将向量记为粗体、小写的符号 （例如，x、y和z)，认为列向量是向量的默认方向，通常用粗体、大写字母来表示矩阵 （例如，X、Y和Z），张量用特殊字体的大写字母表示（例如，X、Y和Z）。</p><h3 id="3-2-张量算法"><a href="#3-2-张量算法" class="headerlink" title="3.2. 张量算法"></a>3.2. 张量算法</h3><p>当我们开始处理图像时，张量将变得更加重要，图像以n维数组形式出现， 其中3个轴对应于高度、宽度，以及一个<em>通道</em>（channel）轴， 用于表示颜色通道（红色、绿色和蓝色）。</p><p>给定具有相同形状的任意两个张量，任何按元素二元运算的结果都将是相同形状的张量。例如，将两个相同形状的矩阵相加，会在这两个矩阵上执行元素加法。</p><pre><code>A=torch.arange(20,dtype=torch.float32).reshape(5,4)B=A.clone()# 通过分配新内存，将A的一个副本分配给BA, A+B</code></pre><p>两个矩阵的按元素乘法称为<em>Hadamard积</em>（Hadamard product）（数学符号⊙）。</p><p><code>A*B</code></p><p>将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘。</p><p><code>A*2, A+2</code></p><h3 id="3-3-降维"><a href="#3-3-降维" class="headerlink" title="3.3. 降维"></a>3.3. 降维</h3><p>我们可以对任意张量进行的一个有用的操作是计算其元素的和。 在数学表示法中，我们使用∑符号表示求和。在代码中，我们可以调用计算求和的函数<code>x.sum()</code>。</p><p>默认情况下，调用求和函数会沿所有的轴<strong>降低张量的维度</strong>，使它变为一个标量。 我们还可以指定张量沿哪一个轴来通过求和降低维度。 以矩阵为例，为了通过求和所有行的元素来降维（轴0），我们可以在调用函数时指定axis=0。 由于输入矩阵沿0轴降维以生成输出向量，因此输入轴0的维数在输出形状中消失。</p><pre><code>A_sum_axis0=A.sum(axis=0)A_sum_axis0,A_sum_axis0.shape#结果：(tensor([40.,45.,50.,55.]),torch.Size([4]))</code></pre><p>指定axis=1将通过汇总所有列的元素降维（轴1）。因此，输入轴1的维数在输出形状中消失。</p><pre><code>A_sum_axis1=A.sum(axis=1)A_sum_axis1,A_sum_axis1.shape#结果：(tensor([6.,22.,38.,54.,70.]),torch.Size([5]))</code></pre><p>沿着行和列对矩阵求和，等价于对矩阵的所有元素进行求和。如<code>A.sum(axis=[0,1])</code>。</p><p>一个与求和相关的量是<em>平均值</em>（mean或average）。 我们通过将总和除以元素总数来计算平均值。 在代码中，我们可以调用函数来计算任意形状张量的平均值。</p><pre><code>A.mean(),A.sum()/A.numel() #两者等价</code></pre><p>同样，计算平均值的函数也可以沿指定轴降低张量的维度。</p><pre><code>A.mean(axis=0),A.sum(axis=0)/A.shape[0]#结果：(tensor([8.,9.,10.,11.]),tensor([8.,9.,10.,11.]))</code></pre><h4 id="3-3-1-非降维求和"><a href="#3-3-1-非降维求和" class="headerlink" title="3.3.1. 非降维求和"></a>3.3.1. 非降维求和</h4><p>有时在调用函数来计算总和或均值时保持轴数不变会很有用。</p><pre><code>sum_A=A.sum(axis=1,keepdims=True)sum_A#结果：tensor([[6.],[22.],[38.],[54.],[70.]])</code></pre><p>例如，由于<code>sum_A</code>在对每行进行求和后仍保持两个轴，我们可以通过广播将A除以<code>sum_A</code>。如：<code>A/sum_A</code></p><p>如果我们想沿某个轴计算<code>A</code>元素的累积总和， 比如<code>axis=0</code>（按行计算），我们可以调用<code>cumsum</code>函数。 此函数不会沿任何轴降低输入张量的维度。</p><pre><code>A.cumsum(axis=0)#原本A：tensor([[0.,1.,2.,3.],        [4.,5.,6.,7.],        [8.,9.,10.,11.],        [12.,13.,14.,15.],        [16.,17.,18.,19.]])#结果：tensor([[0.,1.,2.,3.],        [4.,6.,8.,10.],        [12.,15.,18.,21.],        [24.,28.,32.,36.],        [40.,45.,50.,55.]])</code></pre><h3 id="3-4-范数"><a href="#3-4-范数" class="headerlink" title="3.4 范数"></a>3.4 范数</h3><p>线性代数中最有用的一些运算符是<em>范数</em>（norm）。 非正式地说，一个向量的<em>范数</em>告诉我们一个向量有多大。 这里考虑的<em>大小</em>（size）概念不涉及维度，而是分量的大小。</p><p>在线性代数中，向量范数是将向量映射到标量的函数f。 给定任意向量x，向量范数要满足一些属性。 第一个性质是如果我们按常数因子α缩放向量的所有元素， 其范数也会按相同常数因子的<em>绝对值</em>缩放。第二个性质是我们熟悉的三角不等式f(x+y)\&lt;=f(x)+f(y)。第三个性质简单地说范数必须是非负的，这是有道理的。因为在大多数情况下，任何东西的最小的<em>大小</em>是0。 最后一个性质要求范数最小为0，当且仅当向量全由0组成。</p><p>假设n维向量x中的元素是x1,…,xn，其L2<em>范数</em>是向量元素平方和的平方根。在代码中，我们可以按如下方式计算向量的L2范数。在深度学习中，我们更经常地使用L2范数的平方。</p><pre><code>u=torch.tensor([3.0,-4.0])torch.norm(u)#结果：tensor(5.)</code></pre><p>L1范数，表示为向量元素的绝对值之和。与L2范数相比，L1范数受异常值的影响较小。 为了计算L1范数，我们将绝对值函数和按元素求和组合起来。</p><pre><code>torch.abs(u).sum()#结果：tensor(7.)</code></pre><p>L2范数和L1范数都是更一般的Lp范数的特例。</p><p>类似于向量的L2范数，矩阵X∈Rm×n的<em>Frobenius范数</em>（Frobenius norm）是矩阵元素平方和的平方根。Frobenius范数满足向量范数的所有性质，它就像是矩阵形向量的L2范数。 调用以下函数将计算矩阵的Frobenius范数。</p><pre><code>torch.norm(torch.ones((4,9)))#结果：tensor(6.)</code></pre><p>在深度学习中，我们经常试图解决优化问题：<em>最大化</em>分配给观测数据的概率;<em>最小化</em>预测和真实观测之间的距离。 用向量表示物品（如单词、产品或新闻文章），以便最小化相似项目之间的距离，最大化不同项目之间的距离。 <strong>目标</strong>，或许是深度学习算法最重要的组成部分（除了数据），通常被表达为范数。</p><h2 id="4-微积分和自动微分"><a href="#4-微积分和自动微分" class="headerlink" title="4. 微积分和自动微分"></a>4. 微积分和自动微分</h2><p>在深度学习中，我们“训练”模型，不断更新它们，使它们在看到越来越多的数据时变得越来越好。 通常情况下，变得更好意味着最小化一个<em>损失函数</em>（loss function）， 即一个衡量“我们的模型有多糟糕”这个问题的分数。 最终，我们真正关心的是生成一个模型，它能够在从未见过的数据上表现良好。 但“训练”模型只能将模型与我们实际能看到的数据相拟合。 因此，我们可以将拟合模型的任务分解为两个关键问题：</p><ul><li><em>优化</em>（optimization）：用模型拟合观测数据的过程；</li><li><em>泛化</em>（generalization）：数学原理和实践者的智慧，能够指导我们生成出有效性超出用于训练的数据集本身的模型。</li></ul><p>总结：</p><ul><li>微分和积分是微积分的两个分支，前者可以应用于深度学习中的优化问题。</li><li>导数可以被解释为函数相对于其变量的瞬时变化率，它也是函数曲线的切线的斜率。</li><li>梯度是一个向量，其分量是多变量函数相对于其所有变量的偏导数。</li><li>链式法则使我们能够微分复合函数。</li><li>深度学习框架可以自动计算导数：我们首先将梯度附加到想要对其计算偏导数的变量上。然后我们记录目标值的计算，执行它的反向传播函数，并访问得到的梯度。</li></ul><h3 id="4-1-导数、微分、偏导"><a href="#4-1-导数、微分、偏导" class="headerlink" title="4.1. 导数、微分、偏导"></a>4.1. 导数、微分、偏导</h3><p>我们首先讨论导数的计算，这是几乎所有深度学习优化算法的关键步骤。 在深度学习中，我们通常选择对于模型参数可微的损失函数。 简而言之，对于每个参数， 如果我们把这个参数<em>增加</em>或<em>减少</em>一个无穷小的量，我们可以知道损失会以多快的速度增加或减少。</p><p>假设我们有一个函数f，其输入和输出都是标量。 导数定义不用多说，如果f′(a)存在，则称f在a处是<em>可微</em>（differentiable）的。如果f在一个区间内的每个数上都是可微的，则此函数在此区间中是可微的。可以将导数定义中的导数f′(x)解释为f(x)相对于x的<em>瞬时</em>（instantaneous）变化率。 所谓的瞬时变化率是基于x中的变化h，且h接近0。</p><p>要了解微分，以及对函数微分的法则。例如Dx^n = nx^(n-1)，其中D为微分运算符。还要了解求偏导，这些都是基础。</p><h3 id="4-2-梯度"><a href="#4-2-梯度" class="headerlink" title="4.2. 梯度"></a>4.2. 梯度</h3><p>我们可以连结一个多元函数对其所有变量的偏导数，以得到该函数的<em>梯度</em>（gradient）向量。 具体而言，设函数f:Rn→R的输入是一个n维向量x=[x1,x2,…,xn]⊤，并且输出是一个标量。 函数f(x)相对于x的梯度是一个包含n个偏导数的向量</p><script type="math/tex; mode=display">\nabla_{\mathbf{x}} f(\mathbf{x})=\left[\frac{\partial f(\mathbf{x})}{\partial x_{1}}, \frac{\partial f(\mathbf{x})}{\partial x_{2}}, \ldots, \frac{\partial f(\mathbf{x})}{\partial x_{n}}\right]^{\top}</script><p>,其中∇xf(x)通常在没有歧义时被∇f(x)取代。梯度对于设计深度学习中的优化算法有很大用处。</p><p>假设x为n维向量，在微分多元函数时经常使用以下规则:</p><ul><li>对于所有 $\mathbf{A} \in \mathbb{R}^{m \times n}$, 都有 $\nabla_{\mathbf{x}} \mathbf{A} \mathbf{x}=\mathbf{A}^{\top}$</li><li>对于所有 $\mathbf{A} \in \mathbb{R}^{n \times m}$, 都有 $\nabla_{\mathbf{x}} \mathbf{x}^{\top} \mathbf{A}=\mathbf{A}$<br>-对于所有 $\mathbf{A} \in \mathbb{R}^{n \times n}$, 都有 $\nabla_{\mathbf{x}} \mathbf{x}^{\top} \mathbf{A} \mathbf{x}=\left(\mathbf{A}+\mathbf{A}^{\top}\right) \mathbf{x}$</li><li>$\nabla_{\mathbf{x}}|\mathbf{x}|^{2}=\nabla_{\mathbf{x}} \mathbf{x}^{\top} \mathbf{x}=2 \mathbf{x}$</li></ul><p>同样，对于任何矩阵X，都有 $\nabla \mathbf{X}|\mathbf{X}|_{F}^{2}=2 \mathbf{X}$。</p><h3 id="4-3-链式法则"><a href="#4-3-链式法则" class="headerlink" title="4.3. 链式法则"></a>4.3. 链式法则</h3><p>然而，上面方法可能很难找到梯度。 这是因为在深度学习中，多元函数通常是<em>复合</em>（composite）的， 所以我们可能没法应用上述任何规则来微分这些函数。 幸运的是，链式法则使我们能够微分复合函数。</p><p>让我们先考虑单变量函数。假设函数y=f(u)和u=g(x)都是可微的，根据链式法则有</p><script type="math/tex; mode=display">\frac{d y}{d x}=\frac{d y}{d u} \frac{d u}{d x}</script><p>当处于函数具有任意数量的变量的情况下。假设可微分函数y有变量u1,u2,…,um，其中每个可微分函数ui都有变量x1,x2,…,xn。 注意，y是x1,x2，…,xn的函数。 对于任意i=1,2,…,n，链式法则给出：</p><script type="math/tex; mode=display">\frac{d y}{d x_{i}}=\frac{d y}{d u_{1}} \frac{d u_{1}}{d x_{i}}+\frac{d y}{d u_{2}} \frac{d u_{2}}{d x_{i}}+\cdots+\frac{d y}{d u_{m}} \frac{d u_{m}}{d x_{i}}</script><h3 id="4-4-自动微分"><a href="#4-4-自动微分" class="headerlink" title="4.4. 自动微分"></a>4.4. 自动微分</h3><p>求导是几乎所有深度学习优化算法的关键步骤。 虽然求导的计算很简单，只需要一些基本的微积分。 但对于复杂的模型，手工进行更新是一件很痛苦的事情（而且经常容易出错）。</p><p>深度学习框架通过自动计算导数，即<em>自动微分</em>（automatic differentiation）来加快求导。 实际中，根据我们设计的模型，系统会构建一个<em>计算图</em>（computational graph）， 来跟踪计算是哪些数据通过哪些操作组合起来产生输出。 自动微分使系统能够随后反向传播梯度。 这里，<em>反向传播</em>（backpropagate）意味着跟踪整个计算图，填充关于每个参数的偏导数。设y为关于x的函数，调用<code>y.backward()</code>反向传播计算x的梯度<code>x.grad</code>。</p><p>例如对于：$e=(a+b) *(b+1)$<br>可以得到计算图如下：<br><img src="/assets/post_img/article34/BP.png" alt="0x02pic1"></p><p>反向传播参考：<a href="https://blog.csdn.net/Weary_PJ/article/details/105706318">https://blog.csdn.net/Weary_PJ/article/details/105706318</a></p><h4 id="4-4-1-非标量变量的反向传播"><a href="#4-4-1-非标量变量的反向传播" class="headerlink" title="4.4.1. 非标量变量的反向传播"></a>4.4.1. 非标量变量的反向传播</h4><p>当y不是标量时，向量y关于向量x的导数的最自然解释是一个矩阵。 对于高阶和高维的y和x，求导的结果可以是一个高阶张量。</p><p>然而，虽然这些更奇特的对象确实出现在高级机器学习中（包括深度学习中）， 但当我们调用向量的反向计算时，我们通常会试图计算一批训练样本中每个组成部分的损失函数的导数。 这里，我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和。</p><h4 id="4-4-2-分离计算"><a href="#4-4-2-分离计算" class="headerlink" title="4.4.2. 分离计算"></a>4.4.2. 分离计算</h4><p>有时，我们希望将某些计算移动到记录的计算图之外。 例如，假设y是作为x的函数计算的，而z则是作为y和x的函数计算的。 想象一下，我们想计算z关于x的梯度，但由于某种原因，我们希望将y视为一个常数， 并且只考虑到x在y被计算后发挥的作用。</p><p>在这里，我们可以分离y来返回一个新变量u，该变量与y具有相同的值， 但丢弃计算图中如何计算y的任何信息。 换句话说，梯度不会向后流经u到x。</p><p>为了实现自动微分，PyTorch跟踪所有涉及张量的操作，可能需要为其计算梯度（即require_grad为True）。 这些操作记录为有向图。在代码上，上述分离计算可以通过使用<strong>detach（）</strong>方法在张量上构造一个新视图，该张量声明为不需要梯度，即从进一步跟踪操作中将其排除在外，因此不记录涉及该视图的子图。即对应上述的情况，令<code>u=y.detach()</code>，使用<code>z=u*x</code>替代原本的<code>z=y*x</code>。</p><h4 id="4-4-3-Python控制流的梯度计算"><a href="#4-4-3-Python控制流的梯度计算" class="headerlink" title="4.4.3. Python控制流的梯度计算"></a>4.4.3. Python控制流的梯度计算</h4><p>使用自动微分的一个好处是：即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度。在下面的代码中，while循环的迭代次数和if语句的结果都取决于输入a的值。</p><pre><code>def f(a):  b=a*2  while b.norm()&lt;1000:    b=b*2  if b.sum()&gt;0:    c=b  else:    c=100*b  return c# 计算梯度a=torch.randn(size=(),requires_grad=True)d=f(a)d.backward()</code></pre><p>我们现在可以分析上面定义的f函数。 请注意，它在其输入a中是分段线性的。 换言之，对于任何a，存在某个常量标量k，使得f(a)=k*a，其中k的值取决于输入a。 因此，我们可以用d/a验证梯度是否正确。<code>a.grad==d/a</code>结果为真。</p><h2 id="5-概率"><a href="#5-概率" class="headerlink" title="5. 概率"></a>5. 概率</h2><p>简单地说，机器学习就是做出预测。</p><p>根据病人的临床病史，我们可能想预测他们在下一年心脏病发作的<em>概率</em>。 在飞机喷气发动机的异常检测中，我们想要评估一组发动机读数为正常运行情况的概率有多大。 在强化学习中，我们希望智能体（agent）能在一个环境中智能地行动。 这意味着我们需要考虑在每种可行的行为下获得高奖励的概率。 当我们建立推荐系统时，我们也需要考虑概率。 例如，假设我们为一家大型在线书店工作，我们可能希望估计某些用户购买特定图书的概率。 为此，我们需要使用概率学。 概率是一种灵活的语言，用于说明我们的确定程度，并且它可以有效地应用于广泛的领域中。</p><p>对于概率（probability）和统计（statistics）：概率是已知模型和参数，推数据。统计是已知数据，推模型和参数。</p><h3 id="5-1-基本概率论"><a href="#5-1-基本概率论" class="headerlink" title="5.1. 基本概率论"></a>5.1. 基本概率论</h3><p>以掷骰子为例，想知道看到1的几率有多大，而不是看到另一个数字。 如果骰子是公平的，那么所有六个结果{1,…,6}都有相同的可能发生， 因此我们可以说1发生的概率为6分之1。</p><p>然而现实生活中，对于我们从工厂收到的真实骰子，我们需要检查它是否有瑕疵。 检查骰子的唯一方法是多次投掷并记录结果。 对于每个骰子，我们将观察到{1,…,6}中的一个值。 对于每个值，一种自然的方法是将它出现的次数除以投掷的总次数， 即此<em>事件</em>（event）概率的<em>估计值</em>。<em>大数定律</em>（law of large numbers）告诉我们： 随着投掷次数的增加，这个估计值会越来越接近真实的潜在概率。</p><p>在统计学中，我们把从概率分布中抽取样本的过程称为<em>抽样</em>（sampling）。 笼统来说，可以把<em>分布</em>（distribution）看作是对事件的概率分配， 稍后我们将给出的更正式定义。 将概率分配给一些离散选择的分布称为<em>多项分布</em>（multinomial distribution）。</p><p>在代码中multinomial.Multinomial函数创建由 total_count 和 probs 或 logits（但不是两者）参数化的多项分布。 probs（概率）的最内层维度索引种类，所有其他维度索引批次。total_count表示总的实验次数。如：<code>multinomial.Multinomial(10,fair_probs).sample()</code>表示在fair_probs的概率下进行了10次实验得到的分布情况。在此基础上除10即可得到真实概率的估计。</p><h4 id="5-1-1-概率论公理"><a href="#5-1-1-概率论公理" class="headerlink" title="5.1.1. 概率论公理"></a>5.1.1. 概率论公理</h4><p>在处理骰子掷出时，我们将集合S={1,2,3,4,5,6}称为<em>样本空间</em>（sample space）或<em>结果空间</em>（outcome space）， 其中每个元素都是<em>结果</em>（outcome）。<em>事件</em>（event）是一组给定样本空间的随机结果。 例如，“看到5”（{5}）和“看到奇数”（{1,3,5}）都是掷出骰子的有效事件。 注意，如果一个随机实验的结果在A中，则事件A已经发生。 也就是说，如果投掷出3点，因为3∈{1,3,5}，我们可以说，“看到奇数”的事件发生了。<em>概率</em>（probability）可以被认为是将集合映射到真实值的函数。</p><p>性质：</p><ul><li>概率非负</li><li>全样本空间概率为1</li><li>对于<em>互斥</em>（mutually exclusive）事件（对于所有i≠j都有Ai∩Aj=∅）的任意一个可数序列A1,A2,…，序列中任意一个事件发生的概率等于它们各自发生的概率之和。</li></ul><h4 id="5-1-2-随机变量"><a href="#5-1-2-随机变量" class="headerlink" title="5.1.2. 随机变量"></a>5.1.2. 随机变量</h4><p>在我们掷骰子的随机实验中，我们引入了<em>随机变量</em>（random variable）的概念。 随机变量几乎可以是任何数量，并且它可以在随机实验的一组可能性中取一个值。 考虑一个随机变量X，其值在掷骰子的样本空间S={1,2,3,4,5,6}中。 我们可以将事件“看到一个5”表示为{X=5}或X=5， 其概率表示为P({X=5})或P(X=5)。 通过P(X=a)，我们区分了随机变量X和X可以采取的值（例如a）。</p><p>然而，这可能会导致繁琐的表示。 为了简化符号，一方面，我们可以将P(X)表示为随机变量X上的<em>分布</em>（distribution）： 分布告诉我们X获得某一值的概率。 另一方面，我们可以简单用P(a)表示随机变量取值a的概率。 由于概率论中的事件是来自样本空间的一组结果，因此我们可以为随机变量指定值的可取范围。 例如，P(1≤X≤3)表示事件{1≤X≤3}， 即{X=1,2,or,3}的概率。 等价地，P(1≤X≤3)表示随机变量X从{1,2,3}中取值的概率。这一节主要讨论离散型随机变量，连续型不咋考虑。</p><h3 id="5-2-处理多个随机变量"><a href="#5-2-处理多个随机变量" class="headerlink" title="5.2. 处理多个随机变量"></a>5.2. 处理多个随机变量</h3><p>例如图像包含数百万像素，因此有数百万个随机变量。 在许多情况下，图像会附带一个<em>标签</em>（label），标识图像中的对象。 我们也可以将标签视为一个随机变量。 我们甚至可以将所有元数据视为随机变量，例如位置、时间、光圈、焦距、ISO、对焦距离和相机类型。 所有这些都是联合发生的随机变量。 当我们处理多个随机变量时，会有若干个变量是我们感兴趣的。</p><h4 id="5-2-1-一些基本概念"><a href="#5-2-1-一些基本概念" class="headerlink" title="5.2.1. 一些基本概念"></a>5.2.1. 一些基本概念</h4><p><strong>联合概率（joint probability）</strong>：$P(A=a, B=b)$，联合概率可以回答：A=a和B=b同时满足的概率是多少的问题，但对于任何a和b的取值，P(A=a,B=b)≤P(A=a)。</p><p><strong>条件概率（conditional probability）</strong>：用P(B=b∣A=a)表示它，它是B=b的概率，前提是A=a已发生。</p><p><strong>贝叶斯定理（Bayes’ theorem）</strong>：根据<em>乘法法则</em>（multiplication rule）可得到 $P(A, B)=P(B \mid A) P(A)$ 。根据对称性, 可得到 $P(A, B)=P(A \mid B) P(B)$ 。根据两式假设P(B)>0，求解其中一个条件变量，我们得到：</p><script type="math/tex; mode=display">P(A \mid B)=\frac{P(B \mid A) P(A)}{P(B)}</script><p>注意这里使用紧凑的表示法： 其中 $P(A, B)$ 是一个联合分布（joint distribution）, $P(A \mid B)$ 是一个条件分布 (conditional distribution)。这种分布可以在给定值 $A=a, B=b$ 上进行求值。</p><p><strong>边际化（marginalization）</strong>：为了能进行事件概率求和，我们需要<em>加法法则</em>（sum rule）， 即B的概率相当于计算A的所有可能选择，并将所有选择的联合概率聚合在一起：</p><script type="math/tex; mode=display">P(B)=\sum_{A} P(A, B)</script><p>这也称为<em>边际化</em>（marginalization）。边际化结果的概率或分布称为<em>边缘概率</em>（marginal probability） 或<em>边缘分布</em>（marginal distribution）。</p><p><strong>依赖（dependence）与独立（independence）</strong>：如果两个随机变量A和B是独立的，意味着事件A的发生跟B事件的发生无关。 在这种情况下，通常将这一点表述为A⊥B。 根据贝叶斯定理，得到P(A∣B)=P(A)。 在所有其他情况下，我们称A和B依赖。 比如，两次连续抛出一个骰子的事件是相互独立的。 相比之下，灯开关的位置和房间的亮度并不是（因为可能存在灯泡坏掉、电源故障，或者开关故障）。两个随机变量是独立的，当且仅当两个随机变量的联合分布是其各自分布的乘积。</p><p><strong>期望（expectation，或平均值（average））</strong>：均值。</p><p><strong>方差（variance）</strong>：数据与平均数之差平方和的平均数，方差的平方根被称为<em>标准差</em>（standared deviation）。随机变量函数的方差衡量的是：当从该随机变量分布中采样不同值x时， 函数值偏离该函数的期望的程度。</p><p><strong>似然函数（Likelihood Function）</strong>：似然也就是可能性。在统计中，似然函数和概率函数是两个不同的概念。<br>对于这个函数： $p(x \mid \theta)$ 输入有两个: $\mathrm{x}$ 表示某一个具体的数据; $\theta$ 表示模型的参数。<br>如果 $\theta$ 是已知确定的, $x$ 是变量, 这个函数叫做概率函数(probability function), 它描述对于不同的样本点 $x$, 其出现概率是多少。<br>如果 $x$ 是已知确定的, $\theta$ 是变量, 这个函数叫做似然函数(likelihood function), 它描述对于不同的模型参数, 出现 $x$ 这个样本点的概率是多少。</p><p><strong>最大似然估计(Maximum Likelihood Estimation, MLE)</strong>：属于统计领域问题，利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值。其中似然即可能性。<br>极大似然估计中采样需满足一个重要的假设，就是所有的采样都是独立同分布的。<br>最大似然估计是怎么做的呢？个人理解：对于一个概率分布（或离散或连续），抽出一个具有n个值的采样（结果采样），确定一个似然函数：$\operatorname{lik}(\theta)=f_{D}\left(x_{1}, x_{2}, \ldots, x_{n} \mid \theta\right)$，在θ的所有取值上，使这个函数最大化。这个使可能性最大的值即被称为θ的最大似然估计。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;p&gt;这一笔记对应预备知识章节，包括数据操作、数据预处理、线性代数、微积分、概率论等。其中很多知识在学数一的时候都更深入理解过了，但是现在发现忘的差不多了，哎～&lt;/p&gt;
&lt;p&gt;对应实践：&lt;a href=&quot;https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x02.ipynb&quot;&gt;https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x02.ipynb&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在M1芯片的设备上使用miniforge安装pytorch：&lt;code&gt;conda install -c pytorch pytorch&lt;/code&gt;&lt;br&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://silencezheng.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>M1芯片VMfusion安装win11教程</title>
    <link href="http://silencezheng.top/2022/04/04/article33/"/>
    <id>http://silencezheng.top/2022/04/04/article33/</id>
    <published>2022-04-04T09:26:33.000Z</published>
    <updated>2022-04-04T09:32:34.875Z</updated>
    
    <content type="html"><![CDATA[<p>首先需要准备的有：VMfusion预览版（支持ARM架构）、Win11ARM版镜像（.iso）</p><span id="more"></span><p>下面开始安装：</p><ol><li>打开VMfusion创建自定虚拟机，操作系统选择Other 64-bit Arm</li><li>下一步虚拟磁盘那块点继续，不用设置</li><li>然后一定要注意选择 <strong>自定设置</strong> ，否则虚拟机会自动启动，就无法安装了。</li><li>自定设置时，cpu推荐4核以上，内存4g以上，硬盘容量52g以上（保险起见，实际30g也够用了）</li><li>dvd设置我们下载的win11镜像后，在启动磁盘处选择dvd启动（不选应该也会默认这个）</li><li>找到该虚拟机的安装位置，查看包内容，编辑vmx文件，把guestOS改为arm-windows11-64</li><li>启动虚拟机，点击任意按键进入安装界面</li><li>选择没有产品密钥，一直进入到选择操作系统界面</li><li>下面需要绕过TPM，首先按住fn+shift+f10打开cmd，输入regedit打开注册表编辑器</li><li>进到HKEY_LOCAL_MACHINE-&gt;SYSTEM-&gt;Setup下，新建项LabConfig</li><li>在该项创建两个DWORD值，key分别为BypassTPMCheck和BypassSecureBootCheck，value都为1</li><li>然后都关闭掉继续进行安装，到网络设置界面发现无法继续，打开cmd准备跳过</li><li>输入taskmgr打开任务管理器，详细信息中找到OOBE开头的进程关闭掉，然后全部关闭返回发现跳过了网络设置步骤</li><li>下面成功进入桌面后，需要配置好网络才能正常使用（装来玩扫雷当我没说），管理员身份打开cmd</li><li>输入命令<code>bcdedit /debug on</code> 启用默认启动项的内核调试</li><li>再输入 <code>bcdedit /dbgsettings net hostip:10.0.0.1 port:55555</code> 将目标计算机配置为使用以太网连接进行调试，并指定主计算机的 IP 地址和主机可用于连接到目标计算机的端口号。</li><li>重启系统后即可连接网络，配置完毕。</li></ol><p>解读：<br>10.X.X.X是私有地址（私有地址是互联网上不使用，而用在局域网络中的地址）<br><code>netstat -ano     查看所有端口使用情况</code><br><code>netstat -aon|findstr &quot;55555&quot; 查看55555端口pid</code><br><code>tasklist|findstr &quot;9088&quot; 查看pid对应程序</code></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;首先需要准备的有：VMfusion预览版（支持ARM架构）、Win11ARM版镜像（.iso）&lt;/p&gt;</summary>
    
    
    
    
    <category term="MacOS" scheme="http://silencezheng.top/tags/MacOS/"/>
    
  </entry>
  
  <entry>
    <title>机器学习概述--《动手学深度学习》笔记0x01</title>
    <link href="http://silencezheng.top/2022/04/01/article32/"/>
    <id>http://silencezheng.top/2022/04/01/article32/</id>
    <published>2022-03-31T17:55:28.000Z</published>
    <updated>2022-05-27T14:15:27.517Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><p>终于得以继续升学，可以做一些自己喜欢的事情了，早就想跟李沐老师学习一下深度学习，该系列为《动手学深度学习v2》的笔记，希望自己能在九月前完结。<br><span id="more"></span></p><h2 id="1-什么是机器学习？"><a href="#1-什么是机器学习？" class="headerlink" title="1. 什么是机器学习？"></a>1. 什么是机器学习？</h2><p>机器学习（machine learning，ML）是一类强大的可以从经验中学习的技术。 通常采用观测数据或与环境交互的形式，机器学习算法会积累更多的经验，其性能也会逐步提高。应用场景可以有以下方面：</p><ul><li>编写一个程序，给出地理信息、卫星图像和一些历史天气信息，来预测明天的天气。</li><li>编写一个程序，给出自然文本表示的问题，并正确回答该问题。</li><li>编写一个程序，给出一张图像，识别出图像所包含的人，并在每个人周围绘制轮廓。</li><li>编写一个程序，向用户推荐他们可能喜欢，但在自然浏览过程中不太可能遇到的产品。</li></ul><p>通过机器学习算法解决问题时，我们不需要设计一个“明确地”实现功能的系统。 相反，我们定义一个灵活的程序算法，其输出由许多参数（parameter）决定。 然后我们使用数据集（dataset）来确定当下的“最佳参数集”，这些参数通过某种性能度量来获取完成任务的最佳性能。</p><p>总而言之，可以将这种“通过用数据集来确定程序行为”的方法看作是“用数据编程”（programming with data）。 比如，我们可以通过向机器学习系统，提供许多猫和狗的图片来设计一个“猫图检测器”。 通过这种方式，检测器最终可以学会：如果输入是猫的图片就输出一个非常大的正数，如果输入是狗的图片就会得出一个非常大的负数。 如果检测器不确定，它会输出接近于零的数…… 这个例子仅仅是机器学习常见应用的冰山一角。</p><h3 id="1-1-参数、模型、学习？"><a href="#1-1-参数、模型、学习？" class="headerlink" title="1.1. 参数、模型、学习？"></a>1.1. 参数、模型、学习？</h3><p>我们可以把<strong>参数（parameter</strong>看作是旋钮，我们可以转动旋钮来调整程序的行为。 </p><p><strong>模型（model）</strong>：任一调整参数后的程序</p><p><strong>模型族</strong>：通过操作参数而生成的所有不同程序（输入-输出映射）的集合</p><p><strong>学习算法（learning algorithm）</strong>：使用数据集来选择参数的元程序</p><p>在我们开始用机器学习算法解决问题之前，我们必须精确地定义问题，确定输入（input）和输出（output）的性质，并选择合适的模型族。</p><p>在机器学习中，<strong>学习（learning）</strong>是一个训练模型的过程。 通过这个过程，我们可以发现正确的参数集，从而使模型强制执行所需的行为。 换句话说，我们用数据<strong>训练（train）</strong>我们的模型。</p><h3 id="1-2-训练？"><a href="#1-2-训练？" class="headerlink" title="1.2. 训练？"></a>1.2. 训练？</h3><p>训练过程通常包含如下步骤：</p><ol><li>从一个随机初始化参数的模型开始，这个模型基本毫不“智能”。</li><li>获取一些数据样本。</li><li>调整参数，使模型在这些样本中表现得更好。</li><li>重复第2步和第3步，直到模型在任务中的表现令你满意。</li></ol><h3 id="1-3-核心组件！"><a href="#1-3-核心组件！" class="headerlink" title="1.3. 核心组件！"></a>1.3. 核心组件！</h3><p>无论我们遇到什么类型的机器学习问题，这些组件都将伴随我们左右：</p><ol><li>我们可以学习的<strong>数据</strong>（data）。</li><li>如何转换数据的<strong>模型</strong>（model）。</li><li>一个<strong>目标函数</strong>（objective function），用来量化模型的有效性。</li><li>调整模型参数以优化目标函数的算法（algorithm）。</li></ol><h4 id="1-3-1-数据"><a href="#1-3-1-数据" class="headerlink" title="1.3.1. 数据"></a>1.3.1. 数据</h4><p>数据集由一个个<strong>样本</strong>组成，样本的说法可以有example, sample，data point或者data instance。</p><p>通常每个样本由一组称为<strong>特征</strong>（features，或协变量（covariates））的属性组成。 机器学习模型会根据这些属性进行预测。 在之前的监督学习问题中，要预测的是一个特殊的属性，它被称为<strong>标签</strong>（label，或目标（target））。</p><p>假设我们处理的是图像数据，每一张单独的照片即为一个样本，它的特征由每个像素数值的有序列表表示。 比如，200✖️200彩色照片由200✖️200✖️3个数值组成，其中的“3”对应于每个空间位置的红、绿、蓝通道的强度。</p><p>当每个样本的特征类别数量都是相同的时候，其特征向量是固定长度的，这个长度被称为数据的<strong>维数</strong>（dimensionality）。 固定长度的特征向量是一个方便的属性，它有助于我们量化学习大量样本。</p><p>然而很多数据不可以用固定长度的向量表示，如果强行规范为固定长度，则会造成信息丢失。与传统机器学习方法相比，深度学习的一个主要优势是可以处理不同长度的数据。同时，虽然数据量越大得到的模型越强大，但首先需要保证数据的正确性。</p><h4 id="1-3-2-模型"><a href="#1-3-2-模型" class="headerlink" title="1.3.2. 模型"></a>1.3.2. 模型</h4><p>深度学习与经典方法的区别主要在于：前者关注的功能强大的模型，这些模型由<strong>神经网络</strong>错综复杂的交织在一起，包含层层数据转换，因此被称为深度学习（deep learning）。</p><h4 id="1-3-3-目标函数就是损失函数？过拟合是什么？"><a href="#1-3-3-目标函数就是损失函数？过拟合是什么？" class="headerlink" title="1.3.3. 目标函数就是损失函数？过拟合是什么？"></a>1.3.3. 目标函数就是损失函数？过拟合是什么？</h4><p>机器学习是“从经验中学习”。 这里所说的“学习”，是指自主提高模型完成某些任务的效能。如何定义提高呢？ 在机器学习中，我们需要定义模型的优劣程度的度量，这个度量在大多数情况是“可优化”的，我们称之为<strong>目标函数</strong>（objective function）。</p><p>我们通常定义一个目标函数，并希望优化它到最低点。 因为越低越好，所以这些函数有时被称为<strong>损失函数</strong>（loss function，或cost function）。</p><p>我们通常将可用数据集分成两部分：<strong>训练集</strong>用于<strong>拟合</strong>模型参数，<strong>测试集</strong>用于评估<strong>拟合</strong>的模型。 然后我们观察模型在这两部分数据集的效能。</p><p>当一个模型在训练集上表现良好，但不能推广到测试集时，我们说这个模型是<strong>“过拟合”</strong>（overfitting）的。</p><h4 id="1-3-4-优化算法"><a href="#1-3-4-优化算法" class="headerlink" title="1.3.4. 优化算法"></a>1.3.4. 优化算法</h4><p>一旦我们获得了一些数据源及其表示、一个模型和一个合适的损失函数，我们接下来就需要一种算法，它能够搜索出<strong>最佳参数，以最小化损失函数</strong>。 深度学习中，大多流行的优化算法通常基于一种基本方法–<strong>梯度下降（gradient descent）</strong>。 </p><p>简而言之，在每个步骤中，梯度下降法都会检查每个参数，看看如果你仅对该参数进行少量变动，训练集损失会朝哪个方向移动。 然后，它在可以减少损失的方向上优化参数。</p><h3 id="1-4-常见机器学习问题"><a href="#1-4-常见机器学习问题" class="headerlink" title="1.4. 常见机器学习问题"></a>1.4. 常见机器学习问题</h3><p>监督学习（supervised learning）、无监督学习、与环境互动、强化学习。</p><h4 id="1-4-1-监督学习"><a href="#1-4-1-监督学习" class="headerlink" title="1.4.1. 监督学习"></a>1.4.1. 监督学习</h4><p><strong>监督学习</strong>（supervised learning）擅长在“给定输入特征”的情况下预测标签。 每个“特征-标签”对都称为一个样本（example）。</p><p> 有时，即使标签是未知的，样本也可以指代输入特征。 我们的目标是生成一个模型，能够将任何输入特征映射到<strong>标签</strong>，即预测。</p><p>在工业中，大部分机器学习的成功应用都是监督学习。 这是因为在一定程度上，许多重要的任务可以清晰地描述为：在给定一组特定的可用数据的情况下，估计未知事物的概率。监督学习发挥作用的前提是数据集正确、真实。</p><p>监督学习的学习过程：</p><ol><li>从已知大量数据样本中随机选取一个子集，为每个样本获取基本的真实标签。 有时，这些样本已有标签（例如，患者是否在下一年内康复？）； 有时，我们可能需要人工标记数据（例如，将图像分类）。 这些输入和相应的标签一起构成了训练数据集。 </li><li>选择有监督的学习算法，它将训练数据集作为输入，并输出一个“完成学习模型”。 </li><li>将之前没见过的样本特征放到这个“完成学习模型”中，使用模型的输出作为相应标签的预测。</li></ol><p>即使使用简单的描述“给定输入特征的预测标签”，监督学习也可以采取多种形式的模型，并且需要大量不同的建模决策，这取决于输入和输出的类型、大小和数量。 例如，我们使用不同的模型来处理“任意长度的序列”或“固定长度的序列”。</p><h5 id="1-4-1-1-回归"><a href="#1-4-1-1-回归" class="headerlink" title="1.4.1.1. 回归"></a>1.4.1.1. 回归</h5><p><strong>回归（regression）</strong>是最简单的监督学习任务之一。当标签取任意数值时，我们称之为回归问题。 我们的目标是生成一个模型，它的预测非常接近实际标签值。总而言之，判断回归问题的一个很好的经验法则是，任何有关“多少”的问题很可能就是回归问题。</p><h5 id="1-4-1-2-分类"><a href="#1-4-1-2-分类" class="headerlink" title="1.4.1.2. 分类"></a>1.4.1.2. 分类</h5><p>回归模型可以很好解决“有多少”的问题，分类模型则关注“哪一个”的问题。 在<strong>分类问题（classification）</strong>中，我们希望模型能够预测样本属于哪个类别（category，正式称为类（class））。 例如，对于手写数字，我们可能有10类，分别数字0到9。</p><p>最简单的分类问题是只有两类，我们称之为“二元分类”。在回归中，我们训练一个回归函数来输出一个数值； 而在分类中，我们训练一个分类器，它的输出即为预测的类别。</p><p>给定一个样本特征，我们的模型为每个可能的类分配一个概率。 比如，之前的猫狗分类例子中，分类器可能会输出图像是猫的概率为0.9，即分类器90%确定图像描绘的是一只猫。 预测类别的概率的大小传达了一种模型的不确定性。</p><p>当我们有两个以上的类别时，我们把这个问题称为<strong>多元分类（multiclass classification）</strong>问题。与解决回归问题不同，分类问题的常见损失函数被称为<strong>交叉熵（cross-entropy）</strong>。</p><p>分类可能变得比二元分类、多元分类复杂得多。 例如，有一些分类任务的变体可以用于寻找层次结构，层次结构假定在许多类之间存在某种关系。 因此，并不是所有的错误都是均等的。 我们宁愿错误地分入一个相关的类别，也不愿错误地分入一个遥远的类别，这通常被称为<strong>层次分类(hierarchical classification)</strong>。</p><h5 id="1-4-1-3-标记问题"><a href="#1-4-1-3-标记问题" class="headerlink" title="1.4.1.3. 标记问题"></a>1.4.1.3. 标记问题</h5><p>有些分类问题很适合于二元分类或多元分类，例如判断图片中是猫或者狗或者猪。但有时候图片中有多种对象（多种动物），或者出现了分类器不认识的新物种，那么分类模型就不适用了，此时我们可能想让模型描绘输入图像的内容。</p><p>学习预测不相互排斥的类别的问题称为<strong>多标签分类（multi-label classification）</strong>。如在对文献进行标记时机器学习算法可以提供临时标签，直到每一篇文章都有严格的人工审核。</p><h5 id="1-4-1-4-搜索"><a href="#1-4-1-4-搜索" class="headerlink" title="1.4.1.4. 搜索"></a>1.4.1.4. 搜索</h5><p>有时，我们不仅仅希望输出为一个类别或一个实值。 在信息检索领域，我们希望对一组项目进行排序。 以网络搜索为例，我们的目标不是简单的“查询（query）-网页（page）”分类，而是在海量搜索结果中找到用户最需要的那部分。 搜索结果的排序也十分重要，我们的学习算法需要输出有序的元素子集。 即使结果集是相同的，集内的顺序有时却很重要。</p><p>该问题的一种可能的解决方案：首先为集合中的每个元素分配相应的相关性分数，然后检索评级最高的元素。谷歌搜索引擎背后最初的秘密武器就是这种评分系统的早期例子，但它的奇特之处在于它不依赖于实际的查询。如今，搜索引擎使用机器学习和用户行为模型来获取网页相关性得分，很多学术会议也致力于这一主题。</p><h5 id="1-4-1-5-推荐系统"><a href="#1-4-1-5-推荐系统" class="headerlink" title="1.4.1.5. 推荐系统"></a>1.4.1.5. 推荐系统</h5><p>另一类与搜索和排名相关的问题是<strong>推荐系统（recommender system）</strong>，它的目标是向特定用户进行“个性化”推荐。 例如，对于电影推荐，科幻迷和喜剧爱好者的推荐结果页面可能会有很大不同。 类似的应用也会出现在零售产品、音乐和新闻推荐等等。</p><p>总的来说，推荐系统会为“给定用户和物品”的匹配性打分，这个“分数”可能是估计的评级或购买的概率。 由此，对于任何给定的用户，推荐系统都可以检索得分最高的对象集，然后将其推荐给用户。以上只是简单的算法，而工业生产的推荐系统要先进得多，它会将详细的用户活动和项目特征考虑在内。 推荐系统算法经过调整，可以捕捉一个人的偏好。</p><p>尽管推荐系统具有巨大的应用价值，但单纯用它作为预测模型仍存在一些缺陷。 首先，我们的数据只包含“审查后的反馈”：用户更倾向于给他们感觉强烈的事物打分。 例如，在五分制电影评分中，会有许多五星级和一星级评分，但三星级却明显很少。 此外，推荐系统有可能形成反馈循环：推荐系统首先会优先推送一个购买量较大（可能被认为更好）的商品，然而目前用户的购买习惯往往是遵循推荐算法，但学习算法并不总是考虑到这一细节，进而更频繁地被推荐。 综上所述，关于如何处理审查、激励和反馈循环的许多问题，都是重要的开放性研究问题。</p><h5 id="1-4-1-6-序列学习"><a href="#1-4-1-6-序列学习" class="headerlink" title="1.4.1.6. 序列学习"></a>1.4.1.6. 序列学习</h5><p>以上大多数问题都具有固定大小的输入和产生固定大小的输出。在这些情况下，模型只会将输入作为生成输出的“原料”，而不会“记住”输入的具体内容。</p><p>但是如果输入是连续的，我们的模型可能就需要拥有“记忆”功能。 比如在处理视频时，每个视频片段可能由不同数量的帧组成。 通过前一帧的图像，我们可能对后一帧中发生的事情更有把握。 语言也是如此，机器翻译的输入和输出都为文字序列。</p><p>这些问题是<strong>序列学习</strong>的实例，是机器学习最令人兴奋的应用之一。 序列学习需要摄取输入序列或预测输出序列，或两者兼而有之。 具体来说，输入和输出都是可变长度的序列，例如机器翻译和从语音中转录文本。 虽然不可能考虑所有类型的序列转换，但以下特殊情况值得一提。</p><ol><li><em>标记和解析</em>。这涉及到用属性注释文本序列。 换句话说，输入和输出的数量基本上是相同的。 例如，我们可能想知道动词和主语在哪里，或者，我们可能想知道哪些单词是命名实体。</li><li><em>自动语音识别</em>。在语音识别中，输入序列是说话人的录音，输出序列是说话人所说内容的文本记录。 它的挑战在于，与文本相比，音频帧多得多（声音通常以8kHz或16kHz采样）。 也就是说，音频和文本之间没有1:1的对应关系，因为数千个样本可能对应于一个单独的单词。 这也是“序列到序列”的学习问题，其中输出比输入短得多。</li><li><em>文本到语音</em>。这与自动语音识别相反。 换句话说，输入是文本，输出是音频文件。 在这种情况下，输出比输入长得多。 虽然人类很容易识判断发音别扭的音频文件，但这对计算机来说并不是那么简单。</li><li><em>机器翻译</em>。 在语音识别中，输入和输出的出现顺序基本相同。 而在机器翻译中，颠倒输入和输出的顺序非常重要。 换句话说，虽然我们仍将一个序列转换成另一个序列，但是输入和输出的数量以及相应序列的顺序大都不会相同。</li></ol><p>其他学习任务也有序列学习的应用。 例如，确定“用户阅读网页的顺序”是二维布局分析问题。 再比如，对话问题对序列的学习更为复杂：确定下一轮对话，需要考虑对话历史状态以及现实世界的知识…… 如上这些都是热门的序列学习研究领域。</p><h4 id="1-4-2-无监督学习"><a href="#1-4-2-无监督学习" class="headerlink" title="1.4.2. 无监督学习"></a>1.4.2. 无监督学习</h4><p>监督学习，即我们向模型提供巨大数据集：每个样本包含特征和相应标签值。</p><p><strong>无监督学习（unsupervised learning）</strong>是一类数据中不含有“目标”的机器学习问题，即老板可能会给你一大堆数据，然后让你用它做一些数据科学研究，却没有对结果有要求。</p><p>无监督学习应用举例：</p><ol><li><strong>聚类（clustering）</strong>问题：没有<strong>标签</strong>的情况下，我们是否能给数据分类呢？比如，给定一组照片，我们能把它们分成风景照片、狗、婴儿、猫和山峰的照片吗？同样，给定一组用户的网页浏览记录，我们能否将具有相似行为的用户聚类呢？</li><li><strong>主成分分析（principal component analysis）</strong>问题：我们能否找到少量的参数来准确地捕捉数据的线性相关属性？比如，一个球的运动轨迹可以用球的速度、直径和质量来描述。再比如，裁缝们已经开发出了一小部分参数，这些参数相当准确地描述了人体的形状，以适应衣服的需要。</li><li><strong>因果关系（causality）</strong>和<strong>概率图模型（probabilistic graphical models）</strong>问题：我们能否描述观察到的许多数据的根本原因？例如，如果我们有关于房价、污染、犯罪、地理位置、教育和工资的人口统计数据，我们能否简单地根据经验数据发现它们之间的关系？</li><li><strong>生成对抗性网络（generative adversarial networks）</strong>：为我们提供一种合成数据的方法，甚至像图像和音频这样复杂的非结构化数据。潜在的统计机制是检查真实和虚假数据是否相同的测试，它是无监督学习的另一个重要而令人兴奋的领域。</li></ol><h4 id="1-4-3-与环境互动"><a href="#1-4-3-与环境互动" class="headerlink" title="1.4.3. 与环境互动"></a>1.4.3. 与环境互动</h4><p>到目前为止，不管是监督学习还是无监督学习，我们都会预先获取大量数据，然后启动模型，不再与环境交互。这里所有学习都是在算法与环境断开后进行的，被称为<strong>离线学习（offline learning）</strong>。这种简单的离线学习有它的魅力。 好的一面是，我们可以孤立地进行模式识别，而不必分心于其他问题。 但缺点是，解决的问题相当有限。</p><p>人们可能会期望人工智能不仅能够做出预测，而且能够<strong>与真实环境互动</strong>。 与预测不同，“与真实环境互动”实际上会影响环境。 这里的人工智能是“智能代理”，而不仅是“预测模型”。 因此，我们必须考虑到它的行为可能会影响未来的观察结果。</p><p>考虑“与真实环境互动”将打开一整套新的建模问题。</p><ul><li>环境还记得我们以前做过什么吗？</li><li>环境是否有助于我们建模？例如，用户将文本读入语音识别器。</li><li>环境是否想要打败模型？例如，一个对抗性的设置，如垃圾邮件过滤或玩游戏？</li><li>环境是否重要？</li><li>环境是否变化？例如，未来的数据是否总是与过去相似，还是随着时间的推移会发生变化？是自然变化还是响应我们的自动化工具而发生变化？</li><li>etc…</li></ul><p>当训练和测试数据不同时，最后一个问题提出了<strong>分布偏移（distribution shift）</strong>的问题。 接下来的强化学习问题，是一类明确考虑与环境交互的问题。</p><h4 id="1-4-4-强化学习"><a href="#1-4-4-强化学习" class="headerlink" title="1.4.4. 强化学习"></a>1.4.4. 强化学习</h4><p><strong>强化学习（reinforcement learning）</strong>使用机器学习开发与环境交互并采取行动。 这可能包括应用到机器人、对话系统，甚至开发视频游戏的人工智能（AI）。</p><p> <strong>深度强化学习（deep reinforcement learning）</strong>将深度学习应用于强化学习的问题，是非常热门的研究领域。 突破性的深度Q网络（Q-network）在雅达利游戏中仅使用视觉输入就击败了人类， 以及 AlphaGo 程序在棋盘游戏围棋中击败了世界冠军，是两个突出强化学习的例子。</p><p>在强化学习问题中，<strong>agent</strong>在一系列的时间步骤上与环境交互。 在每个特定时间点，agent从环境接收一些<strong>观察（observation）</strong>，并且必须选择一个<strong>动作（action）</strong>，然后通过某种机制（有时称为执行器）将其传输回环境，最后agent从环境中获得<strong>奖励（reward）</strong>。 此后新一轮循环开始，agent接收后续观察，并选择后续操作，依此类推。强化学习的目标是产生一个好的<strong>策略（policy）</strong>。强化学习agent选择的“动作”受策略控制，即一个从环境观察映射到行动的功能。</p><p>强化学习框架的通用性十分强大。 例如，我们可以将任何监督学习问题转化为强化学习问题。 假设我们有一个分类问题，我们可以创建一个强化学习agent，每个分类对应一个“动作”。 然后，我们可以创建一个环境，该环境给予agent的奖励。 这个奖励与原始监督学习问题的损失函数是一致的。</p><p>强化学习还可以解决许多监督学习无法解决的问题。 例如，在监督学习中，我们总是希望输入与正确的标签相关联。 但在强化学习中，我们并不假设环境告诉agent每个观测的最优动作。 一般来说，agent只是得到一些奖励。 此外，环境甚至可能不会告诉我们是哪些行为导致了奖励。</p><p>以强化学习在国际象棋的应用为例。 唯一真正的奖励信号出现在游戏结束时：当agent获胜时，agent可以得到奖励1；当agent失败时，agent将得到奖励-1。 因此，强化学习者必须处理<strong>学分分配（credit assignment）</strong>问题：决定哪些行为是值得奖励的，哪些行为是需要惩罚的。</p><p>强化学习可能还必须处理部分可观测性问题。 也就是说，当前的观察结果可能无法阐述有关当前状态的所有信息。 比方说，一个清洁机器人发现自己被困在一个许多相同的壁橱的房子里。 推断机器人的精确位置（从而推断其状态），需要在进入壁橱之前考虑它之前的观察结果。</p><p>最后，在任何时间点上，强化学习agent可能知道一个好的策略，但可能有许多更好的策略从未尝试过的。 强化学习agent必须不断地做出选择：是应该利用当前最好的策略，还是探索新的策略空间（放弃一些短期回报来换取知识）。</p><p>当环境可被完全观察到时，我们将强化学习问题称为<strong>马尔可夫决策过程（markov decision process）</strong>。 当状态不依赖于之前的操作时，我们称该问题为上<strong>下文赌博机（contextual bandit problem）</strong>。 当没有状态，只有一组最初未知回报的可用动作时，这个问题就是经典的<strong>多臂赌博机（multi-armed bandit problem）</strong>。</p><h3 id="1-5-深度学习近况"><a href="#1-5-深度学习近况" class="headerlink" title="1.5. 深度学习近况"></a>1.5. 深度学习近况</h3><p>随机存取存储器没有跟上数据增长的步伐，同时算力的增长速度已经超过了现有数据的增长速度。 这意味着统计模型需要提高内存效率（这通常是通过添加非线性来实现的），同时由于计算预算的增加，能够花费更多时间来优化这些参数。 因此，机器学习和统计的关注点从（广义的）线性模型和核方法转移到了深度神经网络。 这也造就了许多深度学习的中流砥柱，如<strong>多层感知机</strong> 、<strong>卷积神经网络</strong>  、<strong>长短期记忆网络</strong>和<strong>Q学习</strong>  ，在相对休眠了相当长一段时间之后，在过去十年中被“重新发现”。</p><p>最近十年，在统计模型、应用和算法方面的进展就像寒武纪大爆发——历史上物种飞速进化的时期。 事实上，最先进的技术不仅仅是将可用资源应用于几十年前的算法的结果。 下面列举了帮助研究人员在过去十年中取得巨大进步的想法（虽然只触及了皮毛）：</p><ul><li>新的容量控制方法，如dropout，有助于减轻过拟合的危险。这是通过在整个神经网络中应用<strong>噪声注入</strong>来实现的，出于训练目的，用随机变量来代替权重。</li><li><strong>注意力机制</strong>解决了困扰统计学一个多世纪的问题：如何在不增加可学习参数的情况下增加系统的记忆和复杂性。研究人员通过使用只能被视为可学习的指针结构找到了一个优雅的解决方案。不需要记住整个文本序列（例如用于固定维度表示中的机器翻译），所有需要存储的都是指向翻译过程的中间状态的指针。这大大提高了长序列的准确性，因为模型在开始生成新序列之前不再需要记住整个序列。</li><li>多阶段设计。例如，存储器网络和神经编程器-解释器 。它们允许统计建模者描述用于推理的迭代方法。这些工具允许重复修改深度神经网络的内部状态，从而执行推理链中的后续步骤，类似于处理器如何修改用于计算的存储器。</li><li>另一个关键的发展是<strong>生成对抗网络</strong> 的发明。传统模型中，密度估计和生成模型的统计方法侧重于找到合适的概率分布（通常是近似的）和抽样算法。因此，这些算法在很大程度上受到统计模型固有灵活性的限制。<strong>生成式对抗性网络</strong>的关键创新是用具有可微参数的任意算法代替采样器。然后对这些数据进行调整，使得鉴别器（实际上是一个双样本测试）不能区分假数据和真实数据。通过使用任意算法生成数据的能力，它为各种技术打开了密度估计的大门。</li><li>在许多情况下，单个GPU不足以处理可用于训练的大量数据。在过去的十年中，构建并行和分布式训练算法的能力有了显著提高。设计可伸缩算法的关键挑战之一是深度学习优化的主力——<strong>随机梯度下降</strong>，它依赖于相对较小的小批量数据来处理。同时，小批量限制了GPU的效率。因此，在1024个GPU上进行训练，例如每批32个图像的小批量大小相当于总计约32000个图像的小批量。</li><li>并行计算的能力也对强化学习的进步做出了相当关键的贡献。这导致了计算机在围棋、雅达里游戏、星际争霸和物理模拟（例如，使用MuJoCo）中实现超人性能的重大进步。简而言之，如果有大量的（状态、动作、奖励）三元组可用，即只要有可能尝试很多东西来了解它们之间的关系，强化学习就会发挥最好的作用。仿真提供了这样一条途径。</li><li><strong>深度学习框架</strong>在传播思想方面发挥了至关重要的作用。允许轻松建模的第一代框架包括Caffe、Torch和Theano。许多开创性的论文都是用这些工具写的。到目前为止，它们已经被TensorFlow（通常通过其高级API Keras使用）、CNTK、Caffe 2和Apache MXNet所取代。第三代工具，即用于深度学习的命令式工具，使用类似于Python NumPy的语法来描述模型。这个想法被PyTorch、MXNet的Gluon API和Jax都采纳了。</li></ul><p>“系统研究人员构建更好的工具”和“统计建模人员构建更好的神经网络”之间的分工大大简化了工作。 例如，在2014年，对于卡内基梅隆大学机器学习博士生来说，训练线性回归模型曾经是一个不容易的作业问题。 而现在，这项任务只需不到10行代码就能完成，这让每个程序员轻易掌握了它。</p><h3 id="1-6-小结"><a href="#1-6-小结" class="headerlink" title="1.6. 小结"></a>1.6. 小结</h3><p>中间还有成功案例和特点的部分，略微有些难懂，先行跳过，日后再来总结。</p><ul><li>机器学习研究计算机系统如何利用经验（通常是数据）来提高特定任务的性能。它结合了统计学、数据挖掘和优化的思想。通常，它是被用作实现人工智能解决方案的一种手段。</li><li>表示学习作为机器学习的一类，其研究的重点是如何自动找到合适的数据表示方式。深度学习是通过学习多层次的转换来进行的多层次的表示学习。</li><li>深度学习不仅取代了传统机器学习的浅层模型，而且取代了劳动密集型的特征工程。</li><li>最近在深度学习方面取得的许多进展，大都是由廉价传感器和互联网规模应用所产生的大量数据，以及（通过GPU）算力的突破来触发的。</li><li>整个系统优化是获得高性能的关键环节。有效的深度学习框架的开源使得这一点的设计和实现变得非常容易。</li></ul>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;p&gt;终于得以继续升学，可以做一些自己喜欢的事情了，早就想跟李沐老师学习一下深度学习，该系列为《动手学深度学习v2》的笔记，希望自己能在九月前完结。&lt;br&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://silencezheng.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Elastic Search7.x学习笔记</title>
    <link href="http://silencezheng.top/2022/02/13/article31/"/>
    <id>http://silencezheng.top/2022/02/13/article31/</id>
    <published>2022-02-13T07:41:30.000Z</published>
    <updated>2022-05-27T14:15:46.188Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-前言："><a href="#1-前言：" class="headerlink" title="1. 前言："></a>1. 前言：</h2><p>先从官方入门视频、官方文档和一些入门博客入手学习。<br><span id="more"></span><br>推荐博客系列：</p><p><a href="https://www.cnblogs.com/qdhxhz/p/11448451.html">https://www.cnblogs.com/qdhxhz/p/11448451.html</a></p><h2 id="2-入门："><a href="#2-入门：" class="headerlink" title="2. 入门："></a>2. 入门：</h2><ul><li>如何下载／运行 Elasticsearch<ul><li>使用容器环境安装ES和Kibana</li><li>在Linux系统中配置ES集群</li></ul></li><li>Elasticsearch概念简析</li><li>通过REST API执行CRUD<ul><li>ES集群相关命令</li><li>索引CRUD命令</li><li>文档CRUD命令</li><li>举例</li></ul></li><li>关于查询<ul><li>Query查询</li><li>Filter查询</li><li>举例</li><li>复合查询</li></ul></li><li>聚合：Elasticsearch 的面向和分析的主功能<ul><li>聚合概念</li><li>Metric聚合</li><li>Bucket聚合</li></ul></li></ul><h3 id="2-1-如何下载／运行-Elasticsearch，及其先决条件"><a href="#2-1-如何下载／运行-Elasticsearch，及其先决条件" class="headerlink" title="2.1. 如何下载／运行 Elasticsearch，及其先决条件"></a>2.1. 如何下载／运行 Elasticsearch，及其先决条件</h3><p>可以通过直接下载或者包管理工具下载ES和Kibana到本地，也可以使用容器安装ELK。</p><h4 id="2-1-1-使用容器环境安装ES和Kibana"><a href="#2-1-1-使用容器环境安装ES和Kibana" class="headerlink" title="2.1.1. 使用容器环境安装ES和Kibana"></a>2.1.1. 使用容器环境安装ES和Kibana</h4><p>通过docker可以运行单节点ES，也可以运行ES集群（通过compose），这里先安装单节点ES和单节点Kibana组成集群。<br>使用容器环境安装ES：<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html">https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html</a>  </p><ol><li>创建桥接网络elastic  <code>docker network create elastic</code></li><li>拉取es7镜像  <code>docker pull docker.elastic.co/elasticsearch/elasticsearch:7.17.0</code></li><li>创建容器es01-test，网络为elastic，镜像为es7 <code>docker run --name es01-test --net elastic -p 127.0.0.1:9200:9200 -p 127.0.0.1:9300:9300 -e &quot;discovery.type=single-node&quot; docker.elastic.co/elasticsearch/elasticsearch:7.17.0</code>  <strong>注意： 9200 是供 http 访问端口，9300 是供 tcp 访问的端口，如果不做端口映射，浏览器就不能访问 elasticsearch 的服务。</strong></li><li>查看es <code>http://127.0.0.1:9200</code></li></ol><p>使用容器环境安装Kibana：<a href="https://www.elastic.co/guide/en/kibana/current/docker.html">https://www.elastic.co/guide/en/kibana/current/docker.html</a></p><ol><li>在新的终端，拉kibana镜像 <code>docker pull docker.elastic.co/kibana/kibana:7.17.0</code></li><li>跑容器kib01-test <code>docker run --name kib01-test --net elastic -p 127.0.0.1:5601:5601 -e &quot;ELASTICSEARCH_HOSTS=http://es01-test:9200&quot; docker.elastic.co/kibana/kibana:7.17.0</code></li><li>查看kibana <code>http://localhost:5601</code>   这里kibana是没有数据的，需要添加，可以添加一个示例数据，我这里添加web log。</li></ol><p>终止、移除容器：</p><pre><code>docker stop es01-testdocker stop kib01-testdocker network rm elasticdocker rm es01-testdocker rm kib01-test</code></pre><h4 id="2-1-2-在Linux系统上配置Elasticsearch集群"><a href="#2-1-2-在Linux系统上配置Elasticsearch集群" class="headerlink" title="2.1.2. 在Linux系统上配置Elasticsearch集群"></a>2.1.2. 在Linux系统上配置Elasticsearch集群</h4><p>集群基础配置：<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/important-settings.html#path-settings">https://www.elastic.co/guide/en/elasticsearch/reference/7.17/important-settings.html#path-settings</a><br>在elasticsearch.yml中可以配置存储路径（不建议动），集群名称，节点名称，网络等等。<strong>以及重要生产环境配置：discovery和cluster formation</strong>，这两项配置能使集群中的节点发现彼此并选举出一个主节点（master node）。<br>cluster.name：设置集群名称<br>node.name：设置节点名称<br>network.host：设置网络地址，默认是<code>127.0.0.1和[::1]</code>，可以用于开发但是不能用于生产。修改这个IP会使ES从开发模式变成<strong>生产模式</strong>，ES启动检测会发生变化。</p><p><strong>discovery.seed**</strong>_hosts<strong>：默认情况下，没有进行网络配置时，ES会绑定本地IP并监听9300和9305端口，与所在主机上的其他ES节点构成集群。当想要进行集群配置时，该参数可以接受集群中所有符合主节点条件的节点地址构成的 YAML 序列或数组（以 - 开头）。每个地址可以是 IP 地址，也可以是能够通过 DNS 解析为一个或多个 IP 地址的主机名。总之该参数下的节点能够被选举为主节点。</strong>cluster.initial<strong><strong>_master</strong></strong>_nodes<strong>：当第一次启动 Elasticsearch 集群时， 集群引导步骤会确定在第一次选举中计票的符合主节点资格的节点集。在</strong>生产模式<strong>下启动新集群时，必须通过此设置明确列出应在第一次选举中计票的符合主节点选举条件的节点集。</strong>注意！在集群首次启动成功后需要在各节点删除此配置，不要在重启集群或向集群中加入新节点时使用该配置** 该配置项只能使用节点名称（node name），以YAML数组形式列出。</p><h3 id="2-2-Elasticsearch概念简析"><a href="#2-2-Elasticsearch概念简析" class="headerlink" title="2.2. Elasticsearch概念简析"></a>2.2. Elasticsearch概念简析</h3><h4 id="2-2-1-Index"><a href="#2-2-1-Index" class="headerlink" title="2.2.1. Index"></a>2.2.1. Index</h4><p>索引index是文档doc的容器，是一类文档的集合。<br>作为名词，类比关系型数据库，索引相当于SQL中的一个Database。索引由其名称(必须为全小写字符)进行标识。<br>作为动词，含义为保存一个文档到索引的过程。这类似于SQL语句中的 INSERT关键词。如果该文档已存在时那就相当于数据库的UPDATE。<br>关系型数据库通过增加一个B+树索引到指定的列上，以便提升数据检索速度。索引ElasticSearch 使用了一个叫做<strong>倒排索引</strong>的结构来达到相同的目的。<strong>mapping</strong>就是index的结构，相当于关系型数据库中的库表结构</p><h4 id="2-2-2-Document、Doc"><a href="#2-2-2-Document、Doc" class="headerlink" title="2.2.2. Document、Doc"></a>2.2.2. Document、Doc</h4><p>Index 里面单条的记录称为Document（文档）。等同于关系型数据库表中的行。</p><ul><li>_index：文档所属索引名称。</li><li>_type：文档所属类型名。</li><li>_id：Doc的主键。在写入的时候，可以指定该Doc的ID值，如果不指定，则系统自动生成一个唯一的UUID值。</li><li>_version：文档的版本信息。Elasticsearch通过使用version来保证对文档的变更能以正确的顺序执行，避免乱序造成的数据丢失。</li><li>_seq_no：严格递增的顺序号，每个文档一个，Shard级别严格递增，保证后写入的Doc的_seq_no大于先写入的Doc的_seq_no。</li><li>_primary_term：_/primary_term也和_seq_no一样是一个整数，每当Primary Shard发生重新分配时，比如重启，Primary选举等，_primary_term会递增1</li><li>found：查询的ID正确那么ture, 如果 Id 不正确，就查不到数据，found字段就是false。</li><li>_source：文档的原始JSON数据。</li></ul><h4 id="2-2-3-Type（被淘汰）"><a href="#2-2-3-Type（被淘汰）" class="headerlink" title="2.2.3. Type（被淘汰）"></a>2.2.3. Type（被淘汰）</h4><p>在7.0开始，一个索引只能建一个Type为_doc</p><h4 id="2-2-4-Cluster"><a href="#2-2-4-Cluster" class="headerlink" title="2.2.4. Cluster"></a>2.2.4. Cluster</h4><p>ElasticSearch集群实际上是一个分布式系统，它需要具备两个特性：高可用性和可扩展性。</p><ol><li>允许有节点停止服务</li><li>部分节点丢失，不会丢失数据</li><li>随着请求量的不断提升，数据量的不断增长，系统可以将数据分布到其他节点，实现水平扩展</li></ol><p>集群健康值有绿黄红三种状态：</p><ul><li>Green：所有主要分片和复制分片都可用</li><li>Yellow：所有主要分片可用，但不是所有复制分片都可用</li><li>Red：不是所有的主要分片都可用。当集群状态为red，它仍然正常提供服务，它会在现有存活分片中执行请求，我们需要尽快修复故障分片，防止查询数据的丢失。</li></ul><p>集群引导（Bootstrapping）是指首次启动 Elasticsearch集群需要在集群中的一个或多个符合主节点的节点上明确定义初始的主节点集。<br>从技术上讲，在集群中设置一个符合主节点资格的节点就足够cluster.initial_master_nodes了，并且只在设置值中提及该单个节点，但这在集群完全形成之前没有提供容错能力。<br>因此，最好使用至少三个符合主节点资格的节点进行引导，每个节点的cluster.initial_master_nodes设置都包含所有三个节点。</p><h4 id="2-2-5-Node"><a href="#2-2-5-Node" class="headerlink" title="2.2.5. Node"></a>2.2.5. Node</h4><p>节点是一个ElasticSearch的实例，其本质就是一个Java进程。一台机器上可以运行多个ElasticSearch实例，但是建议在生产环境中一台机器上只运行一个ElasticSearch实例。Node是组成集群的一个单独的服务器，用于存储数据并提供集群的搜索和索引功能。<br>与集群一样，节点也有一个唯一名字，默认在节点启动时会生成一个uuid作为节点名，该名字也可以手动指定。<br>单个集群可以由任意数量的节点组成。如果只启动了一个节点，则会形成一个单节点的集群。</p><h4 id="2-2-6-分片（Shard）"><a href="#2-2-6-分片（Shard）" class="headerlink" title="2.2.6. 分片（Shard）"></a>2.2.6. 分片（Shard）</h4><p>Primary Shard(主分片）</p><ul><li>ES中的shard用来解决节点的容量上限问题，通过主分片，可以将数据分布到集群内的所有节点之上。</li><li>一个节点对应一个ES实例</li><li>一个节点可以有多个index（索引）</li><li>一个index可以有多个shard（分片）</li><li>一个分片是一个lucene index（此处的index是lucene自己的概念，与ES的index不是一回事）</li><li>主分片数是在索引创建时指定，后续不允许修改，除非Reindex</li><li>一个索引中的数据保存在多个分片中(默认为一个)，相当于水平分表。一个分片便是一个Lucene 的实例，它本身就是一个完整的搜索引擎。我们的文档被存储和索引到分片内，但是应用程序是直接与索引而不是与分片进行交互。</li></ul><p>Replica Shard（副本）</p><ul><li>服务高可用：由于数据只有一份,如果一个node挂了,那存在上面的数据就都丢了,有了replicas,只要不是存储这条数据的node全挂了,数据就不会丢。因此分片副本不会与主分片分配到同一个节点</li><li>扩展性能：通过在所有replicas上并行搜索提高搜索性能.由于replicas上的数据是近实时的(near realtime),因此所有replicas都能提供搜索功能,通过设置合理的replicas数量可以极高的提高搜索吞吐量</li></ul><p>分片设定</p><ul><li>对于生产环境中分片的设定，需要提前做好容量规划，因为主分片数是在索引创建时预先设定的，后续无法修改。</li><li>分片数过小会导致后续无法增加节点进行水平扩展、导致分片的数据量太大，数据在重新分配时耗时</li><li>分片数过大会影响搜索结果的相关性打分，影响统计结果的准确性、单个节点上过多的分片，会导致资源浪费，同时也会影响性能；</li></ul><h4 id="2-2-7-倒排索引"><a href="#2-2-7-倒排索引" class="headerlink" title="2.2.7. 倒排索引"></a>2.2.7. 倒排索引</h4><p>ES的搜索功能是基于lucene,而lucene搜索的基本原理就是倒叙索引,倒序排序的结果跟分词的类型有关。<br>假设我们搜索谷歌地图之父,搜索流程会是这样分词,分词插件将句子分为3个term 谷歌,地图,之父，然后将这3个term拿到倒叙索引中去查找(会很高效,比如二分查找),如果匹配到了就拿对应的文档id,获得文档内容。<br>TF、IDF在结果排序中起到了作用。</p><h3 id="2-3-通过REST-API执行CRUD"><a href="#2-3-通过REST-API执行CRUD" class="headerlink" title="2.3. 通过REST API执行CRUD"></a>2.3. 通过REST API执行CRUD</h3><h4 id="2-3-1-ES集群相关命令"><a href="#2-3-1-ES集群相关命令" class="headerlink" title="2.3.1. ES集群相关命令"></a>2.3.1. ES集群相关命令</h4><p>_cat系列提供了一系列查询Elasticsearch集群状态的接口</p><ul><li>_cat/shards          #查看各shard的详细情况</li><li>_cat/nodes           #查看所有节点信息</li><li>_cat/count           #查看当前集群的doc数量</li><li>_cat/allocation      #查看单节点的shard分配整体情况</li><li>_cat/health          #查看集群当前状态：红、黄、绿</li><li>…etc</li><li>每个命令都支持使用?v参数，让输出内容表格显示表头; pretty则让输出缩进更规范。 如<code>GET _cat/health?v</code></li><li>？pretty参数可以使输出更美观<h4 id="2-3-2-索引CRUD命令"><a href="#2-3-2-索引CRUD命令" class="headerlink" title="2.3.2. 索引CRUD命令"></a>2.3.2. 索引CRUD命令</h4>查询索引举例<br>_cat/indices         #查看集群中所有index的详细信息<br>_cat/indices/{index} #查看集群中指定index的详细信息<br>_cat/segments        #查看各index的segment详细信息,包括segment名, 所属shard, 内存(磁盘)占用大小, 是否刷盘<br>_cat/segments/{index}#查看指定index的segment详细信息<br>_cat/indices?v&amp;health=yellow   #查询健康状态为yellow的索引<br>_cat/indices?v&amp;health=yellow&amp;s=docs.count:desc #根据文档数量进行索引排序</li></ul><p>创建索引举例<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">PUT student</span><br><span class="line">&#123;</span><br><span class="line">  &quot;settings&quot;:&#123;</span><br><span class="line">    <span class="string">&quot;number_of_shards&quot;</span>:<span class="number">1</span>,</span><br><span class="line">    <span class="string">&quot;number_of_replicas&quot;</span>:<span class="number">1</span></span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;mappings&quot;:&#123;</span><br><span class="line">      <span class="string">&quot;properties&quot;</span>:&#123;</span><br><span class="line">        <span class="string">&quot;name&quot;</span>:&#123;<span class="string">&quot;type&quot;</span>:<span class="string">&quot;text&quot;</span>&#125;,</span><br><span class="line">        &quot;sid&quot;:&#123;<span class="string">&quot;type&quot;</span>:<span class="string">&quot;integer&quot;</span>&#125;,</span><br><span class="line">        &quot;<span class="selector-tag">address</span>&quot;:&#123;<span class="string">&quot;type&quot;</span>:<span class="string">&quot;keyword&quot;</span>&#125;,</span><br><span class="line">        &quot;age&quot;:&#123;<span class="string">&quot;type&quot;</span>:<span class="string">&quot;integer&quot;</span>&#125;,</span><br><span class="line">        &quot;interests&quot;:&#123;<span class="string">&quot;type&quot;</span>:<span class="string">&quot;text&quot;</span>&#125;,</span><br><span class="line">        &quot;birthday&quot;:&#123;<span class="string">&quot;type&quot;</span>:<span class="string">&quot;date&quot;</span>&#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure><br>查看创建结果<code>GET /_cat/indices/student?v</code></p><p>更新索引<br>使用PUT也可以更新索引</p><p>删除索引<br><code>curl -X DELETE &quot;localhost:9200/index-name&quot;</code></p><h4 id="2-3-3-文档CRUD命令"><a href="#2-3-3-文档CRUD命令" class="headerlink" title="2.3.3. 文档CRUD命令"></a>2.3.3. 文档CRUD命令</h4><p>POST /uri用于创建，DELETE /uri/xxx用于删除，PUT /uri/xxx用于更新或创建，GET /uri/xxx用于查询。<br>POST与PUT的区别:</p><ol><li>在ES中,如果不确定文档的ID，那么就需要用POST，它可以自己生成唯一的文档ID。如果确定文档的ID，那么就可以用PUT，当然也可以用POST，它们都可以创建或修改文档（如果是修改，那么_version版本号提高1）</li><li>PUT、GET、DELETE是幂等的，而POST并不一定是幂等。如果你对POST也指定了文档ID,那它其实和PUT没啥区别，那它就是幂等。如果你没有指定文档ID那么就不是幂等操作了，因为同一数据，你执行多次POST，那么生成多个UUID的文档，也就是每POST一次都会新增一条数据</li></ol><p>创建文档：<br>PUT方式：<code>PUT  /student/_doc/1 &#123;&quot;name&quot;: &quot;xxx&quot;,&quot;country&quot;: &quot;tj&quot;,&quot;age&quot;: &quot;3&quot;,&quot;date&quot;: &quot;2019-09-04&quot;&#125;</code><br>POST方式：<code>POST  /student/_doc &#123;&quot;name&quot;: &quot;xxx&quot;,&quot;country&quot;: &quot;tj&quot;,&quot;age&quot;: &quot;3&quot;,&quot;date&quot;: &quot;2019-09-04&quot;&#125;</code></p><pre><code>- POST也可以指定文档ID,如果指定文档ID,那么就和PUT没有区别。ID不存在则创建，存在则更新并且\_version版本+1.- 批量插入：通过\_bulk接口，先声明所属index（一个json），而后跟n条数据（n个json）</code></pre><p>查看文档：<code>GET  /student/_doc/1</code><br>更新文档：<br>PUT和POST执行的时候，如果指定的文档ID存在，那么就可以执行更新操作。不过它们执行的是全量更新，如果需要单独对某字段更新我们可以使用关键字_update。如<code>POST /student/_update/1 &#123;&quot;doc&quot; : &#123;&quot;age&quot;: 5&#125;&#125;</code><br>还可以通过<code>POST student/_update_by_query</code>api进行更新操作，也就是通过query进行更新<br>删除文档：<code>DELETE /&#123;index&#125;/&#123;type&#125;/&#123;id&#125;</code></p><h4 id="举点例子，实践一下上面的概念"><a href="#举点例子，实践一下上面的概念" class="headerlink" title="举点例子，实践一下上面的概念"></a>举点例子，实践一下上面的概念</h4><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line">查看es页面的接口信息：</span><br><span class="line">GET / </span><br><span class="line"></span><br><span class="line">查看集群文档数量：</span><br><span class="line">GET <span class="regexp">/_cat/</span><span class="keyword">count</span>?v</span><br><span class="line"></span><br><span class="line">查看集群中所有index的详细信息</span><br><span class="line">GET <span class="regexp">/_cat/i</span>ndices</span><br><span class="line"></span><br><span class="line">创建索引：</span><br><span class="line">PUT student</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;settings&quot;</span>:&#123;</span><br><span class="line">    <span class="string">&quot;number_of_shards&quot;</span>:<span class="number">1</span>,</span><br><span class="line">    <span class="string">&quot;number_of_replicas&quot;</span>:<span class="number">1</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">&quot;mappings&quot;</span>:&#123;</span><br><span class="line">      <span class="string">&quot;properties&quot;</span>:&#123;</span><br><span class="line">        <span class="string">&quot;name&quot;</span>:&#123;<span class="string">&quot;type&quot;</span>:<span class="string">&quot;text&quot;</span>&#125;,</span><br><span class="line">        <span class="string">&quot;sid&quot;</span>:&#123;<span class="string">&quot;type&quot;</span>:<span class="string">&quot;integer&quot;</span>&#125;,</span><br><span class="line">        <span class="string">&quot;address&quot;</span>:&#123;<span class="string">&quot;type&quot;</span>:<span class="string">&quot;keyword&quot;</span>&#125;,</span><br><span class="line">        <span class="string">&quot;age&quot;</span>:&#123;<span class="string">&quot;type&quot;</span>:<span class="string">&quot;integer&quot;</span>&#125;,</span><br><span class="line">        <span class="string">&quot;interests&quot;</span>:&#123;<span class="string">&quot;type&quot;</span>:<span class="string">&quot;text&quot;</span>&#125;,</span><br><span class="line">        <span class="string">&quot;birthday&quot;</span>:&#123;<span class="string">&quot;type&quot;</span>:<span class="string">&quot;date&quot;</span>&#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125; </span><br><span class="line">如果不创建索引直接插入数据，则会发生动态mapping</span><br><span class="line"></span><br><span class="line">删除索引：</span><br><span class="line"><span class="keyword">DELETE</span> student  </span><br><span class="line">      </span><br><span class="line">添加数据：</span><br><span class="line">POST student<span class="regexp">/_doc/</span><span class="number">1</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;name&quot;</span>:<span class="string">&quot;李明&quot;</span>,</span><br><span class="line">  <span class="string">&quot;sid&quot;</span>:<span class="number">1</span>,</span><br><span class="line">  <span class="string">&quot;address&quot;</span>:<span class="string">&quot;北京&quot;</span>,</span><br><span class="line">  <span class="string">&quot;age&quot;</span>:<span class="number">32</span>,</span><br><span class="line">  <span class="string">&quot;interests&quot;</span>:<span class="string">&quot;摄影 旅游&quot;</span>,</span><br><span class="line">  <span class="string">&quot;birthday&quot;</span>:<span class="string">&quot;1990-06-19&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">查询数据：</span><br><span class="line">GET student<span class="regexp">/_doc/</span><span class="number">1</span></span><br><span class="line"></span><br><span class="line">更新数据：</span><br><span class="line">PUT student<span class="regexp">/_doc/</span><span class="number">1</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;name&quot;</span>:<span class="string">&quot;李明&quot;</span>,</span><br><span class="line">  <span class="string">&quot;sid&quot;</span>:<span class="number">1</span>,</span><br><span class="line">  <span class="string">&quot;address&quot;</span>:<span class="string">&quot;天津&quot;</span>,</span><br><span class="line">  <span class="string">&quot;age&quot;</span>:<span class="number">32</span>,</span><br><span class="line">  <span class="string">&quot;interests&quot;</span>:<span class="string">&quot;摄影 打游戏&quot;</span>,</span><br><span class="line">  <span class="string">&quot;birthday&quot;</span>:<span class="string">&quot;1990-06-19&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">删除数据：</span><br><span class="line"><span class="keyword">DELETE</span> student<span class="regexp">/_doc/</span><span class="number">1</span></span><br><span class="line"></span><br><span class="line">批量添加数据：</span><br><span class="line">POST _bulk</span><br><span class="line">&#123;<span class="string">&quot;index&quot;</span>:&#123;<span class="string">&quot;_index&quot;</span>:<span class="string">&quot;student&quot;</span>&#125;&#125;</span><br><span class="line">&#123; <span class="string">&quot;name&quot;</span>:<span class="string">&quot;李明&quot;</span>,<span class="string">&quot;sid&quot;</span>:<span class="number">1</span>,<span class="string">&quot;address&quot;</span>:<span class="string">&quot;天津&quot;</span>,<span class="string">&quot;age&quot;</span>:<span class="number">32</span>,<span class="string">&quot;interests&quot;</span>:<span class="string">&quot;摄影 打游戏&quot;</span>,<span class="string">&quot;birthday&quot;</span>:<span class="string">&quot;1990-06-19&quot;</span>&#125;</span><br><span class="line">&#123;<span class="string">&quot;index&quot;</span>:&#123;<span class="string">&quot;_index&quot;</span>:<span class="string">&quot;student&quot;</span>&#125;&#125;</span><br><span class="line">&#123; <span class="string">&quot;name&quot;</span>:<span class="string">&quot;王一&quot;</span>,<span class="string">&quot;sid&quot;</span>:<span class="number">2</span>,<span class="string">&quot;address&quot;</span>:<span class="string">&quot;北京&quot;</span>,<span class="string">&quot;age&quot;</span>:<span class="number">27</span>,<span class="string">&quot;interests&quot;</span>:<span class="string">&quot;旅游 画画&quot;</span>,<span class="string">&quot;birthday&quot;</span>:<span class="string">&quot;1995-07-19&quot;</span>&#125;</span><br><span class="line">&#123;<span class="string">&quot;index&quot;</span>:&#123;<span class="string">&quot;_index&quot;</span>:<span class="string">&quot;student&quot;</span>&#125;&#125;</span><br><span class="line">&#123; <span class="string">&quot;name&quot;</span>:<span class="string">&quot;李二&quot;</span>,<span class="string">&quot;sid&quot;</span>:<span class="number">3</span>,<span class="string">&quot;address&quot;</span>:<span class="string">&quot;天津&quot;</span>,<span class="string">&quot;age&quot;</span>:<span class="number">22</span>,<span class="string">&quot;interests&quot;</span>:<span class="string">&quot;读书 打游戏&quot;</span>,<span class="string">&quot;birthday&quot;</span>:<span class="string">&quot;2000-08-19&quot;</span>&#125;</span><br><span class="line">&#123;<span class="string">&quot;index&quot;</span>:&#123;<span class="string">&quot;_index&quot;</span>:<span class="string">&quot;student&quot;</span>&#125;&#125;</span><br><span class="line">&#123; <span class="string">&quot;name&quot;</span>:<span class="string">&quot;张三&quot;</span>,<span class="string">&quot;sid&quot;</span>:<span class="number">4</span>,<span class="string">&quot;address&quot;</span>:<span class="string">&quot;上海&quot;</span>,<span class="string">&quot;age&quot;</span>:<span class="number">42</span>,<span class="string">&quot;interests&quot;</span>:<span class="string">&quot;滑雪 打篮球&quot;</span>,<span class="string">&quot;birthday&quot;</span>:<span class="string">&quot;1980-09-19&quot;</span>&#125;</span><br><span class="line">&#123;<span class="string">&quot;index&quot;</span>:&#123;<span class="string">&quot;_index&quot;</span>:<span class="string">&quot;student&quot;</span>&#125;&#125;</span><br><span class="line">&#123; <span class="string">&quot;name&quot;</span>:<span class="string">&quot;刘墉&quot;</span>,<span class="string">&quot;sid&quot;</span>:<span class="number">5</span>,<span class="string">&quot;address&quot;</span>:<span class="string">&quot;香港&quot;</span>,<span class="string">&quot;age&quot;</span>:<span class="number">17</span>,<span class="string">&quot;interests&quot;</span>:<span class="string">&quot;摄影 打游戏 购物&quot;</span>,<span class="string">&quot;birthday&quot;</span>:<span class="string">&quot;2005-06-30&quot;</span>&#125;</span><br><span class="line">&#123;<span class="string">&quot;index&quot;</span>:&#123;<span class="string">&quot;_index&quot;</span>:<span class="string">&quot;student&quot;</span>&#125;&#125;</span><br><span class="line">&#123; <span class="string">&quot;name&quot;</span>:<span class="string">&quot;令狐冲&quot;</span>,<span class="string">&quot;sid&quot;</span>:<span class="number">6</span>,<span class="string">&quot;address&quot;</span>:<span class="string">&quot;天津&quot;</span>,<span class="string">&quot;age&quot;</span>:<span class="number">23</span>,<span class="string">&quot;interests&quot;</span>:<span class="string">&quot;写代码 打游戏&quot;</span>,<span class="string">&quot;birthday&quot;</span>:<span class="string">&quot;1999-06-25&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">查询全部：</span><br><span class="line">GET student/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;match_all&quot;</span>: &#123;&#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">删除全部doc：</span><br><span class="line">POST student/_delete_by_query</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;query&quot;</span>:&#123;</span><br><span class="line">    <span class="string">&quot;match_all&quot;</span>: &#123;&#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="2-4-关于查询"><a href="#2-4-关于查询" class="headerlink" title="2.4. 关于查询"></a>2.4. 关于查询</h3><p>简单查询：就是按照文档_id，通过ES的RESTFUL API 的GET请求来进行文档查询<br>条件查询：</p><ul><li>条件查询请求的协议方法是POST方法(为什么使用get也可以查询)</li><li>条件查询需要使用_search关键字</li><li>条件查询需要在请求体内将条件写在一个json体内，如下面的query和filter<br>聚合查询：aggs是聚合查询的关键词<br>对mapping中的字段type，text表示可以进行全文搜索、进行分词的字段，keyword表示用于聚合查询、统计分析的字段，同一个字段可同时支持这两种type</li></ul><h4 id="2-4-1-Query查询"><a href="#2-4-1-Query查询" class="headerlink" title="2.4.1. Query查询"></a>2.4.1. Query查询</h4><p>Query context(查询上下文)这种语句在执行时既要计算文档是否匹配，还要计算文档相对于其他文档的匹配度有多高，匹配度越高，_score 分数就越高</p><ul><li>match查询<ul><li>match query: 知道分词器的存在，会对filed进行分词操作，然后再查询</li><li>match_all: 查询所有文档</li><li>multi_match: 可以指定多个字段</li><li>match_phrase: 短语匹配查询，ElasticSearch引擎首先分析（analyze）查询字符串，从分析后的文本中构建短语查询，这意味着必须匹配短语中的所有分词，并且保证各个分词的相对位置不变</li><li>文档字段属性如果是一个keyword类型，那就需要完全匹配才能命中。好比这个字段值是12345，那么你不论是1234还是123456都不会命中。</li><li>如果是match_phrase,那就是真正的包含关系。好比这个字段值是12345，那么你是1234就会命中，而123456不会命中。因为12345包含1234而不包含123456。</li></ul></li><li>term查询和terms查询<ul><li>term query: 会去倒排索引中寻找确切的term，它并不知道分词器的存在。这种查询适合keyword 、numeric、date</li><li>term:查询某个字段为该关键词的文档（它是相等关系而不是包含关系）</li><li>terms:查询某个字段里含有多个关键词的文档</li></ul></li><li>范围查询<ul><li>range: 实现范围查询</li><li>from, to：指定范围，如年龄、日期等</li><li>include_lower: 是否包含范围的左边界，默认是true</li><li>include_upper: 是否包含范围的右边界，默认是true</li></ul></li><li>wildcard查询<ul><li>允许使用通配符* 和?来进行查询, *代表0个或多个字符, ? 代表任意一个字符</li><li>如<code>&quot;name&quot;: &quot;徐*&quot;  &quot;name&quot;: &quot;徐小?&quot;</code></li><li>中文数据在使用通配符时可能有bug</li></ul></li><li>控制查询返回的数量和位置，通过from，size</li><li>通过_source指定返回的字段，或也可以控制显示要的字段和去除不需要的字段，此时可使用通配符*</li><li>通过sort:[{}]进行排序，如其中可填入<code>&quot;age&quot;:&#123;&quot;order&quot;: &quot;desc&quot;&#125;</code></li><li>fuzzy实现模糊查询，模糊查询可以在Match和 Multi-Match查询中使用以便解决拼写的错误，模糊度是基于Levenshteindistance计算与原单词的距离</li><li>highlight高亮搜索结果</li></ul><h4 id="2-4-2-Filter查询"><a href="#2-4-2-Filter查询" class="headerlink" title="2.4.2. Filter查询"></a>2.4.2. Filter查询</h4><ul><li>Filter context(过滤上下文)过滤上下文中的语句在执行时只关心文档是否和查询匹配，不会计算匹配度，也就是得分（score）</li><li>filter是不计算相关性的，同时可以cache。因此，filter速度要快于query</li><li>将上面的query替换为post_filter</li></ul><h4 id="2-4-3-复合查询"><a href="#2-4-3-复合查询" class="headerlink" title="2.4.3. 复合查询"></a>2.4.3. 复合查询</h4><h5 id="2-4-3-1-布尔查询bool-query"><a href="#2-4-3-1-布尔查询bool-query" class="headerlink" title="2.4.3.1. 布尔查询bool query"></a>2.4.3.1. 布尔查询bool query</h5><p>概念、特点:</p><ul><li>通过布尔逻辑将较小的查询组合成较大的查询</li><li>子查询可以任意顺序出现</li><li>可以嵌套多个查询，包括bool查询</li><li>如果bool查询中没有must条件，should中必须至少满足一条才会返回结果<br>操作符:</li><li>must：必须出现在匹配文档中 ，贡献算分</li><li>must_not：必须不能出现在匹配文档中，但不贡献算分 </li><li>should：选择性匹配，至少满足一条，贡献算分</li><li>filter：过滤子句，必须出现在匹配文档中，但不贡献算分</li></ul><h5 id="2-4-3-2-提高查询boosting-query"><a href="#2-4-3-2-提高查询boosting-query" class="headerlink" title="2.4.3.2. 提高查询boosting query"></a>2.4.3.2. 提高查询boosting query</h5><p>在bool复合查询我们可以通过 must_not+must 先剔除不想匹配的文档，再获取匹配的文档，但是有一种场景就是我并不需要完全剔除，而是把需要剔除的那部分文档的分数降低。这个时候就可以使用boosting query<br>boosting需要搭配三个关键字：positive , negative , negative_boost</p><ul><li>只有匹配了 positive查询 的文档才会被包含到结果集中</li><li>但是在匹配了 positive 的同时匹配了negative查询 的文档会被降低相关度并也包含在结果中，通过将文档原本的_score和negative_boost参数进行相乘来得到新的_score</li><li>所以negative_boost参数一般小于1.0，这样才能降低得分，如该参数等于0.5时匹配 negative 时会降低一半得分<br>应用场景:<br>例如搜索“苹果公司”相关信息，查询中用于匹配的信息为“苹果”。那么我们查询的条件是：must = ‘苹果’。但如果你的文档是 ‘苹果树’,’苹果水果’，那么此时如果匹配到其实没有任何意义。那么我们修改查询条件为：must = ‘苹果’ AND must_not = ‘树 or 水果’ 因为我们要查的苹果公司相关信息，如果你是苹果树那对我来讲确实是不匹配，所以直接过滤掉。但直接过滤会过于粗暴，因为一个文档中包含’苹果’和’树’那不代表一定是苹果树，而可能是 ‘苹果公司组织员工一起去种树’ 那么这条文档理应出现，此时产生了 boosting 应用的场景</li></ul><h5 id="2-4-3-3-固定分数查询constant-score"><a href="#2-4-3-3-固定分数查询constant-score" class="headerlink" title="2.4.3.3. 固定分数查询constant_score"></a>2.4.3.3. 固定分数查询constant_score</h5><p>常量分值查询，目的就是返回指定的score，一般都结合filter使用，因为filter context忽略score<br>例如 constant_score 中先加入一个filter，后跟<strong>boost参数</strong>，此时所有结果文档的score都会变为boost参数指定的值</p><h5 id="2-4-3-4-最佳匹配查询dis-max"><a href="#2-4-3-4-最佳匹配查询dis-max" class="headerlink" title="2.4.3.4.最佳匹配查询dis_max"></a>2.4.3.4.最佳匹配查询dis_max</h5><p>如果在查询中有多个匹配条件，则结果的得分只是取分数最高的那个query的分数<br>可以通过指定 <strong>tie**</strong>_breaker** 这个参数将其他匹配语句的评分也考虑其中，但最佳匹配语句依然占最终结果里的很大一部分<br>tie_breaker 参数提供了一种 dis_max 和 bool 之间的折中选择，它的评分方式如下：</p><ul><li>获得最佳匹配语句的评分 _score</li><li>将其他匹配语句的评分结果与 tie_breaker 相乘</li><li>对以上评分求和并规范化<br>tie_breaker 可以是 0 到 1 之间的浮点数，其中 0 代表使用 dis_max 最佳匹配语句的普通逻辑， 1 表示所有匹配语句同等重要。最佳的精确值需要根据数据与查询调试得出，但是合理值应该与零接近（处于 0.1 - 0.4 之间），这样就不会颠覆 dis_max 最佳匹配性质的根本</li></ul><h5 id="2-4-3-5-函数查询function-score-（简单介绍）"><a href="#2-4-3-5-函数查询function-score-（简单介绍）" class="headerlink" title="2.4.3.5. 函数查询function_score （简单介绍）"></a>2.4.3.5. 函数查询function_score （简单介绍）</h5><p>概念：</p><ul><li>function_score是处理分值计算过程的终极工具，它让你能够对所有匹配了主查询的每份文档调用一个函数来调整甚至是完全替换原来的_score</li><li>要使用function_score，用户必须定义一个查询和一个或多个函数，这些函数计算查询返回的每个文档的新分数<br>一些预定义的函数：</li><li>weight: 对每份文档适用一个简单的提升，且该提升不会被归约，例如当weight为2时，结果为2 * _score</li><li>field_value_factor: 使用文档中某个字段的值来改变_score，比如将受欢迎程度或者投票数量考虑在内</li><li>random_score: 使用一致性随机分值计算来对每个用户采用不同的结果排序方式，对相同用户仍然使用相同的排序方式</li><li>衰减函数Decay Function<ul><li>linear，exp，gauss等</li></ul></li><li>script_score: 使用自定义的脚本来完全控制分值计算逻辑。如果你需要以上预定义函数之外的功能，可以根据需要通过脚本进行实现。<br>使用场景举例：</li><li>假设有一个资讯类APP，我们希望让人阅读量高的文章出现在结果列表的头部，但是主要的排序依据仍然是全文搜索分值</li><li>当用户搜索酒店，它的要求是 1、离他目前位置1Km内 2、价格在500元内。如果我们只是使用一个 filter 排除所有市中心方圆 1KM以外的酒店，再用一个filter排除每晚价格超过500元的酒店，这种作法太过强硬，可能有一间房在2K米，但是超级便宜一晚只要100元，用户可能会因此愿意妥协住这间房。<br>官方文档： <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html">https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html</a></li></ul><h4 id="举点Query和Filter的例子，数据接用上个例子中的。"><a href="#举点Query和Filter的例子，数据接用上个例子中的。" class="headerlink" title="举点Query和Filter的例子，数据接用上个例子中的。"></a>举点Query和Filter的例子，数据接用上个例子中的。</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line">查询兴趣里包含<span class="string">&#x27;旅游&#x27;</span>的：</span><br><span class="line"><span class="builtin-name">GET</span> student/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;query&quot;</span>:&#123;</span><br><span class="line">    <span class="string">&quot;match&quot;</span>:&#123;<span class="string">&quot;interests&quot;</span>: <span class="string">&quot;旅游&quot;</span>&#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">这里只要interests包含<span class="string">&#x27;旅游&#x27;</span>,<span class="string">&#x27;旅&#x27;</span>,<span class="string">&#x27;游&#x27;</span>的都会命中</span><br><span class="line"></span><br><span class="line">查询name和address包含<span class="string">&#x27;北&#x27;</span>：</span><br><span class="line"><span class="builtin-name">GET</span> student/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;query&quot;</span>:&#123;</span><br><span class="line">    <span class="string">&quot;multi_match&quot;</span>: &#123;</span><br><span class="line">      <span class="string">&quot;query&quot;</span>: <span class="string">&quot;北&quot;</span>,</span><br><span class="line">      <span class="string">&quot;fields&quot;</span>:[<span class="string">&quot;name&quot;</span>,<span class="string">&quot;address&quot;</span>]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">我们索引结构中，address属性是一个keyword类型，它是需要完全匹配，而不是包含的关系，所以只有当query北京时才能被查到</span><br><span class="line"></span><br><span class="line">查询兴趣里包含<span class="string">&#x27;摄影&#x27;</span>的：</span><br><span class="line"><span class="builtin-name">GET</span> student/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;query&quot;</span>:&#123;</span><br><span class="line">    <span class="string">&quot;match_phrase&quot;</span>:&#123;<span class="string">&quot;interests&quot;</span>: <span class="string">&quot;摄影&quot;</span>&#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">这里和match的区别是这里是真正包含<span class="string">&#x27;摄影&#x27;</span>，而不是只要满足其中一个字就会被模糊命中</span><br><span class="line"></span><br><span class="line">terms查询地址等于<span class="string">&quot;香港&quot;</span>或<span class="string">&quot;北京&quot;</span>的：</span><br><span class="line"><span class="builtin-name">GET</span> student/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;query&quot;</span>:&#123;</span><br><span class="line">    <span class="string">&quot;terms&quot;</span>:&#123;</span><br><span class="line">      <span class="string">&quot;address&quot;</span>:[<span class="string">&quot;香港&quot;</span>,<span class="string">&quot;北京&quot;</span>]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">如果仅检索<span class="string">&#x27;香&#x27;</span>或<span class="string">&#x27;北&#x27;</span>那是无法命中的，因为keyword需要完全匹配才能命中，如果查询一个地址则使用term</span><br><span class="line"></span><br><span class="line">按成绩排序：</span><br><span class="line"><span class="builtin-name">GET</span> student/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;query&quot;</span>:&#123;</span><br><span class="line">    <span class="string">&quot;match_all&quot;</span>: &#123;&#125;</span><br><span class="line">  &#125;,</span><br><span class="line"> <span class="string">&quot;sort&quot;</span>:[&#123;</span><br><span class="line">        <span class="string">&quot;age&quot;</span>:&#123;<span class="string">&quot;order&quot;</span>: <span class="string">&quot;desc&quot;</span>&#125;</span><br><span class="line">      &#125;]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">范围查询：</span><br><span class="line"><span class="builtin-name">GET</span> student/_search</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;query&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;range&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;birthday&quot;</span>: &#123;</span><br><span class="line">                <span class="string">&quot;from&quot;</span>: <span class="string">&quot;1950-01-11&quot;</span>,</span><br><span class="line">                <span class="string">&quot;to&quot;</span>: <span class="string">&quot;2000-01-11&quot;</span>,</span><br><span class="line">                 <span class="string">&quot;include_lower&quot;</span>: <span class="literal">true</span>,</span><br><span class="line">                <span class="string">&quot;include_upper&quot;</span>: <span class="literal">false</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">通配符查询：</span><br><span class="line"><span class="builtin-name">GET</span> student/_search</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;query&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;wildcard&quot;</span>: &#123;</span><br><span class="line">             <span class="string">&quot;name&quot;</span>: <span class="string">&quot;李*&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">？符号中文似乎无法使用</span><br><span class="line"></span><br><span class="line">Filter查询age为23或32的：</span><br><span class="line"><span class="builtin-name">GET</span> student/_search</span><br><span class="line">&#123; </span><br><span class="line">  <span class="string">&quot;post_filter&quot;</span>:&#123;</span><br><span class="line">    <span class="string">&quot;terms&quot;</span>:&#123;<span class="string">&quot;age&quot;</span>:[32,23]&#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="2-5-聚合：Elasticsearch-的面向和分析的主功能"><a href="#2-5-聚合：Elasticsearch-的面向和分析的主功能" class="headerlink" title="2.5.  聚合：Elasticsearch 的面向和分析的主功能"></a>2.5.  聚合：Elasticsearch 的面向和分析的主功能</h3><p>在Mysql中，我们可以获取一组数据的 最大值(Max)、最小值(Min)。同样我们能够对这组数据进行 分组(Group)。那么对于Elasticsearch中我们也可以实现同样的功能。<br><strong>官方对聚合有四个关键字: Metric(指标）、Bucketing(桶）、Matrix(矩阵）、Pipeline(管道</strong><br>聚合是Elasticsearch除全文检索功能外提供的针对Elasticsearch数据做统计分析的功能。它的实时性高,所有的计算结果都是即时返回。</p><ul><li>Metric(指标):   指标分析类型，如计算最大值、最小值、平均值等等 （对桶内的文档进行聚合分析的操作）</li><li>Bucket(桶):     分桶类型，类似SQL中的GROUP BY语法 （满足特定条件的文档的集合）</li><li>Pipeline(管道): 管道分析类型，基于上一级的聚合分析结果进行再分析</li><li>Matrix(矩阵):   矩阵分析类型（聚合是一种面向数值型的聚合，用于计算一组文档字段中的统计信息）</li><li>在查询请求体中以aggregations节点按规定语法定义聚合分析，可简写为aggs，如<code>&#123;&quot;aggs&quot; : &#123;&quot;&lt;aggregation_name&gt;&quot; :&#123;&quot;&lt;aggregation_type&gt;&quot; : &#123;&lt;aggregation_body&gt; &#125; &#125; &#125; &#125;</code> </li></ul><h4 id="2-5-1-Metric聚合"><a href="#2-5-1-Metric聚合" class="headerlink" title="2.5.1. Metric聚合"></a>2.5.1. Metric聚合</h4><p>官方文档：<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-metrics.html">https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-metrics.html</a><br>简介：<br>桶(Bucket)能让我们划分文档到有意义的集合，但是最终我们需要的是对这些桶内的文档进行一些<strong>指标</strong>的计算。分桶是一种达到目的地的手段：它提供了一种给文档分组的方法来让我们可以计算感兴趣的指标。<br>大多数指标是简单的数学运算（如：最小值、平均值、最大值、汇总），这些是通过文档的值来计算的。<br>Metric聚合分析分为单值分析和多值分析两类</p><ul><li>单值分析：只输出一个分析结果，如min、max、avg、sum…</li><li>多值分析：输出多个分析结果，如stats、percentile、extended_stats、percentile_rank…<br>1、Avg（均值）<br>计算从聚合文档中提取的数值的平均数<br>如：<code>&#123;&quot;aggs&quot; : &#123;&quot;avg_grade&quot; : &#123; &quot;avg&quot; : &#123; &quot;field&quot; : &quot;grade&quot; &#125; &#125;&#125;&#125;</code><br>2、Max&amp;Min<br>计算从聚合文档中提取的数值的最值<br>如： <code>&quot;max_price&quot; : &#123; &quot;max&quot; : &#123; &quot;field&quot; : &quot;price&quot; &#125; &#125;</code><br>3、Sum（总和）<br>计算从聚合文档中提取的数值的总和<br>如： <code>&quot;hat_prices&quot; : &#123; &quot;sum&quot; : &#123; &quot;field&quot; : &quot;price&quot; &#125; &#125;</code><br>4、Cardinality(唯一值)<br>cardinality 求唯一值，即不重复的字段有多少（相当于mysql中的distinct）<br>如： <code>&quot;type_count&quot; : &#123;&quot;cardinality&quot; : &#123;&quot;field&quot; : &quot;type&quot;&#125; &#125;</code><br>5、Stats（统计）<br>stats 统计，请求后会直接显示多种聚合结果，count、min、max、avg、sum等<br>如： <code>&quot;grades_stats&quot; : &#123; &quot;stats&quot; : &#123; &quot;field&quot; : &quot;grade&quot; &#125; &#125;</code><br>6、Percentiles（文档占比）<br>对指定字段的值按从小到大累计每个值对应的文档数的占比，返回指定占比比例对应的值，如对字段 “load_time” 使用percentiles返回<code>&quot;50.0&quot;: 445.0,</code>，则表示占比为百分之五十的文档”load_time”值小于等于445<br>下面给几种使用方法：</li></ul><ul><li>默认取百分比，则按照[ 1, 5, 25, 50, 75, 95, 99 ]来统计，如： <code>&quot;percentiles&quot; : &#123; &quot;field&quot; : &quot;load_time&quot; &#125;</code></li><li>指定分位值，则可在body内增加如<code>&quot;percents&quot; : [95, 99, 99.9]</code></li><li>Keyed Response<ul><li>默认情况下，keyed标志设置为true，它将唯一的字符串键与每个存储桶相关联，并将范围作为哈希而不是数组返回</li><li>也可以修改keyed为false，在body内<code>&quot;keyed&quot;: false</code>，这样则会返回一个数组，通过结果也可以看出percentiles中键值对是按照百分比作键，字段值作值构成的<br>7、Percentile_Ranks<br>percentiles是通过指定百分比求文档值，这里通过文档值求百分比，即字段值作键，百分比作值<br>如： <code>&quot;percentile_ranks&quot; : &#123; &quot;field&quot; : &quot;load_time&quot; , &quot;values&quot; : [500,600] &#125;</code> ，返回结果若为 <code>&quot;500.0&quot;: 55.1, &quot;600.0&quot;: 64.0</code> 则表示 时间小于500的文档占比为55.1%，时间小于600的文档占比为64%<br>注意这里一定要通过values指定值数组<br>8、Top_Hits<br>一般用于分桶后获取该桶内匹配前n的文档列表，例如可以展示出某一个网站文档浏览前N名的文档</li></ul></li></ul><h4 id="2-5-2-Bucket聚合"><a href="#2-5-2-Bucket聚合" class="headerlink" title="2.5.2. Bucket聚合"></a>2.5.2. Bucket聚合</h4><p>简介：<br>简单来说桶就是满足特定条件的文档的集合，它会遍历文档中的内容，凡是符合某一要求的就放入一个桶中，分桶相当于 SQL 中的 group by。<br>当聚合开始被执行，每个文档里面的值通过计算来决定符合哪个桶的条件，如果匹配到，文档将放入相应的桶并接着开始聚合操作。<br>桶也可以被嵌套在其他桶里面。<br>1、Terms Aggregation<br>根据某一项的每个唯一的值的聚合<br>如：<code>&quot;terms&quot; : &#123; &quot;field&quot; : &quot;brand&quot; &#125;</code>，根据品牌分桶<br>常见使用：</p><ul><li>若只显示数量前n的桶，可通过size参数控制，如在body中加入<code>&quot;size&quot; : 3</code> 则只显示前三的桶</li><li>若分桶后需要排序，可通过order参数控制，如在body中加入<code>&quot;order&quot; : &#123; &quot;_count&quot; : &quot;asc&quot; &#125;</code></li><li>若要显示文档数量不小于n的桶，可通过<code>min_doc_count</code>参数控制，如： <code>&quot;min_doc_count&quot;: 3</code> 则只显示文档数量大于等于3的桶</li><li>也可使用精确的指定字段值分组，通过include参数控制，如： <code>&quot;include&quot; : [&quot;BMW&quot;, &quot;Audi&quot;]</code> 则只通过宝马和奥迪分桶<br>2、Filter Aggregation&amp;Filters Aggregation<br>Filter Aggregation指具体的域和具体的值，可以说是在 Terms Aggregation 的基础上进行了过滤，只对特定的值进行了聚合<br>Filter Aggreagtion 只能指定一个过滤条件，响应也只是单个桶。如果想要只对多个特定值进行聚合，使用 Filter Aggreagtion 只能进行多次请求。而使用 Filters Aggreagation 就可以解决上述的问题，它可以指定多个过滤条件，也是说可以对多个特定值进行聚合<br>3、Histogram Aggregation<br>Histogram与Terms聚合类似，都是数据分组，区别是Terms是按照Field的值分组，而Histogram可以按照指定的<strong>间隔</strong>对Field进行分组<br>如: <code>&quot;histogram&quot; : &#123;&quot;field&quot; : &quot;price&quot; , &quot;interval&quot; : 10000&#125;</code>表示根据价格区间为10000分桶<br>4、Range Aggregation<br>根据用户传递的范围参数作为桶，进行相应的聚合。在同一个请求中，可以传递多组范围，每组范围作为一个桶<br>如： <code>&quot;range&quot; : &#123; &quot;field&quot; : &quot;price&quot;, &quot;ranges&quot; : [ &#123; &quot;to&quot; : 50000 &#125;,&#123; &quot;from&quot; : 5000, &quot;to&quot; : 80000 &#125;, &#123; &quot;from&quot; : 80000 &#125; ] &#125;</code> 通过ranges参数的数组传递了三个范围，则返回结果中包含三个桶<br>也可以在ranges中指定key的名称，如： <code>[&#123; &quot;key&quot; : &quot;xiaoyu&quot;,  &quot;to&quot; : 50000 &#125;]</code><br>5、Date Aggregation<br>Date Aggregation是针对于时间格式数据的直方图聚合，基本的特性与 Histogram Aggregation 一致<br>可以实现按指定时间区间分桶等date相关操作，具体参见官方文档</li></ul><h2 id="3-深入"><a href="#3-深入" class="headerlink" title="3. 深入"></a>3. 深入</h2><ul><li>原理探究<ul><li>score</li><li>分词器</li><li>倒排索引</li></ul></li><li>安全性<ul><li>X-Pack</li></ul></li></ul><h3 id="3-1-原理探究"><a href="#3-1-原理探究" class="headerlink" title="3.1. 原理探究"></a>3.1. 原理探究</h3><h4 id="3-1-1-score"><a href="#3-1-1-score" class="headerlink" title="3.1.1. score"></a>3.1.1. score</h4><h4 id="3-1-2-分词器"><a href="#3-1-2-分词器" class="headerlink" title="3.1.2. 分词器"></a>3.1.2. 分词器</h4><h4 id="3-1-3-倒排索引"><a href="#3-1-3-倒排索引" class="headerlink" title="3.1.3. 倒排索引"></a>3.1.3. 倒排索引</h4><h3 id="3-2-安全性"><a href="#3-2-安全性" class="headerlink" title="3.2. 安全性"></a>3.2. 安全性</h3><p>Elasticsearch早期版本安全部分收费（7.1 &amp; 6.8 版本之前），实际中各个公司6.x，5.x,2.x,1.x都有在用，且非少数。多数只是使用nginx代理防护，不暴露端口，不外网映射。</p><p>如果9200或者改成其他端口的ES暴露在公网，一旦被扫到，集群及数据会受到灾难式影响。</p><h4 id="3-2-1-X-Pack"><a href="#3-2-1-X-Pack" class="headerlink" title="3.2.1. X-Pack"></a>3.2.1. X-Pack</h4><h5 id="3-2-1-1-啥是X-Pack"><a href="#3-2-1-1-啥是X-Pack" class="headerlink" title="3.2.1.1. 啥是X-Pack"></a>3.2.1.1. 啥是X-Pack</h5><p>X-Pack是Elastic Stack扩展功能，提供安全性，警报，监视，报告，机器学习和许多其他功能。 ES7.0+之后，默认情况下，当安装Elasticsearch时，会安装X-Pack，无需单独再安装（也就是内置了）。自6.8以及7.1+版本之后，基础级安全永久免费。</p><p>基础级安全包括：加密通信、基于角色的访问控制、文件和原生身份验证、Kibana功能控制、Kibana Spaces和API密钥管理。</p><p>安全等级有：Minimal、Basic和Basic+TLS for REST，基础级指Basic。</p><p>Basic security比Minimal security多了节点间的TLS（加密通信和身份验证）。</p><h5 id="3-2-1-2-如何配置X-Pack（基础级安全）"><a href="#3-2-1-2-如何配置X-Pack（基础级安全）" class="headerlink" title="3.2.1.2. 如何配置X-Pack（基础级安全）"></a>3.2.1.2. 如何配置X-Pack（基础级安全）</h5><p>默认情况下，拥有安全免费许可证时，Elasticsearch安全功能被禁用。 要启用安全功能，需要先在es主配置文件中添加<code>xpack.security.enabled: true</code>。生产模式下若进行此配置则必须完成Basic security的所有配置，否则集群无法启动。</p><p><strong>一、为内建用户创建密码</strong></p><p>在Minimal和Basic下，只需要为elastic和kibana_system用户创建密码。</p><p>1、在集群的每个节点上启动Elasticsearch</p><p>2、<code>./bin/elasticsearch-setup-passwords interactive</code> 为每个用户创建密码，注意该命令可以在集群的任一节点上运行，但一个集群只能运行一次。interactive换成auto会自动生成密码，该命令只能运行一次！</p><p><strong>二、使用用户名和密码登录Kibana</strong></p><p>当安全特性被启动后，用户必须通过合法的用户名和密码才能登录Kibana。刚刚配置好的内建用户kibana_system用于配置Kibana的某些后台任务，但并不能作为个人用户登录Kibana界面。要想登录Kibana界面需要使用超级用户”elastic”。</p><p>1、在kibana.yml中添入<code>elasticsearch.username: &quot;kibana_system&quot;</code></p><p>2、<code>./bin/kibana-keystore create</code> 创建Kibana密钥库（keystone）</p><p>3、<code>./bin/kibana-keystore add elasticsearch.password</code> 输入之前定义的密码</p><p>4、重启Kibana</p><p>5、使用”elastic”超级用户登录Kibana界面</p><p><strong>三、配置节点间传输层安全（TLS）</strong></p><p>在集群中有多个节点时，必须配置节点间的传输层安全！否则处于生产模式下的Elasticsearch集群不会被启动。该安全应用包含了节点间的通信加密和身份验证。</p><p>（画外音，教程中出现传输层协议特指ES用于节点间交互的协议，transport用于区分http端口和节点间通信端口transport port，虽然TLS和transport port都出现了transport一词，但表示不同含义，我们在将TLS应用到http port和transport port）</p><p><strong>四、将TLS应用到节点间加密通信</strong></p><p>节点间的通信被transport模块处理，为了保护集群必须确保节点间通信的加密和验证工作正常，即互相的TLS。节点间通信使用证书确定身份，推荐做法是集群选择一个信任的CA，当新节点要加入集群时需要提供该CA的签名的证书。使用elasticsearch-certutil工具为集群生成CA。</p><p>1、<code>./bin/elasticsearch-certutil ca</code>使用工具，提示后选择默认文件名”elastic-stack-ca.p12”，这个文件包含生成CA的公共证书和它为每个节点签名的私钥。</p><p>2、为生成的CA选择一个密码，可以为空（非生产）。</p><p>3、在同一个节点上运行<code>./bin/elasticsearch-certutil cert --ca elastic-stack-ca.p12</code> 该命令使用刚才生成的CA为集群上的节点们生成一个证书和一把私钥。</p><p>4、 提示后首先输入CA的密码，然后为节点证书选择一个密码并使用默认文件名”elastic-certificates.p12”，这个文件包含了节点证书、节点私钥和CA证书。</p><p>5、最后在集群的所有节点上，在elasticsearch的config目录中放入刚才生成的节点证书文件”elastic-certificates.p12”，他们都是相同的。</p><p>以上步骤为生成CA和证书，下面开始布置加密节点间通信。由于Elasticseach会定期（默认5s）监视一些配置文件（包括CA和证书），所以不用停止集群就可以进行下列步骤。</p><p>1、打开ES主配置文件，对所有集群中的节点配置统一的集群名称cluster.name，并对各个节点配置各自的节点名称node.name</p><p>2、在所有节点的主配置文件下加入以下配置（由于默认证书文件名一致）</p><pre><code>xpack.security.transport.ssl.enabled: truexpack.security.transport.ssl.verification_mode: certificate xpack.security.transport.ssl.client_authentication: requiredxpack.security.transport.ssl.keystore.path: elastic-certificates.p12xpack.security.transport.ssl.truststore.path: elastic-certificates.p12</code></pre><p>3、如果在创建节点证书（elastic-certificates.p12）时设定了密码，则进行以下两项配置将证书密码加入Elasticsearch密钥库（keystore），在所有节点上。</p><p><code>./bin/elasticsearch-keystore add xpack.security.transport.ssl.keystore.secure_password</code></p><p><code>./bin/elasticsearch-keystore add xpack.security.transport.ssl.truststore.secure_password</code></p><p>4、重启整个集群，确保每个节点的ES都进行了重启，否则可能无法通信。</p><p><strong>五、将TLS应用到HTTP层</strong></p><p>之前已经将TLS在传输层进行了应用，现在要应用到HTTP层。分为两个部分，为Elasticsearch加密HTTP客户端通信和为Kibana加密HTTP客户端通信，Kibana部分包括浏览器与Kibana间的通信加密与Kibana与Elasticsearch间的通信加密。</p><p>1、 停止各节点服务，在任一节点上ESHOME运行<code>./bin/elasticsearch-certutil http</code>。该命令生成一个zip压缩文件，其中包含用于 Elasticsearch 和 Kibana 的证书和密钥。每个文件夹都包含一个README.txt 解释如何使用这些文件。详细步骤如下：</p><ol><li>当询问您是否要生成 CSR 时，请输入n。</li><li>当询问您是否要使用现有 CA 时，输入y。</li><li>输入您的 CA 的路径。elastic-stack-ca.p12这是您为集群生成的文件的绝对路径。</li><li>输入您的 CA 的密码。</li><li>输入证书的到期值，您可以输入年、月或日的有效期。例如，输入90D表示90 天。</li><li>当询问您是否要为每个节点生成一个证书时，输入y。节点的名称，如ESNODE-001等。每个证书都有自己的私钥，并针对特定的主机名或 IP 地址颁发。</li><li>出现提示时，输入集群中第一个节点的名称，即生成节点证书（”elastic-certificates.p12”）时使用的节点名称。</li><li>输入连接到您的第一个节点的所有主机名，这些主机名将作为 DNS 名称添加到证书的Subject Alternative Name（主题备用名称 SAN) 字段中。列出需要通过 HTTPS 连接到集群的每个主机名和变体。主机名如ZMC-MacBook-Air.local。</li><li>输入客户端可用于连接到您的节点的 IP 地址，通常为本机地址。然后会有新增证书选项，选择确认为节点二创建。总之集群中的节点都需要有这些证书。</li></ol><p>2、为每个节点生成证书后，在出现提示时输入您的私钥密码，可以为空。</p><p>3、解压生成的elasticsearch-ssl-http.zip文件。此压缩文件包含 Elasticsearch 和 Kibana 的目录。如果对多个节点创建了证书，那么elasticsearch目录下会根据节点名分为若干个子目录。</p><pre><code>/elasticsearch|_ README.txt|_ http.p12|_ sample-elasticsearch.yml /kibana|_ README.txt|_ elasticsearch-ca.pem|_ sample-kibana.yml</code></pre><p>4、在集群的每个节点上进行操作，复制<strong>对应的</strong>http.p12文件到ESHOME下config文件夹中，然后修改主配置文件，增加如下配置。注意一定要对应节点的文件。</p><pre><code>xpack.security.http.ssl.enabled: truexpack.security.http.ssl.keystore.path: http.p12</code></pre><p>最后<code>./bin/elasticsearch-keystore add xpack.security.http.ssl.keystore.secure_password</code>添加之前设置的私钥密码到elasticsearch密钥库。</p><p>5、启动ES，Elasticsearch加密HTTP客户端通信配置完毕。此时Kibana将无法使用，需要进行进一步加密配置。</p><p>下面配置Kibana加密部分。</p><p>1、第一部分创建的zip文件中包含elasticsearch-ca.pem文件，使用该文件配置Kibana使其信任HTTP 层的 Elasticsearch CA。复制该文件到Kibana的config目录下。</p><p>2、在kibana.yml下增加并修改配置如下，指定认证文件目录，并提供Elasticsearch集群特定的HTTPS URL。目录可以用相对的，如config/.pem。</p><pre><code>elasticsearch.ssl.certificateAuthorities: $KBN_PATH_CONF/elasticsearch-ca.pemelasticsearch.hosts: https://&lt;your_elasticsearch_host&gt;:9200</code></pre><p>3、重启Kibana使配置生效。如果配置了Elastic monitoring在集群上，也可以将监控功能加密（即通过HTTPS方式）。至此Kibana至ES加密配置完成，Kibana可以正常工作，但从浏览器访问时仍然是http连接。</p><p>4、要对浏览器到Kibana间通信进行加密，首先需要为Kibana创建服务器证书和私钥。回到Elasticsearch目录下输入命令，</p><p><code>./bin/elasticsearch-certutil csr -name kibana-server -dns example.com,www.example.com</code> 这个CSR中包含普通名称kibana-server和两个SAN，example.com和www.example.com。</p><p>5、CSR即Certificate Signing Request，证书签名请求，它包含 CA 用于生成和签署安全证书的信息。上述命令生成一个名为csr-bundle.zip的压缩文件，目录如下。</p><pre><code>/kibana-server|_ kibana-server.csr|_ kibana-server.key</code></pre><p>6、解压csr-bundle.zip文件，获取未签名的安全证书kibana-server.csr和未加密的私钥kibana-server.key。</p><p>7、将kibana-server.csr证书签名请求发送给您的内部 CA 或受信任的 CA 进行签名以获得签名证书。签名文件可以是不同的格式，例如crt文件，即kibana-server.crt。这里可以选择使用openssl进行签名，<code>openssl  x509 -req -days 1800 -in kibana-server.csr -signkey kibana-server.key -out kibana-server.crt</code></p><p>8、打开kibana.yml增加以下配置。同样，目录可以用相对的。</p><pre><code>server.ssl.certificate: $KBN_PATH_CONF/kibana-server.crtserver.ssl.key: $KBN_PATH_CONF/kibana-server.keyserver.ssl.enabled: true</code></pre><p>9、启动Kibana完成配置，配置完成后，则必须通过https连接Kibana。</p><p><strong>六、配置Logstash适应HTTPS</strong></p><p><code>bin/elasticsearch-certutil cert --ca elastic-stack-ca.p12 --ip 192.168.0.123 --name LOGSTASH-001</code></p><p>复制证书到Logstash服务器。由于logstash不能使用PKCS#12类型的证书，运行命令<code>openssl pkcs12 -in LOGSTASH-001.p12 -clcerts -nokeys -chain -out ca.pem</code>转换证书格式。</p><p>logstash.yml增加配置如下：</p><pre><code>xpack.monitoring.enabled: truexpack.monitoring.elasticsearch.username: elasticxpack.monitoring.elasticsearch.password: weblogxpack.monitoring.elasticsearch.hosts: [&quot;https://192.168.0.xxx:9200&quot;]xpack.monitoring.elasticsearch.ssl.certificate_authority: &quot;/etc/logstash/ca.pem&quot;xpack.monitoring.elasticsearch.ssl.verification_mode: certificatexpack.monitoring.elasticsearch.sniffing: false</code></pre><p>Conf文件输出插件增加配置如下，并把http修改为https。</p><pre><code>cacert =&gt; &quot;/etc/logstash/ca.pem&quot;ssl =&gt; truessl_certificate_verification =&gt; false</code></pre>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-前言：&quot;&gt;&lt;a href=&quot;#1-前言：&quot; class=&quot;headerlink&quot; title=&quot;1. 前言：&quot;&gt;&lt;/a&gt;1. 前言：&lt;/h2&gt;&lt;p&gt;先从官方入门视频、官方文档和一些入门博客入手学习。&lt;br&gt;</summary>
    
    
    
    
    <category term="ElasticSearch" scheme="http://silencezheng.top/tags/ElasticSearch/"/>
    
  </entry>
  
  <entry>
    <title>Spring boot官网Guide学习笔记</title>
    <link href="http://silencezheng.top/2022/01/29/article30/"/>
    <id>http://silencezheng.top/2022/01/29/article30/</id>
    <published>2022-01-28T16:42:17.000Z</published>
    <updated>2022-05-27T14:17:14.032Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言："><a href="#前言：" class="headerlink" title="前言："></a>前言：</h2><p>先从各种guide入手，先对Spring程序结构体系有一个宏观了解，再结合文档进行精细化学习，文档地址（2.6.2版本）：<a href="https://docs.spring.io/spring-boot/docs/current/reference/html/">https://docs.spring.io/spring-boot/docs/current/reference/html/</a><br><span id="more"></span></p><ul><li><a href="#前言">前言：</a></li><li><a href="#第一个demo">第一个demo</a></li><li><a href="#构建restful风格的服务">构建RESTful风格的服务</a></li><li><a href="#连接mysql">连接MySQL</a></li><li><a href="#使用restful风格的服务">使用RESTful风格的服务</a></li><li><a href="#用spring-boot构建web应用">用spring boot构建Web应用</a></li><li><a href="#使用-spring-mvc-提供-web-内容">使用 Spring MVC 提供 Web 内容</a></li><li><a href="#保护-web-应用程序确认用户">保护 Web 应用程序，确认用户</a></li></ul><h2 id="第一个demo"><a href="#第一个demo" class="headerlink" title="第一个demo"></a>第一个demo</h2><p><a href="https://spring.io/quickstart">https://spring.io/quickstart</a></p><ol><li>@SpringBootApplication、@RestController、@GetMapping</li><li>本地配置tomcat要开权限，不然idea不能启动tomcat</li><li>代码示例：<figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">@SpringBootApplication</span></span><br><span class="line"><span class="variable">@RestController</span></span><br><span class="line">public class DemoApplication &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="selector-tag">public</span> <span class="selector-tag">static</span> <span class="selector-tag">void</span> <span class="selector-tag">main</span>(String[] args) &#123;</span><br><span class="line"><span class="selector-tag">SpringApplication</span><span class="selector-class">.run</span>(DemoApplication.class, args);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@<span class="selector-tag">GetMapping</span>(<span class="string">&quot;/hello&quot;</span>)</span><br><span class="line"><span class="selector-tag">public</span> <span class="selector-tag">String</span> <span class="selector-tag">hello</span>(<span class="variable">@RequestParam</span>(value = <span class="string">&quot;name&quot;</span>, defaultValue = <span class="string">&quot;World&quot;</span>) String name) &#123;</span><br><span class="line"><span class="selector-tag">return</span> <span class="selector-tag">String</span><span class="selector-class">.format</span>(<span class="string">&quot;Hello %s!&quot;</span>, name);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ol><h2 id="构建RESTful风格的服务"><a href="#构建RESTful风格的服务" class="headerlink" title="构建RESTful风格的服务"></a>构建RESTful风格的服务</h2><p><a href="https://spring.io/guides/gs/rest-service/#initial">https://spring.io/guides/gs/rest-service/#initial</a></p><ol><li>通过 mvn clean package将app进行打包（jar），终端可以直接运行java -jar</li><li>jar包和war包的区别，spring boot项目的jar包内建服务器可以直接运行</li><li>不需要web.xml，纯Java</li></ol><h2 id="连接MySQL"><a href="#连接MySQL" class="headerlink" title="连接MySQL"></a>连接MySQL</h2><p><a href="https://spring.io/guides/gs/accessing-data-mysql/">https://spring.io/guides/gs/accessing-data-mysql/</a></p><ol><li>需要添加新依赖，bash进入mysql时若用sudo命令要先输入sudo密码再输入root密码</li><li>MySQL创建新用户后的%指允许在任何主机登陆，可换为ip或localhost（仅在本地）</li><li>在resources中修改application.properties文件，了解spring.jpa.hibernate.ddl-auto的四个类型，数据库用户和scheme也从这里配置</li><li>了解Hibernate ORM ，Hibernate 将 Java 类映射到数据库表中，从 Java 数据类型中映射到 SQL 数据类型中。此demo通过Hibernate将user类映射到MySQL中，可通过Datagrip查看</li><li>用postman发送请求测试接口</li><li>进行安全测试，防止sql注入攻击，将用户权限设置为只能对数据操作而无法涉及数据表结构（scheme） </li></ol><h2 id="使用RESTful风格的服务"><a href="#使用RESTful风格的服务" class="headerlink" title="使用RESTful风格的服务"></a>使用RESTful风格的服务</h2><p><a href="https://spring.io/guides/gs/consuming-rest/">https://spring.io/guides/gs/consuming-rest/</a></p><ol><li>一般可以通过浏览器访问URL来使用一个web服务，但用变成方式来使用RESTful风格的服务是更有用的方法,这个demo要从接口中取数据放到springboot的控制台中以log形式显示</li><li>由于demo给定的接口无法访问（可能是墙的原因），CommandLineRunner启动会失败，故无法查看结果，接口应获得的内容如下：<figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"><span class="symbol"></span></span><br><span class="line"><span class="symbol">   type:</span> <span class="string">&quot;success&quot;</span>,</span><br><span class="line"><span class="symbol"></span></span><br><span class="line"><span class="symbol">   value:</span> &#123;</span><br><span class="line"><span class="symbol"></span></span><br><span class="line"><span class="symbol">      id:</span> <span class="number">10</span>,</span><br><span class="line"><span class="symbol"></span></span><br><span class="line"><span class="symbol">      quote:</span> <span class="string">&quot;Really loving Spring Boot, makes stand alone Spring apps easy.&quot;</span></span><br><span class="line"></span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">&#125; 为一个两层结构的json对象</span><br></pre></td></tr></table></figure></li><li>@JsonIgnoreProperties表示任何未绑定在此类型中的属性都应被忽略，要将接口获得的数据与自定义的类型绑定（反序列化），需要使定义的变量名与json对象的键名完全一致，如果不匹配可以用@JsonProperty注释来指定json对象中的某个键（这也可以用于处理含特殊字符或其他不便于作为变量名出现的json键）</li></ol><h2 id="用spring-boot构建Web应用"><a href="#用spring-boot构建Web应用" class="headerlink" title="用spring boot构建Web应用"></a>用spring boot构建Web应用</h2><p><a href="https://spring.io/guides/gs/spring-boot/">https://spring.io/guides/gs/spring-boot/</a></p><ol><li>CommandLineRunner, ApplicationContext, DispatcherServlet</li><li>添加单元测试，先添加maven依赖. MockMvc, 两种测试。</li><li>Spring Boot Actuator，帮助监控和管理web应用。添加依赖到maven，通过<a href="http://localhost:8080/actuator">http://localhost:8080/actuator</a> 可访问。详细见：<a href="https://docs.spring.io/spring-boot/docs/2.5.0/reference/htmlsingle/#actuator">https://docs.spring.io/spring-boot/docs/2.5.0/reference/htmlsingle/#actuator</a></li><li>JMX，是Java Management Extensions(Java管理扩展)的缩写，是一个为应用程序植入管理功能的框架。</li><li>For more details about each of these REST endpoints and how you can tune their settings with an application.properties file (in src/main/resources), see the <a href="https://docs.spring.io/spring-boot/docs/2.5.0/reference/htmlsingle/#production-ready-endpoints">https://docs.spring.io/spring-boot/docs/2.5.0/reference/htmlsingle/#production-ready-endpoints</a></li><li>Groovy 另一门语言（基于Java）</li></ol><h2 id="使用-Spring-MVC-提供-Web-内容"><a href="#使用-Spring-MVC-提供-Web-内容" class="headerlink" title="使用 Spring MVC 提供 Web 内容"></a>使用 Spring MVC 提供 Web 内容</h2><p><a href="https://spring.io/guides/gs/serving-web-content/">https://spring.io/guides/gs/serving-web-content/</a></p><ol><li>Model object, easy to use in view template</li><li>Thymeleaf, 一个现代的服务器端 Java 模板引擎，适用于 Web 和独立环境。添加spring-boot-starter-thymeleaf</li><li>Spring-boot-devtools, 可以实现指定目录（默认为classpath路径）下的文件进行更改后，项目自动重启，更改后的代码自动生效，热部署节省了“编写更改代码、重新启动应用程序并刷新浏览器以查看更改”这一过程的时间，添加依赖</li><li>Spring boot app serves static content from resources in the classpath at /static (or /public). The index.html resource is used as a welcome page.</li></ol><h2 id="保护-Web-应用程序，确认用户"><a href="#保护-Web-应用程序，确认用户" class="headerlink" title="保护 Web 应用程序，确认用户"></a>保护 Web 应用程序，确认用户</h2><p><a href="https://spring.io/guides/gs/securing-web/">https://spring.io/guides/gs/securing-web/</a></p><ol><li>分两部分，先构建一个app(Spring MVC)，再用Spring Security做登录界面</li><li>Spring MVC负责请求的转发和视图管理</li><li>If Spring Security is on the classpath, Spring Boot automatically secures all HTTP endpoints with “basic” authentication. </li><li>第二部分，首先添加ss的两个依赖（本身和测试），然后编写安全类</li><li>根据配置，Spring Security提供了一个过滤器，可以拦截请求和验证用户的过滤器。如果用户无法验证，页面将被重定向到/login？error，您的页面显示适当的错误信息。成功退出登录后，您的申请将发送到/login?logout，您的页面将显示适当的成功信息。</li><li>更新hello页面，We display the username by using Spring Security’s integration with HttpServletRequest#getRemoteUser(). The “Sign Out” form submits a POST to /logout. Upon successfully logging out, it redirects the user to /login?logout.</li><li>关于Thymeleaf:<figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">$</span>&#123;user.name&#125;表示“获取用户的变量，并调用其getName()方法”</span><br><span class="line">通过标签中的th:text属性来填充该标签的一段内容，意思是<span class="symbol">$</span>表达式只能写在th标签内部,不然不会生效,使用th:text标签的值替换<span class="built_in">div</span>标签里面的值,至于<span class="built_in">div</span>里面的原有的值只是为了给前端开发时做展示用的.</span><br><span class="line">这样的话很好的做到了前后端分离.意味着<span class="built_in">div</span>标签中的内容会被表达式<span class="symbol">$</span>&#123;session.book&#125;的值所替代，无论模板中它的内容是什么，之所以在模板中“多此一举“地填充它的内容，完全是为了它能够作为原型在浏览器中直接显示出来。</span><br><span class="line">访问spring-mvc中<span class="keyword">model</span>的属性，语法格式为“<span class="symbol">$</span>&#123;&#125;”，如<span class="symbol">$</span>&#123;user.id&#125;可以获取<span class="keyword">model</span>里的user对象的id属性 </span><br><span class="line">为了模板更加易用，Thymeleaf还提供了一系列Utility对象（内置于Context中），可以通过#直接访问。例如<span class="symbol">$</span>&#123; #dates.format(dateVar, <span class="string">&#x27;dd/MMM/yyyy HH:mm&#x27;</span>)&#125; 使用java.util.Date的功能方法类</span><br></pre></td></tr></table></figure></li><li>结果是智能允许名为user密码为password的用户登录</li></ol>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言：&quot;&gt;&lt;a href=&quot;#前言：&quot; class=&quot;headerlink&quot; title=&quot;前言：&quot;&gt;&lt;/a&gt;前言：&lt;/h2&gt;&lt;p&gt;先从各种guide入手，先对Spring程序结构体系有一个宏观了解，再结合文档进行精细化学习，文档地址（2.6.2版本）：&lt;a href=&quot;https://docs.spring.io/spring-boot/docs/current/reference/html/&quot;&gt;https://docs.spring.io/spring-boot/docs/current/reference/html/&lt;/a&gt;&lt;br&gt;</summary>
    
    
    
    
    <category term="Java" scheme="http://silencezheng.top/tags/Java/"/>
    
    <category term="Spring" scheme="http://silencezheng.top/tags/Spring/"/>
    
  </entry>
  
  <entry>
    <title>双重指针，指针的指针！</title>
    <link href="http://silencezheng.top/2021/07/13/article29/"/>
    <id>http://silencezheng.top/2021/07/13/article29/</id>
    <published>2021-07-13T09:09:17.000Z</published>
    <updated>2022-05-18T13:48:27.585Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>说来惭愧，从大一完了以后接触Java就觉得C好笨啊（其实是我笨，C语言yyds），于是现在重学数据结构的时候，总是遇到C语言的一些问题，今天终于对双重指针有了一定理解，但还有一些问题，待日后学习再解决吧。对了，此问题是由严蔚敏数据结构中，稀疏矩阵压缩存储的十字链表引申而来。<br><span id="more"></span></p><h2 id="我的理解"><a href="#我的理解" class="headerlink" title="我的理解"></a>我的理解</h2><p>首先为了对新手友好，先放如下代码解决一下typedef方面的问题。<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">typedef struct LNode&#123;</span><br><span class="line">    int data;</span><br><span class="line">    struct LNode *<span class="keyword">next</span>;</span><br><span class="line">&#125;LNode, *LinkList;</span><br><span class="line"><span class="regexp">//</span>等价于下面：</span><br><span class="line"><span class="regexp">//</span>struct LNode&#123;</span><br><span class="line"><span class="regexp">//</span>    int data;</span><br><span class="line"><span class="regexp">//</span>    struct LNode *<span class="keyword">next</span>;</span><br><span class="line"><span class="regexp">//</span>&#125;</span><br><span class="line"><span class="regexp">//</span>typedef struct LNode LNode;</span><br><span class="line"><span class="regexp">//</span>typedef struct LNode *LinkList;</span><br><span class="line"></span><br></pre></td></tr></table></figure><br>然后就正式开始解决双重指针的问题啦，我将我的解释写在了注释中，读者可以边读代码边看注释进行理解（当然我的理解也可能有问题，欢迎大家评论告诉我～）。下面代码可以extern到main.c中跑一下，注意头文件。<br><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//  双重指针.c</span></span><br><span class="line"><span class="comment">//  test</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//  Created by silenceZheng on 2021/7/13.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">ANode</span>&#123;</span></span><br><span class="line">    <span class="keyword">int</span> data;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">ANode</span> *<span class="title">next</span>;</span></span><br><span class="line">&#125;ANode, * ALink;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span>&#123;</span></span><br><span class="line">    ALink *start;</span><br><span class="line">&#125;AList;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">pointerAsArray_int</span><span class="params">(<span class="keyword">int</span> data)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span>* a = (<span class="keyword">int</span> *) <span class="built_in">malloc</span>(<span class="number">6</span>*<span class="built_in"><span class="keyword">sizeof</span></span>(<span class="keyword">int</span>));</span><br><span class="line">    a[<span class="number">3</span>] = data;</span><br><span class="line">    a[<span class="number">5</span>] = <span class="number">7</span>;</span><br><span class="line">    <span class="keyword">return</span> a[<span class="number">3</span>];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">pointerAsArray_ptr</span><span class="params">(<span class="keyword">int</span> data)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// 分配空间</span></span><br><span class="line">    AList* alist = (AList *)<span class="built_in">malloc</span>(<span class="built_in"><span class="keyword">sizeof</span></span>(AList));</span><br><span class="line">    ALink* s = (ALink *)<span class="built_in">malloc</span>(<span class="number">3</span>*<span class="built_in"><span class="keyword">sizeof</span></span>(ALink));</span><br><span class="line">    ALink node1 = (ALink)<span class="built_in">malloc</span>(<span class="built_in"><span class="keyword">sizeof</span></span>(ANode));</span><br><span class="line">    ALink node2 = (ALink)<span class="built_in">malloc</span>(<span class="built_in"><span class="keyword">sizeof</span></span>(ANode));</span><br><span class="line">    ALink node3 = (ALink)<span class="built_in">malloc</span>(<span class="built_in"><span class="keyword">sizeof</span></span>(ANode));</span><br><span class="line">    <span class="comment">// 赋值</span></span><br><span class="line">    node1-&gt;data = <span class="number">1</span>;</span><br><span class="line">    node2-&gt;data = <span class="number">22</span>;</span><br><span class="line">    node3-&gt;data = data;</span><br><span class="line">    node1-&gt;next = node2;</span><br><span class="line">    node2-&gt;next = node3;</span><br><span class="line">    node3-&gt;next = <span class="literal">NULL</span>;</span><br><span class="line">    <span class="comment">// 这里ALink*与上面int*的实质是一样的，只不过是“指针的指针”，ALink*指向ALink，其值为ALink的地址。</span></span><br><span class="line">    <span class="comment">// 由于我分配了3个ALink的空间，所以可以存放3个ALink成为数组。</span></span><br><span class="line">    <span class="comment">// 而ALink实质是ANode的指针，也就是结构体指针，所以需要再为ALink指向的东西（ANode）分配空间。</span></span><br><span class="line">    <span class="comment">// 这样就构成了AList中ALink*是一个指针，也是一个数组的头指针</span></span><br><span class="line">    <span class="comment">// 该数组中的每一个元素都是一个指针，指向一个ANode节点。</span></span><br><span class="line">    <span class="comment">// 下面开始将结点放到数组中。</span></span><br><span class="line">    s[<span class="number">0</span>] = node1;</span><br><span class="line">    s[<span class="number">1</span>] = node2;</span><br><span class="line">    s[<span class="number">2</span>] = node3;</span><br><span class="line">    alist-&gt;start = s;</span><br><span class="line">    <span class="comment">// 验证结果</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;%d\n&quot;</span>,alist-&gt;start[<span class="number">2</span>]-&gt;data);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;%d\n&quot;</span>,alist-&gt;start[<span class="number">0</span>]-&gt;data);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;%d\n&quot;</span>,alist-&gt;start[<span class="number">0</span>]-&gt;next-&gt;next-&gt;data);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="comment">// 说明与问题</span></span><br><span class="line">    <span class="comment">// ANode使用链表节点的结构是随便用的，事实上并不对阐述“指针的指针”这一概念有任何帮助。</span></span><br><span class="line">    <span class="comment">// 问题1. s[500] = node3; printf(&quot;%d\n&quot;,alist-&gt;start[500]-&gt;data);</span></span><br><span class="line">    <span class="comment">//       上面语句依然可以正确输出参数值，而实际上我只给ALink指针分配了3个ALink大小的空间。</span></span><br><span class="line">    <span class="comment">//       如果这样，那么为ALink*分配空间有什么意义？ 或者说分配的空间是固定的？而不分配</span></span><br><span class="line">    <span class="comment">//       则有可能丢失数据？</span></span><br><span class="line">    <span class="comment">// 问题2.    printf(&quot;%lu\n&quot;,sizeof(ANode));</span></span><br><span class="line">    <span class="comment">//          printf(&quot;%lu\n&quot;,sizeof(ALink));</span></span><br><span class="line">    <span class="comment">//          printf(&quot;%lu\n&quot;,sizeof(AList));</span></span><br><span class="line">    <span class="comment">//      输出结果分别为16、8、8，在我的电脑里int的size是4，为什么ANode是8？</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;说来惭愧，从大一完了以后接触Java就觉得C好笨啊（其实是我笨，C语言yyds），于是现在重学数据结构的时候，总是遇到C语言的一些问题，今天终于对双重指针有了一定理解，但还有一些问题，待日后学习再解决吧。对了，此问题是由严蔚敏数据结构中，稀疏矩阵压缩存储的十字链表引申而来。&lt;br&gt;</summary>
    
    
    
    
    <category term="数据结构" scheme="http://silencezheng.top/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
    <category term="C语言" scheme="http://silencezheng.top/tags/C%E8%AF%AD%E8%A8%80/"/>
    
  </entry>
  
  <entry>
    <title>Mac下配置MAP环境（mysql+apache+php）</title>
    <link href="http://silencezheng.top/2021/07/11/article28/"/>
    <id>http://silencezheng.top/2021/07/11/article28/</id>
    <published>2021-07-11T09:03:38.000Z</published>
    <updated>2022-05-25T08:09:44.963Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>发个库存，前一阵在Mac上捣鼓PHP，不想用MAMP，就自己搭了一下，凑活能用吧～<br>本文的目的是实现类WAMP集成环境的配置效果。</p><p>22.05.25更新：MacOS 12中，php.ini和apache中有关php的配置都没有了，也不准备继续学PHP了，就先这样吧。<br><span id="more"></span></p><h2 id="Mysql"><a href="#Mysql" class="headerlink" title="Mysql"></a>Mysql</h2><ol><li>下载MySQL社区版，下载地址：<a href="https://dev.mysql.com/downloads/mysql/。">https://dev.mysql.com/downloads/mysql/。</a></li><li>点击安装，会让你设置密码，或提供一个随机密码，如果提供随机密码一定要记住。</li><li>打开系统偏好，启动数据库服务。</li><li>mysql -u root -p登陆。</li></ol><h2 id="Apache"><a href="#Apache" class="headerlink" title="Apache"></a>Apache</h2><ol><li>使用Mac自带的Apache服务器。</li><li>sudo apachectl start启动服务器，浏览器输入localhost出现it works证明开启成功。</li><li>到/etc下把php.ini搞出来。</li><li>系统自带的服务器位于/etc/apache2下，打开httpd.conf配置文件，可以先备份一下。</li><li>LoadModule php7_module libexec/apache2/libphp7.so打开支持PHP7。</li><li>Include /private/etc/apache2/extra/httpd-vhosts.conf打开支持虚拟主机配置文件。</li><li>打开虚拟主机配置文件，同样可以先备份一下。</li><li> <figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="section">&lt;VirtualHost *<span class="number">:80</span>&gt;</span></span><br><span class="line">    <span class="attribute">ServerAdmin</span> webmaster@mysite.local</span><br><span class="line">    <span class="attribute"><span class="nomarkup">DocumentRoot</span></span> <span class="string">&quot;/Library/WebServer/Documents/testApache&quot;</span></span><br><span class="line">    <span class="attribute"><span class="nomarkup">ServerName</span></span> mysite.local</span><br><span class="line">    <span class="attribute">ErrorLog</span> <span class="string">&quot;/private/var/log/apache2/mysite.local-error_log&quot;</span></span><br><span class="line">    <span class="attribute">CustomLog</span> <span class="string">&quot;/private/var/log/apache2/mysite.local-access_log&quot;</span> common</span><br><span class="line"><span class="section">&lt;/VirtualHost&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure> 加入到文件中，这里描述的虚拟主机域名为mystie.local，根目录为/Library/WebServer/Documents/testApache。</li><li>编辑/etc/hosts文件，将域名绑定到127.0.0.1。 <figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">127.0.0.1</span>       localhost mysite.local</span><br></pre></td></tr></table></figure></li><li>sudo apachectl restart重启服务器，在上述根目录中加入index.php测试配置。</li></ol><h2 id="PHP"><a href="#PHP" class="headerlink" title="PHP"></a>PHP</h2><ol><li>Mac有自带PHP，这里演示使用homebrew下载新版本PHP配置。</li><li>brew install php@7.4下载7.4版本的PHP。</li><li>brew services start php@7.4启动PHP服务。</li><li>打开.bash_profile配置文件，添加 export PATH=”/opt/homebrew/opt/php@7.4/bin/:$PATH”将刚下的PHP加入环境变量。（这里的意思就是在PATH前加入该路径，:是Mac中环境变量的分隔符，和windows中;等同。）</li><li>打开zsh配置文件.zshrc，加入source ~/.bash_profile，使其刷新环境。</li><li>source ～/.zshrc，输入php -v检查是否安装成功。</li></ol>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;发个库存，前一阵在Mac上捣鼓PHP，不想用MAMP，就自己搭了一下，凑活能用吧～&lt;br&gt;本文的目的是实现类WAMP集成环境的配置效果。&lt;/p&gt;
&lt;p&gt;22.05.25更新：MacOS 12中，php.ini和apache中有关php的配置都没有了，也不准备继续学PHP了，就先这样吧。&lt;br&gt;</summary>
    
    
    
    
    <category term="MacOS" scheme="http://silencezheng.top/tags/MacOS/"/>
    
    <category term="PHP" scheme="http://silencezheng.top/tags/PHP/"/>
    
    <category term="MySQL" scheme="http://silencezheng.top/tags/MySQL/"/>
    
    <category term="Apache" scheme="http://silencezheng.top/tags/Apache/"/>
    
  </entry>
  
  <entry>
    <title>Hexo博客个性化设置（yilia主题）</title>
    <link href="http://silencezheng.top/2021/07/11/article27/"/>
    <id>http://silencezheng.top/2021/07/11/article27/</id>
    <published>2021-07-11T08:54:12.000Z</published>
    <updated>2021-07-11T09:01:49.062Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>最近依然在学习考研，祝自己能上岸吧，上岸后还有很多想做的事，学学Spring全家桶啥的，提升一下自己的技能库。最近发现李沐大神的动手学深度学习v2不错，茶余饭后看一看提升一下科研素质，以后发论文没准还是得靠这个，想用Java搞科研还没看出有什么门路。还准备每周在b站发俩做力扣的实战视频，水着玩儿也提高一下算法能力。说回正题，这个博文记录一下我在我的博客所做的一些个性化设置，持续更新～<br><span id="more"></span></p><h2 id="yilia添加字数统计和阅读时长功能"><a href="#yilia添加字数统计和阅读时长功能" class="headerlink" title="yilia添加字数统计和阅读时长功能"></a>yilia添加字数统计和阅读时长功能</h2><ol><li>安装 hexo-wordcount<br>在博客目录下打开Git Bash Here，输入命令:<br>npm i —save hexo-wordcount</li><li>文件配置<br>在theme\yilia\layout_partial\post下创建word.ejs文件：<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">style</span>=<span class="string">&quot;margin-top:10px;&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">&quot;post-time&quot;</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">&quot;post-meta-item-icon&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">&quot;fa fa-keyboard-o&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">i</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">&quot;post-meta-item-text&quot;</span>&gt;</span>  字数统计: <span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">&quot;post-count&quot;</span>&gt;</span>&lt;%= wordcount(post.content) %&gt;字<span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">&quot;post-time&quot;</span>&gt;</span></span><br><span class="line">      <span class="symbol">&amp;nbsp;</span> | <span class="symbol">&amp;nbsp;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">&quot;post-meta-item-icon&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">&quot;fa fa-hourglass-half&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">i</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">&quot;post-meta-item-text&quot;</span>&gt;</span>  阅读时长: <span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">&quot;post-count&quot;</span>&gt;</span>&lt;%= min2read(post.content) %&gt;分<span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure>然后在themes/yilia/layout/_partial/article.ejs中添加:<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;article-inner&quot;</span>&gt;</span></span></span><br><span class="line"><span class="xml">    &lt;% if (post.link || post.title)</span><span class="xquery">&#123; %&gt;</span></span><br><span class="line"><span class="xquery">      <span class="xml"><span class="tag">&lt;<span class="name">header</span> <span class="attr">class</span>=<span class="string">&quot;article-header&quot;</span>&gt;</span></span></span></span><br><span class="line"><span class="xml"><span class="xquery">        &lt;%- partial(&#x27;post/title&#x27;, </span><span class="xquery">&#123;class_name: <span class="string">&#x27;article-title&#x27;</span>&#125;</span></span><span class="xml">) %&gt;</span></span><br><span class="line"><span class="xml">        &lt;% if (!post.noDate)</span><span class="xquery"><span class="xquery">&#123; %&gt;</span></span></span><br><span class="line"><span class="xquery"><span class="xquery">        &lt;<span class="meta">%-</span> partial(<span class="string">&#x27;post/date&#x27;</span>, &#123;class_name: <span class="string">&#x27;archive-article-date&#x27;</span>, date_format: null&#125;</span></span><span class="xml">) %&gt;</span></span><br><span class="line"><span class="xml">        <span class="comment">&lt;!-- 需要添加的位置 --&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="comment">&lt;!-- 开始添加字数统计--&gt;</span></span></span><br><span class="line"><span class="xml">        &lt;% if(theme.word_count &amp;&amp; !post.no_word_count)</span><span class="xquery"><span class="xquery">&#123;%&gt;</span></span></span><br><span class="line"><span class="xquery"><span class="xquery">          &lt;<span class="meta">%-</span> partial(<span class="string">&#x27;post/word&#x27;</span>) %&gt;</span></span></span><br><span class="line"><span class="xquery"><span class="xquery">          &lt;% &#125;</span></span><span class="xml"> %&gt;</span></span><br><span class="line"><span class="xml">        <span class="comment">&lt;!-- 添加完成 --&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">        &lt;% &#125; %&gt;</span></span><br><span class="line"><span class="xml">      <span class="tag">&lt;/<span class="name">header</span>&gt;</span></span></span><br></pre></td></tr></table></figure></li><li>开启功能<br>在站点的_config.yml中添加下面代码:<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 是否开启字数统计</span></span><br><span class="line"><span class="comment">#不需要使用，直接设置值为false，或注释掉</span></span><br><span class="line"><span class="attr">word_count:</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="添加不蒜子统计插件"><a href="#添加不蒜子统计插件" class="headerlink" title="添加不蒜子统计插件"></a>添加不蒜子统计插件</h2><ol><li>配置是否开启不蒜子访问量统计功能<br>在themes/yilia/_config.yml添加属性<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 是否开启访问量统计功能(不蒜子)</span></span><br><span class="line"><span class="attr">busuanzi:</span></span><br><span class="line"> <span class="attr">enable:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></li><li>引入不蒜子并添加站点访问量<br>在themes/yilia/layout/_partial/footer.ejs末尾footer标签前添加如下代码<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;% if (theme.busuanzi &amp;&amp; theme.busuanzi.enable)&#123; %&gt;</span><br><span class="line">        <span class="comment">&lt;!-- 不蒜子统计 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">span</span> <span class="attr">id</span>=<span class="string">&quot;busuanzi_container_site_pv&quot;</span>&gt;</span></span><br><span class="line">                本站总访问量<span class="tag">&lt;<span class="name">span</span> <span class="attr">id</span>=<span class="string">&quot;busuanzi_value_site_pv&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">span</span>&gt;</span>次</span><br><span class="line">        <span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">&quot;post-meta-divider&quot;</span>&gt;</span>|<span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">span</span> <span class="attr">id</span>=<span class="string">&quot;busuanzi_container_site_uv&quot;</span> <span class="attr">style</span>=<span class="string">&#x27;display:none&#x27;</span>&gt;</span></span><br><span class="line">                本站访客数<span class="tag">&lt;<span class="name">span</span> <span class="attr">id</span>=<span class="string">&quot;busuanzi_value_site_uv&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">span</span>&gt;</span>人</span><br><span class="line">        <span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">script</span> <span class="attr">async</span> <span class="attr">src</span>=<span class="string">&quot;//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line">  &lt;% &#125; %&gt;</span><br></pre></td></tr></table></figure></li><li>添加文章访问量<br>这种形式是：外面不显示，只在文章里面才显示，位置为右上角时间的右边。<br>在themes/yilia/layout/_partial/post/date.ejs开头添加如下代码<figure class="highlight mojolicious"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml">&lt;%</span><span class="perl"> <span class="keyword">if</span> (theme.busuanzi &amp;&amp; theme.busuanzi.enable &amp;&amp; !<span class="keyword">index</span>)&#123; </span><span class="xml">%&gt;</span></span><br><span class="line"><span class="xml">        <span class="comment">&lt;!-- 不蒜子统计 --&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">span</span> <span class="attr">id</span>=<span class="string">&quot;busuanzi_container_page_pv&quot;</span> <span class="attr">style</span>=<span class="string">&#x27;display:none&#x27;</span> <span class="attr">class</span>=<span class="string">&quot;&lt;%=</span></span></span><span class="perl"> class_name </span><span class="xml"><span class="tag"><span class="string">%&gt;&quot;</span>&gt;</span></span></span><br><span class="line"><span class="xml">              <span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">&quot;icon-smile icon&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">i</span>&gt;</span> 阅读数：<span class="tag">&lt;<span class="name">span</span> <span class="attr">id</span>=<span class="string">&quot;busuanzi_value_page_pv&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">span</span>&gt;</span>次</span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;/<span class="name">span</span>&gt;</span></span></span><br><span class="line"><span class="xml">&lt;%</span><span class="perl"> &#125; </span><span class="xml">%&gt;</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="在hexo博客养一只猫"><a href="#在hexo博客养一只猫" class="headerlink" title="在hexo博客养一只猫"></a>在hexo博客养一只猫</h2><ol><li><p>安装模块</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">npm <span class="keyword">install</span> --save hexo-helper-live2d</span><br><span class="line"></span><br><span class="line">这里解释一下npm <span class="keyword">install</span>的命令含义：</span><br><span class="line">npm <span class="keyword">install</span> moduleName <span class="comment"># 安装模块到项目目录</span></span><br><span class="line"></span><br><span class="line">npm <span class="keyword">install</span> -g moduleName <span class="comment"># -g 意思是将模块安装到全局，具体安装到磁盘哪个位置，要看 npm config prefix 的位置。</span></span><br><span class="line"></span><br><span class="line">npm <span class="keyword">install</span> --save moduleName <span class="comment"># --save的意思是将模块安装到项目目录下，并在package文件的dependencies节点写入依赖。</span></span><br><span class="line"></span><br><span class="line">npm <span class="keyword">install</span> --save-dev moduleName <span class="comment"># --save -dev的意思是将模块安装到项目目录下，并在package文件的devDependencies节点写入依赖。</span></span><br></pre></td></tr></table></figure></li><li><p>安装模型</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">用 npm <span class="keyword">install</span> 模型名字安装。</span><br><span class="line">模型列表如下：</span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li><p>在hexo或者主题的_config.yml文件中添加如下配置</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Live2D:</span></span><br><span class="line"><span class="comment">## https://github.com/EYHN/hexo-helper-live2d</span></span><br><span class="line"><span class="attr">live2d:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">scriptFrom:</span> <span class="string">local</span> <span class="comment"># 默认</span></span><br><span class="line">  <span class="attr">pluginModelPath:</span> <span class="string">assets/</span> <span class="comment"># 模型文件相对与插件根目录路径</span></span><br><span class="line">  <span class="attr">pluginRootPath:</span> <span class="string">live2dw/</span></span><br><span class="line">  <span class="attr">pluginJsPath:</span> <span class="string">lib/</span></span><br><span class="line">  <span class="attr">tagMode:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">debug:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">model:</span></span><br><span class="line">  <span class="attr">use:</span> <span class="string">live2d-widget-model-wanko</span></span><br><span class="line">  <span class="attr">scale:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">hHeadPos:</span> <span class="number">0.5</span></span><br><span class="line">  <span class="attr">vHeadPos:</span> <span class="number">0.618</span></span><br><span class="line"><span class="attr">display:</span></span><br><span class="line">  <span class="attr">position:</span> <span class="string">left</span></span><br><span class="line">  <span class="attr">width:</span> <span class="number">150</span> <span class="comment"># 显示位置及大小</span></span><br><span class="line">  <span class="attr">height:</span> <span class="number">300</span></span><br><span class="line">  <span class="attr">hOffset:</span> <span class="number">0</span></span><br><span class="line">  <span class="attr">vOffset:</span> <span class="number">-20</span></span><br><span class="line"><span class="attr">mobile:</span></span><br><span class="line">  <span class="attr">show:</span> <span class="literal">false</span> <span class="comment"># 手机显示开关，建议关闭</span></span><br><span class="line"><span class="attr">react:</span></span><br><span class="line">  <span class="attr">opacity:</span> <span class="number">0.7</span></span><br></pre></td></tr></table></figure></li><li><p>遇到的问题…<br>在我配置好一切后，出现了不能更换模型的bug，怎么更改配置也没有用，我就尝试使用npm audit fix修复，看了一下报告大概是hexo等等插件的版本问题，然后强制fix了一波以后还是没好。。。一气之下我就把下载的东西都卸了，现在audit一下果然没问题了，然后重新用npm装一遍。。果然发现了些问题。</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">npm</span> WARN deprecated fsevents@<span class="number">1</span>.<span class="number">2</span>.<span class="number">13</span>: fsevents <span class="number">1</span> will break <span class="literal">on</span> node v<span class="number">14</span>+ and could be using insecure binaries. Upgrade to fsevents <span class="number">2</span>.</span><br><span class="line"><span class="attribute">npm</span> WARN deprecated chokidar@<span class="number">2</span>.<span class="number">1</span>.<span class="number">8</span>: Chokidar <span class="number">2</span> will break <span class="literal">on</span> node v<span class="number">14</span>+. Upgrade to chokidar <span class="number">3</span> with <span class="number">15</span>x less dependencies.</span><br><span class="line"><span class="attribute">npm</span> WARN deprecated hexo-bunyan@<span class="number">1</span>.<span class="number">0</span>.<span class="number">0</span>: Please see https://github.com/hexojs/hexo-bunyan/issues/<span class="number">17</span></span><br><span class="line"><span class="attribute">npm</span> WARN deprecated core-js@<span class="number">2</span>.<span class="number">6</span>.<span class="number">12</span>: core-js@&lt;<span class="number">3</span>.<span class="number">3</span> is no longer maintained and not recommended for usage due to the number of issues. Because of the V<span class="number">8</span> engine whims, feature detection in old core-js versions could cause a slowdown up to <span class="number">100</span>x even if nothing is polyfilled. Please, upgrade your dependencies to the actual version of core-js.</span><br></pre></td></tr></table></figure><p>从上面的警告可以看出我目前使用的nodejs15版本对有些插件的支持是比较差的，按照提示我运行audit fix来修复一下，结果修复完出来了更多issue。。从8个变成13了😅，试了一遍还是改不了模型和位置，索性不安了。</p></li></ol>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;最近依然在学习考研，祝自己能上岸吧，上岸后还有很多想做的事，学学Spring全家桶啥的，提升一下自己的技能库。最近发现李沐大神的动手学深度学习v2不错，茶余饭后看一看提升一下科研素质，以后发论文没准还是得靠这个，想用Java搞科研还没看出有什么门路。还准备每周在b站发俩做力扣的实战视频，水着玩儿也提高一下算法能力。说回正题，这个博文记录一下我在我的博客所做的一些个性化设置，持续更新～&lt;br&gt;</summary>
    
    
    
    
    <category term="Hexo" scheme="http://silencezheng.top/tags/Hexo/"/>
    
    <category term="yilia" scheme="http://silencezheng.top/tags/yilia/"/>
    
  </entry>
  
  <entry>
    <title>Hexo博客迁移之windows to macOS</title>
    <link href="http://silencezheng.top/2021/05/28/article26/"/>
    <id>http://silencezheng.top/2021/05/28/article26/</id>
    <published>2021-05-28T09:40:00.000Z</published>
    <updated>2022-05-18T13:56:09.517Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>之前在windows电脑上迁移过博客，但是没有记录过程，这次正好换到另一台电脑，迁移一下博客并记录一下。碰到了不少小坑～ 网上的方式各种麻烦，感觉我目前用的这种最简单粗暴。<br><span id="more"></span></p><h3 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h3><ol><li>node.js环境, git环境搭建好</li><li>cnpm替换npm，安装hexo框架cnpm install -g hexo-cli</li></ol><h3 id="迁移"><a href="#迁移" class="headerlink" title="迁移"></a>迁移</h3><ol><li>新建目录，hexo init创建博客。</li><li>cnpm install —save hexo-deployer-git，在blog目录下安装git部署插件。</li><li>将原博客的下列文件复制到新博客目录中<figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">_config.yml</span><br><span class="line"> <span class="keyword">package</span>.json (不清楚有没有用到)</span><br><span class="line"> scaffolds/ (不清楚有没有用到)</span><br><span class="line"> <span class="keyword">source</span>/</span><br><span class="line"> themes/</span><br></pre></td></tr></table></figure></li><li>hexo clean &amp;&amp; hexo g</li><li>hexo s本地测试, 没问题就可以发布了。</li></ol><h3 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h3><ol><li>因为我原博客中添加了字数统计插件，所以第一次进行完第四步测试会报错，npm i —save hexo-wordcount在目录中添加插件解决。</li><li>当我首次发布到远端时，报错fatal: unable to access ‘<a href="https://github.com/silenceZheng66/silenceZheng66.github.io.git/">https://github.com/silenceZheng66/silenceZheng66.github.io.git/</a>‘: LibreSSL SSL_connect: SSL_ERROR_SYSCALL in connection to github.com:443，看网上说是因为设置了https代理，<br>但当我去掉代理也没有反应，输入以下命令则可以成功发布。<br>git config —global http.sslBackend “openssl” </li><li>当解决2错误后，我注意到有另一个报错：FATAL err: Error: Spawn failed 大多是因为git进行push或者hexo d的时候改变了一些.deploy_git文件下的内容，解决办法如下：<figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1. 删除<span class="string">.deploy_git</span>文件夹<span class="params">(如果没有跳过这步。)</span></span><br><span class="line">2. 输入git config <span class="params">--global</span> core.autocrlf <span class="literal">false</span></span><br><span class="line">然后，依次执行：</span><br><span class="line">hexo clean</span><br><span class="line">hexo g</span><br><span class="line">hexo d</span><br><span class="line">成功！</span><br></pre></td></tr></table></figure></li></ol><p>原理：<br><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>、不同操作系统下，处理行尾结束符的方法是不同的：</span><br><span class="line">    windows下：<span class="literal">CRLF</span>（表示句尾使用回车换行两个字符，即windows下的<span class="string">&quot;\r\n&quot;</span>换行）</span><br><span class="line">    unix下：LF（表示句尾，只使用换行）</span><br><span class="line">    mac下：<span class="literal">CR</span>（表示只使用回车）</span><br><span class="line">    </span><br><span class="line"><span class="number">2</span>、Git下处理“换行”（<span class="built_in">line</span> ending）</span><br><span class="line">    core.autocrlf是git中负责处理lineending的变量，可以设置<span class="number">3</span>个值：<span class="literal">true</span>，<span class="literal">false</span>，input。</span><br><span class="line">    （<span class="number">1</span>）设置为<span class="literal">true</span>【config <span class="comment">--global core.autocrlf true】</span></span><br><span class="line">          当设置成<span class="literal">true</span>时，这意味着你在任何时候添加(<span class="built_in">add</span>)文件到git仓库时，git都会视为它是一个文本文件(<span class="keyword">text</span> <span class="built_in">file</span>)。</span><br><span class="line">　　　     它将把<span class="literal">crlf</span>变成LF。</span><br><span class="line">    （<span class="number">2</span>）设置为<span class="literal">false</span>【config <span class="comment">--global core.autocrlf false】</span></span><br><span class="line">          当设置成<span class="literal">false</span>时，<span class="built_in">line</span> endings将不做转换操作。文本文件保持原来的样子。</span><br><span class="line">    （<span class="number">3</span>）设置为input时，添加文件git仓库时，git把<span class="literal">crlf</span>编程lf。当有人Check代码时还是lf方式。因此在window操作系统下，不要使用这个设置。</span><br></pre></td></tr></table></figure></p><ol><li>某一篇博客中可能存在”’{连着#“字符串，去掉它，或者去掉”#”.否则generate时会报错！</li><li>github现在需要用token才能上传了，在config.yml里把url改成https://&lt;your_token&gt;@github.com/<USERNAME>/<REPO>.git，token从setting-&gt;developer tools里面申请。 (22.05.05更新)</li></ol>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;之前在windows电脑上迁移过博客，但是没有记录过程，这次正好换到另一台电脑，迁移一下博客并记录一下。碰到了不少小坑～ 网上的方式各种麻烦，感觉我目前用的这种最简单粗暴。&lt;br&gt;</summary>
    
    
    
    
    <category term="Hexo" scheme="http://silencezheng.top/tags/Hexo/"/>
    
    <category term="MacOS" scheme="http://silencezheng.top/tags/MacOS/"/>
    
  </entry>
  
</feed>
