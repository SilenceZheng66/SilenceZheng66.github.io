<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>silenceZheng66-BLOG</title>
  
  <subtitle>log</subtitle>
  <link href="http://silencezheng.top/atom.xml" rel="self"/>
  
  <link href="http://silencezheng.top/"/>
  <updated>2022-07-28T14:15:11.939Z</updated>
  <id>http://silencezheng.top/</id>
  
  <author>
    <name>silenceZheng66</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Python机器学习库简介</title>
    <link href="http://silencezheng.top/2022/07/28/article52/"/>
    <id>http://silencezheng.top/2022/07/28/article52/</id>
    <published>2022-07-28T14:09:25.000Z</published>
    <updated>2022-07-28T14:15:11.939Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>学深度学习也有一段时间了，想着同时也需要看一看机器学习的算法，对机器学习的基础有一个全面些的了解，过程中发现对个别机器学习库的了解不多，写个博客简单总结一下。</p><p>主要介绍的库有：</p><ul><li>pandas</li><li>numpy</li><li>scipy</li><li>sklearn<span id="more"></span><h2 id="NumPy"><a href="#NumPy" class="headerlink" title="NumPy"></a>NumPy</h2>NumPy是使用Python进行科学计算的基础软件包。以纯数学的矩阵计算为基础。<br>核心功能包括：</li><li>功能强大的N维数组对象。</li><li>精密广播功能函数。</li><li>集成 C/C+和Fortran 代码的工具。</li><li>强大的线性代数、傅立叶变换和随机数功能。</li></ul><p>NumPy 最重要的一个特点是其 N 维数组对象 ndarray，它是一系列同类型数据的集合，以 0 下标为开始进行集合中元素的索引。ndarray 对象是用于存放同类型元素的多维数组。ndarray 中的每个元素在内存中都有相同存储大小的区域。</p><p>ndarray对象的内容可以通过索引或切片来访问和修改，与 Python 中 list 的切片操作一样。ndarray 数组可以基于 0 - n 的下标进行索引，切片对象可以通过内置的 slice 函数，并设置 start, stop 及 step 参数进行，从原数组中切割出一个新数组。</p><h2 id="Pandas"><a href="#Pandas" class="headerlink" title="Pandas"></a>Pandas</h2><p>Pandas是一个强大的分析结构化数据的工具集；它的使用基础是Numpy（提供高性能的矩阵运算）；用于数据挖掘和数据分析，同时也提供数据清洗功能。</p><p>Series是一种类似于一维数组的对象，由一组数据(各种NumPy数据类型)以及一组与之相关的数据标签(即索引)组成。仅由一组数据也可产生简单的Series对象。<br><img src="/assets/post_img/article52/Series.jpeg" alt="Series"><br>DataFrame是Pandas中核心的一个表格型的数据结构，包含有一组有序的列，每列可以是不同的值类型(数值、字符串、布尔型等)，DataFrame即有行索引也有列索引，可以被看做是由Series组成的字典。</p><p>Pandas 适用于处理以下类型的数据：</p><ul><li>与 SQL 或 Excel 表类似的，含异构列的表格数据;</li><li>有序和无序（非固定频率）的时间序列数据;</li><li>带行列标签的矩阵数据，包括同构或异构型数据;</li><li>任意其它形式的观测、统计数据集, 数据转入 Pandas 数据结构时不必事先标记。</li></ul><p>利用Pandas做数据清洗也是一个非常常见的应用，例如空值、重复数据和错误数据的清洗。</p><h2 id="SciPy"><a href="#SciPy" class="headerlink" title="SciPy"></a>SciPy</h2><p>SciPy 是基于 Numpy 的科学计算库，用于数学、科学、工程学等领域，很多有一些高阶抽象和物理模型需要使用 SciPy。</p><p>NumPy 能够做一些基础的分析或变换，比如转置/逆矩阵/均值方差的计算等; SciPy则可以提供高阶的分析，比如拟合/回归/参数估计等。</p><p>SciPy被组织成覆盖不同科学计算领域的子包，如下：</p><div class="table-container"><table><thead><tr><th style="text-align:center">子包</th><th style="text-align:center">应用</th></tr></thead><tbody><tr><td style="text-align:center">scipy.cluster</td><td style="text-align:center">矢量量化/Kmeans</td></tr><tr><td style="text-align:center">scipy.constants</td><td style="text-align:center">物理和数学常数</td></tr><tr><td style="text-align:center">scipy.fftpack</td><td style="text-align:center">傅里叶变换</td></tr><tr><td style="text-align:center">scipy.integrate</td><td style="text-align:center">集成例程</td></tr><tr><td style="text-align:center">scipy.interpolate</td><td style="text-align:center">插值</td></tr><tr><td style="text-align:center">scipy.io</td><td style="text-align:center">数据输入和输出</td></tr><tr><td style="text-align:center">scipy.linalg</td><td style="text-align:center">线性代数例程</td></tr><tr><td style="text-align:center">scipy.ndimage</td><td style="text-align:center">n维图像包</td></tr><tr><td style="text-align:center">scipy.odr</td><td style="text-align:center">正交距离回归</td></tr><tr><td style="text-align:center">scipy.optimize</td><td style="text-align:center">优化</td></tr><tr><td style="text-align:center">scipy.signal</td><td style="text-align:center">信号处理</td></tr><tr><td style="text-align:center">scipy.sparse</td><td style="text-align:center">稀疏矩阵</td></tr><tr><td style="text-align:center">scipy.spatial</td><td style="text-align:center">空间数据结构和算法</td></tr><tr><td style="text-align:center">scipy.special</td><td style="text-align:center">任何特殊的数学函数</td></tr><tr><td style="text-align:center">scipy.stats</td><td style="text-align:center">统计</td></tr></tbody></table></div><h2 id="Sklearn"><a href="#Sklearn" class="headerlink" title="Sklearn"></a>Sklearn</h2><p>全称Scikit-learn，Scikit-learn是一个开源的机器学习库，它支持有监督和无监督的学习。它还提供了用于模型拟合，数据预处理，模型选择和评估以及许多其他实用程序的各种工具。</p><p>支持机器学习的六大任务模块：分类（Classification）、回归(Regression)、聚类（Clustering）、降维、模型选择和预处理。</p><p>分类：识别某个对象属于哪个类别，常用的算法有：SVM（支持向量机）、nearest neighbors（最近邻）、random forest（随机森林），常见的应用有：垃圾邮件识别、图像识别。</p><p>回归：预测与对象相关联的连续值属性，常见的算法有：SVR（支持向量机）、 ridge regression（岭回归）、Lasso，常见的应用有：药物反应，预测股价。</p><p>聚类：将相似对象自动分组，常用的算法有：k-Means、 spectral clustering、mean-shift，常见的应用有：客户细分，分组实验结果。</p><p>降维：减少要考虑的随机变量的数量，常见的算法有：PCA（主成分分析）、feature selection（特征选择）、non-negative matrix factorization（非负矩阵分解），常见的应用有：可视化，提高效率。</p><p>模型选择：比较，验证，选择参数和模型，常用的模块有：grid search（网格搜索）、cross validation（交叉验证）、 metrics（度量）。它的目标是通过参数调整提高精度。</p><p>预处理：特征提取和归一化，常用的模块有：preprocessing，feature extraction，常见的应用有：把输入数据（如文本）转换为机器学习算法可用的数据。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;学深度学习也有一段时间了，想着同时也需要看一看机器学习的算法，对机器学习的基础有一个全面些的了解，过程中发现对个别机器学习库的了解不多，写个博客简单总结一下。&lt;/p&gt;
&lt;p&gt;主要介绍的库有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;pandas&lt;/li&gt;
&lt;li&gt;numpy&lt;/li&gt;
&lt;li&gt;scipy&lt;/li&gt;
&lt;li&gt;sklearn</summary>
    
    
    
    
    <category term="Python" scheme="http://silencezheng.top/tags/Python/"/>
    
    <category term="机器学习" scheme="http://silencezheng.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习计算--《动手学深度学习》笔记0x06</title>
    <link href="http://silencezheng.top/2022/07/24/article51/"/>
    <id>http://silencezheng.top/2022/07/24/article51/</id>
    <published>2022-07-24T04:27:41.000Z</published>
    <updated>2022-07-24T04:31:03.103Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><p>本章将深入探索深度学习计算的关键组件， 即模型构建、参数访问与初始化、设计自定义层和块、将模型读写到磁盘， 以及利用GPU实现显著的加速。<br>本节知识极其重要，开始真正窥探深度学习框架的使用。我的所有笔记，都是以PyTorch为基础的（PyTorch不支持的部分使用TensorFlow）。</p><span id="more"></span><h3 id="0-1-结论"><a href="#0-1-结论" class="headerlink" title="0.1. 结论"></a>0.1. 结论</h3><ul><li>一个块可以由许多层组成；一个块可以由许多块组成。</li><li>块可以包含代码。</li><li>块负责大量的内部处理，包括参数初始化和反向传播。</li><li>层和块的顺序连接由Sequential块处理。</li><li>有多种方法可以访问、初始化和绑定模型参数，以及使用自定义初始化方法。</li><li>延后初始化使框架能够自动推断参数形状，使修改模型架构变得容易，避免了一些常见的错误。</li><li>可以通过模型传递数据，使框架最终初始化参数。</li><li>可以通过基本层类设计自定义层，允许我们定义灵活的新层。</li><li>层可以有局部参数，这些参数可以通过内置函数创建。</li><li>save和load函数可用于张量对象的文件读写。</li><li>可以通过参数字典保存和加载网络的全部参数。</li><li>保存架构必须在代码中完成，而不是在参数中完成。</li><li>可以指定用于存储和计算的设备，例如CPU或GPU。默认情况下，数据在主内存中创建，然后使用CPU进行计算。</li><li>深度学习框架要求计算的所有输入数据都在同一设备上，无论是CPU还是GPU。</li><li>不经意地移动数据可能会显著降低性能。一个典型的错误如下：计算GPU上每个小批量的损失，并在命令行中将其报告给用户（或将其记录在NumPy ndarray中）时，将触发全局解释器锁，从而使所有GPU阻塞。最好是为GPU内部的日志分配内存，并且只移动较大的日志。</li></ul><h2 id="1-层和块"><a href="#1-层和块" class="headerlink" title="1. 层和块"></a>1. 层和块</h2><p>具有单一输出的线性模型中整个模型只有一个输出：单个神经网络 （1）接受一些输入； （2）生成相应的标量输出； （3）具有一组相关 参数（parameters），更新这些参数可以优化某目标函数。</p><p>当考虑具有多个输出的网络时， 我们利用矢量化算法来描述整层神经元。 像单个神经元一样，层（1）接受一组输入， （2）生成相应的输出， （3）由一组可调整参数描述。 当我们使用softmax回归时，一个单层本身就是模型。</p><p>对于多层感知机而言，整个模型及其组成层都是上述架构。 整个模型接受原始输入（特征），生成输出（预测）， 并包含一些参数（所有组成层的参数集合）。 同样，每个单独的层接收输入（由前一层提供）， 生成输出（到下一层的输入），并且具有一组可调参数， 这些参数根据从下一层反向传播的信号进行更新。</p><p>事实证明，研究讨论“比单个层大”但“比整个模型小”的组件更有价值，如在计算机视觉中广泛流行的ResNet-152架构就有数百层， 这些层是由层组（groups of layers）的重复模式组成。</p><p>为了实现这些复杂的网络，我们引入了<em>神经网络块</em>的概念。 块（block）可以描述单个层、由多个层组成的组件或整个模型本身。 使用块进行抽象的一个好处是可以将一些块组合成更大的组件， 这一过程通常是递归的，如下图所示。 通过定义代码来按需生成任意复杂度的块，我们可以通过简洁的代码实现复杂的神经网络。<br><img src="/assets/post_img/article51/blocks.svg" alt="blocks"></p><p>从编程的角度来看，块由类（class）表示。 它的任何子类都必须定义一个将其输入转换为输出的正向传播函数， 并且必须存储任何必需的参数（有些块不需要任何参数）。 最后，为了计算梯度，块必须具有反向传播函数。 在定义我们自己的块时，由于自动微分提供了一些后端实现，我们只需要考虑正向传播函数和必需的参数。</p><p>回顾一下多层感知机，下面的代码生成一个网络，其中包含一个具有256个单元和ReLU激活函数的全连接隐藏层， 然后是一个具有10个隐藏单元且不带激活函数的全连接输出层。<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from torch<span class="selector-class">.nn</span> import functional as F</span><br><span class="line"></span><br><span class="line">net = nn<span class="selector-class">.Sequential</span>(nn<span class="selector-class">.Linear</span>(<span class="number">20</span>, <span class="number">256</span>), nn<span class="selector-class">.ReLU</span>(), nn<span class="selector-class">.Linear</span>(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">X = torch<span class="selector-class">.rand</span>(<span class="number">2</span>, <span class="number">20</span>)</span><br><span class="line"><span class="function"><span class="title">net</span><span class="params">(X)</span></span></span><br></pre></td></tr></table></figure><br>这里通过实例化nn.Sequential来构建模型，层的执行顺序是作为参数传递的。 简而言之，n<em>n.Sequential定义了一种特殊的Module，即在PyTorch中表示一个块的类，它维护了一个由Module组成的有序列表。</em><br>值得注意：<br>1、两个全连接层都是Linear类的实例， Linear类是Module的子类。<br>2、通过net(X)调用模型来获得模型的输出，实际上是net.<strong>call</strong>(X)的简写。 这个正向传播函数非常简单：它将列表中的每个块连接在一起，将每个块的输出作为下一个块的输入。</p><h3 id="1-1-自定义块"><a href="#1-1-自定义块" class="headerlink" title="1.1. 自定义块"></a>1.1. 自定义块</h3><p>通过实现一个块（在torch中为nn.Module)来了解块是如何工作的，首先总结一下块的基本功能：</p><ol><li>将输入数据作为其正向传播函数的参数。</li><li>通过正向传播函数来生成输出。输出的形状可能与输入的形状不同。</li><li>计算其输出关于输入的梯度，可通过其反向传播函数进行访问。通常这是自动发生的。</li><li>存储和访问正向传播计算所需的参数。</li><li>根据需要初始化模型参数。</li></ol><p>下面从零开始编写一个块，它含有一个多层感知机，其具有256个隐藏单元的隐藏层和一个10维输出层。MLP类继承了表示块的类。 我们的实现只需要提供我们自己的构造函数（Python中的<strong>init</strong>函数）和正向传播函数。<br><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span>(<span class="title">nn</span>.<span class="title">Module</span>):</span></span><br><span class="line">    <span class="comment"># 用模型参数声明层。这里声明两个全连接的层</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span></span>(<span class="keyword">self</span>):</span><br><span class="line">        <span class="comment"># 调用MLP的父类Module的构造函数来执行必要的初始化。</span></span><br><span class="line">        <span class="comment"># 这样，在类实例化时也可以指定其他函数参数，例如模型参数params</span></span><br><span class="line">        <span class="keyword">super</span>().__init__()</span><br><span class="line">        <span class="keyword">self</span>.hidden = nn.Linear(<span class="number">20</span>, <span class="number">256</span>)  <span class="comment"># 隐藏层</span></span><br><span class="line">        <span class="keyword">self</span>.<span class="keyword">out</span> = nn.Linear(<span class="number">256</span>, <span class="number">10</span>)  <span class="comment"># 输出层</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义模型的正向传播，即如何根据输入X返回所需的模型输出</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span></span>(<span class="keyword">self</span>, X):</span><br><span class="line">        <span class="comment"># 这里使用ReLU的函数版本，其在nn.functional模块中定义。</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">self</span>.<span class="keyword">out</span>(F.relu(<span class="keyword">self</span>.hidden(X)))</span><br></pre></td></tr></table></figure></p><p>此处的前向传播函数以X作为输入， 计算带有激活函数的隐藏表示，并输出其未规范化的输出值。<br>在这个MLP实现中，两个层都是实例变量，在每次调用前向传播函数时调用这些层。 注意一些关键细节： 首先我们的<strong>init</strong>函数通过<code>super().__init__()</code>调用父类的<strong>init</strong>函数， 省去了重复编写模版代码的痛苦。 然后我们实例化两个全连接层，分别为self.hidden和self.out。<br>注意：除非我们需要实现一个新的运算符，否则我们不必担心反向传播函数或参数初始化，系统将自动生成这些。</p><figure class="highlight dos"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 使用自定义块！</span><br><span class="line"><span class="built_in">net</span> = MLP()</span><br><span class="line"><span class="built_in">net</span>(X)</span><br></pre></td></tr></table></figure><p>块的一个主要优点是它的多功能性。 可以利用子类化块以创建层（如全连接层的类）、 整个模型或具有中等复杂度的各种组件。</p><h3 id="1-2-顺序块"><a href="#1-2-顺序块" class="headerlink" title="1.2. 顺序块"></a>1.2. 顺序块</h3><p>Sequential的设计是为了把其他模块串起来。为了构建自己的简化顺序块MySequential， 我们只需要定义两个关键函数：</p><ol><li>一种将块逐个追加到列表中的函数。</li><li>一种正向传播函数，用于将输入按追加块的顺序传递给块组成的“链条”。</li></ol><p>下面的MySequential类提供了与默认Sequential类相同的功能：<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySequential</span>(<span class="title">nn</span>.<span class="title">Module</span>):</span></span><br><span class="line">    <span class="comment"># __init__函数将每个模块逐个添加到有序字典_modules中</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>, *args)</span></span>:</span><br><span class="line">        <span class="keyword">super</span>().__init__()</span><br><span class="line">        <span class="keyword">for</span> idx, <span class="class"><span class="keyword">module</span> <span class="title">in</span> <span class="title">enumerate</span>(<span class="title">args</span>):</span></span><br><span class="line">            <span class="comment"># 这里module是Module子类的一个实例，</span></span><br><span class="line">            <span class="comment"># 把它保存在&#x27;Module&#x27;类的成员变量_modules中。module的类型是OrderedDict。</span></span><br><span class="line">            <span class="keyword">self</span>._modules[str(idx)] = <span class="class"><span class="keyword">module</span></span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(<span class="keyword">self</span>, X)</span></span>:</span><br><span class="line">        <span class="comment"># OrderedDict保证了按照成员添加的顺序遍历它们</span></span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> <span class="keyword">self</span>._modules.values():</span><br><span class="line">            X = block(X)</span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><br>使用_modules属性的主要优点是：在模块的参数初始化过程中，系统知道在_modules字典中查找需要初始化参数的子块。（默认规定）</p><p>当MySequential的正向传播函数被调用时，每个添加的块都按照它们被添加的顺序执行。</p><p>使用我们自定义的MySequential类重新实现多层感知机：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net = MySequential(nn<span class="selector-class">.Linear</span>(<span class="number">20</span>, <span class="number">256</span>), nn<span class="selector-class">.ReLU</span>(), nn<span class="selector-class">.Linear</span>(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line"><span class="function"><span class="title">net</span><span class="params">(X)</span></span></span><br></pre></td></tr></table></figure></p><h3 id="1-3-在正向传播函数中执行代码"><a href="#1-3-在正向传播函数中执行代码" class="headerlink" title="1.3. 在正向传播函数中执行代码"></a>1.3. 在正向传播函数中执行代码</h3><p>Sequential类使模型构造变得简单，允许我们组合新的架构，而不必定义自己的类。 然而并不是所有的架构都是简单的顺序架构，当需要更强的灵活性时，我们需要定义自己的块。 例如，我们可能希望在前向传播函数中执行Python的控制流。 此外，我们可能希望执行任意的数学运算，而不是简单地依赖预定义的神经网络层。</p><p>到目前为止, 我们网络中的所有操作都对网络的激活值及网络的参数起作用。然而有时可能希望合并一些常数参数(constant parameter)，即那些既不是上一层的结果也不是可更新参数的项。例如,我们需要一个计算函数 $f(\mathbf{x}, \mathbf{w})=c \cdot \mathbf{w}^{\top} \mathbf{x}$ 的层, 其中 $\mathbf{x}$ 是输入, $\mathbf{w}$ 是参数, $c$ 是某个在优化过程中没有更新的指定常量。<br>对此情况，书中实现了一个FixedHiddenMLP类, 如下所示：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FixedHiddenMLP</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 不计算梯度的随机权重参数。因此其在训练期间保持不变</span></span><br><span class="line">        <span class="comment"># 常量参数c</span></span><br><span class="line">        self.rand_weight = torch.rand((<span class="number">20</span>, <span class="number">20</span>), requires_grad=<span class="literal">False</span>)</span><br><span class="line">        self.linear = nn.Linear(<span class="number">20</span>, <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        X = self.linear(X)</span><br><span class="line">        <span class="comment"># 使用创建的 常量参数 以及relu和mm函数</span></span><br><span class="line">        X = F.relu(torch.mm(X, self.rand_weight) + <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 复用全连接层。这相当于两个全连接层共享参数</span></span><br><span class="line">        X = self.linear(X)</span><br><span class="line">        <span class="comment"># 控制流</span></span><br><span class="line">        <span class="keyword">while</span> X.<span class="built_in">abs</span>().<span class="built_in">sum</span>() &gt; <span class="number">1</span>:</span><br><span class="line">            X /= <span class="number">2</span></span><br><span class="line">        <span class="keyword">return</span> X.<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure></p><p>在这个FixedHiddenMLP模型中实现了一个隐藏层，其权重（self.rand_weight）在实例化时被随机初始化，之后为常量。 这个权重不是一个模型参数，因此它永远不会被反向传播更新。 然后，神经网络将这个固定层的输出通过一个全连接层。</p><p>在返回输出之前, 模型做了一些不寻常的事情: 它运行了一个while循环, 在 $L_{1}$ 范数大于 1 的条件下, 将 输出向量除以 2 , 直到它满足条件为止。最后, 模型返回了X中所有项的和。这个操作可能不会常用于在任何实际任务中, 只是展示了如何将任意代码集成到神经网络计算的流程中。</p><p>使用该类实现多层感知机，same thing：<br><figure class="highlight isbl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">net</span> = <span class="function"><span class="title">FixedHiddenMLP</span>()</span></span><br><span class="line"><span class="function"><span class="title">net</span>(<span class="variable">X</span>)</span></span><br></pre></td></tr></table></figure></p><p>混合搭配各种组合块的实现举例：<br><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">NestMLP</span>(<span class="title">nn</span>.<span class="type">Module</span>):</span></span><br><span class="line"><span class="class">    def __init__(<span class="title">self</span>):</span></span><br><span class="line"><span class="class">        super().__init__()</span></span><br><span class="line"><span class="class">        self.net = nn.<span class="type">Sequential</span>(<span class="title">nn</span>.<span class="type">Linear</span>(20, 64), nn.<span class="type">ReLU</span>(),</span></span><br><span class="line"><span class="class">                                 nn.<span class="type">Linear</span>(64, 32), nn.<span class="type">ReLU</span>())</span></span><br><span class="line"><span class="class">        self.linear = nn.<span class="type">Linear</span>(32, 16)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    def forward(<span class="title">self</span>, <span class="type">X</span>):</span></span><br><span class="line"><span class="class">        return self.linear(<span class="title">self</span>.<span class="title">net</span>(<span class="type">X</span>))</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">chimera = nn.<span class="type">Sequential</span>(<span class="type">NestMLP</span>(), nn.<span class="type">Linear</span>(16, 20), <span class="type">FixedHiddenMLP</span>())</span></span><br><span class="line"><span class="class">chimera(<span class="type">X</span>)</span></span><br></pre></td></tr></table></figure></p><h3 id="1-4-效率问题"><a href="#1-4-效率问题" class="headerlink" title="1.4. 效率问题"></a>1.4. 效率问题</h3><p>我们在一个高性能的深度学习库中进行了大量的字典查找、 代码执行和许多其他的Python代码。<br>Python的全局解释器锁问题很可能导致运行速度变慢。 CPython 中，全局解释器锁或GIL是一个互斥锁，用于保护对 Python 对象的访问，防止多个线程同时执行 Python 字节码。GIL 防止竞争条件并确保线程安全。这个互斥锁是必要的，主要是因为 CPython 的内存管理不是线程安全的。<br>在深度学习环境中，速度极快的GPU可能要等到CPU运行Python代码后才能运行另一个作业。</p><h2 id="2-参数管理"><a href="#2-参数管理" class="headerlink" title="2. 参数管理"></a>2. 参数管理</h2><p>在选择了架构并设置了超参数后，就进入了训练阶段。 此时，我们的目标是找到使损失函数最小化的模型参数值。 经过训练后，将需要使用这些参数来做出未来的预测。<br>此外，有时我们希望提取参数，以便在其他环境中复用它们， 将模型保存下来，以便它可以在其他软件中执行， 或者为了获得科学的理解而进行检查。</p><p>此节将介绍以下内容：</p><ul><li>访问参数，用于调试、诊断和可视化。</li><li>参数初始化。</li><li>在不同模型组件间共享参数。</li></ul><p>首先，对于一个单隐含层的多层感知机进行研究：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line"></span><br><span class="line">net = nn<span class="selector-class">.Sequential</span>(nn<span class="selector-class">.Linear</span>(<span class="number">4</span>, <span class="number">8</span>), nn<span class="selector-class">.ReLU</span>(), nn<span class="selector-class">.Linear</span>(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line">X = torch<span class="selector-class">.rand</span>(size=(<span class="number">2</span>, <span class="number">4</span>))</span><br><span class="line"><span class="function"><span class="title">net</span><span class="params">(X)</span></span></span><br></pre></td></tr></table></figure></p><h3 id="2-1-参数访问"><a href="#2-1-参数访问" class="headerlink" title="2.1. 参数访问"></a>2.1. 参数访问</h3><p>当通过Sequential类定义模型时， 我们可以通过索引来访问模型的任意层。 这就像模型是一个列表一样，每层的参数都在其属性中。例如检查第二个全连接层的参数：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="title">print</span><span class="params">(net[<span class="number">2</span>].state_dict()</span></span>)</span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">OrderedDict</span>([(&#x27;weight&#x27;, tensor([[ <span class="number">0</span>.<span class="number">3231</span>, -<span class="number">0</span>.<span class="number">3373</span>,  <span class="number">0</span>.<span class="number">1639</span>, -<span class="number">0</span>.<span class="number">3125</span>,  <span class="number">0</span>.<span class="number">0527</span>, -<span class="number">0</span>.<span class="number">2957</span>,  <span class="number">0</span>.<span class="number">0192</span>,  <span class="number">0</span>.<span class="number">0039</span>]])), (&#x27;bias&#x27;, tensor([-<span class="number">0</span>.<span class="number">2930</span>]))])</span><br></pre></td></tr></table></figure><br>结果说明了这个全连接层包含两个参数，分别是该层的权重和偏置。 两者都存储为单精度浮点数（float32）。 注意参数名称允许唯一标识每个参数，即使在包含数百个层的网络中也是如此。</p><h4 id="2-1-1-目标参数"><a href="#2-1-1-目标参数" class="headerlink" title="2.1.1. 目标参数"></a>2.1.1. 目标参数</h4><p>层中的每个参数都表示为参数类的一个实例。 要对参数执行任何操作，首先需要访问底层的数值。 有几种方法可以做到这一点。有些比较简单，而另一些则比较通用。<br>下面的代码从第二个全连接层（即第三个神经网络层）提取偏置， 提取后返回的是一个参数类实例，并进一步访问该参数的值。<br><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span>(<span class="keyword">type</span>(<span class="keyword">net</span>[2].<span class="keyword">bias</span>))</span><br><span class="line"><span class="keyword">print</span>(<span class="keyword">net</span>[2].<span class="keyword">bias</span>)</span><br><span class="line"><span class="keyword">print</span>(<span class="keyword">net</span>[2].<span class="keyword">bias</span>.data)</span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;class <span class="string">&#x27;torch.nn.parameter.Parameter&#x27;</span>&gt;</span><br><span class="line">Parameter containing:</span><br><span class="line"><span class="function"><span class="title">tensor</span><span class="params">([-<span class="number">0.2930</span>], requires_grad=True)</span></span></span><br><span class="line"><span class="function"><span class="title">tensor</span><span class="params">([-<span class="number">0.2930</span>])</span></span></span><br></pre></td></tr></table></figure><br>参数是复合的对象，包含值、梯度和额外信息。 这就是我们需要显式参数值的原因。 除了值之外，还可以访问每个参数的梯度。 在上面这个网络中，由于我们还没有调用反向传播，所以参数的梯度处于初始状态。<br><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net[2]<span class="string">.weight.grad</span> == None <span class="comment"># 为真</span></span><br></pre></td></tr></table></figure></p><h4 id="2-1-2-一次性访问所有参数"><a href="#2-1-2-一次性访问所有参数" class="headerlink" title="2.1.2. 一次性访问所有参数"></a>2.1.2. 一次性访问所有参数</h4><p>有些情况需要访问全部参数，甚至是递归的访问嵌套块的参数。<br>对比访问第一个全连接层的参数和访问所有层的参数：<br><figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># python星号变量的作用是将变量拆分为单个元素</span></span><br><span class="line">print(*[(<span class="built_in">name</span>, <span class="built_in">param</span>.shape) <span class="keyword">for</span> <span class="built_in">name</span>, <span class="built_in">param</span> <span class="built_in">in</span> net[<span class="number">0</span>].named_parameters()])</span><br><span class="line">print(*[(<span class="built_in">name</span>, <span class="built_in">param</span>.shape) <span class="keyword">for</span> <span class="built_in">name</span>, <span class="built_in">param</span> <span class="built_in">in</span> net.named_parameters()])</span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(<span class="symbol">&#x27;weight</span>&#x27;, torch.Size([<span class="name">8</span>, <span class="number">4</span>])) (<span class="symbol">&#x27;bias</span>&#x27;, torch.Size([<span class="name">8</span>]))</span><br><span class="line">(<span class="symbol">&#x27;0.weight</span>&#x27;, torch.Size([<span class="name">8</span>, <span class="number">4</span>])) (<span class="symbol">&#x27;0.bias</span>&#x27;, torch.Size([<span class="name">8</span>])) (<span class="symbol">&#x27;2.weight</span>&#x27;, torch.Size([<span class="name">1</span>, <span class="number">8</span>])) (<span class="symbol">&#x27;2.bias</span>&#x27;, torch.Size([<span class="name">1</span>]))</span><br></pre></td></tr></table></figure><br>可以看到访问所有层参数时，以<code>数字.参数名</code>的形式表示参数，这为我们提供了另一种访问网络参数的方式：<br><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># 访问网络第三层的bias参数值</span></span><br><span class="line"><span class="title">net</span>.state_dict()[&#x27;<span class="number">2.</span>bias&#x27;].<span class="class"><span class="keyword">data</span></span></span><br></pre></td></tr></table></figure></p><h4 id="2-1-3-从嵌套块收集参数"><a href="#2-1-3-从嵌套块收集参数" class="headerlink" title="2.1.3. 从嵌套块收集参数"></a>2.1.3. 从嵌套块收集参数</h4><p>探索如果将多个块相互嵌套，参数命名约定是如何工作的。<br>定义一个生成块的函数（“块工厂”），然后将这些块组合到更大的块中，打印最终的网络：<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">def block1():</span><br><span class="line">    return nn.<span class="built_in">Sequential</span>(nn.<span class="built_in">Linear</span>(<span class="number">4</span>, <span class="number">8</span>), nn.<span class="built_in">ReLU</span>(),</span><br><span class="line">                         nn.<span class="built_in">Linear</span>(<span class="number">8</span>, <span class="number">4</span>), nn.<span class="built_in">ReLU</span>())</span><br><span class="line"></span><br><span class="line">def <span class="built_in">block2</span>():</span><br><span class="line">    net = nn.<span class="built_in">Sequential</span>()</span><br><span class="line">    # <span class="number">4</span>个块<span class="number">1</span>堆叠</span><br><span class="line">    for i in <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">        # 利用add_module嵌套</span><br><span class="line">        net.<span class="built_in">add_module</span>(f<span class="string">&#x27;block &#123;i&#125;&#x27;</span>, <span class="built_in">block1</span>())</span><br><span class="line">    return net</span><br><span class="line"></span><br><span class="line"># 组合块<span class="number">2</span>和一个全连接层</span><br><span class="line">rgnet = nn.<span class="built_in">Sequential</span>(<span class="built_in">block2</span>(), nn.<span class="built_in">Linear</span>(<span class="number">4</span>, <span class="number">1</span>))</span><br><span class="line"><span class="built_in">rgnet</span>(X)</span><br><span class="line"></span><br><span class="line"># 打印设计好的网络</span><br><span class="line"><span class="built_in">print</span>(rgnet)</span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">Sequential(</span><br><span class="line">  (0): Sequential(</span><br><span class="line">    (block 0): Sequential(</span><br><span class="line">      (0): Linear(<span class="attribute">in_features</span>=4, <span class="attribute">out_features</span>=8, <span class="attribute">bias</span>=<span class="literal">True</span>)</span><br><span class="line">      (1): ReLU()</span><br><span class="line">      (2): Linear(<span class="attribute">in_features</span>=8, <span class="attribute">out_features</span>=4, <span class="attribute">bias</span>=<span class="literal">True</span>)</span><br><span class="line">      (3): ReLU()</span><br><span class="line">    )</span><br><span class="line">    (block 1): Sequential(</span><br><span class="line">      (0): Linear(<span class="attribute">in_features</span>=4, <span class="attribute">out_features</span>=8, <span class="attribute">bias</span>=<span class="literal">True</span>)</span><br><span class="line">      (1): ReLU()</span><br><span class="line">      (2): Linear(<span class="attribute">in_features</span>=8, <span class="attribute">out_features</span>=4, <span class="attribute">bias</span>=<span class="literal">True</span>)</span><br><span class="line">      (3): ReLU()</span><br><span class="line">    )</span><br><span class="line">    (block 2): Sequential(</span><br><span class="line">      (0): Linear(<span class="attribute">in_features</span>=4, <span class="attribute">out_features</span>=8, <span class="attribute">bias</span>=<span class="literal">True</span>)</span><br><span class="line">      (1): ReLU()</span><br><span class="line">      (2): Linear(<span class="attribute">in_features</span>=8, <span class="attribute">out_features</span>=4, <span class="attribute">bias</span>=<span class="literal">True</span>)</span><br><span class="line">      (3): ReLU()</span><br><span class="line">    )</span><br><span class="line">    (block 3): Sequential(</span><br><span class="line">      (0): Linear(<span class="attribute">in_features</span>=4, <span class="attribute">out_features</span>=8, <span class="attribute">bias</span>=<span class="literal">True</span>)</span><br><span class="line">      (1): ReLU()</span><br><span class="line">      (2): Linear(<span class="attribute">in_features</span>=8, <span class="attribute">out_features</span>=4, <span class="attribute">bias</span>=<span class="literal">True</span>)</span><br><span class="line">      (3): ReLU()</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (1): Linear(<span class="attribute">in_features</span>=4, <span class="attribute">out_features</span>=1, <span class="attribute">bias</span>=<span class="literal">True</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><br>因为层是分层嵌套的，所以可以像通过嵌套列表索引一样访问它们。<br>下面访问第一个主要的块中第二个子块的第一层的偏置项：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rgnet<span class="selector-attr">[0]</span><span class="selector-attr">[1]</span><span class="selector-attr">[0]</span><span class="selector-class">.bias</span>.data</span><br></pre></td></tr></table></figure></p><h3 id="2-2-参数初始化"><a href="#2-2-参数初始化" class="headerlink" title="2.2. 参数初始化"></a>2.2. 参数初始化</h3><p>深度学习框架提供默认随机初始化， 也允许用户创建自定义初始化方法， 满足通过其他规则实现初始化权重的需要。</p><p>对于PyTorch框架来说，默认情况下，PyTorch会根据一个范围均匀地初始化权重和偏置矩阵， 这个范围是根据输入和输出维度计算出的。 PyTorch的nn.init模块提供了多种预置初始化方法。</p><h4 id="2-2-1-内置初始化"><a href="#2-2-1-内置初始化" class="headerlink" title="2.2.1. 内置初始化"></a>2.2.1. 内置初始化</h4><p>首先调用内置的初始化器,下面的代码将所有权重参数初始化为标准差为0.01的高斯(正态）随机变量，且将偏置参数设置为0。<br><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def init<span class="constructor">_normal(<span class="params">m</span>)</span>:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">type</span>(m)<span class="operator"> == </span>nn.Linear:</span><br><span class="line">        nn.init.normal<span class="constructor">_(<span class="params">m</span>.<span class="params">weight</span>, <span class="params">mean</span>=0, <span class="params">std</span>=0.01)</span></span><br><span class="line">        nn.init.zeros<span class="constructor">_(<span class="params">m</span>.<span class="params">bias</span>)</span></span><br><span class="line"># 注意用法，apply</span><br><span class="line">net.apply(init_normal)</span><br><span class="line">net<span class="literal">[<span class="number">0</span>]</span>.weight.data<span class="literal">[<span class="number">0</span>]</span>, net<span class="literal">[<span class="number">0</span>]</span>.bias.data<span class="literal">[<span class="number">0</span>]</span></span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="name">tensor</span>([<span class="number">-0.0017</span>,  <span class="number">0.0232</span>, <span class="number">-0.0026</span>,  <span class="number">0.0026</span>]), tensor(<span class="number">0</span>.))</span><br></pre></td></tr></table></figure><br>还可以将所有参数初始化为给定的常数，比如初始化为1：<br><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def init<span class="constructor">_constant(<span class="params">m</span>)</span>:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">type</span>(m)<span class="operator"> == </span>nn.Linear:</span><br><span class="line">        nn.init.constant<span class="constructor">_(<span class="params">m</span>.<span class="params">weight</span>, 1)</span></span><br><span class="line">        nn.init.zeros<span class="constructor">_(<span class="params">m</span>.<span class="params">bias</span>)</span></span><br><span class="line">net.apply(init_constant)</span><br></pre></td></tr></table></figure><br>还可以对某些块应用不同的初始化方法。 例如，使用Xavier初始化方法初始化第一个神经网络层，然后将第三个神经网络层初始化为常量值42：<br><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">def</span> xavier(m):</span><br><span class="line">    <span class="keyword">if</span> <span class="class"><span class="keyword">type</span>(<span class="title">m</span>) == nn.<span class="type">Linear</span>:</span></span><br><span class="line">        nn.init.xavier_uniform_(m.weight)</span><br><span class="line"><span class="title">def</span> init_42(m):</span><br><span class="line">    <span class="keyword">if</span> <span class="class"><span class="keyword">type</span>(<span class="title">m</span>) == nn.<span class="type">Linear</span>:</span></span><br><span class="line">        nn.init.constant_(m.weight, <span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="title">net</span>[<span class="number">0</span>].apply(xavier)</span><br><span class="line"><span class="title">net</span>[<span class="number">2</span>].apply(init_42)</span><br><span class="line"><span class="title">print</span>(net[<span class="number">0</span>].weight.<span class="class"><span class="keyword">data</span>[0])</span></span><br><span class="line"><span class="title">print</span>(net[<span class="number">2</span>].weight.<span class="class"><span class="keyword">data</span>)</span></span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">tensor</span>([-<span class="number">0</span>.<span class="number">4645</span>,  <span class="number">0</span>.<span class="number">0062</span>, -<span class="number">0</span>.<span class="number">5186</span>,  <span class="number">0</span>.<span class="number">3513</span>])</span><br><span class="line"><span class="attribute">tensor</span>([[<span class="number">42</span>., <span class="number">42</span>., <span class="number">42</span>., <span class="number">42</span>., <span class="number">42</span>., <span class="number">42</span>., <span class="number">42</span>., <span class="number">42</span>.]])</span><br></pre></td></tr></table></figure></p><h4 id="2-2-2-自定义初始化"><a href="#2-2-2-自定义初始化" class="headerlink" title="2.2.2. 自定义初始化"></a>2.2.2. 自定义初始化</h4><p>有时, 深度学习框架没有提供我们需要的初始化方法。下面的例子中, 我们使用以下的分布为任意权重参数 $w$ 定义初始化方法：</p><script type="math/tex; mode=display">w \sim \begin{cases}U(5,10) & \text { 可能性 } \frac{1}{4} \\ 0 & \text { 可能性 } \frac{1}{2} \\ U(-10,-5) & \text { 可能性 } \frac{1}{4}\end{cases}</script><p>对应<code>my_init</code>：<br><figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def my_init(m):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">type</span>(m) == nn.Linear:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Init&quot;</span>, *[(<span class="keyword">name</span>, param.<span class="built_in">shape</span>)</span><br><span class="line">                        for <span class="keyword">name</span>, param <span class="keyword">in</span> m.named_parameters()][<span class="number">0</span>])</span><br><span class="line">        # -<span class="number">10</span> 到 <span class="number">10</span> 的范围均匀分布</span><br><span class="line">        nn.init.uniform_(m.weight, -<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">        # 这里m.weight.<span class="keyword">data</span>.<span class="built_in">abs</span>() &gt;= <span class="number">5</span>为一个布尔矩阵</span><br><span class="line">        # 将原矩阵与该布尔矩阵按元素乘，小于<span class="number">5</span>的值会变为<span class="number">0</span>，其余元素不变</span><br><span class="line">        m.weight.<span class="keyword">data</span> *= m.weight.<span class="keyword">data</span>.<span class="built_in">abs</span>() &gt;= <span class="number">5</span></span><br><span class="line"></span><br><span class="line">net.apply(my_init)</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">0</span>].weight[:<span class="number">2</span>])</span><br></pre></td></tr></table></figure><br>By the way，参数当然也可以被直接的设置和修改：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net<span class="selector-attr">[0]</span><span class="selector-class">.weight</span><span class="selector-class">.data</span><span class="selector-attr">[:]</span> += <span class="number">1</span></span><br><span class="line">net<span class="selector-attr">[0]</span><span class="selector-class">.weight</span><span class="selector-class">.data</span><span class="selector-attr">[0, 0]</span> = <span class="number">42</span></span><br><span class="line">net<span class="selector-attr">[0]</span><span class="selector-class">.weight</span><span class="selector-class">.data</span><span class="selector-attr">[0]</span></span><br></pre></td></tr></table></figure></p><h3 id="2-3-参数绑定"><a href="#2-3-参数绑定" class="headerlink" title="2.3. 参数绑定"></a>2.3. 参数绑定</h3><p>有时我们希望在多个层间共享参数：可以定义一个稠密层，然后使用它的参数来设置另一个层的参数。<br><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 我们需要给共享层一个名称，以便可以引用它的参数</span></span><br><span class="line"><span class="attribute">shared</span> = nn.Linear(<span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line"><span class="attribute">net</span> = nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(),</span><br><span class="line">                    <span class="attribute">shared</span>, nn.ReLU(),</span><br><span class="line">                    <span class="attribute">shared</span>, nn.ReLU(),</span><br><span class="line">                    <span class="attribute">nn</span>.Linear(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line"><span class="attribute">net</span>(X)</span><br><span class="line"><span class="comment"># 检查参数是否相同</span></span><br><span class="line"><span class="attribute">print</span>(net[<span class="number">2</span>].weight.data[<span class="number">0</span>] == net[<span class="number">4</span>].weight.data[<span class="number">0</span>])</span><br><span class="line"><span class="attribute">net</span>[<span class="number">2</span>].weight.data[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">100</span></span><br><span class="line"><span class="comment"># 确保它们实际上是同一个对象，而不只是有相同的值</span></span><br><span class="line"><span class="attribute">print</span>(net[<span class="number">2</span>].weight.data[<span class="number">0</span>] == net[<span class="number">4</span>].weight.data[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>])</span><br><span class="line">tensor([<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>])</span><br></pre></td></tr></table></figure></p><h2 id="3-延后初始化"><a href="#3-延后初始化" class="headerlink" title="3. 延后初始化"></a>3. 延后初始化</h2><p>非PyTorch的实践中忽略了建立网络时需要做的以下这些事情：</p><ul><li>定义了网络架构，但没有指定输入维度。</li><li>添加层时没有指定前一层的输出维度。</li><li>在初始化参数时，甚至没有足够的信息来确定模型应该包含多少参数。</li></ul><p>但代码却依然能够运行，即便深度学习框架无法判断网络的输入维度是什么。这里的诀窍是框架的延后初始化（defers initialization）， 即直到数据第一次通过模型传递时，框架才会动态地推断出每个层的大小。</p><p>当使用卷积神经网络时，由于输入维度（即图像的分辨率）将影响每个后续层的维数，有了该技术将更加方便。<br>现在我们在编写代码时无须知道维度是什么就可以设置参数， 这种能力可以大大简化定义和修改模型的任务。<br>下面开始研究初始化机制，以TensorFlow为例。似乎PyTorch也准备推出这个功能，为<code>torch.nn.LazyLinear</code>。</p><h3 id="3-1-实例化网络"><a href="#3-1-实例化网络" class="headerlink" title="3.1. 实例化网络"></a>3.1. 实例化网络</h3><p>实例化一个多层感知机：<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow <span class="keyword">as</span> <span class="keyword">tf</span></span><br><span class="line"></span><br><span class="line"># keras是TensorFlow中的高层神经网络API</span><br><span class="line"># Dense层为全连接层，等同Linear</span><br><span class="line">net = <span class="keyword">tf</span>.keras.models.Sequential([</span><br><span class="line">    # <span class="number">256</span>处的参数为units，代表输出维度</span><br><span class="line">    <span class="keyword">tf</span>.keras.layers.Dense(<span class="number">256</span>, activation=<span class="keyword">tf</span>.<span class="keyword">nn</span>.relu),</span><br><span class="line">    <span class="keyword">tf</span>.keras.layers.Dense(<span class="number">10</span>),</span><br><span class="line">])</span><br></pre></td></tr></table></figure><br>此时输入维数是未知的，所以网络不可能知道输入层权重的维数。 因此框架尚未初始化任何参数，通过尝试访问以下参数进行确认：<br><figure class="highlight hy"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[net.layers[i].get_weights() for i in range(<span class="name"><span class="builtin-name">len</span></span>(<span class="name">net.layers</span>))]</span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[], []]</span><br></pre></td></tr></table></figure><br>每个层对象都存在，但权重为空。 使用net.get_weights()将抛出一个错误，因为权重尚未初始化。<br>接下来将数据通过网络，最终使框架初始化参数。<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X = tf<span class="selector-class">.random</span><span class="selector-class">.uniform</span>((<span class="number">2</span>, <span class="number">20</span>))</span><br><span class="line"><span class="function"><span class="title">net</span><span class="params">(X)</span></span></span><br><span class="line"><span class="selector-attr">[w.shape for w in net.get_weights()]</span></span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(<span class="number">20</span>, <span class="number">256</span>), (<span class="number">256</span>,), (<span class="number">256</span>, <span class="number">10</span>), (<span class="number">10</span>,)]</span><br></pre></td></tr></table></figure><br>一旦知道输入维数是20，框架可以通过代入值20来识别第一层权重矩阵的形状。 识别出第一层的形状后，框架处理第二层，依此类推，直到所有形状都已知为止。 注意在这种情况下，只有第一层需要<strong>延迟初始化</strong>，但是框架仍是按顺序初始化的。 等到知道了所有的参数形状，框架就可以初始化参数。</p><h2 id="4-自定义层"><a href="#4-自定义层" class="headerlink" title="4. 自定义层"></a>4. 自定义层</h2><p>深度学习成功背后的一个因素是神经网络的灵活性： 我们可以用创造性的方式组合不同的层，从而设计出适用于各种任务的架构。<br>例如，研究人员发明了专门用于处理图像、文本、序列数据和执行动态规划的层。 当深度学习框架并未提供你需要的层时，则必须构建自定义层。</p><h3 id="4-1-不带参数的层"><a href="#4-1-不带参数的层" class="headerlink" title="4.1. 不带参数的层"></a>4.1. 不带参数的层</h3><p>构造一个没有任何参数的自定义层，CenteredLayer类要从其输入中减去均值，构建它只需继承基础层类并实现前向传播功能：<br><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="title">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">CenteredLayer</span>(<span class="title">nn</span>.<span class="type">Module</span>):</span></span><br><span class="line"><span class="class">    def __init__(<span class="title">self</span>):</span></span><br><span class="line"><span class="class">        super().__init__()</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    def forward(<span class="title">self</span>, <span class="type">X</span>):</span></span><br><span class="line"><span class="class">        return <span class="type">X</span> - <span class="type">X</span>.mean()</span></span><br></pre></td></tr></table></figure><br>向该层提供一些数据，验证它是否能按预期工作：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">layer = CenteredLayer()</span><br><span class="line"><span class="function"><span class="title">layer</span><span class="params">(torch.FloatTensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span></span>)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line"><span class="function"><span class="title">tensor</span><span class="params">([-<span class="number">2</span>., -<span class="number">1</span>.,  <span class="number">0</span>.,  <span class="number">1</span>.,  <span class="number">2</span>.])</span></span></span><br></pre></td></tr></table></figure></p><p>将层作为组件合并到更复杂的模型中：<br><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net = nn.<span class="constructor">Sequential(<span class="params">nn</span>.Linear(8, 128)</span>, <span class="constructor">CenteredLayer()</span>)</span><br></pre></td></tr></table></figure><br>在向该网络发送随机数据后，检查一下均值是否为0：<br><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">Y</span> = net(torch.rand(<span class="number">4</span>, <span class="number">8</span>))</span><br><span class="line"><span class="attribute">Y</span>.mean()</span><br></pre></td></tr></table></figure><br>由于处理的是浮点数，因为存储精度的原因，仍然可能会看到一个非常小的非零数（正常来说Y的均值应该是0）。</p><h3 id="4-2-带参数的层"><a href="#4-2-带参数的层" class="headerlink" title="4.2. 带参数的层"></a>4.2. 带参数的层</h3><p>下面定义具有参数的层， 这些参数可以通过训练进行调整。 我们可以使用内置函数来创建参数，这些函数提供一些基本的管理功能。 比如管理访问、初始化、共享、保存和加载模型参数。 这样做的好处之一是：我们不需要为每个自定义层编写自定义的序列化程序。</p><p>全连接层层需要两个参数，一个用于表示权重，另一个用于表示偏置项。 在这个版本的自定义实现中，使用修正线性单元作为激活函数。该层需要输入参数in_units和units，分别表示输入数和输出数：<br><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># 自定义实现全连接层</span><br><span class="line"><span class="keyword">class</span> <span class="constructor">MyLinear(<span class="params">nn</span>.Module)</span>:</span><br><span class="line">    def <span class="constructor">__init__(<span class="params">self</span>, <span class="params">in_units</span>, <span class="params">units</span>)</span>:</span><br><span class="line">        super<span class="literal">()</span>.<span class="constructor">__init__()</span></span><br><span class="line">        # nn.Parameter处理参数会有很多好处</span><br><span class="line">        # 例如可以使用named<span class="constructor">_parameters()</span>获取参数等</span><br><span class="line">        # 即可以像内置的Module一样使用自定义层</span><br><span class="line">        self.weight = nn.<span class="constructor">Parameter(<span class="params">torch</span>.<span class="params">randn</span>(<span class="params">in_units</span>, <span class="params">units</span>)</span>)</span><br><span class="line">        self.bias = nn.<span class="constructor">Parameter(<span class="params">torch</span>.<span class="params">randn</span>(<span class="params">units</span>,)</span>)</span><br><span class="line">    def forward(self, X):</span><br><span class="line">        # 全连接层计算</span><br><span class="line">        linear = torch.matmul(X, self.weight.data) + self.bias.data</span><br><span class="line">        # 激活函数</span><br><span class="line">        return <span class="module-access"><span class="module"><span class="identifier">F</span>.</span></span>relu(linear)</span><br></pre></td></tr></table></figure><br>实例化MyLinear类并访问其模型参数：<br><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">linear = MyLinear(5, 3)</span><br><span class="line">linear.weight</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor(<span class="comment">[<span class="comment">[ 1.9054, -3.4102, -0.9792]</span>,</span></span><br><span class="line"><span class="comment">        <span class="comment">[ 1.5522,  0.8707,  0.6481]</span>,</span></span><br><span class="line"><span class="comment">        <span class="comment">[ 1.0974,  0.2568,  0.4034]</span>,</span></span><br><span class="line"><span class="comment">        <span class="comment">[ 0.1416, -1.1389,  0.5875]</span>,</span></span><br><span class="line"><span class="comment">        <span class="comment">[-0.7209,  0.4432,  0.1222]</span>]</span>, requires_grad=True)</span><br></pre></td></tr></table></figure><br>使用自定义层直接执行前向传播计算：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="title">linear</span><span class="params">(torch.rand(<span class="number">2</span>, <span class="number">5</span>)</span></span>)</span><br></pre></td></tr></table></figure><br>使用自定义层构建模型：<br><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">net</span> = nn.Sequential(MyLinear(<span class="number">64</span>, <span class="number">8</span>), MyLinear(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line"><span class="attribute">net</span>(torch.rand(<span class="number">2</span>, <span class="number">64</span>))</span><br></pre></td></tr></table></figure></p><h2 id="5-读写文件"><a href="#5-读写文件" class="headerlink" title="5. 读写文件"></a>5. 读写文件</h2><p>有时我们希望保存训练的模型， 以备将来在各种环境中使用（比如在部署中进行预测）。 此外，当运行一个耗时较长的训练过程时， 最佳的做法是定期保存中间结果， 以确保在服务器电源被不小心断掉时不会损失几天的计算结果。<br>本节学习如何加载和存储权重向量和整个模型。</p><h3 id="5-1-加载和保存张量"><a href="#5-1-加载和保存张量" class="headerlink" title="5.1. 加载和保存张量"></a>5.1. 加载和保存张量</h3><p>对于单个张量，可以直接调用load和save函数分别读写它们。 这两个函数都要求我们提供一个名称，save要求将要保存的变量作为输入。<br><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">x = torch.arange(<span class="number">4</span>)</span><br><span class="line">torch.save(x, <span class="string">&#x27;x-file&#x27;</span>)</span><br></pre></td></tr></table></figure><br>将存储在文件中的数据读回内存：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x2 = torch<span class="selector-class">.load</span>(<span class="string">&#x27;x-file&#x27;</span>)</span><br><span class="line">x2</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line"><span class="function"><span class="title">tensor</span><span class="params">([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span></span></span><br></pre></td></tr></table></figure><br>存储一个张量列表，然后读回内存：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">y = torch<span class="selector-class">.zeros</span>(<span class="number">4</span>)</span><br><span class="line">torch<span class="selector-class">.save</span>(<span class="selector-attr">[x, y]</span>,<span class="string">&#x27;x-files&#x27;</span>)</span><br><span class="line">x2, y2 = torch<span class="selector-class">.load</span>(<span class="string">&#x27;x-files&#x27;</span>)</span><br><span class="line">(x2, y2)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">(tensor(<span class="selector-attr">[0, 1, 2, 3]</span>), tensor(<span class="selector-attr">[0., 0., 0., 0.]</span>))</span><br></pre></td></tr></table></figure><br>可以写入或读取从字符串映射到张量的字典，当需要读取或写入模型中的所有权重时很有用：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mydict = &#123;<span class="string">&#x27;x&#x27;</span>: x, <span class="string">&#x27;y&#x27;</span>: y&#125;</span><br><span class="line">torch<span class="selector-class">.save</span>(mydict, <span class="string">&#x27;mydict&#x27;</span>)</span><br><span class="line">mydict2 = torch<span class="selector-class">.load</span>(<span class="string">&#x27;mydict&#x27;</span>)</span><br><span class="line">mydict2</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">&#123;<span class="string">&#x27;x&#x27;</span>: tensor(<span class="selector-attr">[0, 1, 2, 3]</span>), <span class="string">&#x27;y&#x27;</span>: tensor(<span class="selector-attr">[0., 0., 0., 0.]</span>)&#125;</span><br></pre></td></tr></table></figure></p><h3 id="5-2-加载和保存模型参数"><a href="#5-2-加载和保存模型参数" class="headerlink" title="5.2. 加载和保存模型参数"></a>5.2. 加载和保存模型参数</h3><p>深度学习框架提供了内置函数来保存和加载整个网络。注意这将<em>保存模型的参数而不是保存整个模型</em>。<br>例如，如果我们有一个3层多层感知机，我们需要单独指定架构。 因为模型本身可以包含任意代码，难以序列化。 为了恢复模型，我们需要用代码生成架构， 然后从磁盘加载参数。<br>以下面的多层感知机为例：<br><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">MLP</span>(<span class="title">nn</span>.<span class="type">Module</span>):</span></span><br><span class="line"><span class="class">    def __init__(<span class="title">self</span>):</span></span><br><span class="line"><span class="class">        super().__init__()</span></span><br><span class="line"><span class="class">        self.hidden = nn.<span class="type">Linear</span>(20, 256)</span></span><br><span class="line"><span class="class">        self.output = nn.<span class="type">Linear</span>(256, 10)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    def forward(<span class="title">self</span>, <span class="title">x</span>):</span></span><br><span class="line"><span class="class">        return self.output(<span class="type">F</span>.<span class="title">relu</span>(<span class="title">self</span>.<span class="title">hidden</span>(<span class="title">x</span>)))</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">net = <span class="type">MLP</span>()</span></span><br><span class="line"><span class="class"><span class="type">X</span> = torch.randn(<span class="title">size</span>=(2, 20))</span></span><br><span class="line"><span class="class"><span class="type">Y</span> = net(<span class="type">X</span>)</span></span><br></pre></td></tr></table></figure><br>存储模型参数到文件：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch<span class="selector-class">.save</span>(net<span class="selector-class">.state_dict</span>(), <span class="string">&#x27;mlp.params&#x27;</span>)</span><br></pre></td></tr></table></figure><br>恢复模型，实例化了原始多层感知机模型的一个备份。这里不需要随机初始化模型参数，而是直接读取文件中存储的参数：<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">clone</span> = MLP()</span><br><span class="line"><span class="keyword">clone</span>.load_state_dict(torch.load(<span class="string">&#x27;mlp.params&#x27;</span>))</span><br><span class="line"><span class="comment"># 进入评估模式,对这个例子没啥用</span></span><br><span class="line"><span class="keyword">clone</span>.<span class="keyword">eval</span>()</span><br></pre></td></tr></table></figure><br>由于两个实例具有相同的模型参数，在输入相同的X时，两个实例的计算结果应该相同，即：<code>clone(X) == Y</code></p><h2 id="6-GPU"><a href="#6-GPU" class="headerlink" title="6. GPU"></a>6. GPU</h2><p>自2000年以来，GPU性能每十年增长1000倍。本节将讨论如何使用单个GPU，然后是如何使用多个GPU和多个服务器（具有多个GPU）。</p><p>先看看如何使用单个NVIDIA GPU进行计算。<br>首先，确保实验机器至少安装了一个NVIDIA GPU。 然后，下载<a href="https://developer.nvidia.com/cuda-downloads">NVIDIA驱动和CUDA</a> 并按照提示设置适当的路径。 当这些准备工作完成，就可以使用nvidia-smi命令来查看显卡信息。<br>例如：<br><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">Thu Mar 31 23:45:57 2022</span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">|<span class="string"> NVIDIA-SMI 460.27.04    Driver Version: 460.27.04    CUDA Version: 11.2     </span>|</span><br><span class="line">|<span class="string">-------------------------------+----------------------+----------------------+</span></span><br><span class="line"><span class="string"></span>|<span class="string"> GPU  Name        Persistence-M</span>|<span class="string"> Bus-Id        Disp.A </span>|<span class="string"> Volatile Uncorr. ECC </span>|</span><br><span class="line">|<span class="string"> Fan  Temp  Perf  Pwr:Usage/Cap</span>|<span class="string">         Memory-Usage </span>|<span class="string"> GPU-Util  Compute M. </span>|</span><br><span class="line">|<span class="string">                               </span>|<span class="string">                      </span>|<span class="string">               MIG M. </span>|</span><br><span class="line">|<span class="string">===============================+======================+======================</span>|</span><br><span class="line">|<span class="string">   0  Tesla V100-SXM2...  Off  </span>|<span class="string"> 00000000:00:1B.0 Off </span>|<span class="string">                    0 </span>|</span><br><span class="line">|<span class="string"> N/A   44C    P0    77W / 300W </span>|<span class="string">   1612MiB / 16160MiB </span>|<span class="string">     63%      Default </span>|</span><br><span class="line">|<span class="string">                               </span>|<span class="string">                      </span>|<span class="string">                  N/A </span>|</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line">|<span class="string">   1  Tesla V100-SXM2...  Off  </span>|<span class="string"> 00000000:00:1C.0 Off </span>|<span class="string">                    0 </span>|</span><br><span class="line">|<span class="string"> N/A   47C    P0    38W / 300W </span>|<span class="string">      0MiB / 16160MiB </span>|<span class="string">      0%      Default </span>|</span><br><span class="line">|<span class="string">                               </span>|<span class="string">                      </span>|<span class="string">                  N/A </span>|</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line">|<span class="string">   2  Tesla V100-SXM2...  Off  </span>|<span class="string"> 00000000:00:1D.0 Off </span>|<span class="string">                    0 </span>|</span><br><span class="line">|<span class="string"> N/A   44C    P0    77W / 300W </span>|<span class="string">   1624MiB / 16160MiB </span>|<span class="string">     61%      Default </span>|</span><br><span class="line">|<span class="string">                               </span>|<span class="string">                      </span>|<span class="string">                  N/A </span>|</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line">|<span class="string">   3  Tesla V100-SXM2...  Off  </span>|<span class="string"> 00000000:00:1E.0 Off </span>|<span class="string">                    0 </span>|</span><br><span class="line">|<span class="string"> N/A   52C    P0    42W / 300W </span>|<span class="string">      0MiB / 16160MiB </span>|<span class="string">      0%      Default </span>|</span><br><span class="line">|<span class="string">                               </span>|<span class="string">                      </span>|<span class="string">                  N/A </span>|</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line"></span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">|<span class="string"> Processes:                                                                  </span>|</span><br><span class="line">|<span class="string">  GPU   GI   CI        PID   Type   Process name                  GPU Memory </span>|</span><br><span class="line">|<span class="string">        ID   ID                                                   Usage      </span>|</span><br><span class="line">|<span class="string">=============================================================================</span>|</span><br><span class="line">|<span class="string">    0   N/A  N/A     94484      C   ...l-zh-release-0/bin/python     1609MiB </span>|</span><br><span class="line">|<span class="string">    2   N/A  N/A     94484      C   ...l-zh-release-0/bin/python     1621MiB </span>|</span><br><span class="line">+-----------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure><br>在PyTorch中，每个数组都有一个设备（device）， 我们通常将其称为上下文（context）。 默认情况下，所有变量和相关的计算都分配给CPU。 有时上下文可能是GPU。 当我们跨多个服务器部署作业时，事情会变得更加棘手。 通过智能地将数组分配给上下文（设备）， 我们可以最大限度地减少在设备之间传输数据的时间。 例如当在带有GPU的服务器上训练神经网络时，通常希望模型的参数在GPU上。</p><p>要运行此部分中的程序，至少需要两个GPU。（目前一个都没有😭）</p><h3 id="6-1-计算设备"><a href="#6-1-计算设备" class="headerlink" title="6.1. 计算设备"></a>6.1. 计算设备</h3><p>我们可以指定用于存储和计算的设备，如CPU和GPU。 默认情况下，张量是在内存中创建的，然后使用CPU计算它。</p><p>在PyTorch中，CPU和GPU可以用torch.device(‘cpu’) 和torch.device(‘cuda’)表示。 应该注意的是，cpu设备意味着所有物理CPU和内存，这意味着PyTorch的计算将尝试使用所有CPU核心。 然而，gpu设备只代表一个卡和相应的显存。 如果有多个GPU，我们使用torch.device(f’cuda:{i}’) 来表示第i块GPU（i从0开始）。 另外，cuda:0和cuda是等价的。</p><p>对于M1芯片的设备，<code>torch.has_mps</code>可以检查是否有gpu😆。<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"><span class="keyword">from</span> torch import nn</span><br><span class="line"></span><br><span class="line">torch.device(<span class="string">&#x27;cpu&#x27;</span>), torch.device(<span class="string">&#x27;cuda&#x27;</span>), torch.device(<span class="string">&#x27;cuda:1&#x27;</span>)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">(device(<span class="attribute">type</span>=<span class="string">&#x27;cpu&#x27;</span>), device(<span class="attribute">type</span>=<span class="string">&#x27;cuda&#x27;</span>), device(<span class="attribute">type</span>=<span class="string">&#x27;cuda&#x27;</span>, <span class="attribute">index</span>=1))</span><br></pre></td></tr></table></figure><br>查询可用gpu数量：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch<span class="selector-class">.cuda</span><span class="selector-class">.device_count</span>()</span><br></pre></td></tr></table></figure><br>定义两个函数,允许我们在不存在所需所有GPU的情况下运行代码。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">try_gpu</span>(<span class="params">i=<span class="number">0</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;如果存在，则返回gpu(i)，否则返回cpu()&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> torch.cuda.device_count() &gt;= i + <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.device(<span class="string">f&#x27;cuda:<span class="subst">&#123;i&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.device(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">try_all_gpus</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回所有可用的GPU，如果没有GPU，则返回[cpu(),]&quot;&quot;&quot;</span></span><br><span class="line">    devices = [torch.device(<span class="string">f&#x27;cuda:<span class="subst">&#123;i&#125;</span>&#x27;</span>)</span><br><span class="line">             <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(torch.cuda.device_count())]</span><br><span class="line">    <span class="keyword">return</span> devices <span class="keyword">if</span> devices <span class="keyword">else</span> [torch.device(<span class="string">&#x27;cpu&#x27;</span>)]</span><br><span class="line"></span><br><span class="line">try_gpu(), try_gpu(<span class="number">10</span>), try_all_gpus()</span><br></pre></td></tr></table></figure></p><h3 id="6-2-张量与GPU"><a href="#6-2-张量与GPU" class="headerlink" title="6.2. 张量与GPU"></a>6.2. 张量与GPU</h3><p>查询张量所在的设备，默认情况下，张量是在CPU上创建的：<br><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">x</span> = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="attribute">x</span>.device</span><br></pre></td></tr></table></figure><br>需要注意的是，无论何时我们要对多个项进行操作， 它们都必须在同一个设备上。 例如，如果我们对两个张量求和， 我们需要确保两个张量都位于同一个设备上，否则框架将不知道在哪里存储结果，甚至不知道在哪里执行计算。</p><h4 id="6-2-1-存储在GPU上"><a href="#6-2-1-存储在GPU上" class="headerlink" title="6.2.1. 存储在GPU上"></a>6.2.1. 存储在GPU上</h4><p>有几种方法可以在GPU上存储张量。 例如可以在创建张量时指定存储设备。<br>接下来在第一个gpu上创建张量变量X。在GPU上创建的张量只消耗这个GPU的显存。我们可以使用nvidia-smi命令查看显存使用情况。 一般需要确保不创建超过GPU显存限制的数据。<br><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">X</span> = torch.ones(<span class="number">2</span>, <span class="number">3</span>, device=try_gpu())</span><br><span class="line"><span class="attribute">X</span></span><br></pre></td></tr></table></figure><br>假设至少有两个GPU，下面的代码将在第二个GPU上创建一个随机张量:<br><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">Y</span> = torch.rand(<span class="number">2</span>, <span class="number">3</span>, device=try_gpu(<span class="number">1</span>))</span><br><span class="line"><span class="attribute">Y</span></span><br></pre></td></tr></table></figure></p><p>对于M1芯片的设备，可以这样进行操作：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X = torch<span class="selector-class">.ones</span>(<span class="number">2</span>, <span class="number">3</span>, device=torch<span class="selector-class">.device</span>(<span class="string">&#x27;mps&#x27;</span>))</span><br><span class="line"><span class="function"><span class="title">print</span><span class="params">(X.device)</span></span></span><br><span class="line"></span><br><span class="line">Y = torch<span class="selector-class">.ones</span>(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="function"><span class="title">print</span><span class="params">(Y.device)</span></span></span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">mps:</span><span class="number">0</span> <span class="meta"># 并且只有这一个GPU，合理。</span></span><br><span class="line">cpu</span><br></pre></td></tr></table></figure><br>可以看出默认是在cpu上，但是可以指定到mps上。</p><h4 id="6-2-2-复制"><a href="#6-2-2-复制" class="headerlink" title="6.2.2. 复制"></a>6.2.2. 复制</h4><p>如果要计算X + Y则需要决定在哪里执行这个操作。如下图，我们可以将X传输到第二个GPU并在那里执行操作。<br><img src="/assets/post_img/article51/copyto.svg" alt="copyto"><br>不要简单地X加上Y，这会导致异常，运行时引擎不知道该怎么做：它在同一设备上找不到数据会导致失败。 由于Y位于第二个GPU上，所以需要将X移到那里， 然后才能执行相加运算。<br><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Z = X.cuda(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(X)</span><br><span class="line"><span class="built_in">print</span>(Z)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor(<span class="string">[[1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1.]]</span>, device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">tensor(<span class="string">[[1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1.]]</span>, device=<span class="string">&#x27;cuda:1&#x27;</span>)</span><br></pre></td></tr></table></figure><br>现在数据在同一个GPU上（Z和Y都在），可以将它们相加。<br><figure class="highlight tp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Y</span> + <span class="keyword">Z</span></span><br></pre></td></tr></table></figure><br>假设变量Z已经存在于第二个GPU上。 此时调用Z.cuda(1)将返回Z，而不会复制并分配新内存。<br><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Z.cuda(<span class="number">1</span>) <span class="keyword">is</span> Z</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure></p><h4 id="6-2-3-旁注"><a href="#6-2-3-旁注" class="headerlink" title="6.2.3. 旁注"></a>6.2.3. 旁注</h4><p>人们使用GPU来进行机器学习，因为单个GPU相对运行速度快。 但是在设备（CPU、GPU和其他机器）之间传输数据比计算慢得多。 这也使得并行化变得更加困难，因为我们必须等待数据被发送（或者接收）， 然后才能继续进行更多的操作。 这就是为什么拷贝操作要格外小心。 根据经验，多个小操作比一个大操作糟糕得多。 此外，一次执行几个操作比代码中散布的许多单个操作要好得多（除非你确信自己在做什么）。 如果一个设备必须等待另一个设备才能执行其他操作， 那么这样的操作可能会阻塞。 </p><p>最后，当我们打印张量或将张量转换为NumPy格式时， 如果数据不在内存中，框架会首先将其复制到内存中， 这会导致额外的传输开销。 更糟糕的是，它现在受制于全局解释器锁，使得一切都得等待Python完成。</p><h3 id="6-3-神经网络与GPU"><a href="#6-3-神经网络与GPU" class="headerlink" title="6.3. 神经网络与GPU"></a>6.3. 神经网络与GPU</h3><p>神经网络模型可以指定设备,将模型参数放在GPU上:<br><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net = nn.<span class="constructor">Sequential(<span class="params">nn</span>.Linear(3, 1)</span>)</span><br><span class="line">net = net.<span class="keyword">to</span>(device=<span class="keyword">try</span><span class="constructor">_gpu()</span>)</span><br></pre></td></tr></table></figure><br>当输入为GPU上的张量时，模型将在同一GPU上计算结果:<br><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">net(X)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor(<span class="string">[[-0.7803],</span></span><br><span class="line"><span class="string">        [-0.7803]]</span>, device=<span class="string">&#x27;cuda:0&#x27;</span>, grad_fn=&lt;AddmmBackward0&gt;)</span><br></pre></td></tr></table></figure><br>确认模型参数存储在同一个GPU上：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net<span class="selector-attr">[0]</span><span class="selector-class">.weight</span><span class="selector-class">.data</span>.device</span><br></pre></td></tr></table></figure><br>只要所有的数据和参数都在同一个设备上，就可以有效地学习模型。</p><h3 id="6-4-比较M1芯片设备CPU与GPU速度差异"><a href="#6-4-比较M1芯片设备CPU与GPU速度差异" class="headerlink" title="6.4. 比较M1芯片设备CPU与GPU速度差异"></a>6.4. 比较M1芯片设备CPU与GPU速度差异</h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">t1 = <span class="selector-tag">time</span><span class="selector-class">.time</span>()</span><br><span class="line">X = torch<span class="selector-class">.ones</span>(<span class="number">100000</span>, <span class="number">10000</span>, device=torch<span class="selector-class">.device</span>(<span class="string">&#x27;mps&#x27;</span>))</span><br><span class="line"><span class="function"><span class="title">print</span><span class="params">(X.device)</span></span></span><br><span class="line">net = nn<span class="selector-class">.Sequential</span>(nn<span class="selector-class">.Linear</span>(<span class="number">10000</span>, <span class="number">1000</span>))</span><br><span class="line">net = net<span class="selector-class">.to</span>(device=torch<span class="selector-class">.device</span>(<span class="string">&#x27;mps&#x27;</span>))</span><br><span class="line"><span class="function"><span class="title">net</span><span class="params">(X)</span></span></span><br><span class="line"><span class="function"><span class="title">print</span><span class="params">(time.time()</span></span> - t1)</span><br><span class="line"></span><br><span class="line">t2 = <span class="selector-tag">time</span><span class="selector-class">.time</span>()</span><br><span class="line">Y = torch<span class="selector-class">.ones</span>(<span class="number">100000</span>, <span class="number">10000</span>)</span><br><span class="line"><span class="function"><span class="title">print</span><span class="params">(Y.device)</span></span></span><br><span class="line">net2 = nn<span class="selector-class">.Sequential</span>(nn<span class="selector-class">.Linear</span>(<span class="number">10000</span>, <span class="number">1000</span>))</span><br><span class="line"><span class="function"><span class="title">net2</span><span class="params">(Y)</span></span></span><br><span class="line"><span class="function"><span class="title">print</span><span class="params">(time.time()</span></span> - t2)</span><br></pre></td></tr></table></figure><p>输出：<br><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">mps</span>:<span class="number">0</span></span><br><span class="line"><span class="attribute">0</span>.<span class="number">2889399528503418</span></span><br><span class="line"><span class="attribute">cpu</span></span><br><span class="line"><span class="attribute">4</span>.<span class="number">196138620376587</span></span><br></pre></td></tr></table></figure><br>可以看到差异极大。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;p&gt;本章将深入探索深度学习计算的关键组件， 即模型构建、参数访问与初始化、设计自定义层和块、将模型读写到磁盘， 以及利用GPU实现显著的加速。&lt;br&gt;本节知识极其重要，开始真正窥探深度学习框架的使用。我的所有笔记，都是以PyTorch为基础的（PyTorch不支持的部分使用TensorFlow）。&lt;/p&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://silencezheng.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>房价预测--《动手学深度学习》笔记0x05</title>
    <link href="http://silencezheng.top/2022/07/20/article50/"/>
    <id>http://silencezheng.top/2022/07/20/article50/</id>
    <published>2022-07-20T14:13:40.000Z</published>
    <updated>2022-07-20T14:16:47.641Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><p>Kaggle的<a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques">房价预测比赛</a>，数据集由Bart de Cock于2011年收集，涵盖了2006-2010年期间亚利桑那州埃姆斯市的房价。这个数据集是相当通用的，不会需要使用复杂模型架构。 它比哈里森和鲁宾菲尔德的波士顿房价数据集要大得多，也有更多的特征。</p><p>对应实践：<a href="https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x05.ipynb">https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x05.ipynb</a><br><span id="more"></span></p><h2 id="1-下载、缓存数据集"><a href="#1-下载、缓存数据集" class="headerlink" title="1. 下载、缓存数据集"></a>1. 下载、缓存数据集</h2><p>实现几个函数来方便下载数据，这些函数是用来下载、管理多个数据集的。</p><p>字典DATA_HUB将 数据集名称的字符串 映射到 数据集相关的二元组上， 这个二元组包含数据集的url和验证文件完整性的sha-1密钥。 所有这样的数据集都托管在地址为DATA_URL的站点上，这个链接是d2l的数据托管地址。<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">DATA_HUB</span> = dict()</span><br><span class="line"><span class="attr">DATA_URL</span> = <span class="string">&#x27;http://d2l-data.s3-accelerate.amazonaws.com/&#x27;</span></span><br></pre></td></tr></table></figure></p><p>实现一个download函数，当本地不存在数据时，下载数据集，当本地存在时，对本地数据集的sha-1密钥与数据托管地址中的比对，验证符合后使用本地数据集。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download</span>(<span class="params">name, cache_dir=os.path.join(<span class="params"><span class="string">&#x27;..&#x27;</span>, <span class="string">&#x27;data&#x27;</span></span>)</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载一个DATA_HUB中的文件，返回本地文件名&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> name <span class="keyword">in</span> DATA_HUB, <span class="string">f&quot;<span class="subst">&#123;name&#125;</span> 不存在于 <span class="subst">&#123;DATA_HUB&#125;</span>&quot;</span></span><br><span class="line">    url, sha1_hash = DATA_HUB[name]</span><br><span class="line">    os.makedirs(cache_dir, exist_ok=<span class="literal">True</span>)</span><br><span class="line">    fname = os.path.join(cache_dir, url.split(<span class="string">&#x27;/&#x27;</span>)[-<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">if</span> os.path.exists(fname):</span><br><span class="line">        sha1 = hashlib.sha1()</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(fname, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">                data = f.read(<span class="number">1048576</span>)</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> data:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                sha1.update(data)</span><br><span class="line">        <span class="keyword">if</span> sha1.hexdigest() == sha1_hash:</span><br><span class="line">            <span class="keyword">return</span> fname  <span class="comment"># 命中缓存</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;正在从<span class="subst">&#123;url&#125;</span>下载<span class="subst">&#123;fname&#125;</span>...&#x27;</span>)</span><br><span class="line">    r = requests.get(url, stream=<span class="literal">True</span>, verify=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(fname, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(r.content)</span><br><span class="line">    <span class="keyword">return</span> fname</span><br></pre></td></tr></table></figure><br>实现两个实用函数： 一个将下载并解压缩文件， 另一个是将d2l书中使用的所有数据集从DATA_HUB下载到缓存目录中。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download_extract</span>(<span class="params">name, folder=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载并解压zip/tar文件&quot;&quot;&quot;</span></span><br><span class="line">    fname = download(name)</span><br><span class="line">    base_dir = os.path.dirname(fname)</span><br><span class="line">    data_dir, ext = os.path.splitext(fname)</span><br><span class="line">    <span class="keyword">if</span> ext == <span class="string">&#x27;.zip&#x27;</span>:</span><br><span class="line">        fp = zipfile.ZipFile(fname, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    <span class="keyword">elif</span> ext <span class="keyword">in</span> (<span class="string">&#x27;.tar&#x27;</span>, <span class="string">&#x27;.gz&#x27;</span>):</span><br><span class="line">        fp = tarfile.<span class="built_in">open</span>(fname, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">assert</span> <span class="literal">False</span>, <span class="string">&#x27;只有zip/tar文件可以被解压缩&#x27;</span></span><br><span class="line">    fp.extractall(base_dir)</span><br><span class="line">    <span class="keyword">return</span> os.path.join(base_dir, folder) <span class="keyword">if</span> folder <span class="keyword">else</span> data_dir</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download_all</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载DATA_HUB中的所有文件&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> DATA_HUB:</span><br><span class="line">        download(name)</span><br></pre></td></tr></table></figure></p><h2 id="2-访问和读取数据集"><a href="#2-访问和读取数据集" class="headerlink" title="2. 访问和读取数据集"></a>2. 访问和读取数据集</h2><p>竞赛数据分为训练集和测试集。 每条记录都包括房屋的属性值和属性。这些特征由各种数据类型组成。 例如，建筑年份由整数表示，屋顶类型由离散类别表示，其他特征由浮点数表示。<br>这就是现实让事情变得复杂的地方：例如，一些数据完全丢失了，缺失值被简单地标记为“NA”。<br>比赛最终要求我们预测测试集的价格。我们将划分训练集以创建验证集，但是在将预测结果上传到Kaggle之后，只能在官方测试集中评估模型。</p><p>数据集可以直接在Kaggle上下载，也可以用d2l的HUB下载。总之，最后得到<code>all_features = pd.concat((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:]))</code> （去掉第一列是因为该数据集中第一列是ID属性，对模型和预测没有帮助；去掉train_data的最后一列，因为是标签）。</p><p><code>pd.concat()</code> 可以沿着指定的轴将多个dataframe或者series拼接到一起，在这里就沿第一维（纵向）拼接到一起。</p><h2 id="3-数据预处理"><a href="#3-数据预处理" class="headerlink" title="3. 数据预处理"></a>3. 数据预处理</h2><p>建模之前先对数据进行预处理。<br>首先将所有缺失的值替换为相应特征的平均值。然后，为了将所有特征放在一个共同尺度上，将特征重新缩放到零均值和统一方差来标准化数据，通过以下公式：</p><script type="math/tex; mode=display">x \leftarrow \frac{x - \mu}{\sigma}，\mu和\sigma分别表示通过已有数据计算出的对应特征的均值和标准差。</script><p>替换后的特征具有零均值和统一的方差，也就是说缩放后的每个特征均值都为零且方差为实际方差, 即 $E\left[\frac{x-\mu}{\sigma}\right]=\frac{\mu-\mu}{\sigma}=0$ 和 $E\left[(x-\mu)^{2}\right]=\left(\sigma^{2}+\mu^{2}\right)-2 \mu^{2}+\mu^{2}=\sigma^{2}$（回忆$D(X)=E\left((X-E(X))^{2}\right)=E\left(X^{2}\right)-E^{2}(X)$）。</p><p>标准化数据有两个原因: 一是方便优化。二是在不明确特征相关性的情况下不让惩罚分配给某一特征的系数比分配给其他特征的系数更大。</p><figure class="highlight sas"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 若无法获得测试数据，则可根据训练数据计算均值和标准差</span><br><span class="line">numeric_features = all_features.dtypes[all_features.dtypes != <span class="string">&#x27;object&#x27;</span>].<span class="meta">index</span></span><br><span class="line">all_features[numeric_features] = all_features[numeric_features].apply(</span><br><span class="line">    lambda <span class="meta">x</span>: (<span class="meta">x</span> - <span class="meta">x</span><span class="meta">.mean(</span>)) / (<span class="meta">x</span><span class="meta">.std(</span>)))</span><br><span class="line"># 在标准化数据之后，所有原值为均值的元素消失，可以将缺失值设置为0</span><br><span class="line">all_features[numeric_features] = all_features[numeric_features].fillna(0)</span><br></pre></td></tr></table></figure><p>这里有个值得注意的点就是筛选<code>dtype != &#39;object&#39;</code>的元素，NumPy中数组存储为连续的内存块，通常具有单一数据类型(例如整数、浮点数或固定长度的字符串)，然后内存中的位被解释为具有该数据类型的值。<br><img src="/assets/post_img/article50/pandasObject.jpg" alt="po"><br>但用object类型创建数组是不同的，数组占用的内存此时由指向存储在内存中其他位置的 Python 对象的指针构成。</p><p>pandas的Dataframe使用了NumPy的object类型，Pandas存储字符串时正是使用这种类型，每一个object类型元素事实上是一个指针。</p><p>再处理离散值，使用独热编码替换它们，这样做将离散的字符串特征值也用数字表示出来。<br>例如，“MSZoning”包含值“RL”和“Rm”。 我们将创建两个新的指示器特征“MSZoning_RL”和“MSZoning_RM”，其值为0或1。 根据独热编码，如果“MSZoning”的原始值为“RL”， 则：“MSZoning_RL”为1，“MSZoning_RM”为0。 pandas软件包可以自动实现这一点。<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># “Dummy_na=True”将“na”（缺失值）视为有效的特征值，并为其创建一个特征。</span></span><br><span class="line">all_features = pd.get_dummies(all_features, <span class="attribute">dummy_na</span>=<span class="literal">True</span>)</span><br><span class="line">all_features.shape</span><br></pre></td></tr></table></figure><br>由于我们对所有字符串类型的特征都应用了独热编码（该编码会使每一个值都形成一个特征），最终此转换会将特征的总数量从79个增加到331个。<br>最后通过values属性从pandas格式中提取NumPy格式，并将其转换为pytorch张量表示用于训练。<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">n_train = train_data<span class="selector-class">.shape</span><span class="selector-attr">[0]</span></span><br><span class="line">train_features = torch<span class="selector-class">.tensor</span>(all_features<span class="selector-attr">[:n_train]</span><span class="selector-class">.values</span>, dtype=torch.float32)</span><br><span class="line">test_features = torch<span class="selector-class">.tensor</span>(all_features<span class="selector-attr">[n_train:]</span><span class="selector-class">.values</span>, dtype=torch.float32)</span><br><span class="line">train_labels = torch<span class="selector-class">.tensor</span>(</span><br><span class="line">    train_data<span class="selector-class">.SalePrice</span><span class="selector-class">.values</span><span class="selector-class">.reshape</span>(-<span class="number">1</span>, <span class="number">1</span>), dtype=torch.float32)</span><br></pre></td></tr></table></figure></p><h2 id="4-训练"><a href="#4-训练" class="headerlink" title="4. 训练"></a>4. 训练</h2><p>首先训练一个带有损失平方的线性模型作为基线（baseline）模型，让我们直观地知道最好的模型有超出简单的模型多少。</p><p>线性模型是最简单的深度学习模型（对比赛并无帮助），仅提供了一种查看数据中是否存在有意义的信息的功能。 如果它不能做得比随机猜测更好，那很可能存在数据处理错误。</p><p>对于房价的预测我们更多关心相对值, 而不是绝对值，即更关心相对误差 $\frac{y-\hat{y}}{y}$, 而不是绝对误差 $y-\hat{y}_{\text {。 }}$ 对一个房子价格的预测误差要与其本身的数量级进行比较，对千万级房子的价格预测出现10万左右的偏差时，比对百万级房子或价格更低的房子产生同样的误差更容易令人接受。</p><p>解决这个问题的一种方法是用价格预测的对数来衡量差异。事实上, 这也是比赛中官方用来评价提交质量的误差指标。即将 $\delta$ for $|\log y-\log \hat{y}| \leq \delta$ 转换为 $e^{-\delta} \leq \frac{\hat{y}}{y} \leq e^{\delta}$ 。这使得预测价格的对数与真实标签价格的对数之间出现以下均方根误差（Root Mean Square Error）：</p><script type="math/tex; mode=display">\sqrt{\frac{1}{n} \sum_{i=1}^{n}\left(\log y_{i}-\log \hat{y}_{i}\right)^{2}}</script><p>训练函数将借助Adam优化器，Adam优化器的主要吸引力在于它对初始学习率不那么敏感。</p><p>到这里，排除数据处理部分，基本上完成的工作就是：<br>1、提取了输入的特征张量<br>2、写一个获取单层网络的函数，输入特征输出标签<br>3、选定了对数RMSE的误差计算方法，并实现他<br>4、选定Adam优化器，实现了训练过程</p><p>以上代码详见实践对应仓库。</p><h2 id="5-K折交叉验证"><a href="#5-K折交叉验证" class="headerlink" title="5. K折交叉验证"></a>5. K折交叉验证</h2><p>K折交叉验证有助于模型选择和超参数调整。 首先定义一个函数get_k_fold_data，在K折交叉验证过程中返回第 $i$ 折的数据。 具体地说，它选择第 $i$ 个切片作为验证数据，其余部分作为训练数据。 注意，这并不是处理数据的最有效方法，如果我们的数据集大得多，会有其他解决办法。 k_fold函数则负责在K折交叉验证中训练K次后，返回训练和验证误差的平均值。</p><p>总之，实现了K折交叉验证的部分，重新强调一下K折交叉验证解决的问题：在训练数据稀缺、无法构成足够的验证集的情况下，估计模型的误差。</p><h2 id="6-模型选择"><a href="#6-模型选择" class="headerlink" title="6. 模型选择"></a>6. 模型选择</h2><p>到了这里，剩下的事情就是调参啦！<br><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">k, num_epochs, lr, weight_decay, batch_size = <span class="number">5</span>, <span class="number">100</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">64</span></span><br><span class="line">train_l, valid_l = k<span class="constructor">_fold(<span class="params">k</span>, <span class="params">train_features</span>, <span class="params">train_labels</span>, <span class="params">num_epochs</span>, <span class="params">lr</span>, <span class="params">weight_decay</span>, <span class="params">batch_size</span>)</span></span><br></pre></td></tr></table></figure><br>可调整的超参数有：验证折数、训练周期、学习率、权重和批次大小。</p><p>目前为止，我们还没有开始真正的预测，只是用比赛中的训练数据，通过K折交叉验证来选择模型（调整超参数），使模型能够达到较好的误差，之后，我们再将模型应用于全部数据集，即包含测试数据集。</p><p>有时一组超参数的训练误差可能非常低，但K折交叉验证的误差要高得多，这表明模型过拟合了。<br>在训练过程中监控训练误差和验证误差，较少的过拟合可能表明现有数据可以支撑一个更强大的模型，较大的过拟合可能意味着可以通过正则化技术来获益。</p><h2 id="7-预测、提交"><a href="#7-预测、提交" class="headerlink" title="7. 预测、提交"></a>7. 预测、提交</h2><p>选定了模型之后，就可以将模型训练并应用于测试集进行预测，如果测试集上的预测与K折交叉验证过程中的预测相似，则可以将结果提交给Kaggle，判断预测标签与实际标签的差异。</p><p>提交后得到的score应该就是误差了，误差为0表示完全正确！</p><h2 id="8-小结"><a href="#8-小结" class="headerlink" title="8. 小结"></a>8. 小结</h2><ul><li><p>真实数据通常混合了不同的数据类型，需要进行预处理。</p></li><li><p>常用的预处理方法：将实值数据重新缩放为零均值和单位方法；用均值替换缺失值。</p></li><li><p>将类别特征转化为指标特征，可以使我们把这个特征当作一个独热向量来对待。</p></li><li><p>我们可以使用K折交叉验证来选择模型并调整超参数。</p></li><li><p>对数对于相对误差很有用。</p></li></ul><h2 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h2><p>改进模型来提高分数，应用多层、暂退法等技术。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;p&gt;Kaggle的&lt;a href=&quot;https://www.kaggle.com/c/house-prices-advanced-regression-techniques&quot;&gt;房价预测比赛&lt;/a&gt;，数据集由Bart de Cock于2011年收集，涵盖了2006-2010年期间亚利桑那州埃姆斯市的房价。这个数据集是相当通用的，不会需要使用复杂模型架构。 它比哈里森和鲁宾菲尔德的波士顿房价数据集要大得多，也有更多的特征。&lt;/p&gt;
&lt;p&gt;对应实践：&lt;a href=&quot;https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x05.ipynb&quot;&gt;https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x05.ipynb&lt;/a&gt;&lt;br&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://silencezheng.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Swing布局及快速开发</title>
    <link href="http://silencezheng.top/2022/06/29/article49/"/>
    <id>http://silencezheng.top/2022/06/29/article49/</id>
    <published>2022-06-29T06:48:59.000Z</published>
    <updated>2022-06-29T06:55:02.032Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Swing的各种布局方式介绍及利用IDEA GUI Designer进行快速开发（可生成源代码）。</p><span id="more"></span><h2 id="BorderLayout（边框布局）"><a href="#BorderLayout（边框布局）" class="headerlink" title="BorderLayout（边框布局）"></a>BorderLayout（边框布局）</h2><p>Window、JFrame 和 JDialog 的默认布局管理器。边框布局管理器将窗口分为 5 个区域：North、South、East、West 和 Center。</p><p><img src="/assets/post_img/article49/border.jpg" alt="BL"></p><p>边框布局管理器并不要求所有区域都必须有组件，如果四周的区域（North、South、East 和 West 区域）没有组件，则由 Center 区域去补充。如果单个区域中添加的不只一个组件，那么后来添加的组件将覆盖原来的组件，所以，区域中只显示最后添加的一个组件。</p><p>BorderLayout的构造方法有：<br><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="constructor">BorderLayout()</span>：创建一个 Border 布局，组件之间没有间隙。</span><br><span class="line"></span><br><span class="line"><span class="constructor">BorderLayout(<span class="params">int</span> <span class="params">hgap</span>,<span class="params">int</span> <span class="params">vgap</span>)</span>：创建一个 Border 布局，其中 hgap 表示组件之间的横向间隔；vgap 表示组件之间的纵向间隔，单位是像素。</span><br></pre></td></tr></table></figure></p><h2 id="FlowLayout（流式布局）"><a href="#FlowLayout（流式布局）" class="headerlink" title="FlowLayout（流式布局）"></a>FlowLayout（流式布局）</h2><p>JPanel 和 JApplet 的默认布局管理器。</p><p>FlowLayout 会将组件按照从上到下、从左到右的放置规律逐行进行定位。与其他布局管理器不同的是，FlowLayout 布局管理器不限制它所管理组件的大小，而是允许它们有自己的最佳大小（Preferred Size）。</p><p>FlowLayout的构造方法：<br><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="constructor">FlowLayout()</span>：创建一个布局管理器，使用默认的</span><br><span class="line">居中对齐方式和默认 <span class="number">5</span> 像素的水平和垂直间隔。</span><br><span class="line"></span><br><span class="line"><span class="constructor">FlowLayout(<span class="params">int</span> <span class="params">align</span>)</span>：创建一个布局管理器，使用默认 <span class="number">5</span> 像素的水平和垂直间隔。</span><br><span class="line">其中，align 表示组件的对齐方式，对齐的值必须是 FlowLayoutLEFT、FlowLayout.RIGHT 或 FlowLayout.CENTER，</span><br><span class="line">指定组件在这一行的位置是居左对齐、居右对齐或居中对齐。</span><br><span class="line"></span><br><span class="line"><span class="constructor">FlowLayout(<span class="params">int</span> <span class="params">align</span>, <span class="params">int</span> <span class="params">hgap</span>,<span class="params">int</span> <span class="params">vgap</span>)</span>：创建一个布局管理器，其中 align 表示组件的对齐方式；</span><br><span class="line">hgap 表示组件之间的横向间隔；vgap 表示组件之间的纵向间隔，单位是像素。</span><br></pre></td></tr></table></figure></p><h2 id="CardLayout（卡片布局）"><a href="#CardLayout（卡片布局）" class="headerlink" title="CardLayout（卡片布局）"></a>CardLayout（卡片布局）</h2><p>CardLayout（卡片布局管理器）能够帮助用户实现多个成员共享同一个显示空间，并且一次只显示一个容器组件的内容，该布局将容器分成许多层，每层的显示空间占据整个容器的大小，但每层只允许放置一个组件。</p><p>CardLayout的构造方法：<br><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="constructor">CardLayout()</span>：构造一个新布局，默认间隔为<span class="number">0</span>。</span><br><span class="line"></span><br><span class="line"><span class="constructor">CardLayout(<span class="params">int</span> <span class="params">hgap</span>, <span class="params">int</span> <span class="params">vgap</span>)</span>：创建布局，并指定组件间的水平间隔（hgap）和垂直间隔（vgap）。</span><br></pre></td></tr></table></figure></p><p>下面是一个示例，介绍如何使用卡片布局实现分层显示：<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="regexp">//</span> 假设有两个JPanel p1和p2</span><br><span class="line">JPanel cards=new JPanel(new CardLayout());</span><br><span class="line">cards.add(p1,<span class="string">&quot;card1&quot;</span>);    <span class="regexp">//</span>向卡片式布局面板中添加面板<span class="number">1</span></span><br><span class="line">cards.add(p2,<span class="string">&quot;card2&quot;</span>);    <span class="regexp">//</span>向卡片式布局面板中添加面板<span class="number">2</span></span><br><span class="line"><span class="regexp">//</span> 显示面板</span><br><span class="line">CardLayout cl=(CardLayout)(cards.getLayout());</span><br><span class="line">cl.show(cards,<span class="string">&quot;card1&quot;</span>);    <span class="regexp">//</span>调用show()方法显示面板<span class="number">1</span></span><br></pre></td></tr></table></figure></p><h2 id="GridLayout（网格布局）"><a href="#GridLayout（网格布局）" class="headerlink" title="GridLayout（网格布局）"></a>GridLayout（网格布局）</h2><p>GridLayout（网格布局管理器）为组件的放置位置提供了更大的灵活性。它将区域分割成行数（rows）和列数（columns）的网格状布局，组件按照由左至右、由上而下的次序排列填充到各个单元格中。</p><p>GridLayout的构造方法：<br><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="constructor">GridLayout(<span class="params">int</span> <span class="params">rows</span>,<span class="params">int</span> <span class="params">cols</span>)</span>：创建一个指定行（rows）和列（cols）的网格布局。</span><br><span class="line">布局中所有组件的大小一样，组件之间没有间隔。</span><br><span class="line"></span><br><span class="line"><span class="constructor">GridLayout(<span class="params">int</span> <span class="params">rows</span>,<span class="params">int</span> <span class="params">cols</span>,<span class="params">int</span> <span class="params">hgap</span>,<span class="params">int</span> <span class="params">vgap</span>)</span>：创建一个指定行（rows）和列（cols）的网格布局，</span><br><span class="line">并且可以指定组件之间横向（hgap）和纵向（vgap）的间隔，单位是像素。</span><br></pre></td></tr></table></figure></p><p>GridLayout 布局管理器总是忽略组件的最佳大小，而是根据提供的行和列进行平分。该布局管理的所有单元格的宽度和高度都是一样的。这也是一个缺点，在实际使用中，通常通过与其他布局进行协作，才能达到一个很好的展示效果。</p><h2 id="GridBagLayout（网格包布局）"><a href="#GridBagLayout（网格包布局）" class="headerlink" title="GridBagLayout（网格包布局）"></a>GridBagLayout（网格包布局）</h2><p>GridBagLayout（网格包布局管理器）是在网格基础上提供复杂的布局，是最灵活、 最复杂的布局管理器。GridBagLayout 不需要组件的尺寸一致，允许组件扩展到多行多列。每个 GridBagLayout 对象都维护了一组动态的矩形网格单元，每个组件占一个或多个单元，所占有的网格单元称为组件的显示区域。</p><p>GridBagLayout 所管理的每个组件都与一个 GridBagConstraints 约束类的对象相关。这个约束类对象指定了组件的显示区域在网格中的位置，以及在其显示区域中应该如何摆放组件。除了组件的约束对象，GridBagLayout 还要考虑每个组件的最小和首选尺寸，以确定组件的大小。</p><p>为了有效地利用网格包布局，在向容器中添加组件时，必须定制某些组件的相关约束对象。GridBagConstraints 对象的定制是通过下列变量实现的：</p><ol><li>gridx 和 gridy<br>用来指定组件左上角在网格中的行和列。容器中最左边列的 gridx 为 0，最上边行的 gridy 为 0。这两个变量的默认值是 GridBagConstraints.RELATIVE，表示对应的组件将放在前一个组件的右边或下面。</li><li>gridwidth 和 gridheight<br>用来指定组件显示区域所占的列数和行数，以网格单元为单位，默认值为 1。</li><li>fill<br>指定组件填充网格的方式，可以是如下值：<ul><li>GridBagConstraints.NONE（默认值）</li><li>GridBagConstraints.HORIZONTAL（组件横向充满显示区域，但是不改变组件高度）</li><li>GridBagConstraints.VERTICAL（组件纵向充满显示区域，但是不改变组件宽度）</li><li>GridBagConstraints.BOTH（组件横向、纵向充满其显示区域）。</li></ul></li><li>ipadx 和 ipady<br>指定组件显示区域的内部填充，即在组件最小尺寸之外需要附加的像素数，默认值为 0。</li><li>insets<br>指定组件显示区域的外部填充，即组件与其显示区域边缘之间的空间，默认组件没有外部填充。</li><li>anchor<br>指定组件在显示区域中的摆放位置。可选值有：<ul><li>GridBagConstraints.CENTER（默认值）</li><li>GridBagConstraints.NORTH</li><li>GridBagConstraints.NORTHEAST</li><li>GridBagConstraints.EAST</li><li>GridBagConstraints.SOUTH</li><li>GridBagConstraints.SOUTHEAST</li><li>GridBagConstraints.WEST</li><li>GridBagConstraints.SOUTHWEST</li><li>GridBagConstraints.NORTHWEST</li></ul></li><li>weightx 和 weighty<br>用来指定在容器大小改变时，增加或减少的空间如何在组件间分配，默认值为 0，即所有的组件将聚拢在容器的中心，多余的空间将放在容器边缘与网格单元之间。weightx 和 weighty 的取值一般在 0.0 与 1.0 之间，数值大表明组件所在的行或者列将获得更多的空间。</li></ol><h2 id="BoxLayout（盒布局）"><a href="#BoxLayout（盒布局）" class="headerlink" title="BoxLayout（盒布局）"></a>BoxLayout（盒布局）</h2><p>BoxLayout（盒布局管理器）通常和 Box 容器联合使用。</p><p>Box 类有以下两个静态方法：<br><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">createHorizontalBox()：返回一个 <span class="keyword">Box </span>对象，它采用水平 <span class="keyword">BoxLayout，</span></span><br><span class="line"><span class="keyword"></span>即 <span class="keyword">BoxLayout </span>沿着水平方向放置组件，让组件在容器内从左到右排列。</span><br><span class="line"></span><br><span class="line">createVerticalBox()：返回一个 <span class="keyword">Box </span>对象，它采用垂直 <span class="keyword">BoxLayout，</span></span><br><span class="line"><span class="keyword"></span>即 <span class="keyword">BoxLayout </span>沿着垂直方向放置组件，让组件在容器内从上到下进行排列。</span><br></pre></td></tr></table></figure></p><p>Box 还提供了用于决定组件之间间隔的静态方法：</p><div class="table-container"><table><thead><tr><th style="text-align:center">方法</th><th style="text-align:center">说明</th></tr></thead><tbody><tr><td style="text-align:center">static Component createHorizontalGlue()</td><td style="text-align:center">创建一个不可见的、可以被水平拉伸和收缩的组件</td></tr><tr><td style="text-align:center">static Component createVerticalGlue()</td><td style="text-align:center">创建一个不可见的、可以被垂直拉伸和收缩的组件</td></tr><tr><td style="text-align:center">static Component createHorizontalStrut(int width)</td><td style="text-align:center">创建一个不可见的、固定宽度的组件</td></tr><tr><td style="text-align:center">static Component createVerticalStrut(int height)</td><td style="text-align:center">创建一个不可见的、固定高度的组件</td></tr><tr><td style="text-align:center">static Component createRigidArea(Dimension d)</td><td style="text-align:center">创建一个不可见的、总是具有指定大小的组件</td></tr></tbody></table></div><p>BoxLayout 类只有一个构造方法:<br><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="constructor">BoxLayout(Container <span class="params">c</span>,<span class="params">int</span> <span class="params">axis</span>)</span></span><br><span class="line">Container 是一个容器对象，即该布局管理器在哪个容器中使用。</span><br><span class="line">axis 用来决定容器上的组件水平（X_AXIS）或垂直（Y_AXIS）放置,可以使用 BoxLayout 类访问这两个属性。</span><br></pre></td></tr></table></figure></p><h2 id="使用GUI-Designer快速开发"><a href="#使用GUI-Designer快速开发" class="headerlink" title="使用GUI Designer快速开发"></a>使用GUI Designer快速开发</h2><p>通常在编写Swing程序时，各种布局需要交错在一起使用，从而达到更好的界面效果。</p><p>InteliJ IDEA自带的可视化创建工具可以为图形界面的开发提供一定程度上的便利，下图是GUI Designer的设置：<br><img src="/assets/post_img/article49/GD-setting.jpg" alt="settings"></p><p>该工具提供了两种生成GUI界面的方式，一种是直接产生编译后的二进制class文件，一种是在运行时生成Java源代码，但是该源代码并非原生的Swing代码，而是调用了JetBrains自家开发的库产生的源代码。</p><p>在可视化编辑的时候也可以选择一些布局管理工具（不包括Box布局），设置UI组件的隐私性等等。</p><p>如果想要保留源代码的话，可以在确定界面符合要求后，运行一次程序产生源代码，然后删除掉跟随类的form文件即可，否则每次运行程序都会重置界面源代码。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;Swing的各种布局方式介绍及利用IDEA GUI Designer进行快速开发（可生成源代码）。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Java" scheme="http://silencezheng.top/tags/Java/"/>
    
    <category term="Swing" scheme="http://silencezheng.top/tags/Swing/"/>
    
    <category term="IDEA" scheme="http://silencezheng.top/tags/IDEA/"/>
    
  </entry>
  
  <entry>
    <title>Swing事件响应三种方式</title>
    <link href="http://silencezheng.top/2022/06/26/article48/"/>
    <id>http://silencezheng.top/2022/06/26/article48/</id>
    <published>2022-06-26T08:30:14.000Z</published>
    <updated>2022-06-26T08:31:37.446Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>简单总结Swing开发中对组件添加事件响应的三种方法。<br>参考：<a href="https://blog.csdn.net/moridehuixiang/article/details/45394577">https://blog.csdn.net/moridehuixiang/article/details/45394577</a><br><span id="more"></span></p><h2 id="事件响应"><a href="#事件响应" class="headerlink" title="事件响应"></a>事件响应</h2><p>在Swing中,事件响应是通过监听器对象来处理事件的方式实行的,这种方式被称为事件委托模型。</p><p>以JButton举例,它内部有一个名为listenerList的链表,在点击按钮时,会产生一个ActionEvent事件,此后内部会依次调用位于listenerList中的每一个实现ActionListener接口的类的实例的actionPerformed方法,这就是事件响应的过程。</p><p>当调用JButton的addActionListener方法时, 外部实现了ActionListene接口的类的实例的指针就被放入了listenerList中,当按钮点击事件产生时,这个实例的actionPerformed方法就会被调用,从而按钮的点击事件处理就被委托到了该实例中进行处理。</p><h2 id="实现方式"><a href="#实现方式" class="headerlink" title="实现方式"></a>实现方式</h2><p>实现ActionListener接口的三种方式如下：<br>1.实现一个ActionListener接口的子类,再把按钮的事件响应委托给这个子类的实例处理.这种方式并不常用。</p><p>2.让界面类（通常是继承了JFrame）实现ActionListener接口,再把事件响应委托给界面类。这种方式适合于处理一些短小简单或要求内聚的事件响应。</p><p>3.用匿名类实现ActionListener接口,再把事件委托给这个匿名类的实例，这种方式是Swing事件处理的主流。</p><h2 id="方法一：创建一个实现了ActionListener接口的子类"><a href="#方法一：创建一个实现了ActionListener接口的子类" class="headerlink" title="方法一：创建一个实现了ActionListener接口的子类"></a>方法一：创建一个实现了ActionListener接口的子类</h2><figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ButtonActionListener</span> <span class="keyword"><span class="keyword">implements</span> <span class="type">ActionListener</span></span></span>&#123;</span><br><span class="line">  <span class="keyword">public</span> void actionPerformed(ActionEvent e) &#123;</span><br><span class="line">    <span class="keyword">String</span> buttonText=((JButton)e.getSource()).getText();</span><br><span class="line">    System.out.println(<span class="string">&quot;你按下了&quot;</span> + buttonText);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">button.addActionListener(<span class="keyword">new</span> <span class="type">ButtonActionListener</span>());</span><br></pre></td></tr></table></figure><h2 id="方法二：整个窗体类实现ActionListener接口"><a href="#方法二：整个窗体类实现ActionListener接口" class="headerlink" title="方法二：整个窗体类实现ActionListener接口"></a>方法二：整个窗体类实现ActionListener接口</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">MyWindow</span> <span class="keyword">extends</span> <span class="title">JFrame</span> <span class="title">implements</span> <span class="title">ActionListener</span></span>&#123;</span><br><span class="line">  public <span class="type">MyFrame</span>() &#123;</span><br><span class="line">    button.addActionListener(<span class="keyword">this</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  public void actionPerformed(<span class="type">ActionEvent</span> e) &#123;</span><br><span class="line">    <span class="keyword">if</span>(e.getSource()==button)&#123;</span><br><span class="line">      <span class="type">System</span>.out.println(<span class="string">&quot;你按下了&quot;</span> + button.getText());</span><br><span class="line">    &#125;   </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="方法三：匿名类方式添加"><a href="#方法三：匿名类方式添加" class="headerlink" title="方法三：匿名类方式添加"></a>方法三：匿名类方式添加</h2><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">button.add<span class="constructor">ActionListener(<span class="params">new</span> ActionListener()</span> &#123;</span><br><span class="line">      public void action<span class="constructor">Performed(ActionEvent <span class="params">e</span>)</span> &#123;</span><br><span class="line">        <span class="module-access"><span class="module"><span class="identifier">System</span>.</span></span>out.println(<span class="string">&quot;你按下了&quot;</span> + button.get<span class="constructor">Text()</span>);</span><br><span class="line">      &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;简单总结Swing开发中对组件添加事件响应的三种方法。&lt;br&gt;参考：&lt;a href=&quot;https://blog.csdn.net/moridehuixiang/article/details/45394577&quot;&gt;https://blog.csdn.net/moridehuixiang/article/details/45394577&lt;/a&gt;&lt;br&gt;</summary>
    
    
    
    
    <category term="Java" scheme="http://silencezheng.top/tags/Java/"/>
    
    <category term="Swing" scheme="http://silencezheng.top/tags/Swing/"/>
    
  </entry>
  
  <entry>
    <title>线程同步、异步</title>
    <link href="http://silencezheng.top/2022/06/23/article47/"/>
    <id>http://silencezheng.top/2022/06/23/article47/</id>
    <published>2022-06-23T09:43:55.000Z</published>
    <updated>2022-06-23T09:48:56.891Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>之前也做过不少的分布式、多线程系统，对线程同步异步的理解还是不到位，现归纳总结一下。<br><span id="more"></span></p><h2 id="多线程"><a href="#多线程" class="headerlink" title="多线程"></a>多线程</h2><p>多线程并发程序通过同时执行多个任务来提高 CPU 利用率，线程通过共享对象引用和成员变量相互通信。</p><p>当多个线程共享同一内存（资源）时，多个对同一数据执行不同操作的线程可能会相互交错，并在内存中创建不一致的数据，产生线程干扰错误。</p><p>使线程安全的办法有许多，如：</p><ol><li>同步</li><li>施加多个线程对同一对象的访问限制</li><li>将变量声明为final</li><li>将变量声明为volatile</li><li>创建不可变对象</li><li>等等</li></ol><p>也就是说，在多线程的场景中，同步是维护线程安全的一种方式。<a href="https://blog.csdn.net/u011033906/article/details/53840525">这篇文章</a>可以学习借鉴一下。</p><h2 id="同步、异步"><a href="#同步、异步" class="headerlink" title="同步、异步"></a>同步、异步</h2><p><strong>同步（synchronized）</strong>：A线程要请求某资源，但是此资源正在被B线程使用中，因为同步机制存在，A线程阻塞，等待资源可用再进行。</p><p><strong>异步（asynchronized）</strong>：A线程要请求某资源，但是此资源正在被B线程使用中，因为没有同步机制存在，A线程可以请求得到此资源，无需等待。</p><p>同步方法调用一旦开始，调用者必须等到方法调用返回后，才能继续后续的行为。<br>异步方法调用更像一个消息传递，一旦开始，方法调用就会立即返回，调用者就可以继续后续的操作。而异步方法通常会在另外一个线程中“真实”地执行着。整个过程，不会阻碍调用者的工作（即不阻碍当前进程）。最终通过状态来通知调用者，或通过回调函数来处理执行结果。</p><p>同步显然是安全的，但使系统性能降低；异步速度快，但可能导致死锁，引发更大安全问题。</p><h2 id="阻塞与同步"><a href="#阻塞与同步" class="headerlink" title="阻塞与同步"></a>阻塞与同步</h2><p>同步与异步和<strong>阻塞机制</strong>不能混为一谈，虽然通常有所关联。</p><p>同步和异步强调的是消息通信机制，而 阻塞和非阻塞 强调的是程序在等待调用结果（消息，返回值）时的状态。</p><p>阻塞时，在调用结果返回前，当前线程会被挂起，并在得到结果之后返回。<br>非阻塞调用指在不能立刻得到结果之前，该调用不会阻塞当前线程。</p><p>对于同步调用来说，很多时候当前线程还是激活的状态，只是从逻辑上当前函数没有返回而已，即同步等待时什么都不干，白白占用着资源。</p><h2 id="举例说说"><a href="#举例说说" class="headerlink" title="举例说说"></a>举例说说</h2><h3 id="同步"><a href="#同步" class="headerlink" title="同步"></a>同步</h3><p>Tomcat，通过自建的线程池实现同步阻塞式IO模型。详细见<a href="https://juejin.cn/post/6977719729947738142">这篇文章</a></p><h3 id="异步"><a href="#异步" class="headerlink" title="异步"></a>异步</h3><p>Node.js 是一个基于 Chrome V8 引擎的 JavaScript 运行环境。 Node.js 使用了一个事件驱动、非阻塞式 I/O 的模型。Node.js是单线程的，利用异步IO和事件驱动（回调函数）来解决高并发的问题。详细可以看看<a href="https://blog.csdn.net/fengqiaojiangshui/article/details/55819930">这篇文章</a>。</p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>随想随写写，后续再完善吧，有很多值得深挖的点。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;之前也做过不少的分布式、多线程系统，对线程同步异步的理解还是不到位，现归纳总结一下。&lt;br&gt;</summary>
    
    
    
    
    <category term="编程思想" scheme="http://silencezheng.top/tags/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/"/>
    
  </entry>
  
  <entry>
    <title>VSCode代码段使用</title>
    <link href="http://silencezheng.top/2022/06/19/article46/"/>
    <id>http://silencezheng.top/2022/06/19/article46/</id>
    <published>2022-06-19T11:18:38.000Z</published>
    <updated>2022-06-19T11:24:44.113Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>越用VSCode越有一种轻便感，麻雀虽小，五脏俱全，又有微软在做背书，同步设置什么的也很方便，要是国产也能有这样的好工具就好了。<br>之前在编写博客时，总觉得代码块、标题的输入有些麻烦，偶然发现可以使用代码段功能来解决这一问题，遂记录一下。<br><span id="more"></span></p><h2 id="代码段功能"><a href="#代码段功能" class="headerlink" title="代码段功能"></a>代码段功能</h2><p>VSCode提供的代码段功能，主要是通过输入预先设置好的“前缀”字符，触发输出对应的代码段。</p><p>该功能具有三种作用域：</p><ol><li>全局（global），该类代码段被存放在软件内部的文件中，不会随着项目的关闭而失效。在创建后除非手动删除该代码段文件，否则它会一直存在。</li><li>文件夹（folder），该类代码段被存放在选定目录下<code>.vscode</code>隐藏文件夹中，这个代码段只适用于当前文件夹。</li><li>文件类型（file type），该类代码段与全局作用域代码段类似，被存放在软件内部的文件中。但这类代码段只适用于用户指定的文件类型，严格的匹配文件后缀。</li></ol><p>三类代码段文件的书写格式是一致的，给个简单的例子如下：<br><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;Add a Markdown code block&quot;</span>:&#123;</span><br><span class="line"><span class="attr">&quot;scope&quot;</span>: <span class="string">&quot;&quot;</span>,</span><br><span class="line"><span class="attr">&quot;prefix&quot;</span>: <span class="string">&quot;amcb&quot;</span>,</span><br><span class="line"><span class="attr">&quot;body&quot;</span>: [</span><br><span class="line"><span class="string">&quot;```&quot;</span>,</span><br><span class="line"><span class="string">&quot;$1&quot;</span>,</span><br><span class="line"><span class="string">&quot;```&quot;</span></span><br><span class="line">],</span><br><span class="line"><span class="attr">&quot;description&quot;</span>: <span class="string">&quot;Add a Markdown code block&quot;</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>所有的代码段都书写在<code>xx.code-snippets</code>文件中的花括号内，每一个代码段包含四个属性：<br><strong>scope</strong>：作用文件类型，多种类型之间用逗号隔开。如果值为空，或不写该属性，默认所有类型文件都支持该代码段。同时该属性仅支持全局和文件夹作用域，因为文件类型作用域代码段本身已经指定了该属性。<br><strong>prefix</strong>： 触发代码段的“前缀”字符。<br><strong>body</strong>：代码段主体内容，格式为<em>字符串</em>数组，也就是说每行代码都需要用引号括起。<br><strong>description</strong>：代码块的描述，输入“前缀”时会有提示。</p><p>除此之外，在代码段主体的书写上还有一些规则：<br><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1、每个字符串元素就代表一行，在单行内可以使用\r或者使用\<span class="keyword">n</span>换行。</span><br><span class="line">2、行内不能使用<span class="keyword">tab</span>键缩进，只能使用空格或者\t缩进。</span><br><span class="line">3、用<span class="variable">$1</span>表示敲击回车或者<span class="keyword">tab</span>键一次后光标定位的位置。<span class="variable">$2</span>，<span class="variable">$3</span>，<span class="variable">$4</span>，...同理。</span><br></pre></td></tr></table></figure></p><h2 id="利用代码段功能方便Markdown博客编写"><a href="#利用代码段功能方便Markdown博客编写" class="headerlink" title="利用代码段功能方便Markdown博客编写"></a>利用代码段功能方便Markdown博客编写</h2><p>了解这个功能后，就可以应用它来帮助在VSCode中编写Markdown格式的博客。</p><p>我通常习惯在一个固定的工作区编写我的博客，所以新建一个“文件夹域”的代码段文件，如下：<br><img src="/assets/post_img/article46/md文章增强.png" alt="wzzq"></p><p>书写代码段：<br><figure class="highlight ceylon"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">   <span class="string">&quot;Add a Markdown code block&quot;</span>:&#123;</span><br><span class="line"><span class="string">&quot;scope&quot;</span>: <span class="string">&quot;markdown&quot;</span>,</span><br><span class="line"><span class="string">&quot;prefix&quot;</span>: <span class="string">&quot;amcb&quot;</span>,</span><br><span class="line"><span class="string">&quot;body&quot;</span>: [</span><br><span class="line"><span class="string">&quot;``<span class="subst">`<span class="string">&quot;,</span></span></span></span><br><span class="line"><span class="string"><span class="subst"><span class="string">&quot;</span><span class="number">$1</span><span class="string">&quot;,</span></span></span></span><br><span class="line"><span class="string"><span class="subst"><span class="string">&quot;</span></span>```&quot;</span></span><br><span class="line">],</span><br><span class="line"><span class="string">&quot;description&quot;</span>: <span class="string">&quot;Add a Markdown code block&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>这时在.md文件中还不能直接使用，因为VSCode默认没有开启quickSuggestions。</p><p>打开当前工作区的<code>settings.json</code>，添加如下配置：<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&quot;<span class="selector-attr">[markdown]</span>&quot;:&#123;</span><br><span class="line">    <span class="string">&quot;editor.quickSuggestions&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;other&quot;</span>: <span class="string">&quot;on&quot;</span>,</span><br><span class="line">        <span class="string">&quot;comments&quot;</span>: <span class="string">&quot;off&quot;</span>,</span><br><span class="line">        <span class="string">&quot;strings&quot;</span>: <span class="string">&quot;off&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>这样以后，虽然代码段是有提示了:<br><img src="/assets/post_img/article46/amcb.png" alt="amcb"><br>可是还有很多不必要的提示，比如前文中出现的语句也会被加入到提示中来，有些麻烦：<br><img src="/assets/post_img/article46/麻烦.png" alt="trouble"></p><p>于是我到VSCode<a href="https://code.visualstudio.com/docs/getstarted/settings#_languagespecific-editor-settings">官方文档</a>中寻找，看看有没有什么办法，发现这其实是因为”editor.quickSuggestions”这个设置的属性粒度不够细致造成的，它将文件的内容区分为字符串、注释和其他，也就是说只要我启用了其他，那么文件中非注释和非字符串的内容都会被提示，我的个人建议是增加一个snippets属性，将该属性从other中划分出来，就可以完美的解决我的需求😄。</p><p>顺便一提VSCode有两种设置方式，用户设置和工作区（Workplace）设置，前者应用于全局，后者存在于项目根目录的<code>.vscode</code>默认文件夹内，可以对当前项目覆盖用户设置（优先级更高）。<br><img src="/assets/post_img/article46/vsc设置.png" alt="setting"><br>关于更多设置优先级相关，<a href="https://code.visualstudio.com/docs/getstarted/settings#_settings-precedence">官网</a>有更详细的解释。所以之前做的配置实际上是覆盖了用户配置。</p><p>最终还是选择了这种配置：<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="regexp">//</span> on、off、onlySnippets</span><br><span class="line"><span class="string">&quot;editor.tabCompletion&quot;</span>: <span class="string">&quot;onlySnippets&quot;</span></span><br></pre></td></tr></table></figure><br>即：使用tab自动补全，但仅对于代码段使用。<br>这样虽然不会有输入建议提示，但胜在简洁，只需要把代码段的前缀设置的相对短，并记住即可，基本上我常用的也就是Markdown代码块和标题，用于填补Markdown All in One插件的缺点。</p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>后续可能给VSCode提个issue，建议一下关于粒度划分问题。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;越用VSCode越有一种轻便感，麻雀虽小，五脏俱全，又有微软在做背书，同步设置什么的也很方便，要是国产也能有这样的好工具就好了。&lt;br&gt;之前在编写博客时，总觉得代码块、标题的输入有些麻烦，偶然发现可以使用代码段功能来解决这一问题，遂记录一下。&lt;br&gt;</summary>
    
    
    
    
    <category term="VSCode" scheme="http://silencezheng.top/tags/VSCode/"/>
    
  </entry>
  
  <entry>
    <title>.gitignore文件使用</title>
    <link href="http://silencezheng.top/2022/06/18/article45/"/>
    <id>http://silencezheng.top/2022/06/18/article45/</id>
    <published>2022-06-18T15:21:59.000Z</published>
    <updated>2022-06-18T15:35:33.561Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>记录一下如何编写git忽略文件，文件的格式，以及使用pycharm插件帮助生成。<br><span id="more"></span></p><h2 id="Git-ignore文件"><a href="#Git-ignore文件" class="headerlink" title="Git ignore文件"></a>Git ignore文件</h2><p>Git的忽略文件名为 <code>.gitignore</code>，在这个文件中列出那些不希望添加到git中的文件名后，当使用<code>git add .</code>时这些文件就会被自动忽略掉。</p><p>忽视文件的格式很简单，同时也支持格式匹配（包括文件和目录），用 <code>*</code> 表示省略，用<code>#</code>表示注释，如：<br><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># Java class <span class="keyword">Files</span></span><br><span class="line"><span class="comment">*.class</span></span><br><span class="line"></span><br><span class="line"># Package <span class="keyword">Files</span></span><br><span class="line"><span class="comment">*.jar</span></span><br><span class="line"><span class="comment">*.war</span></span><br><span class="line"><span class="comment">*.ear</span></span><br><span class="line"></span><br><span class="line"># 忽略名称中末尾为bin的目录</span><br><span class="line"><span class="comment">*bin/</span></span><br><span class="line"></span><br><span class="line"># 忽略名称中间包含bin的目录</span><br><span class="line"><span class="comment">*bin*/</span></span><br></pre></td></tr></table></figure></p><p>GitHub也给出了各种各样项目的忽视文件模版，见<a href="https://github.com/github/gitignore">这里</a>。</p><h2 id="忽视文件的一般原则"><a href="#忽视文件的一般原则" class="headerlink" title="忽视文件的一般原则"></a>忽视文件的一般原则</h2><p>忽略操作系统自动生成的文件，比如缩略图等。</p><p>忽略编译生成的中间文件、可执行文件等，也就是如果一个文件是通过另一个文件自动生成的，那自动生成的文件就没必要放进版本库，比如Java编译产生的.class文件。</p><p>忽略自己带有敏感信息的配置文件，比如存放口令的配置文件。</p><h2 id="Pycharm添加git忽视文件"><a href="#Pycharm添加git忽视文件" class="headerlink" title="Pycharm添加git忽视文件"></a>Pycharm添加git忽视文件</h2><p>下载.ignore插件：<br><img src="/assets/post_img/article45/plugin.png" alt=".ignore"></p><p>然后在项目根目录上右键移到New，就可以看到.ignore的选项。从中选择gitignore文件，还可以选择一些模版创建，也可以直接建立空的忽视文件。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;记录一下如何编写git忽略文件，文件的格式，以及使用pycharm插件帮助生成。&lt;br&gt;</summary>
    
    
    
    
    <category term="Git" scheme="http://silencezheng.top/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>多层感知机--《动手学深度学习》笔记0x04</title>
    <link href="http://silencezheng.top/2022/06/16/article44/"/>
    <id>http://silencezheng.top/2022/06/16/article44/</id>
    <published>2022-06-16T07:03:39.000Z</published>
    <updated>2022-06-18T07:00:39.691Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><p>这一章开始学习真正的深度网络。<br>最简单的深度网络称为多层感知机。多层感知机由多层神经元组成， 每一层与它的上一层相连，从中接收输入； 同时每一层也与它的下一层相连，影响当前层的神经元。<br>这一章从基本的概念介绍开始讲起，包括过拟合、欠拟合和模型选择。 为了解决这些问题，本章将介绍权重衰减和暂退法等正则化技术，以及将讨论数值稳定性和参数初始化相关的问题。最后应用一个真实的案例：房价预测。</p><p>对应实践：<a href="https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x04.ipynb">https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x04.ipynb</a><br><span id="more"></span></p><h3 id="0-1-结论"><a href="#0-1-结论" class="headerlink" title="0.1. 结论"></a>0.1. 结论</h3><ul><li>多层感知机通过激活函数+隐藏层摆脱线性模型的限制。</li><li>欠拟合是指模型无法继续减少训练误差。过拟合是指训练误差远小于验证误差。</li><li>由于不能基于训练误差来估计泛化误差，因此简单地最小化训练误差并不一定意味着泛化误差的减小。机器学习模型需要注意防止过拟合，即防止泛化误差过大。</li><li>验证集可以用于模型选择，但不能过于随意地使用它。</li><li>应该选择一个复杂度适当的模型，避免使用数量不足的训练样本。简单模型导致欠拟合，复杂模型导致过拟合。</li><li>正则化是处理过拟合的常用方法：在训练集的损失函数中加入惩罚项，以降低学习到的模型的复杂度。</li><li>保持模型简单的一个特别的选择是使用$L_2$惩罚的权重衰减。这会导致学习算法在更新步骤中递减权重。权重衰减功能在深度学习框架的优化器中提供。</li><li>在同一训练代码实现中，不同的参数集可以有不同的更新行为。</li><li>暂退法在前向传播过程中，计算每一内部层的同时丢弃一些神经元。</li><li>暂退法可以避免过拟合，它通常与控制权重向量的维数和大小结合使用的。</li><li>暂退法将活性值$h$替换为具有期望值$h$的随机变量。</li><li>暂退法仅在训练期间使用。</li><li>前向传播在神经网络定义的计算图中按顺序计算和存储中间变量，它的顺序是从输入层到输出层。</li><li>反向传播按相反的顺序（从输出层到输入层）计算和存储神经网络的中间变量和参数的梯度。</li><li>在训练深度学习模型时，前向传播和反向传播是相互依赖的。</li><li>训练比预测需要更多的内存。</li><li>梯度消失和梯度爆炸是深度网络中常见的问题。在参数初始化时需要非常小心，以确保梯度和参数可以得到很好的控制。</li><li>需要用启发式的初始化方法来确保初始梯度既不太大也不太小</li><li>ReLU激活函数缓解了梯度消失问题，这样可以加速收敛。</li><li>随机初始化是保证在进行优化前打破对称性的关键。</li><li>Xavier初始化表明，对于每一层，输出的方差不受输入数量的影响，任何梯度的方差不受输出数量的影响。</li><li>在许多情况下，训练集和测试集并不来自同一个分布。这就是所谓的分布偏移。</li><li>真实风险是从真实分布中抽取的所有数据的总体损失的预期。然而，这个数据总体通常是无法获得的。经验风险是训练数据的平均损失，用于近似真实风险。在实践中，我们进行经验风险最小化。</li><li>在相应的假设条件下，可以在测试时检测并纠正协变量偏移和标签偏移。在测试时，不考虑这种偏移可能会成为问题。</li></ul><h2 id="1-多层感知机"><a href="#1-多层感知机" class="headerlink" title="1. 多层感知机"></a>1. 多层感知机</h2><p>开始对深度神经网络的探索！</p><h3 id="1-1-隐藏层"><a href="#1-1-隐藏层" class="headerlink" title="1.1. 隐藏层"></a>1.1. 隐藏层</h3><p>在线性神经网络章节描述了仿射变换， 它是一种带有偏置项的线性变换。<br>softmax回归模型通过单个仿射变换将输入直接映射到输出，然后进行softmax操作。<br>如果我们的标签通过仿射变换后确实与我们的输入数据相关，那么这种方法确实足够了。 但是，仿射变换中的线性是一个很强的假设。</p><h4 id="1-1-1-线性模型可能会出错"><a href="#1-1-1-线性模型可能会出错" class="headerlink" title="1.1.1. 线性模型可能会出错"></a>1.1.1. 线性模型可能会出错</h4><p>线性意味着单调假设： 任何特征的增大都会导致模型输出的增大（如果对应的权重为正）， 或者导致模型输出的减小（如果对应的权重为负）。<br>有时这是有道理的。 例如，如果我们试图预测一个人是否会偿还贷款。 我们可以认为，在其他条件不变的情况下， 收入较高的申请人比收入较低的申请人更有可能偿还贷款。<br>但是，虽然收入与还款概率存在单调性，但它们不是线性相关的。 收入从0增加到5万，可能比从100万增加到105万带来更大的还款可能性。 处理这一问题的一种方法是对我们的数据进行预处理， 使线性变得更合理，如使用收入的对数作为我们的特征。</p><p>然而我们可以很容易找出违反单调性的例子。 例如，我们想要根据体温预测死亡率。 对于体温高于37摄氏度的人来说，温度越高风险越大。 然而，对于体温低于37摄氏度的人来说，温度越高风险就越低。 在这种情况下，我们也可以通过一些巧妙的预处理来解决问题。 例如，我们可以使用与37摄氏度的距离作为特征。</p><p>但如果是对于图像分类问题，单个像素的强度大小对预测整张图片为某个类别的作用并非简单的线性关系。任何像素的重要性都以复杂的方式取决于该像素的上下文（周围像素的值）。 </p><p>然而数据可能会有一种考虑到特征之间的相关交互作用的表示，在这个表示的基础上可以使用线性模型。对于深度神经网络，我们使用观测数据来联合学习<em>隐藏层</em>表示和应用于该表示的线性预测器。</p><h4 id="1-1-2-在网络中加入隐藏层"><a href="#1-1-2-在网络中加入隐藏层" class="headerlink" title="1.1.2. 在网络中加入隐藏层"></a>1.1.2. 在网络中加入隐藏层</h4><p>隐藏层也称隐含层。<br>可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制， 使其能处理更普遍的函数关系类型。 要做到这一点，最简单的方法是将许多<em>全连接层</em>堆叠在一起。 每一层都输出到上面的层，直到生成最后的输出。<br>我们可以把前L-1层看作表示，把最后一层(输出）看作线性预测器。 这种架构通常称为多层感知机（multilayer perceptron），通常缩写为MLP。 下面，我们以图的方式描述了多层感知机（单隐藏层的多层感知机）。<br><img src="/assets/post_img/article44/隐藏层.svg" alt="mlp"></p><p>这个多层感知机有4个输入，3个输出，其隐藏层包含5个隐藏单元。 输入层不涉及任何计算，因此使用此网络产生输出只需要实现隐藏层和输出层的计算。 因此，这个多层感知机中的层数为2。 注意，这两个层都是全连接的。 每个输入都会影响隐藏层中的每个神经元， 而隐藏层中的每个神经元又会影响输出层中的每个神经元。</p><p>但全连接层的多层感知机的参数开销可能很高，在不改变输入或输出大小的情况下，需要在参数节约和模型有效性之间进行权衡。</p><h4 id="1-1-3-从线性到非线性，激活函数！"><a href="#1-1-3-从线性到非线性，激活函数！" class="headerlink" title="1.1.3. 从线性到非线性，激活函数！"></a>1.1.3. 从线性到非线性，激活函数！</h4><p>我们通过矩阵 $\mathbf{X} \in \mathbb{R}^{n \times d}$ 来表示 $n$ 个样本的小批量，其中每个样本具有$d$个输入特征。对于具有$h$个隐藏单元的单隐藏层多层感知机， 用 $\mathbf{H} \in \mathbb{R}^{n \times h}$ 表示隐藏层的输出，称为隐藏表示（hidden representations）。 在数学或代码中，$\mathbf{H}$ 也被称为隐藏层变量（hidden-layer variable）或隐藏变量（hidden variable）。</p><p>因为隐藏层和输出层都是全连接的， 所以我们有隐藏层权重 $\mathbf{W}^{(1)} \in \mathbb{R}^{d \times h}$ 和隐藏层偏置 $\mathbf{b}^{(1)} \in \mathbb{R}^{1 \times h}$ 以及输出层权重 $\mathbf{W}^{(2)} \in \mathbb{R}^{h \times q}$ 和输出层偏置 $\mathbf{b}^{(2)} \in \mathbb{R}^{1 \times q}$。 形式上，我们按如下方式计算单隐藏层多层感知机的输出 $\mathbf{O} \in \mathbb{R}^{n \times q}$：</p><script type="math/tex; mode=display">\begin{split}\begin{aligned} \mathbf{H} & = \mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}, \\ \mathbf{O} & = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}. \end{aligned}\end{split}</script><p>首先回顾一下什么是仿射函数：<br>仿射函数即由由1阶多项式构成的函数，一般形式为 f (x) = A x + b，这里，A 是一个 m×k 矩阵，x 是一个 k 向量,b是一个m向量，实际上反映了一种从 k 维到 m 维的空间映射关系。</p><p>添加隐藏层之后，模型现在需要跟踪和更新额外的参数（权重和偏移），但却仍然是线性模型！因为隐藏单元由输入的仿射函数给出，而输出（在softmax操作前）只是隐藏单元的仿射函数。仿射的仿射依然是仿射函数（线性模型可以表示任意一个仿射函数）。<br>下面证明这一等价性，即对于任意权重值，只需合并隐藏层，便可产生具有参数 $\mathbf{W} = \mathbf{W}^{(1)}\mathbf{W}^{(2)}$ 和  $\mathbf{b} = \mathbf{b}^{(1)} \mathbf{W}^{(2)} + \mathbf{b}^{(2)}$ 的等价单层模型（即同样是线性模型）：</p><script type="math/tex; mode=display">\mathbf{O} = (\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)})\mathbf{W}^{(2)} + \mathbf{b}^{(2)} = \mathbf{X} \mathbf{W}^{(1)}\mathbf{W}^{(2)} + \mathbf{b}^{(1)} \mathbf{W}^{(2)} + \mathbf{b}^{(2)} = \mathbf{X} \mathbf{W} + \mathbf{b}</script><p>因此为了克服线性模型的限制，还需要一个额外的关键要素：在仿射变换之后对每个隐藏单元应用非线性的<strong>激活函数</strong>（activation function）   $\sigma$。 <strong>激活函数</strong>的输出（例如，$\sigma(\cdot)$）被称为活性值（activations）。<br>有了激活函数，就不可能再将我们的多层感知机退化成线性模型，如下：</p><script type="math/tex; mode=display">\begin{split}\begin{aligned} \mathbf{H} & = \sigma(\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}), \\ \mathbf{O} & = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}.\\ \end{aligned}\end{split}</script><p>由于 $\mathbf{X}$ 中的每一行对应于小批量中的一个样本， 出于记号习惯的考量， 我们定义非线性函数 $\sigma$ 也以按行的方式作用于其输入， 即一次计算一个样本（与之前的softmax函数相同）。<br>这里我们应用于隐藏层的激活函数通常不仅按行操作，也按元素操作。 这意味着在计算每一层的线性部分之后，我们可以计算每个活性值，而不需要查看其他隐藏单元所取的值。</p><p>为了构建更通用的多层感知机， 我们可以继续堆叠这样的隐藏层， 例如$\mathbf{H}^{(1)} = \sigma_1(\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}$ 和 $\mathbf{H}^{(2)} = \sigma_2(\mathbf{H}^{(1)} \mathbf{W}^{(2)} + \mathbf{b}^{(2)})$， 一层叠一层，从而产生更有表达能力的模型。</p><h4 id="1-1-4-通用近似定理（Universal-Approximation-Theorem）"><a href="#1-1-4-通用近似定理（Universal-Approximation-Theorem）" class="headerlink" title="1.1.4. 通用近似定理（Universal Approximation Theorem）"></a>1.1.4. 通用近似定理（Universal Approximation Theorem）</h4><p>人工神经网络最有价值的地方可能就在于，它可以在理论上证明：“一个包含足够多隐含层神经元的多层前馈网络，能以任意精度逼近任意预定的连续函数”。这个定理即为通用近似定理，这里的“Universal”，也有人将其翻译成“万能的”，也有译为“万能逼近定理”。</p><p>也就是说，通用近似定理告诉我们，不管连续函数$f(x)$在形式上有多复杂，我们总能确保找到一个神经网络，对任何可能的输入$x$，以任意高的精度（通过增加隐含层神元的个数来提升近似的精度）近似输出$f(x)$（即使函数有多个输入和输出）。</p><p>多层感知机可以通过隐藏神经元，捕捉到输入之间复杂的相互作用， 这些神经元依赖于每个输入的值。 我们可以很容易地设计隐藏节点来执行任意计算。即使是网络只有一个隐藏层，给定足够的神经元和正确的权重， 我们可以对任意函数建模，尽管实际中学习该函数是很困难的。</p><p>虽然一个单隐层网络能学习任何函数， 但我们不应该尝试使用单隐藏层网络来解决所有问题，因为这种网络结构可能会格外庞大，进而无法正确地学习和泛化。通过使用更深（而不是更广）的网络，可以更容易地逼近许多函数。 </p><p>参考：<br><a href="https://blog.csdn.net/qq_41554005/article/details/110821533">通用近似定理（学习笔记）</a></p><h3 id="1-2-激活函数（activation-function）"><a href="#1-2-激活函数（activation-function）" class="headerlink" title="1.2. 激活函数（activation function）"></a>1.2. 激活函数（activation function）</h3><p>激活函数通过计算加权和并加上偏置来确定神经元是否应该被激活，它们将输入信号转换为输出的可微运算。 大多数激活函数都是<em>非线性</em>的。激活函数是深度学习的基础，下面简要介绍一些常见的激活函数。</p><h4 id="1-2-1-ReLU函数"><a href="#1-2-1-ReLU函数" class="headerlink" title="1.2.1. ReLU函数"></a>1.2.1. ReLU函数</h4><p>最受欢迎的激活函数是修正线性单元（Rectified linear unit，ReLU），因为它实现简单，同时在各种预测任务中表现良好。<br>ReLU提供了一种非常简单的非线性变换。 给定元素$x$，ReLU函数被定义为该元素与0的最大值：</p><script type="math/tex; mode=display">\operatorname{ReLU}(x) = \max(x, 0)</script><p>图像：<br><img src="/assets/post_img/article44/relu.png" alt="relu"><br>通俗地说，ReLU函数通过将相应的活性值设为0，仅保留正元素并丢弃所有负元素。ReLU激活函数是分段线性的。</p><p>ReLU导数图像：<br><img src="/assets/post_img/article44/grelu.png" alt="relu"><br>当输入为负时，ReLU函数的导数为0，而当输入为正时，ReLU函数的导数为1。 注意，当输入值精确等于0时，ReLU函数不可导。此时默认使用左侧的导数，即当输入为0时导数为0。可以忽略这种情况，因为输入可能永远都不会是0。<br>“如果微妙的边界条件很重要，我们很可能是在研究数学而非工程”。</p><p>使用ReLU的原因是，它求导表现得特别好：要么让参数消失，要么让参数通过。 这使得优化表现得更好，并且ReLU减轻了困扰以往神经网络的梯度消失问题（稍后详细介绍）。</p><p>ReLU函数有许多变体，包括参数化ReLU函数（pReLU）。该变体为ReLU添加了一个线性项，因此即使参数是负的，某些信息仍然可以通过：</p><script type="math/tex; mode=display">\operatorname{pReLU}(x) = \max(0, x) + \alpha \min(0, x)</script><h4 id="1-2-2-sigmoid函数"><a href="#1-2-2-sigmoid函数" class="headerlink" title="1.2.2. sigmoid函数"></a>1.2.2. sigmoid函数</h4><p>对于一个定义域在$\mathbb{R}$中的输入， sigmoid函数将输入变换为区间(0, 1)上的输出。 因此，sigmoid通常称为挤压函数（squashing function）： 它将范围（-inf, inf）中的任意输入压缩到区间（0, 1）中的某个值：</p><script type="math/tex; mode=display">\operatorname{sigmoid}(x) = \frac{1}{1 + \exp(-x)}</script><p>图像：<br><img src="/assets/post_img/article44/sigmoid.png" alt="relu"><br>当输入接近0时，sigmoid函数接近线性变换。</p><p>在最早的神经网络中，科学家们专注于阈值单元。阈值单元在其输入低于某个阈值时取值0，当输入超过阈值时取值1。<br>当人们逐渐关注到到基于梯度的学习时， sigmoid函数是一个自然的选择，因为它是一个平滑的、可微的阈值单元近似。 当我们想要将输出视作二元分类问题的概率时， sigmoid仍然被广泛用作输出单元上的激活函数 （可以将sigmoid视为softmax的特例）。</p><p>sigmoid函数的导数为下面的公式：</p><script type="math/tex; mode=display">\frac{d}{dx} \operatorname{sigmoid}(x) = \frac{\exp(-x)}{(1 + \exp(-x))^2} = \operatorname{sigmoid}(x)\left(1-\operatorname{sigmoid}(x)\right)</script><p>sigmoid导数图像：<br><img src="/assets/post_img/article44/gsigmoid.png" alt="relu"><br>当输入为0时，sigmoid函数的导数达到最大值0.25； 而输入在任一方向上越远离0点时，导数越接近0。</p><p>然而，sigmoid在隐藏层中已经较少使用，它在大部分时候被更简单、更容易训练的ReLU所取代。 在后面关于循环神经网络的章节中将描述利用sigmoid单元来控制时序信息流的架构。</p><h4 id="1-2-3-tanh函数"><a href="#1-2-3-tanh函数" class="headerlink" title="1.2.3. tanh函数"></a>1.2.3. tanh函数</h4><p>tanh(双曲正切)函数也能将其输入压缩转换到区间(-1, 1)上。 tanh函数的公式如下：</p><script type="math/tex; mode=display">\operatorname{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}</script><p>图像：<br><img src="/assets/post_img/article44/tanh.png" alt="relu"><br>当输入在0附近时，tanh函数接近线性变换。 函数的形状类似于sigmoid函数，不同的是tanh函数关于坐标系原点中心对称。</p><p>tanh函数的导数是：</p><script type="math/tex; mode=display">\frac{d}{dx} \operatorname{tanh}(x) = 1 - \operatorname{tanh}^2(x)</script><p>tanh导数图像：<br><img src="/assets/post_img/article44/gtanh.png" alt="relu"><br>当输入接近0时，tanh函数的导数接近最大值1。与sigmoid函数导数图像中看到的类似，输入在任一方向上越远离0点，导数越接近0。</p><h4 id="1-2-4-关于激活函数的小结"><a href="#1-2-4-关于激活函数的小结" class="headerlink" title="1.2.4. 关于激活函数的小结"></a>1.2.4. 关于激活函数的小结</h4><p>这些知识只是掌握了一个类似于1990年左右深度学习从业者的工具。<br>使用深度学习框架只需几行代码就可以快速构建模型，而以前训练这些网络需要研究人员编写数千行的C或Fortran代码。<br>这些激活函数在框架中可以直接调用，如<code>y = torch.relu(x)</code>。</p><h3 id="1-3-实现多层感知机"><a href="#1-3-实现多层感知机" class="headerlink" title="1.3. 实现多层感知机"></a>1.3. 实现多层感知机</h3><p><strong>从零实现</strong>：<br>继续使用Fashion-MNIST图像分类数据集，并与之前softmax回归的结果进行比较。将每个图像视为具有784个输入特征（使用reshape将每个二维图像转换为一个长度为784的向量）和10个类的简单分类数据集，实现一个具有单隐藏层的多层感知机，包含256个隐藏单元。选择2的若干次幂作为层的宽度（隐含层单元的个数）是因为考虑到内存在硬件中的分配和寻址方式，这么做往往可以在计算上更高效。<br>对于每一层都要记录一个权重矩阵和一个偏置向量，并为损失关于这些参数的梯度分配内存。<br>手动实现一个简单的多层感知机是很容易的。然而如果有大量的层，从零开始实现多层感知机会变得很麻烦（例如，要命名和记录大量参数）。<br><strong>框架实现</strong>：<br>使用高级API更简洁地实现多层感知机。<br>对于相同的分类问题，多层感知机的实现与softmax回归的实现相同，只是多层感知机的实现里增加了带有激活函数的隐藏层。</p><h2 id="2-模型选择、欠拟合和过拟合"><a href="#2-模型选择、欠拟合和过拟合" class="headerlink" title="2. 模型选择、欠拟合和过拟合"></a>2. 模型选择、欠拟合和过拟合</h2><p>机器学习问题中，我们的目标是发现模式（pattern），确定模型真正发现了一种可泛化的模式而不是简单的记住了所有数据。<br>我们的目标是发现某些模式， 这些模式捕捉到了我们训练集潜在总体的规律。 如果成功做到了这点，即使是对以前从未遇到过的个体， 模型也可以成功地评估风险。 如何发现可以泛化的模式是机器学习的根本问题。</p><p>困难在于，在训练模型时，我们只能访问数据中的小部分样本。 最大的公开图像数据集包含大约一百万张图像。 而在大部分时候，我们只能从数千或数万个数据样本中学习。 在大型医院系统中，我们可能会访问数十万份医疗记录。 当我们使用有限的样本时，可能会遇到这样的问题： 当收集到更多的数据时，会发现之前找到的明显关系并不成立。</p><p>模型在训练数据上拟合的比在潜在分布中更接近的现象称为过拟合（overfitting）， 用于对抗过拟合的技术称为正则化（regularization）。<br>比如在使用Fashion-MNIST数据集时如果有足够多的神经元、层数和训练迭代周期， 模型最终可以在训练集上达到完美的精度，但此时在测试集上的准确性却下降了。</p><h3 id="2-1-训练误差和泛化误差"><a href="#2-1-训练误差和泛化误差" class="headerlink" title="2.1. 训练误差和泛化误差"></a>2.1. 训练误差和泛化误差</h3><p>误差越小，模型越有效。<br><strong>训练误差</strong>（training error）是指， 模型在训练数据集上计算得到的误差。<br><strong>泛化误差</strong>（generalization error）是指， 模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。</p><p>泛化误差永远不能被准确地计算出。 这是因为无限多的数据样本是一个虚构的对象。 实际中只能通过将模型应用于一个独立的测试集来估计泛化误差， 该测试集由随机选取的、未曾在训练集中出现的数据样本构成。</p><p>对于之前提到的记住所有数据的模式举一个例子，应用查表法来进行预测，对于$28\times28$的灰度图像，如果每个像素可以取$256$个灰度值中的一个， 则有$256^{784}$个可能的图像。 这意味着指甲大小的低分辨率灰度图像的数量比宇宙中的原子要多得多。 即使我们可能遇到这样的数据，我们也不可能存储整个查找表。</p><h4 id="2-1-1-独立同分布假设"><a href="#2-1-1-独立同分布假设" class="headerlink" title="2.1.1. 独立同分布假设"></a>2.1.1. 独立同分布假设</h4><p>监督学习情景中， 通常会假设训练数据和测试数据都是从相同的分布中独立提取的（独立同分布）。 这意味着对数据进行采样的过程没有进行“记忆”。 换句话说，抽取的第2个样本和第3个样本的相关性， 并不比抽取的第2个样本和第200万个样本的相关性更强。</p><p>但这个假设很多时候是有漏洞的，例如一个模型应用在多个数据集时，不同数据集的分布可能都是不同的。其次，抽样过程可能与时间有关，从而违反独立性。有时轻微违法独立同分布假设时模型依然可以运行良好，但另一些时候在违背独立同分布的数据上应用模型会导致错误。</p><p>当训练模型时，我们试图找到一个能够尽可能拟合训练数据的函数。 但是如果它执行地“太好了”，而不能对看不见的数据做到很好泛化，就会导致过拟合。 这种情况正是我们想要避免或控制的。 深度学习中有许多启发式的技术旨在防止过拟合。</p><h4 id="2-1-2-模型复杂性"><a href="#2-1-2-模型复杂性" class="headerlink" title="2.1.2. 模型复杂性"></a>2.1.2. 模型复杂性</h4><p>模型复杂性由什么构成是一个复杂的问题。 一个模型是否能很好地泛化取决于很多因素。 例如，具有更多参数的模型可能被认为更复杂， 参数有更大取值范围的模型可能更为复杂。 通常对于神经网络，我们认为需要更多训练迭代的模型比较复杂， 而需要“早停”（early stopping）的模型（即较少训练迭代周期）就不那么复杂。</p><p>很难比较本质上不同大类的模型之间（例如，决策树与神经网络）的复杂性。统计学家认为，能够轻松解释任意事实的模型是复杂的， 而表达能力有限但仍能很好地解释数据的模型可能更有现实用途。<br>几个影响模型泛化的因素：</p><ol><li>可调整参数的数量。当可调整参数的数量（有时称为自由度）很大时，模型往往更容易过拟合。</li><li>参数采用的值。当权重的取值范围较大时，模型可能更容易过拟合。</li><li>训练样本的数量。即使你的模型很简单，也很容易过拟合只包含一两个样本的训练集。而过拟合一个有数百万个样本的数据集则需要一个极其灵活的模型。<h3 id="2-2-模型选择"><a href="#2-2-模型选择" class="headerlink" title="2.2. 模型选择"></a>2.2. 模型选择</h3>在机器学习中，我们通常在评估几个候选模型后选择最终的模型。 这个过程叫做模型选择。<br>有时进行比较的模型在本质上是完全不同的（比如，决策树与线性模型）。 又有时我们需要比较不同的超参数设置下的同一类模型。</li></ol><p>例如，训练多层感知机模型时，我们可能希望比较具有不同数量的隐藏层、不同数量的隐藏单元以及不同的的激活函数组合的模型。 为了确定候选模型中的最佳模型，我们通常会使用验证集。</p><h4 id="2-2-1-验证集"><a href="#2-2-1-验证集" class="headerlink" title="2.2.1. 验证集"></a>2.2.1. 验证集</h4><p>原则上，在确定所有的超参数之前，我们不希望用到测试集。 如果在模型选择过程中使用测试数据，可能会有过拟合测试数据的风险。过拟合了训练数据，还可以在测试数据上的评估来判断过拟合。 但我们无法知晓模型是否过拟合了测试数据。<br>因此决不能依靠测试数据进行模型选择，但也不能仅仅依靠训练数据来选择模型，因为我们无法估计训练数据的泛化误差。</p><p>同时虽然理想情况下我们只会使用测试数据一次，以评估最好的模型或比较一些模型效果，但现实是测试数据很少在使用一次后被丢弃。我们很少能有充足的数据来对每一轮实验采用全新测试集。</p><p>解决此问题的常见做法是将数据分成三份， 除了训练和测试数据集之外，还增加一个验证数据集（validation dataset）， 也叫验证集（validation set）。<br>但现实是验证数据和测试数据之间的边界也十分模糊。d2l书中每次实验都在使用训练集和验证集，而没有真正的测试集，实验报告的准确度都是验证集准确度。</p><h4 id="2-2-2-K-折交叉验证"><a href="#2-2-2-K-折交叉验证" class="headerlink" title="2.2.2.  $K$折交叉验证"></a>2.2.2.  $K$折交叉验证</h4><p>当训练数据稀缺时，可能无法提供足够的数据来构成一个合适的验证集。 一个解决方案是采用$K$折交叉验证。<br>原始训练数据被分成$K$个不重叠的子集。 然后执行$K$次模型训练和验证，每次在$K-1$个子集上进行训练， 并在剩余的一个子集（在该轮中没有用于训练的子集）上进行验证。 最后，通过对$K$次实验的结果取平均来估计训练和验证误差。</p><h3 id="2-3-欠拟合，过拟合"><a href="#2-3-欠拟合，过拟合" class="headerlink" title="2.3. 欠拟合，过拟合"></a>2.3. 欠拟合，过拟合</h3><p>比较训练和验证误差时要注意两种常见的情况： 欠拟合，过拟合。<br><strong>欠拟合</strong>：训练误差和验证误差都很严重，但它们之间仅有一点差距。如果模型不能降低训练误差，这可能意味着模型过于简单（即表达能力不足），无法捕获试图学习的模式。 又由于训练和验证误差之间的泛化误差很小， 我们有理由相信可以用一个更复杂的模型降低训练误差。 这种现象被称为欠拟合（underfitting）。</p><p><strong>过拟合</strong>：当我们的训练误差明显低于验证误差时要小心， 这表明严重的过拟合（overfitting）。过拟合并不总是一件坏事，特别是在深度学习领域，最好的预测模型在训练数据上的表现往往比在保留（验证）数据上好得多。最终通常更关心验证误差，而不是训练误差和验证误差之间的差距。</p><p>是否过拟合或欠拟合可能取决于模型复杂性和可用训练数据集的大小。</p><h4 id="2-3-1-模型复杂性"><a href="#2-3-1-模型复杂性" class="headerlink" title="2.3.1. 模型复杂性"></a>2.3.1. 模型复杂性</h4><p>关于过拟合和模型复杂性的经典直觉：给定由单个特征$x$和对应实数标签$y$组成的训练数据， 我们试图找到下面的$d$阶多项式来估计标签$y$。</p><script type="math/tex; mode=display">\hat{y}= \sum_{i=0}^d x^i w_i</script><p>这只是一个线性回归问题，特征是$x$的幂给出的， 模型的权重是$w_i$给出的，偏置是$w_0$给出的 （因为对于所有的$x$都有$x^0$ = 1）。线性回归问题可以使用平方误差作为损失函数。</p><p>高阶多项式函数比低阶多项式函数复杂得多。高阶多项式的参数较多，模型函数的选择范围较广。 因此在固定训练数据集的情况下， 高阶多项式函数相对于低阶多项式的训练误差应该始终更低（最坏也是相等）。 其实当数据样本包含了$x$的不同值时， 函数阶数等于数据样本数量的多项式函数可以完美拟合训练集（因为y就是一个多项式，通过优化参数总可以表示出来y）。下图直观地描述了多项式的阶数和欠拟合与过拟合之间的关系。<br><img src="/assets/post_img/article44/模型复杂度对拟合的影响.svg" alt="fit"></p><h4 id="2-3-2-数据集大小"><a href="#2-3-2-数据集大小" class="headerlink" title="2.3.2. 数据集大小"></a>2.3.2. 数据集大小</h4><p>训练数据集中的样本越少，我们就越有可能（且更严重地）过拟合。 随着训练数据量的增加，泛化误差通常会减小。一般来说，更多的数据不会有什么坏处。<br>对于固定的任务和数据分布，模型复杂性和数据集大小之间通常存在关系。 给出更多的数据，我们可能会尝试拟合一个更复杂的模型。 能够拟合更复杂的模型可能是有益的。 如果没有足够的数据，简单的模型可能更有用。 对于许多任务，深度学习只有在有数千个训练样本时才优于线性模型。 从一定程度上来说，深度学习目前的生机要归功于廉价存储、互联设备以及数字化经济带来的海量数据集。</p><h3 id="2-4-多项式回归"><a href="#2-4-多项式回归" class="headerlink" title="2.4. 多项式回归"></a>2.4. 多项式回归</h3><p>通过多项式拟合来探索上述概念。 一样放到notebook中，话说如果都放到notebook中我为什么不把博客格式都做成notebook…<br>给定$x$，使用以下三阶多项式来生成训练和测试数据的标签：</p><script type="math/tex; mode=display">y = 5 + 1.2x - 3.4\frac{x^2}{2!} + 5.6 \frac{x^3}{3!} + \epsilon \text{ where } \epsilon \sim \mathcal{N}(0, 0.1^2)</script><p>噪声项$\epsilon$服从均值为0且标准差为0.1的正态分布。 在优化的过程中，我们通常希望避免非常大的梯度值或损失值，所以将特征从$x^i$调整为$\frac{x^i}{i!}$，这样可以避免很大的$i$带来的特别大的指数值。训练集和测试集各生成100个样本。</p><p>分别对3阶、2阶（线性函数）和高阶多项式训练并观察结果，可以看出3阶是正常的，线性函数会产生欠拟合问题，而高阶（20）则会产生过拟合问题。<br>结果说明2阶多项式模型过于简单。而在20阶多项式模型的实验中，实际上只有前5阶有非零的权值，其余阶均为0，这种情况下，高阶多项式模型的参数过多，但训练样本只有100个，数量较少，模型无法从中学习到高阶权值应为0的特点，即模型被数据中的噪声轻易的影响了。虽然高阶模型能够得到更小的训练误差，但在测试集上的表现会比在训练集上要差，故过拟合。<br>但对于这个实验，使用高阶多项式模型的损失仍然小于线性模型（在同样的迭代周期下）。</p><h2 id="3-权重衰减"><a href="#3-权重衰减" class="headerlink" title="3. 权重衰减"></a>3. 权重衰减</h2><p>正则化就是把额外的约束或者惩罚项加到已有模型（损失函数）上，以防止过拟合并提高泛化能力。假设我们已经拥有尽可能多的高质量数据，我们便可以将重点放在正则化技术上。</p><p>在多项式回归的例子中， 我们可以通过调整拟合多项式的阶数来限制模型的容量。事实上这种限制特征的数量是缓解过拟合的一种常用技术，但这种直接丢弃特征的做法可能过于生硬。 多项式对多变量数据的自然扩展称为单项式（monomials），也就是多变量幂的乘积。单项式的阶数是幂的和。 例如，$x_1^2 x_2$和$x_3 x_5^2$都是3次单项式。<br>随着阶数$d$的增长，带有阶数$d$的项数迅速增加。 给定$k$个变量，阶数为$d$的项的个数为：（这个式子的含义就是从k-1+d个中取k-1个，即$C^{k-1}_{k-1+d}$）</p><script type="math/tex; mode=display">{k - 1 + d} \choose {k - 1}</script><p>例如对于3个变量（x1，x2，x3）的多项式，阶数为1的项有3个（x1，x2，x3），对应从3个（3-1+1）中选2个（3-1），也就是$C^2_3 = C^1_3 = 3$。具体计算方式如下（就是排列组合）：<br><img src="/assets/post_img/article44/排列组合.jpeg" alt="排列组合"><br>那么对于2阶项就对应从4个（3-1+2）中选2个，即$C^2_4 = 6种$，分别是：x1x2，x1x3，x2x3和三个变量单独的2次项。<br>因此即使是阶数上的微小变化，也会显著增加模型的复杂性。仅仅通过简单的限制特征数量（在多项式回归中体现为限制阶数）可能仍然使模型在过简单和过复杂中徘徊，我们需要一个更细粒度的工具来调整函数的复杂性，使其达到一个合适的平衡位置。</p><h3 id="3-1-范数与权重衰减"><a href="#3-1-范数与权重衰减" class="headerlink" title="3.1. 范数与权重衰减"></a>3.1. 范数与权重衰减</h3><p>回顾范数的三个性质，以及$L_1$范数：向量元素的绝对值之和 和 $L_2$范数：向量元素平方和的平方根。</p><p>在训练参数化机器学习模型时， 权重衰减（weight decay）是最广泛使用的正则化的技术之一， 它通常也被称为$L_2$正则化。 这项技术通过函数与零的距离来衡量函数的复杂度， 因为在所有函数$f$中，函数$f = 0$（所有输入都得到值$0$） 在某种意义上是最简单的。 但是应该如何精确地测量一个函数和零之间的距离并没有标准答案。</p><p>一种简单的方法是通过线性函数 $f(\mathbf{x}) = \mathbf{w}^\top \mathbf{x}$ 中的权重向量的某个范数来度量其复杂性（这是一种正则化线性模型）， 例如$| \mathbf{w} |^2$。 要保证权重向量比较小， 最常用方法是将其范数作为惩罚项加到最小化损失的问题中。 （为什么权重大会导致模型复杂度提高？）</p><p>这样一来训练目标就由最小化训练标签上的预测损失变为最小化预测损失和惩罚项之和。 此时如果权重向量增长的太大， 我们的学习算法可能会更集中于最小化权重范数$| \mathbf{w} |^2$（简而言之我们想起到的效果就是在最小化损失函数的同时保证权重不太大，至于为什么这样做，不太懂）。现在回顾一下之前的线性回归例子。其中损失由下式给出：</p><script type="math/tex; mode=display">L(\mathbf{w}, b) = \frac{1}{n}\sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2</script><p>$\mathbf{x}^{(i)}$是样本$i$的特征， $y^{(i)}$是样本$i$的标签， $(\mathbf{w}, b)$是权重和偏置参数。 为了惩罚权重向量的大小， 我们必须以某种方式在损失函数中添加$| \mathbf{w} |^2$， 但是模型应该如何平衡这个新的额外惩罚的损失？通常通过正则化常数 $\lambda$ 来描述这种权衡， 这是一个非负超参数，我们使用验证数据拟合：</p><script type="math/tex; mode=display">L(\mathbf{w}, b) + \frac{\lambda}{2} \|\mathbf{w}\|^2</script><p>这样操作后对于$\lambda = 0$，保持了原来的损失函数。 对于$\lambda &gt; 0$，则可以通过这一惩罚项来增大损失函数，从而使参数向$| \mathbf{w} |$更小的方向进行优化（我的理解是这样的）。 这里仍然除以$2$是因为当求导时， $2$和$1/2$会抵消，以确保更新表达式看起来既漂亮又简单。 关于为什么这里使用范数的平方而不是标准范数（即欧几里得距离）则是为了便于计算，通过使用$L_2$范数的平方（之前也提到过机器学习经常使用该范数的平方而不是它本身），我们去掉平方根，留下权重向量每个分量的平方和。 这使得惩罚的导数很容易计算，即惩罚项导数的和等于和的导数。</p><p>此外，关于为什么我们首先使用$L_2$范数，而不是$L_1$范数。这是因为这个选择在整个统计领域中都是有效的和受欢迎的。 $L_2$正则化线性模型构成经典的岭回归（ridge regression）算法， $L_1$正则化线性回归是统计学中类似的基本模型， 通常被称为套索回归（lasso regression）。 </p><p>使用$L_2$范数的一个原因是它对权重向量的大分量施加了巨大的惩罚。 这使得我们的学习算法偏向于在大量特征上均匀分布权重的模型。 在实践中，这可能使它们对单个变量中的观测误差更为稳定。 相比之下，$L_1$惩罚会导致模型将权重集中在一小部分特征上， 而将其他权重清除为零。 这称为<strong>特征选择</strong>（feature selection），这可能是其他场景下需要的。<br>$L_2$正则化回归的小批量随机梯度下降更新如下式，其中$\mathcal{B}$为小批量，$\eta$为预先确定的正数，$\lambda$是正则化常数 ：</p><script type="math/tex; mode=display">\begin{aligned} \mathbf{w} & \leftarrow \left(1- \eta\lambda \right) \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right). \end{aligned}</script><p>我们依然是根据估计值与观测值之间的差异来更新$\mathbf{w}$，但同时也在试图将$\mathbf{w}$的大小缩小到零（有点没看懂）。 这就是为什么这种方法有时被称为权重衰减。 我们仅考虑惩罚项，优化算法在训练的每一步衰减权重。 与特征选择相比，权重衰减为我们提供了一种连续的机制来调整函数的复杂度。 较小的$\lambda$值对应较少约束的$\mathbf{w}$， 而较大的$\lambda$值对$\mathbf{w}$的约束更大（这里约束就是使值更小的意思吧）。</p><p>是否对相应的偏置$b^2$（为什么是b方？可能也是范数平方吧）进行惩罚在不同的实践中会有所不同， 在神经网络的不同层中也会有所不同。 通常，网络输出层的偏置项不会被正则化。</p><h3 id="3-2-权重衰减演示的从零实现"><a href="#3-2-权重衰减演示的从零实现" class="headerlink" title="3.2. 权重衰减演示的从零实现"></a>3.2. 权重衰减演示的从零实现</h3><p>通过高维线性回归演示如何实现权重衰减。（见实践）<br>简单说就是先生成了一些高维（200个特征）的数据，用如下公式：</p><script type="math/tex; mode=display">y = 0.05 + \sum_{i = 1}^d 0.01 x_i + \epsilon \text{ where } \epsilon \sim \mathcal{N}(0, 0.01^2)</script><p>标签是关于输入的线性函数。 标签同时被均值为0，标准差为0.01高斯噪声破坏。</p><p>然后将$L_2$的平方惩罚添加到原始目标函数中。最终通过对比使用正则化和不使用正则化的过拟合优化情况可以看到正则化后测试误差减小。</p><h3 id="3-3-权重衰减演示的框架实现"><a href="#3-3-权重衰减演示的框架实现" class="headerlink" title="3.3. 权重衰减演示的框架实现"></a>3.3. 权重衰减演示的框架实现</h3><p>由于权重衰减在神经网络优化中很常用， 深度学习框架为了便于我们使用权重衰减， 将权重衰减集成到优化算法中，以便与任何损失函数结合使用。 此外，这种集成还有计算上的好处， 允许在不增加任何额外的计算开销的情况下向算法中添加权重衰减。 由于更新的权重衰减部分仅依赖于每个参数的当前值， 因此优化器必须至少接触每个参数一次。</p><h2 id="4-暂退法（Dropout）"><a href="#4-暂退法（Dropout）" class="headerlink" title="4. 暂退法（Dropout）"></a>4. 暂退法（Dropout）</h2><p>在概率角度看，可以通过以下论证来证明这一技术的合理性： 我们已经假设了一个先验，即权重的值取自均值为0的正态分布。我们希望模型深度挖掘特征，即将其权重分散到许多特征中， 而不是过于依赖少数潜在的虚假关联。</p><h3 id="4-1-重新审视过拟合"><a href="#4-1-重新审视过拟合" class="headerlink" title="4.1. 重新审视过拟合"></a>4.1. 重新审视过拟合</h3><p>当面对更多的特征而样本不足时，线性模型往往会过拟合。 相反，当给出更多样本而不是特征，通常线性模型不会过拟合。<br>但线性模型泛化的可靠性是有代价的，线性模型不会考虑到特征之间的交互作用。对于每个特征，线性模型必须指定正的或负的权重，而忽略其他特征。</p><p>泛化性和灵活性之间的这种基本权衡被描述为偏差-方差权衡（bias-variance tradeoff）。 线性模型有很高的偏差：它们只能表示一小类函数。 然而，这些模型的方差很低（泛化性好）：它们在不同的随机数据样本上可以得出相似的结果。</p><p>深度神经网络位于偏差-方差谱的另一端。 与线性模型不同，神经网络并不局限于单独查看每个特征，而是学习特征之间的交互。 例如，神经网络可能推断“尼日利亚”和“西联汇款”一起出现在电子邮件中表示垃圾邮件， 但单独出现则不表示垃圾邮件。</p><p>即使我们有比特征多得多的样本，深度神经网络也有可能过拟合。深度网络的泛化性质令人费解，而这种泛化性质的数学基础仍然是悬而未决的研究问题。</p><h3 id="4-2-扰动的稳健性"><a href="#4-2-扰动的稳健性" class="headerlink" title="4.2. 扰动的稳健性"></a>4.2. 扰动的稳健性</h3><p>在探究泛化性之前，我们先来定义一下什么是一个“好”的预测模型？ 我们期待“好”的预测模型能在未知的数据上有很好的表现： 经典泛化理论认为，为了缩小训练和测试性能之间的差距，应该以简单的模型为目标。 简单性以较小维度的形式展现，此外，参数的范数也代表了一种有用的简单性度量。</p><p>简单性的另一个角度是平滑性，即函数不应该对其输入的微小变化敏感。 例如，当我们对图像进行分类时，我们预计向像素添加一些随机噪声应该是基本无影响的。1995年，克里斯托弗·毕晓普用数学证实了“要求函数光滑”和“要求函数对输入的随机噪声具有适应性”之间的联系。</p><p>在2014年，斯里瓦斯塔瓦等人就如何将毕晓普的想法应用于网络的内部层提出了一个想法： 在训练过程中，他们建议在计算后续层之前向网络的每一层注入噪声。 因为当训练一个有多层的深层网络时，注入噪声只会在输入-输出映射上增强平滑性。</p><p>这个想法被称为<strong>暂退法（dropout）</strong>。 暂退法在<strong>前向传播</strong>过程中，计算每一内部层的同时注入噪声，这已经成为训练神经网络的常用技术。 这种方法之所以被称为暂退法，因为我们从表面上看是在训练过程中丢弃（drop out）一些神经元。 在整个训练过程的每一次迭代中，标准暂退法包括在计算下一层之前将当前层中的一些节点置零。<br>暂退法的原始论文提到了一个关于有性繁殖的类比： 神经网络过拟合与每一层都依赖于前一层激活值相关，称这种情况为“共适应性”。 作者认为，暂退法会破坏共适应性，就像有性生殖会破坏共适应的基因一样。</p><p>那么关键的挑战就是如何注入这种噪声。 一种想法是以一种无偏向（unbiased）的方式注入噪声。 这样在固定住其他层时，每一层的期望值等于没有噪音时的值。</p><p>在毕晓普的工作中，他将正态分布的噪声添加到线性模型的输入中。 在每次训练迭代中，他将从均值为零的分布$\epsilon \sim \mathcal{N}(0,\sigma^2)$ 采样噪声添加到输入$\mathbf{x}$， 从而产生扰动点$\mathbf{x}’ = \mathbf{x} + \epsilon$， 预期是$E[\mathbf{x}’] = \mathbf{x}$。</p><p>在标准暂退法正则化中，通过按保留（未丢弃）的节点的分数进行规范化来消除每一层的偏差。 换言之，每个中间活性值$h$以暂退概率$p$由随机变量$h’$替换，如下所示：</p><script type="math/tex; mode=display">\begin{split}\begin{aligned} h' = \begin{cases} 0 & \text{ 概率为 } p \\ \frac{h}{1-p} & \text{ 其他情况} \end{cases} \end{aligned}\end{split}</script><p>根据此模型的设计，其期望值保持不变，即$E[h’] = h$。</p><h3 id="4-3-暂退法实践"><a href="#4-3-暂退法实践" class="headerlink" title="4.3. 暂退法实践"></a>4.3. 暂退法实践</h3><p>下图为dropout前后的多层感知机，在左图中，删除了$h_2$和$h_5$， 因此输出的计算不再依赖于$h_2$或$h_5$，并且它们各自的梯度在执行反向传播时也会消失。 这样，输出层的计算不能过度依赖于$h_1, \ldots, h_5$的任何一个元素。<br><img src="/assets/post_img/article44/dropout.svg" alt="暂退法"><br>通常在测试时不使用暂退法（训练时用）。 给定一个训练好的模型和一个新的样本，我们不会丢弃任何节点，因此不需要标准化。 然而也有一些例外：一些研究人员在测试时使用暂退法， 用于估计神经网络预测的“不确定性”： 如果通过许多不同的暂退法遮盖后得到的预测结果都是一致的，那么我们可以说网络发挥更稳定。</p><h3 id="4-4-暂退法的实现"><a href="#4-4-暂退法的实现" class="headerlink" title="4.4. 暂退法的实现"></a>4.4. 暂退法的实现</h3><p>从零实现和框架实现具体见<a href="https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x04.ipynb">对应实践</a>,几个可以注意的点如下：</p><ul><li>在靠近输入层的地方设置较低的暂退概率。</li><li>框架中，在训练时，Dropout层将根据指定的暂退概率随机丢弃上一层的输出（相当于下一层的输入）。 在测试时，Dropout层仅传递数据。</li></ul><h2 id="5-前向传播与反向传播"><a href="#5-前向传播与反向传播" class="headerlink" title="5. 前向传播与反向传播"></a>5. 前向传播与反向传播</h2><p>前向传播（forward propagation或forward pass） 指的是：按顺序（从输入层到输出层）计算和存储神经网络中每层的结果。</p><p>反向传播（backward propagation或backpropagation）指的是计算神经网络参数梯度的方法。 简言之，该方法根据微积分中的链式规则，按相反的顺序从输出层到输入层遍历网络。 该算法存储了计算某些参数梯度时所需的任何中间变量（偏导数）。假设有函数 $Y=f(\mathrm{X})$ 和 $\mathrm{Z}=g(\mathrm{Y})$, 其中输入 和输出 $X, Y, Z$ 是任意形状的张量。利用链式法则, 我们可以计算Z关于 $X$ 的导数</p><script type="math/tex; mode=display">\frac{\partial \mathrm{Z}}{\partial \mathrm{X}}=\operatorname{prod}\left(\frac{\partial \mathrm{Z}}{\partial \mathrm{Y}}, \frac{\partial \mathrm{Y}}{\partial \mathrm{X}}\right)</script><p>这里使用prod运算符在执行必要的操作 (如换位和交换输入位置) 后将其参数相乘。对于向量只是矩阵矩阵乘法。对于高维张量则使用适当的对应项。运算符prod指代了所有的这些符号。</p><p>在训练神经网络时，在初始化模型参数后， 我们交替使用前向传播和反向传播，利用反向传播给出的梯度来更新模型参数。 注意，反向传播重复利用前向传播中存储的中间值，以避免重复计算。 带来的影响之一是我们需要保留中间值，直到反向传播完成。 这也是训练比单纯的预测需要更多的内存（显存）的原因之一。 此外，这些中间值的大小与网络层的数量和批量的大小大致成正比。 因此，使用更大的批量来训练更深层次的网络更容易导致内存不足（out of memory）错误。</p><h2 id="6-数值稳定性和模型初始化"><a href="#6-数值稳定性和模型初始化" class="headerlink" title="6. 数值稳定性和模型初始化"></a>6. 数值稳定性和模型初始化</h2><p>到目前为止学习的每个模型都是根据某个预先指定的分布来初始化模型的参数。但事实上这种初始化方案并不是理所当然的，初始化方案的选择在神经网络学习中起着举足轻重的作用，它对保持数值稳定性至关重要。 此外，这些初始化方案的选择可以与非线性激活函数的选择有趣的结合在一起。 我们选择哪个函数以及如何初始化参数可以决定优化算法收敛的速度有多快。 糟糕选择可能会导致我们在训练时遇到<strong>梯度爆炸</strong>或<strong>梯度消失</strong>。<br>一些有用的启发式方法在整个深度学习生涯中都很有用。</p><h3 id="6-1-梯度消失和梯度爆炸"><a href="#6-1-梯度消失和梯度爆炸" class="headerlink" title="6.1. 梯度消失和梯度爆炸"></a>6.1. 梯度消失和梯度爆炸</h3><p>在深度神经网络中输出关于任何一组参数（权重）的梯度都可以表示n个矩阵与梯度向量的乘积。其中n个矩阵和梯度向量取决于关于哪组参数求梯度，设当前深度神经网络为L层，当关于 $l$ 层参数时，n为 $L-l$ ，梯度向量为$\partial_{\mathbf{W}}(l) \boldsymbol{h}^{(l)}$。<br>注：每一层 $l$ 由变换 $f_{l}$ 定义, 该变换的参数为权重 $\mathbf{W}^{(l)}$, 其隐藏变量是 $\mathbf{h}^{(l)}$ (令 $\mathbf{h}^{(0)}=输入\mathbf{x}$ )。</p><p>这样不稳定的梯度（太多概率相乘）造成两个问题：<br>一是容易受到数值下溢影响（概率乘积过小），即便对概率取对数，将数值表示的压力从尾数转移到指数，也有可能出现数值溢出问题（当概率趋近零，取对数后对数值趋近负无穷）。</p><p>二是威胁到优化算法的稳定性。 此时面临的问题要么是<strong>梯度爆炸</strong>（gradient exploding）问题： 参数更新过大，破坏了模型的稳定收敛； 要么是<strong>梯度消失</strong>（gradient vanishing）问题： 参数更新过小，在每次更新时几乎不会移动，导致模型无法学习。</p><h4 id="6-1-1-梯度消失（gradient-vanishing）"><a href="#6-1-1-梯度消失（gradient-vanishing）" class="headerlink" title="6.1.1. 梯度消失（gradient vanishing）"></a>6.1.1. 梯度消失（gradient vanishing）</h4><p>sigmoid函数曾经是导致梯度消失问题的一个常见的原因。<br><img src="/assets/post_img/article44/sigmoid_gradient.svg" alt="sg"><br>当sigmoid函数的输入很大或是很小时，它的梯度都会消失。 此外，当反向传播通过许多层时，除非我们在刚刚好的地方， 这些地方sigmoid函数的输入接近于零，否则整个乘积的梯度可能会消失。 当我们的网络有很多层时，除非我们很小心，否则在某一层可能会切断梯度。 这个问题曾经困扰着深度网络的训练，因此，更稳定的ReLU系列函数已经成为从业者的默认选择（虽然在神经科学的角度看起来不太合理）。</p><h4 id="6-1-2-梯度爆炸（gradient-exploding）"><a href="#6-1-2-梯度爆炸（gradient-exploding）" class="headerlink" title="6.1.2. 梯度爆炸（gradient exploding）"></a>6.1.2. 梯度爆炸（gradient exploding）</h4><p>当梯度爆炸是由于深度网络的初始化所导致时，我们没有机会让梯度下降优化器收敛。</p><h4 id="6-1-3-打破对称性"><a href="#6-1-3-打破对称性" class="headerlink" title="6.1.3. 打破对称性"></a>6.1.3. 打破对称性</h4><p>神经网络设计中的另一个问题是其参数化所固有的对称性。例如对于简单的多层感知机，每一层的隐藏单元之间具有排列对称性。</p><p>例如对于一个含两个隐藏单元隐藏层的多层感知机，输出层将两个隐藏单元的多层感知机转换为仅一个输出单元。如果将隐藏层的所有参数初始化为 $\mathbf{W}^{(1)}=c ， c$ 为常量。在这种情况下, 在前向传播期间, 两个隐藏单元采用相同的输入和参数, 产生相同的激活, 该激活被送到输出单元。在反向传播期间, 根据参数 $\mathbf{W}^{(1)}$ 对输出单元进行微分, 得到一个梯度, 其元素都取相同的值。因此，在基于梯度的迭代 (例如, 小批量随机梯度下降）之后, $\mathbf{W}^{(1)}$ 的所有元素仍然采用相同的值。这样的迭代永远不会打破对称性, 则可能永远也无法实现网络的表达能力。隐藏层的行为就好像只有一个单元。 小批量随机梯度下降不会打破这种对称性，但暂退法正则化可以。</p><h3 id="6-2-参数初始化"><a href="#6-2-参数初始化" class="headerlink" title="6.2. 参数初始化"></a>6.2. 参数初始化</h3><p>减缓上述三个问题的一种办法是在参数初始化方面进行改良，在优化期间做一些工作和适当的正则化也可以进一步提高稳定性。</p><h4 id="6-2-1-默认初始化"><a href="#6-2-1-默认初始化" class="headerlink" title="6.2.1. 默认初始化"></a>6.2.1. 默认初始化</h4><p>如果不人为指定初始化方法，框架将使用默认的随机初始化方法，对于中等难度的问题，这种方法通常很有效。</p><h4 id="6-2-2-Xavier初始化"><a href="#6-2-2-Xavier初始化" class="headerlink" title="6.2.2. Xavier初始化"></a>6.2.2. Xavier初始化</h4><p>对于只有线性计算的全连接层的输出$o_{i}$，设有$n_{\mathrm{in}}$个输入 $x_{j}$ 及对应的相关权重 $w_{i j}$，则输出可由下式表示：</p><script type="math/tex; mode=display">o_{i}=\sum_{j=1}^{n_{\text {in }}} w_{i j} x_{j}</script><p>权重 $w_{i j}$ 都是从同一分布中独立抽取的。假设该分布（并不一定是正态分布，只是需要均值和方差存在）具有零均值和方差 $\sigma^{2}$，同时假设层 $x_{j}$ 的输入也具有零均值和方差 $\gamma^{2}$, 它们独立于 $w_{i j}$ 并且彼此独立。在这种假设下就可以按如下方式计算 $o_{i}$ 的期望和方差：</p><script type="math/tex; mode=display">\begin{aligned}E\left[o_{i}\right] &=\sum_{j=1}^{n_{\text {in }}} E\left[w_{i j} x_{j}\right] \\&=\sum_{j=1}^{n_{\text {in }}} E\left[w_{i j}\right] E\left[x_{j}\right] \\&=0, \\\operatorname{Var}\left[o_{i}\right] &=E\left[o_{i}^{2}\right]-\left(E\left[o_{i}\right]\right)^{2} \\&=\sum_{j=1}^{n_{\text {in }}} E\left[w_{i j}^{2} x_{j}^{2}\right]-0 \\&=\sum_{j=1}^{n_{\text {in }}} E\left[w_{i j}^{2}\right] E\left[x_{j}^{2}\right] \\&=n_{\text {in }} \sigma^{2} \gamma^{2}\end{aligned}</script><p>所以保持输出与输入的方差不变的一种方法是设置 $n_{\text {in }} \sigma^{2}=1$ 。 在反向传播过程中也是类似的问题, 梯度从更靠近输出的层传播的。使用与正向传播相同的推断, 可以得出除非 $n_{\text {out }} \sigma^{2}=1$, 否则梯度的方差可能会增大（$n_{\text {out }}$ 是该层的输出的数量）。但很明显不可能同时满足这两个条件（$n_{\text {in}}$和$n_{\text {out}}$相等且与$\sigma^{2}$乘积为1）。则只需满足：</p><script type="math/tex; mode=display">\frac{1}{2}\left(n_{\text {in }}+n_{\text {out }}\right) \sigma^{2}=1 \text { 或等价于 } \sigma=\sqrt{\frac{2}{n_{\text {in }}+n_{\mathrm{out}}}}</script><p>这就是现在标准且实用的Xavier初始化的基础，它以其提出者第一作者的名字命名。</p><p>通常, Xavier初始化从均值为零, 方差 $\sigma^{2}=\frac{2}{n_{\text {in }}+n_{\text {aut }}}$ 的正态分布中采样权重。我们也可以利用 Xavier的直觉来选择从均匀分布中抽取权重时的方差。注意均匀分布 $U(-a, a)$ 的方差为 $\frac{a^{2}}{3}$ 。将 $\frac{a^{2}}{3}$ 代 入到 $\sigma^{2}$ 的条件中，将得到初始化值域：</p><script type="math/tex; mode=display">U\left(-\sqrt{\frac{6}{n_{\text {in }}+n_{\text {out }}}}, \sqrt{\frac{6}{n_{\text {in }}+n_{\text {out }}}}\right) .</script><p>虽然上述的“不存在非线性计算”的假设在神经网络中很容易被违反, 但Xavier初始化方法在实践中被证明是有效的。</p><h4 id="6-2-3-更多初始化方法"><a href="#6-2-3-更多初始化方法" class="headerlink" title="6.2.3. 更多初始化方法"></a>6.2.3. 更多初始化方法</h4><p>上面的推理仅仅触及了现代参数初始化方法的皮毛。 深度学习框架通常实现十几种不同的启发式方法。 参数初始化一直是深度学习基础研究的热点领域。 其中包括专门用于参数绑定（共享）、超分辨率、序列模型和其他情况的启发式算法。</p><h2 id="7-环境和分布偏移"><a href="#7-环境和分布偏移" class="headerlink" title="7. 环境和分布偏移"></a>7. 环境和分布偏移</h2><p>许多机器学习应用中存在的问题之一就是在将基于模型的决策引入环境时可能会破坏模型。比如用户根据模型的某些特点，做出对应更改从而获取非法利益。</p><h3 id="7-1-分布偏移的类型"><a href="#7-1-分布偏移的类型" class="headerlink" title="7.1. 分布偏移的类型"></a>7.1. 分布偏移的类型</h3><p>分布偏移，在我理解就是两个（或多个）数据集间分布不同的情况，比如在训练集和测试集之间，如果没有一个关于两者间相互关系的预估，那将不能学习到有用的模型。<br>基于对未来数据可能发生变化的一些限制性假设，有些算法可以检测这种偏移，甚至可以动态调整，提高原始分类器的精度。</p><h4 id="7-1-1-协变量偏移（covariate-shift）"><a href="#7-1-1-协变量偏移（covariate-shift）" class="headerlink" title="7.1.1. 协变量偏移（covariate shift）"></a>7.1.1. 协变量偏移（covariate shift）</h4><p>在不同分布偏移中, 协变量偏移可能是最为广泛研究的。 协变量偏移是指：输入的分布可能随时间而改变, 但标签函数（即条件分布 $P(y \mid \mathbf{x})$ ）没有改变。<br>统计学家称之为协变量偏移是因为这个问题是由于协变量（特征）分布的变化而产生的。具体的例子比如在训练分类器时，训练集为真实图片，而测试集为卡通图片。<br>当认为 $\mathbf{x}$ 导致 $y$ 时, 标签偏移是一个合理的假设。</p><h4 id="7-1-2-标签偏移（label-shift）"><a href="#7-1-2-标签偏移（label-shift）" class="headerlink" title="7.1.2. 标签偏移（label shift）"></a>7.1.2. 标签偏移（label shift）</h4><p>标签偏移（label shift）描述了与协变量偏移相反的问题。它假设标签边缘概率 $P(y)$ 可以改变, 但是类别条件分布 $P(\mathbf{x} \mid y)$ 在不同的领域之间保持不变。<br>当认为 $y$ 导致 $\mathbf{x}$ 时, 标签偏移是一个合理的假设。例如预测患者的疾病, 我们可能根据症状来判断, 即使疾病的相对流行率随着时间的推移而变化。标签偏移在这里是恰当的假设, 因为疾病会引起症状，输入为症状（不变），输出为可能的疾病（随时间而分布变化）。</p><p>有时标签偏移和协变量偏移假设可以同时成立。在这些情况下，使用基于标签偏移假设的方法通常是有利的。这是因为这些方法倾向于包含看起来像标签 (低维) 的对象, 而不是像输入（高维）的对象。</p><h4 id="7-1-3-概念偏移（concept-shift）"><a href="#7-1-3-概念偏移（concept-shift）" class="headerlink" title="7.1.3. 概念偏移（concept shift）"></a>7.1.3. 概念偏移（concept shift）</h4><p>当标签的定义发生变化时，就会出现这种问题。精神疾病的诊断标准、所谓的时髦、以及工作头衔等等，都是概念偏移的日常映射。</p><h3 id="7-2-分布偏移纠正"><a href="#7-2-分布偏移纠正" class="headerlink" title="7.2. 分布偏移纠正"></a>7.2. 分布偏移纠正</h3><p>许多情况下训练分布与测试分布是不同的，有时这对模型没什么影响，有时则需要运用一些手段去应对这种偏移。</p><h4 id="7-2-1-经验风险与实际风险"><a href="#7-2-1-经验风险与实际风险" class="headerlink" title="7.2.1. 经验风险与实际风险"></a>7.2.1. 经验风险与实际风险</h4><p>在模型的训练期间，训练数据 ${(\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_n, y_n)}$  的特征和相关的标签经过迭代，在每一个小批量之后更新模型 $f$ 的参数。为了简单起见，我们先不考虑正则化，此时极大地降低了训练损失（即经验风险最小化）：</p><script type="math/tex; mode=display">\underset{f}{\operatorname{minimize}} \frac{1}{n} \sum_{i=1}^{n} l\left(f\left(\mathbf{x}_{i}\right), y_{i}\right),</script><p>其中 $l$ 是损失函数, 用来度量给定标签 $y_{i}$, 预测 $f\left(\mathbf{x}_{i}\right)$ 的 “糟糕程度”。 统计学家称上式中的这一项 $l\left(f\left(\mathbf{x}_{i}\right), y_{i}\right)$ 为经验风险。 经验风险 (empirical risk) 是为了近似真实风险（true risk）。</p><p>真实风险是整个训练数据上的平均损失, 即从其真实分布 $p(\mathbf{x}, y)$ 中抽取的所有数据的总体损失的期望值：</p><script type="math/tex; mode=display">E_{p(\mathbf{x}, y)}[l(f(\mathbf{x}), y)]=\iint l(f(\mathbf{x}), y) p(\mathbf{x}, y) d \mathbf{x} d y</script><p>然而在实践中通常无法获得总体数据。因此, 经验风险最小化是一种实用的机器学习策略，希望能近似最小化真实风险。</p><h4 id="7-2-2-协变量偏移纠正"><a href="#7-2-2-协变量偏移纠正" class="headerlink" title="7.2.2. 协变量偏移纠正"></a>7.2.2. 协变量偏移纠正</h4><p><a href="https://zh-v2.d2l.ai/chapter_multilayer-perceptrons/environment.html#subsec-covariate-shift-correction">下面三种纠正原理</a>暂时放一下，详细看还是得多读几遍书。<br>给出完整的协变量偏移纠正算法，假设我们有一个训练集 ${(\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_n, y_n)}$ 和一个未标记的测试集 ${\mathbf{u}_1, \ldots, \mathbf{u}_m}$ 对于协变量偏移，我们假设 $1 \leq i \leq n$ 的 $\mathbf{x}_{i}$ 来自某个源分布 $q(\mathbf{x})$, $\mathbf{u}_{i}$ 来自目标分布 $p(\mathbf{x})$。以下是纠正协变量偏移的典型算法：</p><ol><li>生成一个二元分类训练集： ${(\mathbf{x}_1, -1), \ldots, (\mathbf{x}_n, -1), (\mathbf{u}_1, 1), \ldots, (\mathbf{u}_m, 1)}$ 。</li><li>用对数几率回归训练二元分类器得到函数 $h_{\circ}$</li><li>使用 $\beta_{i}=\exp \left(h\left(\mathbf{x}_{i}\right)\right)$ 或更好的 $\beta_{i}=\min \left(\exp \left(h\left(\mathbf{x}_{i}\right)\right), c\right)$ (c为常量）对训练数据进行加权。</li><li>使用权重 $\beta_{i}$ 进行”加权经验风险最小化“中 ${(\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_n, y_n)}$ 的训练。</li></ol><p>上述算法依赖于一个重要的假设：需要目标分布(例如, 测试分布)中的每个数据样本在训练时出现的概率非零。如果找到 $p(\mathbf{x})&gt;0$ 但 $q(\mathbf{x})=0$ 的点，那么相应的重要性权重会是无穷大（因为 $\beta_{i} \stackrel{\operatorname{def}}{=} \frac{p\left(\mathbf{x}_{i}\right)}{q\left(\mathbf{x}_{i}\right)}$）。</p><p>加权经验风险最小化:</p><script type="math/tex; mode=display">\underset{f}{\operatorname{minimize}} \frac{1}{n} \sum_{i=1}^{n} \beta_{i} l\left(f\left(\mathbf{x}_{i}\right), y_{i}\right) .</script><h4 id="7-2-3-标签偏移纠正"><a href="#7-2-3-标签偏移纠正" class="headerlink" title="7.2.3. 标签偏移纠正"></a>7.2.3. 标签偏移纠正</h4><p>标签偏移的一个好处是，如果我们在源分布上有一个相当好的模型，那么我们可以得到对这些权重的一致估计，而不需要处理周边的其他维度。 在深度学习中，输入往往是高维对象（如图像），而标签通常是低维（如类别）。</p><h4 id="7-2-4-概念偏移纠正"><a href="#7-2-4-概念偏移纠正" class="headerlink" title="7.2.4. 概念偏移纠正"></a>7.2.4. 概念偏移纠正</h4><p>概念偏移很难用原则性的方式解决。 例如，在一个问题突然从“区分猫和狗”偏移为“区分白色和黑色动物”的情况下， 除了从零开始收集新标签和训练没有其他的办法。 但在实践中这种极端的偏移是罕见的，通常情况下，概念的变化总是缓慢的。 在这种情况下，我们可以使用与训练网络相同的方法，使其适应数据的变化。 换言之，我们使用新数据更新现有的网络权重，而不是从头开始训练。</p><h3 id="7-3-学习问题的分类方法"><a href="#7-3-学习问题的分类方法" class="headerlink" title="7.3. 学习问题的分类方法"></a>7.3. 学习问题的分类方法</h3><p>机器学习研究的是计算机怎样模拟人类的学习行为，以获取新的知识或技能，并重新组织已有的知识结构使之不断改善自身。<br>机器学习能解决的问题包括以下几种（<a href="https://blog.csdn.net/lovenankai/article/details/99965501">参考</a>）：</p><ol><li>分类问题：根据数据样本上抽取出的特征，判定其属于有限个类别中的哪一个。</li><li>回归问题：根据数据样本上抽取出的特征，预测一个连续值的结果。</li><li>聚类问题：根据数据样本上抽取出的特征，让样本抱抱团(相近/相关的样本在一团内)。</li></ol><p>在机器学习模拟学习的方法上，有许多分类，如批量学习、强化学习等等，目前就是在讨论这方面（这些学习问题）的类别有哪些。</p><h4 id="7-3-1-批量学习（batch-learning）"><a href="#7-3-1-批量学习（batch-learning）" class="headerlink" title="7.3.1. 批量学习（batch learning）"></a>7.3.1. 批量学习（batch learning）</h4><p>在批量学习中, 我们可以访问一组训练特征和标签 ${(\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_n, y_n)}$, 使用这些特性和标签训练 $f(\mathbf{x})$ 。 然后部署此模型来对来自同一分布的新数据 $(\mathbf{x}, y)$ 进行评分。 例如, 我们可以根据猫和狗的大量图片训练一个猫检测器。 一旦我们训练了它, 就把它作为智能猫门计算视觉系统的一部分, 来控制只允许猫进入该门。 然后这个系统会被安装在客户家中, 基本再也不会更新。</p><h4 id="7-3-2-在线学习（online-learning）"><a href="#7-3-2-在线学习（online-learning）" class="headerlink" title="7.3.2. 在线学习（online learning）"></a>7.3.2. 在线学习（online learning）</h4><p>“在线”逐个学习数据$\left(\mathbf{x}_{i}, y_{i}\right)$。我们首先观测到$\mathbf{x}_{i}$，然后得出一个估计值 $f\left(\mathbf{x}_{i}\right)$, 当完成估计后，我们才观测到 $y_{i}$。然后根据模型的决定（估计）, 给予模型奖励或惩罚。<br>例如, 我们预测明天的股票价格来根据这个预测进行交易。在一天结束时, 我们会评估模型的预测是否盈利。<br>在线学习的循环如下，这样看起来就清晰易懂了：</p><script type="math/tex; mode=display">\text { model } f_{t} \longrightarrow \text { data } \mathbf{x}_{t} \longrightarrow \text { estimate } f_{t}\left(\mathbf{x}_{t}\right) \longrightarrow \text { observation } y_{t} \longrightarrow \operatorname{loss} l\left(y_{t}, f_{t}\left(\mathbf{x}_{t}\right)\right) \longrightarrow \operatorname{model} f_{t+1}</script><h4 id="7-3-3-老虎机（bandits）"><a href="#7-3-3-老虎机（bandits）" class="headerlink" title="7.3.3. 老虎机（bandits）"></a>7.3.3. 老虎机（bandits）</h4><p>在一个老虎机问题中，只有有限数量的手臂可以拉动。 也就是说我们可以采取的行动是有限的。 老虎机问题是一个相较于上述问题更简单的情景，可以获得更强的最优性理论保证。 这个问题经常被视为一个单独的学习问题的情景。</p><h4 id="7-3-4-控制"><a href="#7-3-4-控制" class="headerlink" title="7.3.4. 控制"></a>7.3.4. 控制</h4><p>在很多情况下，环境（许多算法形成的环境模型）会记住我们所做的事，虽然不一定是以对抗的方式，并且环境会根据记忆做出相应的反应。 例如，咖啡锅炉控制器将根据之前是否加热锅炉来观测到不同的温度。 在这种情况下，PID（比例—积分—微分）控制器算法是一个流行的选择，温度PID控制器的原理是将温度偏差的比例、积分和微分通过线性组合构成控制量，对控制对象进行控制。</p><blockquote><p>这里还是有点没看明白，英文翻译的不好。 22.06.16</p></blockquote><p>近年来，控制理论（如PID的变体）也被用于自动调整超参数， 以获得更好的解构和重建质量，提高生成文本的多样性和生成图像的重建质量。</p><h4 id="7-3-5-强化学习（reinforcement-learning）"><a href="#7-3-5-强化学习（reinforcement-learning）" class="headerlink" title="7.3.5. 强化学习（reinforcement learning）"></a>7.3.5. 强化学习（reinforcement learning）</h4><p>强化学习强调如何基于环境而行动，以取得最大化的预期利益。 国际象棋、围棋、西洋双陆棋或星际争霸都是强化学习的应用实例，如果在游戏中采取某种策略可以取得较高的得分，那么就进一步「强化」这种策略，以期继续取得较好的结果。</p><h4 id="7-3-6-基于应用环境选择学习方法"><a href="#7-3-6-基于应用环境选择学习方法" class="headerlink" title="7.3.6. 基于应用环境选择学习方法"></a>7.3.6. 基于应用环境选择学习方法</h4><p>上述不同情况之间的一个关键区别是： 在静止环境中可能一直有效的相同策略，在环境能够改变的情况下可能不会始终有效。<br>环境变化的速度和方式在很大程度上决定了我们可以采用的算法类型。 例如，如果我们知道事情只会缓慢地变化，就可以迫使任何估计也只能缓慢地发生改变。 如果我们知道环境可能会瞬间发生变化，但这种变化非常罕见，我们就可以在使用算法时考虑到这一点。<br>当一个数据科学家试图解决的问题会随着时间的推移而发生变化时这些类型的知识至关重要。</p><h3 id="7-4-机器学习中的公平、责任和透明度"><a href="#7-4-机器学习中的公平、责任和透明度" class="headerlink" title="7.4. 机器学习中的公平、责任和透明度"></a>7.4. 机器学习中的公平、责任和透明度</h3><p>当部署机器学习系统时，你不仅仅是在优化一个预测模型，通常是在提供一个会被用来（部分或完全）进行自动化决策的工具。 这些技术系统可能会通过其进行的决定而影响到每个人的生活。<br>从 预测 到 决策 的飞跃不仅提出了新的技术问题， 而且还提出了一系列必须仔细考虑的伦理问题。</p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>Kaggle房价预测比赛的实践，放到下一篇笔记中吧。<br>还遇到了kramed渲染不了公式中大括号的问题，先将就，等我学了JS回来再修复。<br>还发现了链接图片的原理，前面加斜杠是表示在域名下一级。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;p&gt;这一章开始学习真正的深度网络。&lt;br&gt;最简单的深度网络称为多层感知机。多层感知机由多层神经元组成， 每一层与它的上一层相连，从中接收输入； 同时每一层也与它的下一层相连，影响当前层的神经元。&lt;br&gt;这一章从基本的概念介绍开始讲起，包括过拟合、欠拟合和模型选择。 为了解决这些问题，本章将介绍权重衰减和暂退法等正则化技术，以及将讨论数值稳定性和参数初始化相关的问题。最后应用一个真实的案例：房价预测。&lt;/p&gt;
&lt;p&gt;对应实践：&lt;a href=&quot;https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x04.ipynb&quot;&gt;https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x04.ipynb&lt;/a&gt;&lt;br&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://silencezheng.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>回调函数</title>
    <link href="http://silencezheng.top/2022/06/07/article43/"/>
    <id>http://silencezheng.top/2022/06/07/article43/</id>
    <published>2022-06-07T08:20:56.000Z</published>
    <updated>2022-06-07T08:43:00.930Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>回调函数（callback）是一个常用的概念，最近在写代码的过程中遇到了相关问题，写个文梳理一下。以下都是个人的浅显理解，如有错误请指正。<br><span id="more"></span></p><h2 id="什么是回调函数？"><a href="#什么是回调函数？" class="headerlink" title="什么是回调函数？"></a>什么是回调函数？</h2><p>回调函数，本身实际上就是普通的功能函数，当功能函数以参数的方式被传入到其他函数中时，它便成为了回调函数。</p><p>下面我们将回调过程中的函数分为两部分，调用者称为调用函数，被调用者称为回调函数。</p><h2 id="为什么要使用回调函数？"><a href="#为什么要使用回调函数？" class="headerlink" title="为什么要使用回调函数？"></a>为什么要使用回调函数？</h2><p>回调函数首先的作用就是解耦，调用函数不用关心回调函数的具体实现，只是按照其格式拿来即用就可以了。<br>回调函数的一个直观作用就是可以将事件与函数绑定，当事件发生时触发回调函数。（也是目前我遇到的场景）<br>关于其他的作用，我认为<a href="https://blog.csdn.net/qiuhuanghe/article/details/109245579">这篇文章</a>讲的还不错，可以看一下学习学习。<br>关于同步回调和异步回调，<a href="https://cloud.tencent.com/developer/article/1373683">这篇文章</a>可以看看。</p><h2 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h2><p>应用场景是这样的：在编写一个游戏时，需要通过鼠标点击组件来使用人物背包中的物品。</p><p>其中背包组件类的实现大致如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">InventoryView</span>(<span class="params">tk.Frame</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, master: <span class="type">Union</span>[tk.Tk, tk.Frame], **kwargs</span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(master)</span><br><span class="line">        self.callback = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_click_callback</span>(<span class="params">self, callback: <span class="type">Callable</span>[[<span class="built_in">str</span>], <span class="literal">None</span>]</span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Sets the function to be called when an item is clicked.</span></span><br><span class="line"><span class="string">        The provided callback function should take one argument: the string name of the item.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.callback = callback</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_draw_item</span>(<span class="params">self, name: <span class="built_in">str</span>, num: <span class="built_in">int</span>, colour: <span class="built_in">str</span></span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Creates and binds (if a callback exists) a single tk.Label in the InventoryView frame.</span></span><br><span class="line"><span class="string">        name is the name of the item, num is the quantity currently in the users inventory,</span></span><br><span class="line"><span class="string">        and colour is the background colour for this item label (determined by the type of item).</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        label = tk.Label(self, text=<span class="string">f&quot;<span class="subst">&#123;name&#125;</span>: <span class="subst">&#123;<span class="built_in">str</span>(num)&#125;</span>&quot;</span>, background=colour)</span><br><span class="line">        label.bind(<span class="string">&quot;&lt;Button-1&gt;&quot;</span>, self.callback)</span><br><span class="line">        label.pack(side=<span class="string">&#x27;top&#x27;</span>, fill=tk.X)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">draw_inventory</span>(<span class="params">self, inventory: Inventory</span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        ...</span><br><span class="line"></span><br></pre></td></tr></table></figure><br>类中的callback为回调函数（这里也就是”物品使用函数“），通过与label组件上的鼠标左键点击事件绑定，实现点击调用“物品使用函数”。 可以看出，这里背包并不需要关心物品是如何被使用的，他只是接收一个回调函数，并将这个函数与背包中的每一个物品（label）绑定。</p><p>但这里其实是简化过的版本，真正在使用时，由于“物品使用函数”需要通过接收一个str类型的参数来分辨被使用的物品是何类型，所以不能直接绑定（tkinter的bind方法会将事件作为参数传入到绑定的回调函数中，这个参数表与我们规定的“物品使用函数”参数表不同）。</p><p>所以对于想要使用Event的同时绑定带参函数的情况，需要中间函数来匹配两边的规则（“物品使用函数”和bind函数）。如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_draw_item</span>(<span class="params">self, name: <span class="built_in">str</span>, num: <span class="built_in">int</span>, colour: <span class="built_in">str</span></span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Creates and binds (if a callback exists) a single tk.Label in the InventoryView frame.</span></span><br><span class="line"><span class="string">    name is the name of the item, num is the quantity currently in the users inventory,</span></span><br><span class="line"><span class="string">    and colour is the background colour for this item label (determined by the type of item).</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">handlerAdaptor</span>(<span class="params">function, **kwds</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">lambda</span> event, fun = function, kwds = kwds: fun(event, **kwds)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">handler</span>(<span class="params">event, item_name</span>):</span></span><br><span class="line">        self.callback(item_name)</span><br><span class="line"></span><br><span class="line">    label = tk.Label(self, text=<span class="string">f&quot;<span class="subst">&#123;name&#125;</span>: <span class="subst">&#123;<span class="built_in">str</span>(num)&#125;</span>&quot;</span>, background=colour)</span><br><span class="line">    label.bind(<span class="string">&quot;&lt;Button-1&gt;&quot;</span>, handlerAdaptor(handler, item_name=name))</span><br><span class="line">    label.pack(side=<span class="string">&#x27;top&#x27;</span>, fill=tk.X)</span><br></pre></td></tr></table></figure><br>这里我们在函数内部又定义了两个中间函数用来实现目的，handler增加了event参数来接收bind函数交给回调函数的Event，但此时不能直接绑定，因为绑定时无处获取Event作为实参传入。</p><p>所以又定义了handlerAdaptor来实现一个不含event在参数表中的函数，使用lambda表达式来解决这一问题，虽然参数表中不包含event，但在实际调用的过程中，bind函数依然会传入event参数（我猜想bind传入的参数的名字恰好就是event，所以这个匿名函数才能成立），使得lambda表达式的参数表匹配，返回调用handler的结果。</p><p>写两个中间函数的好处是（其实也比较牵强）：handler函数可以规范一下回调函数的参数表命名，我们可以不去管self.callback中的参数实际名字是如何，直接用item_name这一参数名去传入即可。</p><p>但实际上，如果self.callback的参数表是已知的，我们只需要一个中间函数即可实现“使用Event的同时绑定带参函数”，如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_draw_item</span>(<span class="params">self, name: <span class="built_in">str</span>, num: <span class="built_in">int</span>, colour: <span class="built_in">str</span></span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">middle</span>(<span class="params">function, **kwds</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">lambda</span> event, fun = function, kwds = kwds: fun(**kwds)</span><br><span class="line"></span><br><span class="line">    label = tk.Label(self, text=<span class="string">f&quot;<span class="subst">&#123;name&#125;</span>: <span class="subst">&#123;<span class="built_in">str</span>(num)&#125;</span>&quot;</span>, background=colour)</span><br><span class="line">    <span class="comment"># 这里已知回调函数中的参数名为item_name</span></span><br><span class="line">    label.bind(<span class="string">&quot;&lt;Button-1&gt;&quot;</span>, middle(self.callback, item_name=name))</span><br><span class="line">    label.pack(side=<span class="string">&#x27;top&#x27;</span>, fill=tk.X)</span><br></pre></td></tr></table></figure><br>总的来说，使用lambda表达式就可以解决这个问题～<br>参考：<a href="https://blog.csdn.net/tinym87/article/details/6957438">https://blog.csdn.net/tinym87/article/details/6957438</a></p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>理解的还是很基础，只是知道个大概，以后用到了再总结补充吧。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;回调函数（callback）是一个常用的概念，最近在写代码的过程中遇到了相关问题，写个文梳理一下。以下都是个人的浅显理解，如有错误请指正。&lt;br&gt;</summary>
    
    
    
    
    <category term="Python" scheme="http://silencezheng.top/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch适配M1芯片测试</title>
    <link href="http://silencezheng.top/2022/06/01/article42/"/>
    <id>http://silencezheng.top/2022/06/01/article42/</id>
    <published>2022-05-31T17:03:49.000Z</published>
    <updated>2022-05-31T17:08:06.334Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近用mac学习深度学习时越来越感觉硬件的重要…随便跑个小demo都温度90+，听闻最近Pytorch适配了M1芯片，正好试试能否有所提升！<br><span id="more"></span></p><h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><p>miniforge是要安的，<code>bash Miniforge3-MacOSX-arm64.sh</code>。</p><p>然后我们上Pytorch官网看一下情况，可以看到有稳定版和预览版两种，其中预览版是支持GPU加速的。<br>稳定版：<br><img src="/assets/post_img/article42/stable.jpg" alt="stable"><br>预览版：<br><img src="/assets/post_img/article42/preview.png" alt="preview"><br>这里提到的MPS加速是指用苹果的Metal Performance Shaders (MPS)作为Pytorch后端启用GPU加速训练。</p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>我的conda环境之前安装过稳定版：<br><code>conda install pytorch torchvision -c pytorch</code></p><p>这里的 -c 参数是指定channel，即软件下载的渠道。所以conda本身是没有这个包的，要从pytorch的channel下载。</p><p>这时在终端中进入安装稳定版的环境，启用python，输入<code>torch.__version__</code>可以查看到当前的版本是1.8.0（这里挺奇怪的，官网说MacOS不支持1.8的…）:<br><img src="/assets/post_img/article42/原版本.jpg" alt="1.8.0"></p><p>下面我们安装预览版试一下：<br><code>conda install pytorch torchvision -c pytorch-nightly</code></p><p>可以看到下载的已经是1.13版本了：<br><img src="/assets/post_img/article42/1.13版本.jpg" alt="下载提示"></p><p><img src="/assets/post_img/article42/环境中加载成功.jpg" alt="完成"></p><p>安完之后有个无语的问题，<code>conda help</code>不能用了，可能是安装了4.13版本的问题..用-h就好了。</p><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>有一个现成的<a href="https://github.com/rasbt/machine-learning-notes/tree/main/benchmark/pytorch-m1-gpu">测试</a></p><p>但是发现mps和cpu没有区别呀，如图：<br><img src="/assets/post_img/article42/test1.jpg" alt="benchmark1"></p><p>倒是发现了一个BUG，标准化地方要改成如下，否则报错：<br><code>transforms.Normalize((0.5,),(0.5,)),</code></p><p>然后又用我自己的代码小测一下，发现好像…还是没区别：<br><img src="/assets/post_img/article42/test2.jpg" alt="benchmark2"></p><p>结论：Mac目前的GPU加速就是个笑话😅</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;最近用mac学习深度学习时越来越感觉硬件的重要…随便跑个小demo都温度90+，听闻最近Pytorch适配了M1芯片，正好试试能否有所提升！&lt;br&gt;</summary>
    
    
    
    
    <category term="MacOS" scheme="http://silencezheng.top/tags/MacOS/"/>
    
  </entry>
  
  <entry>
    <title>TLS过程简单了解</title>
    <link href="http://silencezheng.top/2022/05/26/article41/"/>
    <id>http://silencezheng.top/2022/05/26/article41/</id>
    <published>2022-05-26T15:12:50.000Z</published>
    <updated>2022-05-26T15:15:03.601Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><p>答辩前夕，补习一下计算机网络知识，因为在部署ELK的过程中频繁的使用到了私钥、证书、签名等等，有必要梳理一下TLS的相关知识。本文参考了谢希仁第八版计算机网络教材，只做简单理解，深入学习还是要仔细啃书。<br>场景有如下几个：</p><ol><li>Elasticsearch节点间通信加密</li><li>Elasticsearch的HTTP通信加密</li><li>Kibana与浏览器间HTTP通信加密<span id="more"></span></li></ol><h2 id="1-什么是TLS？"><a href="#1-什么是TLS？" class="headerlink" title="1. 什么是TLS？"></a>1. 什么是TLS？</h2><p>TLS即Transport Layer Security，传输层安全协议。学过计算机网络的同学都知道，我们常用的网络模型有五层，自底向上依次为物理层、数据链路层、网络层、传输层、应用层。 而TLS事实上是属于传输层与应用层之间的一个安全协议。</p><p>TLS由SSL发展而来，但并不兼容，其优势在于与高层的应用层协议（如HTTP、FTP等）无耦合，应用层协议能透明地运行在TLS协议之上，由TLS协议进行 创建加密通道所需的 协商和认证。应用层协议传送的数据在通过TLS协议时都会被加密，从而保证通信的私密性</p><p>TLS是针对TCP进行加密的，所以任何在TCP基础上运行的应用程序程序都可以使用TLS进行加密。最常用的就是HTTP协议，HTTP在不使用传输层安全协议的时候就直接使用TCP连接，而使用了传输层安全协议后，链接前方就会显示<code>https://</code>的字样。</p><p>目前TLS的最新版本为TLS1.3，在2020年1.0和1.1均已被废弃掉了，现在能够使用的传输层安全协议只有TLS1.2和TLS1.3。</p><h2 id="2-那么TLS是如何工作的"><a href="#2-那么TLS是如何工作的" class="headerlink" title="2. 那么TLS是如何工作的"></a>2. 那么TLS是如何工作的</h2><p>谈论这个之前，我们需要了解一些基本的概念。</p><h3 id="2-1-鉴别、密钥、密码体制"><a href="#2-1-鉴别、密钥、密码体制" class="headerlink" title="2.1. 鉴别、密钥、密码体制"></a>2.1. 鉴别、密钥、密码体制</h3><p>对于安全的计算机网络，<strong>端点鉴别</strong>是一个必不可少的特性，即网络能够鉴别信息的发送方和接收方的真实身份。TLS具有双向鉴别的功能，即客户端可以鉴别服务器，服务器也可以鉴别客户端，但大多数情况下只使用前者（在HTTP中体现为浏览器鉴别服务器）。</p><p><strong>密钥</strong>是数据加密模型中的重要组成部分，通常密钥的形式为一个字符串（比特串），一般来说，一段信息（明文）需要通过加密算法和加密密钥进行加密操作，变成一段密文，接受方通过解密算法和解密密钥求解出原本的信息。</p><p>在这个过程中，产生了两种密码体制，即<strong>对称密码体制</strong>和<strong>公钥密码体制</strong>，这个概念很好理解，对称密码体制中加密密钥==解密密钥，而公钥密码体制中两者不同。本文主要研究公钥密码体制。</p><h3 id="2-2-公钥密码体制"><a href="#2-2-公钥密码体制" class="headerlink" title="2.2. 公钥密码体制"></a>2.2. 公钥密码体制</h3><p>公钥密码体制源于两个需求，一是对称密码体制中密钥分配不便，二是应用中对电子信息的 <strong>数字签名</strong> 需求。数字签名可以绑定信息与某个特定的人。</p><p>公钥密码体制中，存在两种密钥，即<strong>公钥（Public Key）</strong>和<strong>私钥（Secret Key）</strong>。其中公钥为加密密钥，对大众公开；而私钥则为解密密钥，需要保密，公钥与私钥是成对出现的。另外，加密算法与解密算法也都是公开的。</p><p>这种密码体制的通信可以是多对一的<strong>单向保密通信</strong>，即不同人持有公钥对信息加密后发送给同一接收方，但只有持有对应私钥的接收方才能获取其中的信息。</p><h3 id="2-3-用数字签名进行鉴别"><a href="#2-3-用数字签名进行鉴别" class="headerlink" title="2.3. 用数字签名进行鉴别"></a>2.3. 用数字签名进行鉴别</h3><p>数字签名的原理可以用公钥密码体制原理的逆向过程来思考，但该方式并不准确，如下：<br>当某人需要发送进行数字签名的信息时，使用自己的私钥对该信息进行了D运算，得到了一个不可读的密文，并将密文发送给接收方。接收方在收到密文后，使用对应的公钥进行E运算（核实签名），解出了原本的信息，同时证明了该信息来自特定的发送方，因为私钥只有该发送方持有。<br>注意在这个过程中，任何人都可以通过公钥解密出此人的信息，因此数字签名只对信息进行了签名，并没有加密。实现数字签名安全性的关键在于确定没有其他人能够持有发送方的私钥。</p><h3 id="2-4-那公钥是如何分配的？什么是证书签名？"><a href="#2-4-那公钥是如何分配的？什么是证书签名？" class="headerlink" title="2.4. 那公钥是如何分配的？什么是证书签名？"></a>2.4. 那公钥是如何分配的？什么是证书签名？</h3><p>说了这么多，密钥如何分配的问题还没有解决，这在实现公钥密码体制中十分重要，有必要详细的说一说。</p><p>公钥的分配过程中，最最关键的名词就是<strong>CA</strong>和<strong>公钥证书</strong>了。首先，当前流行的办法是找一个通信双方都信任的第三方机构来给拥有公钥的实体颁发一个具有数字签名的数字证书，该证书是公钥与其对应实体进行绑定的证明。这个颁发公钥证书的机构即CA（Certification Authority），通常由政府或知名公司出资建立。</p><p>每个公钥证书中都写有公钥及其拥有者的标识信息，同时包含颁发的CA自己的数字签名。但这个最终的公钥证书在形成前有一个被签名的过程，即公钥的拥有方首先将公钥与自己进行绑定，形成一个未认证的证书，CA将该证书与自己签名放在一起（中间进行一些运算）最终形成公钥证书。这个过程即为证书“签名”的过程。</p><p>当然，任何东西想要扩散起来，离不开标准化，数字证书的格式也是如此。ITU-T制定了<strong>X.509</strong>协议标准，现在的版本是 X.509V3，该标准又称为互联网<strong>公钥基础结构PKI</strong>。具体来说，该标准提出了把多级认证中心连接起来构成树状的认证系统，最高一级的认证中心为<strong>Root CA</strong>，根CA向下的所有链接称为<strong>信任链</strong>，表示这些CA都是可信的。</p><p>现代的操作系统厂商已经把许多主流CA的根证书内置到了操作系统中，这些根证书有CA们的公钥和并且使用CA的私钥进行了自签名。用户只要打开浏览器就可以查到这些根证书，并利用公钥对各网站服务器的证书进行验证。</p><h3 id="2-5-行了，TLS是啥来着？"><a href="#2-5-行了，TLS是啥来着？" class="headerlink" title="2.5. 行了，TLS是啥来着？"></a>2.5. 行了，TLS是啥来着？</h3><p>扯了一圈别的概念，这一节主要还是为了解释TLS是如何工作的…下面来看。<br>前面说到了，TLS的单向鉴别是客户端（浏览器）鉴别服务器，也就是浏览器A要确定服务器B是可信的，这就需要两个前提，一是服务器B需要有一个证书证明自己，二是浏览器A能有办法通过证书证明B是安全的。下面我们假设B已经拥有了由可靠CA颁发的证书。</p><p>首先，执行TLS的前提是TCP连接建立完成，我们假设A和B的连接已经建立。<br>TLS主要分为两个阶段，<strong>握手阶段</strong>和<strong>会话阶段</strong>。在不同的阶段，TLS使用不同的协议：握手协议和记录协议。本文中主要关注握手阶段，会话阶段读者可以自行查阅。</p><p>协商加密算法：<br>1、A向B发送自己选定的加密算法以及密钥交换算法<br>2、B从中确认自己支持的<strong>算法C</strong>，并把证书发送给A<br>服务器鉴别：<br>3、A用B证书中CA的对应公钥（这里的公钥是A本身带有的，不是B证书中包含的）对证书进行鉴别<br>生成主密钥：<br>4、A按照C生成主密钥<br>5、A用B的公钥对主密钥加密并发送给B<br>6、B用私钥解密出相同的主密钥</p><p>上述步骤完成后，A和B拥有了相同的主密钥，可以进行数据会话。但这样会有些不安全，故A和B会各自将主密钥分割成4个不同的密钥：<br>7、A生成对话密钥<br>8、B生成对话密钥</p><p>这样以后，双方各自持有4个密钥。注意这4个密钥也是相同的，分别为A发送数据使用的会话密钥、A发送数据使用的MAC密钥、B发送数据使用的会话密钥和B的MAC密钥。会话密钥也采用对称的形式是因为对称密钥的运行速度快得多。</p><h2 id="3-验证"><a href="#3-验证" class="headerlink" title="3. 验证"></a>3. 验证</h2><p>回到开头所提到的三个场景，都是我在部署ELK过程中使用加密的案例，我们来一一验证，看他们是否是TLS。首先要明确Elasticsearch节点间通信与HTTP通信使用不同的端口，故事实上属于应用层的两个应用。</p><h3 id="3-1-Elasticsearch节点间通信加密"><a href="#3-1-Elasticsearch节点间通信加密" class="headerlink" title="3.1. Elasticsearch节点间通信加密"></a>3.1. Elasticsearch节点间通信加密</h3><p>在这一过程中，大致步骤如下：<br>1、使用工具生成”elastic-stack-ca.p12”，这个文件包含生成CA的公共证书和它为每个节点签名的私钥<br>2、使用刚才生成的CA为集群上的节点们生成一个证书和一把私钥，得到“elastic-certificates.p12”，这个文件包含了节点证书、节点私钥和CA证书。然后在所有节点上放入“elastic-certificates.p12”。</p><p>让我们把这个过程与TLS对应起来，第一步事实上是主节点形成CA的过程，第二步使用CA颁发了证书，该证书存在于所有非主节点上。这样以来，主节点相当于浏览器A，而非主节点相当于服务器B，在通信时，主节点会验证其余节点的证书是否是由自己的CA签名的，这样一来就完成了鉴别，所以是TLS。</p><h3 id="3-2-Elasticsearch的HTTP通信加密（对Kibana和Logstash）"><a href="#3-2-Elasticsearch的HTTP通信加密（对Kibana和Logstash）" class="headerlink" title="3.2. Elasticsearch的HTTP通信加密（对Kibana和Logstash）"></a>3.2. Elasticsearch的HTTP通信加密（对Kibana和Logstash）</h3><p>大致流程如下：<br>1、运行命令利用”elastic-stack-ca.p12”生成一个zip压缩文件，其中包含用于 Elasticsearch 和 Kibana 的证书和私钥。“http.p12”文件为Elasticsearch使用的证书及密钥，“elasticsearch-ca.pem”为Kibana使用的。<br>2、将“http.p12”部署到所有ELasticsearch节点中。<br>3、将“elasticsearch-ca.pem”部署到Kibana中，并修改指向链接为<code>https://</code>格式。<br>4、使用工具利用”elastic-stack-ca.p12”为Logstash节点生成证书，部署到Logstash节点中。</p><p>不难看出，这部分使用的是TLS加密HTTP，从<code>https://</code>就可以看出了，依然是利用第一步产生的CA为各E、L、K节点签名证书，发放私钥。</p><h3 id="3-3-Kibana与浏览器间HTTP通信加密"><a href="#3-3-Kibana与浏览器间HTTP通信加密" class="headerlink" title="3.3. Kibana与浏览器间HTTP通信加密"></a>3.3. Kibana与浏览器间HTTP通信加密</h3><p>不用说就知道了，HTTP加密！标准的TLS！但这里存在另一个有趣的问题，Elasticsearch产生的CA固然可以为其他需要与之通信的程序们签名证书，但浏览器并不认识他，所以，该找谁签名呢？</p><p>先说大致过程：<br>1、使用工具为Kibana创建服务器证书和私钥，产生了两个文件：“kibana-server.csr”和私钥“kibana-server.key”，前者的后缀即证书签名请求，可以知道该文件需要CA签名后才能够使用。<br>2、使用OpenSSL为证书签名，得到签名后的公钥证书。部署公钥证书和私钥到Kibana。</p><p>可以看出，在这一部分中，Kibana作为服务器B的角色出现，他需要寻找一个浏览器A可信任的CA为其签名后才能够与浏览器A（其实是用户们）进行安全通信，这里我因为方便使用到了OpenSSL，但其实这个签名在浏览器眼中是不安全的。</p><p>OpenSSL是一个强大的安全套接字层密码库，Apache使用它加密HTTPS，OpenSSH使用它加密SSH，但它同时还是一个多用途的、跨平台的密码工具。</p><h2 id="4-结语"><a href="#4-结语" class="headerlink" title="4. 结语"></a>4. 结语</h2><p>从头梳理了一下TLS的流程，感觉理解加深了许多，文章写了有两个多小时了吧，希望对想了解TLS的人有一些帮助，文中部分参考了百度百科，嗯就这样。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;p&gt;答辩前夕，补习一下计算机网络知识，因为在部署ELK的过程中频繁的使用到了私钥、证书、签名等等，有必要梳理一下TLS的相关知识。本文参考了谢希仁第八版计算机网络教材，只做简单理解，深入学习还是要仔细啃书。&lt;br&gt;场景有如下几个：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Elasticsearch节点间通信加密&lt;/li&gt;
&lt;li&gt;Elasticsearch的HTTP通信加密&lt;/li&gt;
&lt;li&gt;Kibana与浏览器间HTTP通信加密</summary>
    
    
    
    
    <category term="计算机网络" scheme="http://silencezheng.top/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>JavaFX环境配置--MacOS</title>
    <link href="http://silencezheng.top/2022/05/26/article40/"/>
    <id>http://silencezheng.top/2022/05/26/article40/</id>
    <published>2022-05-26T05:47:36.000Z</published>
    <updated>2022-05-26T05:52:47.996Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>换了MacBook后想打开一下以前写的JavaFX程序，发现我的jdk1.8没有自带这个SDK（据说有打包的jdk），所以需要重新下载并在IDEA里配置，记录一下。<br><span id="more"></span></p><h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><p>这里有ARM架构的SDK（Software Development Kit）<br><a href="https://gluonhq.com/products/javafx/">https://gluonhq.com/products/javafx/</a><br>我下载了18版本。</p><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p>到IDEA项目结构-&gt;Library中添加解压出来的sdk中的lib文件夹。</p><p>然后到运行配置里添加VM Options：<br><code>--module-path &quot;/xxx/javafx-sdk-18.0.1/lib&quot; --add-modules javafx.controls,javafx.fxml</code></p><p>理论上就可以运行了，但是因为我的Java版本是8，不知道是否因为版本不适配，会报错版本问题。所以我又配置了OpenJDK17（已经适配Arm平台了）。然后就可以运行了～</p><p><a href="https://www.oracle.com/java/technologies/downloads/#java17">OpenJDK17</a></p><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>但是我运行成功后，原本程序的功能还是实现不了，Debug了一下发现定位在<code>System.getenv(&quot;COMPUTERNAME&quot;)</code>，原本的程序是在Windows平台写的，是不是MacOS没有这个环境变量呢？</p><p>在Terminal中输入<code>export</code>查看所有的环境变量，发现似乎应该在MacOS中可以使用<strong>HOME</strong>，为当前用户的根目录。修改后解决～</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;换了MacBook后想打开一下以前写的JavaFX程序，发现我的jdk1.8没有自带这个SDK（据说有打包的jdk），所以需要重新下载并在IDEA里配置，记录一下。&lt;br&gt;</summary>
    
    
    
    
    <category term="Java" scheme="http://silencezheng.top/tags/Java/"/>
    
    <category term="MacOS" scheme="http://silencezheng.top/tags/MacOS/"/>
    
  </entry>
  
  <entry>
    <title>Markdown博客编写工具选择</title>
    <link href="http://silencezheng.top/2022/05/22/article39/"/>
    <id>http://silencezheng.top/2022/05/22/article39/</id>
    <published>2022-05-22T13:34:31.000Z</published>
    <updated>2022-06-19T11:24:25.641Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>关于我编写Markdown格式博客的工具选择。先说设备，我现在使用的是M1的MacBook Air。<br>最初使用的是Sublime Text，一个很经典的开源文本编辑器，缺点就是如果不订阅的话保存几次就会弹出广告，故舍弃。<br>然后使用的就是Effie，这是一个国产的笔记软件，十分的简洁，用于编写普通的文本和代码是很好的，直到我需要在笔记中加入公式…遂也舍弃了。<br>然后就是VSCode了，只需要使用几个简单的插件就可以满足我当前的博客编辑需求，公式、无限插图、多层文件夹（当然了）等等，给大家简单分享一下。<br><span id="more"></span></p><h2 id="工具及插件"><a href="#工具及插件" class="headerlink" title="工具及插件"></a>工具及插件</h2><p>工具：Visual Studio Code（神器！）<br>插件：</p><ol><li>yzhang.markdown-all-in-one</li><li>shd101wyy.markdown-preview-enhanced</li></ol><h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p>我的博客是用Hexo搭建的，解释器换用了包含LaTex的公式的Kramed。<br>首先展示一下编辑时的效果：<br><img src="/assets/post_img/article39/总览.jpg" alt="all"></p><p>左侧就是工作区了，这里是Markdown的真实样貌，右边就是预览插件的效果，由于使用了插件2所以可以使用目录进行快速跳转，对于写长文的帮助很大（Effie缺少这个功能）。如图：<br><img src="/assets/post_img/article39/toc.jpg" alt="toc"></p><p>那么插件1能为我们提供什么帮助呢？对我来说，是一个Effie的”低配版“。<br>最常用的应该就是<strong>粗体</strong>、<em>斜体</em>的快捷键，分别是<code>cmd+b</code>和<code>cmd+i</code>，与Effie相同。</p><p>但是对于代码行和代码块似乎没有对应的快捷键，这一点对我来说是不方便的，以后可能会设置一下。</p><p>数学公式方面，插件提供<code>cmd+m</code>可以直接打两个dollar号出来，比较方便。</p><p>预览方面，通常在点开一个Markdown文件时会自动在右侧开启预览，但如果不小心关闭掉了，可以通过<code>cmd+k + v</code>开启，注意这里要先输入<code>cmd+k</code>～</p><p>再下面就是一些高级的功能了，比如：自动生成目录。</p><ul><li><a href="#前言">前言</a></li><li><a href="#工具及插件">工具及插件</a></li><li><a href="#使用">使用</a></li></ul><p>这个似乎没有默认的快捷键，需要使用<code>shift+cmd+p</code>调出命令面板，然后搜索目录就可以找到了，对了，我的VSCode安装了中文插件，所以大多事情可以用中文完成，我认为VSCode的中文插件比其他我用过的IDE都要好，至少用起来不别扭。</p><p>关于插件1就说这么多，再说说插件2，预览时默认使用的是白色背景，对于我这种夜猫子，简直太不友好了。所以需要更换一个深色的。具体就是在插件的设置面板更换Preview Theme为一个你喜欢的主题，我使用的是atom dark。</p><p>以上就是关于我如何编辑Markdown格式博客的全部内容了，如果以后有其他更有用的插件，再继续分享～。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;关于我编写Markdown格式博客的工具选择。先说设备，我现在使用的是M1的MacBook Air。&lt;br&gt;最初使用的是Sublime Text，一个很经典的开源文本编辑器，缺点就是如果不订阅的话保存几次就会弹出广告，故舍弃。&lt;br&gt;然后使用的就是Effie，这是一个国产的笔记软件，十分的简洁，用于编写普通的文本和代码是很好的，直到我需要在笔记中加入公式…遂也舍弃了。&lt;br&gt;然后就是VSCode了，只需要使用几个简单的插件就可以满足我当前的博客编辑需求，公式、无限插图、多层文件夹（当然了）等等，给大家简单分享一下。&lt;br&gt;</summary>
    
    
    
    
    <category term="VSCode" scheme="http://silencezheng.top/tags/VSCode/"/>
    
  </entry>
  
  <entry>
    <title>Hexo博客更换Markdown解释器</title>
    <link href="http://silencezheng.top/2022/05/17/article38/"/>
    <id>http://silencezheng.top/2022/05/17/article38/</id>
    <published>2022-05-17T05:04:36.000Z</published>
    <updated>2022-05-17T05:11:56.050Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>最近在学习深度学习，总是需要在博客中添加一些公式，就选择使用LaTex来添加，结果发现使用hexo默认的hexo-renderer-marked解释器问题很多，还没有办法修改（网上没找到），索性更换一个解释器，记录一下。<br><span id="more"></span></p><h1 id="选择解释器"><a href="#选择解释器" class="headerlink" title="选择解释器"></a>选择解释器</h1><p>选择 hexo-renderer-kramed，原因无他，只是找到了一篇通过修改 hexo-renderer-kramed使hexo对公式的支持达到不错效果的文章，照葫芦画瓢抄起来～<br>参考：<a href="https://blog.csdn.net/weixin_44441126/article/details/119745642">https://blog.csdn.net/weixin_44441126/article/details/119745642</a></p><h1 id="卸载、安装"><a href="#卸载、安装" class="headerlink" title="卸载、安装"></a>卸载、安装</h1><p><code>npm uninstall hexo-renderer-marked --save</code> 卸载默认解释器<br><code>npm install hexo-renderer-kramed --save</code> 安装新的～</p><h1 id="解决问题"><a href="#解决问题" class="headerlink" title="解决问题"></a>解决问题</h1><h2 id="问题1：下划线-被转义为斜体而非LaTeX下标"><a href="#问题1：下划线-被转义为斜体而非LaTeX下标" class="headerlink" title="问题1：下划线_被转义为斜体而非LaTeX下标"></a>问题1：下划线_被转义为斜体而非LaTeX下标</h2><p>如图：<br><img src="/assets/post_img/article38/下划线错误.jpg" alt=""><br>Markdown本身的语法是支持*和_都被转义为斜体的，所以我们需要取消掉kramed对_的转义。</p><p>打开本地hexo文件夹下的/node_modules/kramed/lib/rules/inline.js，找到第20行如下代码：<br><code>em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</code></p><p>把正则中对下划线匹配的部分去掉，修改后如下：<br><code>em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</code></p><p>这样修改以后斜体就只能用*了，不过足够的～</p><h2 id="问题2-反斜杠-被转义为-而非LaTeX换行"><a href="#问题2-反斜杠-被转义为-而非LaTeX换行" class="headerlink" title="问题2:反斜杠\\被转义为\而非LaTeX换行"></a>问题2:反斜杠\\被转义为\而非LaTeX换行</h2><p>如图：<br><img src="/assets/post_img/article38/双反斜杠换行错误.jpg" alt=""></p><p>当公式中出现\\表示换行时，会被kramed渲染为\，导致公式显示异常。<br>找到inline.js中第11行如下代码：<br><figure class="highlight taggerscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">escape: /^<span class="symbol">\\</span>([<span class="symbol">\\</span>`*<span class="symbol">\[</span><span class="symbol">\]</span>()#$+<span class="symbol">\-</span>.!_&gt;])/,</span><br></pre></td></tr></table></figure><br>修改如下：<br><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">escape: /^\\([`*\[\]()#$+\-.!_&gt;])/,`</span><br></pre></td></tr></table></figure></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;最近在学习深度学习，总是需要在博客中添加一些公式，就选择使用LaTex来添加，结果发现使用hexo默认的hexo-renderer-marked解释器问题很多，还没有办法修改（网上没找到），索性更换一个解释器，记录一下。&lt;br&gt;</summary>
    
    
    
    
    <category term="Hexo" scheme="http://silencezheng.top/tags/Hexo/"/>
    
  </entry>
  
  <entry>
    <title>线性神经网络--《动手学深度学习》笔记0x03</title>
    <link href="http://silencezheng.top/2022/05/16/article37/"/>
    <id>http://silencezheng.top/2022/05/16/article37/</id>
    <published>2022-05-16T15:55:05.000Z</published>
    <updated>2022-05-27T14:14:41.763Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><p>从经典算法————<em>线性</em>神经网络开始，介绍神经网络的基础知识。 经典统计学习技术中的线性回归和softmax回归可以视为线性神经网络。</p><p>对应实践：<a href="https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x03.ipynb">https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x03.ipynb</a></p><span id="more"></span><h3 id="0-1-各章总结"><a href="#0-1-各章总结" class="headerlink" title="0.1. 各章总结"></a>0.1. 各章总结</h3><ul><li>机器学习模型中的关键要素是训练数据、损失函数、优化算法，还有模型本身。</li><li>矢量化使数学表达上更简洁，同时运行的更快。</li><li>最小化目标函数和执行极大似然估计等价。</li><li>线性回归模型也是一个简单的神经网络。</li><li>我们可以使用PyTorch的高级API更简洁地实现模型。在PyTorch中，data模块提供了数据处理工具，nn模块定义了大量的神经网络层和常见损失函数。可以通过<code>_</code>结尾的方法将参数替换，从而初始化参数。</li><li>softmax运算获取一个向量并将其映射为概率。</li><li>softmax回归适用于分类问题，它使用了softmax运算中输出类别的概率分布。</li><li>交叉熵是一个衡量两个概率分布之间差异的很好的度量，它测量给定模型编码数据所需的比特数。</li><li>数据迭代器是获得更高性能的关键组件。依靠实现良好的数据迭代器，利用高性能计算来避免减慢训练过程。</li><li>借助softmax回归，我们可以训练多分类的模型。</li><li>训练softmax回归循环模型与训练线性回归模型非常相似：先读取数据，再定义模型和损失函数，然后使用优化算法训练模型。大多数常见的深度学习模型都有类似的训练过程。</li><li>使用深度学习框架的高级API，我们可以更简洁地实现softmax回归。</li><li>从计算的角度来看，实现softmax回归比较复杂。在许多情况下，深度学习框架在这些著名的技巧之外采取了额外的预防措施，来确保数值的稳定性。这使我们避免了在实践中从零开始编写模型时可能遇到的陷阱。</li></ul><h2 id="1-线性回归"><a href="#1-线性回归" class="headerlink" title="1. 线性回归"></a>1. 线性回归</h2><p><em>回归</em>（regression）是能为一个或多个自变量与因变量之间关系建模的一类方法。 在自然科学和社会科学领域，回归经常用来表示输入和输出之间的关系。</p><p>在机器学习领域中的大多数任务通常都与<em>预测</em>（prediction）有关。 当我们想预测一个数值时，就会涉及到回归问题。 常见的例子包括：预测价格（房屋、股票等）、预测住院时间（针对住院病人等）、 预测需求（零售销量等）。 但不是所有的<em>预测</em>都是回归问题，比如分类问题也属于<em>预测</em>，目标是预测数据属于一组类别中的哪一个。</p><h3 id="1-1-线性回归基本元素"><a href="#1-1-线性回归基本元素" class="headerlink" title="1.1. 线性回归基本元素"></a>1.1. 线性回归基本元素</h3><p><em>线性回归</em>（linear regression）基于几个简单的假设： 首先，假设自变量x和因变量y之间的关系是线性的， 即y可以表示为x中元素的加权和，这里通常允许包含观测值的一些噪声； 其次，我们假设任何噪声都比较正常，如噪声遵循正态分布。</p><p>为了解释<em>线性回归</em>，我们举一个实际的例子： 我们希望根据房屋的面积（平方英尺）和房龄（年）来估算房屋价格（美元）。 为了开发一个能预测房价的模型，我们需要收集一个真实的数据集。 这个数据集包括了房屋的销售价格、面积和房龄。 在机器学习的术语中，该数据集称为<em>训练数据集</em>（training data set） 或<em>训练集</em>（training set）。 每行数据（比如一次房屋交易相对应的数据）称为<em>样本</em>（sample）， 也可以称为<em>数据点</em>（data point）或<em>数据样本</em>（data instance）。 我们把试图预测的目标（比如预测房屋价格）称为<em>标签</em>（label）或<em>目标</em>（target）。 预测所依据的自变量（面积和房龄）称为<em>特征</em>（feature）或<em>协变量</em>（covariate）。</p><h4 id="1-1-1-线性模型"><a href="#1-1-1-线性模型" class="headerlink" title="1.1.1. 线性模型"></a>1.1.1. 线性模型</h4><p>线性假设是指目标（房屋价格）可以表示为特征（面积和房龄）的加权和，如下面的式子：</p><script type="math/tex; mode=display">\text { price }=w_{\text {1}} \cdot \text { area }+w_{\text {2}} \cdot \text { age }+b</script><p>其中的w1和w2称为<em>权重</em>（weight），分别对应面积（area）和房龄（age），权重决定了每个特征对我们预测值的影响。b称为<em>偏置</em>（bias）、<em>偏移量</em>（offset）或<em>截距</em>（intercept）。 偏置是指当所有特征都取值为0时，预测值应该为多少。 即使现实中不会有任何房子的面积是0或房龄正好是0年，我们仍然需要偏置项。 如果没有偏置项，我们模型的表达能力将受到限制。 严格来说，上式是输入特征的一个<strong>仿射变换（affine transformation）</strong>。 仿射变换的特点是通过加权和对特征进行<em>线性变换（linear transformation）</em>， 并通过偏置项来进行<em>平移（translation）</em>。</p><p>给定一个数据集，我们的目标是寻找模型的权重w和偏置b， 使得根据模型做出的预测大体符合数据里的真实价格。 输出的预测值由输入特征通过<em>线性模型</em>的仿射变换决定，仿射变换由所选权重和偏置确定。</p><p>而在机器学习领域，我们通常使用的是高维数据集，建模时采用线性代数表示法会比较方便。 当我们的输入包含d个特征时，我们将预测结果$\hat{y}$（通常使用“尖角”符号表示y的估计值）表示为：$\hat{y}=w_{1} x_{1}+\ldots+w_{d} x_{d}+b$.</p><p>将所有特征放到向量x∈Rd中， 并将所有权重放到向量w∈Rd中， 我们可以用点积形式来简洁地表达模型：$\hat{y}=\mathbf{w}^{\top} \mathbf{x}+b$，其中向量x对应于单个数据样本的特征。 用符号表示的矩阵X∈Rn×d可以很方便地引用我们整个数据集的n个样本。 其中，X的每一行是一个样本，每一列是一种特征。</p><p>对于特征集合X，预测值y^∈Rn可以通过矩阵-向量乘法表示为：$\hat{\mathbf{y}}=\mathbf{X} \mathbf{w}+b$。</p><p>这个过程中的求和将使用广播机制。给定训练数据特征X和对应的已知标签y， 线性回归的目标是找到一组<strong>权重向量w和偏置b</strong>：当给定从X的同分布中取样的新样本特征时， 这组权重向量和偏置能够使得新样本预测标签的误差尽可能小。</p><p>虽然我们相信给定x预测y的最佳模型会是线性的， 但我们很难找到一个有n个样本的真实数据集，其中对于所有的1≤i≤n，$\boldsymbol{y}^{(i)}$完全等于$\mathbf{w}^{\top} \mathbf{x}^{(i)}+b$。 无论我们使用什么手段来观察特征X和标签y， 都可能会出现少量的观测误差。 因此，即使确信特征与标签的潜在关系是线性的， 我们也会加入一个噪声项来考虑观测误差带来的影响。</p><p>在开始寻找最好的<em>模型参数</em>（model parameters）w和b之前， 我们还需要两个东西： （1）一种模型质量的度量方式（损失函数）； （2）一种能够更新模型以提高模型预测质量的方法（比如随机梯度下降）。</p><h4 id="1-1-2-损失函数（loss-function）！"><a href="#1-1-2-损失函数（loss-function）！" class="headerlink" title="1.1.2. 损失函数（loss function）！"></a>1.1.2. 损失函数（loss function）！</h4><p>在开始考虑如何用模型<em>拟合</em>（fit）数据之前，我们需要确定一个拟合程度的度量。<strong>损失函数（loss function）</strong>能够量化目标的<em>实际</em>值与<em>预测</em>值之间的差距。 通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。 </p><p>回归问题中最常用的损失函数是平方误差函数。当样本i的预测值为y，其相应的真实标签为ty时， 平方误差可以定义为以下公式：<code>l(w,b)=0.5(y−ty)^2</code>其中常数0.5不会带来本质的差别，但这样在形式上稍微简单一些 （因为当我们对损失函数求导后常数系数为1）。由于训练数据集并不受我们控制，所以经验误差只是关于模型参数的函数。经验误差即l，w和b是线性模型参数。<br>由于平方误差函数中的二次方项， 估计值和观测值之间较大的差异将导致更大的损失。为了度量模型在整个数据集上的质量，我们需计算在训练集n个样本上的<em>损失均值</em>，也就是对每一个样本计算损失然后求和再除样本数。</p><script type="math/tex; mode=display">L(\mathbf{w}, b)=\frac{1}{n} \sum_{i=1}^{n} l^{(i)}(\mathbf{w}, b)=\frac{1}{n} \sum_{i=1}^{n} \frac{1}{2}\left(\mathbf{w}^{\top} \mathbf{x}^{(i)}+b-y^{(i)}\right)^{2}</script><p>在训练模型时，我们希望寻找一组参数（w∗,b∗）， 这组参数能最小化在所有训练样本上的总损失。</p><h4 id="1-1-3-解析解"><a href="#1-1-3-解析解" class="headerlink" title="1.1.3. 解析解"></a>1.1.3. 解析解</h4><p>线性回归刚好是一个很简单的优化问题。 与在d2l中所讲到的其他大部分模型不同，线性回归的解可以用一个公式简单地表达出来， 这类解叫作解析解（analytical solution）。 首先，我们将偏置b合并到参数w中，合并方法是在包含所有参数的矩阵中附加一列。 我们的预测问题是最小化$|\mathbf{y}-\mathbf{X} \mathbf{w}|^{2}$。 这在损失平面上只有一个临界点，这个临界点对应于整个区域的损失极小点。 将损失关于w的导数设为0，得到解析解：</p><script type="math/tex; mode=display">\mathbf{w}^{*}=\left(\mathbf{X}^{\top} \mathbf{X}\right)^{-1} \mathbf{X}^{\top} \mathbf{y}</script><p>像线性回归这样的简单问题存在解析解，但并不是所有的问题都存在解析解。 解析解可以进行很好的数学分析，但解析解对问题的限制很严格，导致它无法广泛应用在深度学习里。</p><h4 id="1-1-4-随机梯度下降（Stochastic-Gradient-Descent）！"><a href="#1-1-4-随机梯度下降（Stochastic-Gradient-Descent）！" class="headerlink" title="1.1.4. 随机梯度下降（Stochastic Gradient Descent）！"></a>1.1.4. 随机梯度下降（Stochastic Gradient Descent）！</h4><p>即使在我们无法得到解析解的情况下，我们仍然可以有效地训练模型。 在许多任务上，那些难以优化的模型效果要更好。 因此，弄清楚如何训练这些难以优化的模型是非常重要的。</p><p>本书中我们用到一种名为<strong>梯度下降</strong>的方法， 这种方法几乎可以优化所有深度学习模型。<strong>它通过不断地在损失函数递减的方向上更新参数来降低误差。</strong></p><p>梯度下降最简单的用法是计算损失函数（数据集中所有样本的损失均值） 关于模型参数的导数（在这里也可以称为梯度）。 但实际中的执行可能会非常慢：因为在每一次更新参数之前，我们必须遍历整个数据集。 因此，我们通常会在每次需要计算更新的时候随机抽取一小批样本， 这种变体叫做<strong>小批量随机梯度下降（minibatch stochastic gradient descent）</strong>。</p><p>在每次迭代中，我们首先随机抽样一个小批量B， 它是由固定数量的训练样本组成的。 然后，我们计算小批量的平均损失关于模型参数的导数（也可以称为梯度）。 最后，我们将梯度乘以一个预先确定的正数η，并从当前参数的值中减掉。</p><p>总结一下，算法的步骤如下： （1）初始化模型参数的值，如随机初始化； （2）从数据集中随机抽取小批量样本且在负梯度的方向上更新参数，并不断迭代这一步骤。</p><p>|B|表示每个小批量中的样本数，这也称为<strong>批量大小（batch size）</strong>。η表示<strong>学习率（learning rate）</strong>。 批量大小和学习率的值通常是手动预先指定，而不是通过模型训练得到的。 这些可以调整但不在训练过程中更新的参数称为<strong>超参数（hyperparameter）</strong>。<strong>调参（hyperparameter tuning</strong>是选择超参数的过程。 超参数通常是我们根据训练迭代结果来调整的， 而训练迭代结果是在独立的<em>验证数据集</em>（validation dataset）上评估得到的。</p><p>在训练了预先确定的若干迭代次数后（或者直到满足某些其他停止条件后）， 我们记录下模型参数的估计值，表示为$\hat{\mathbf{w}}, \hat{b}$。 但是，即使我们的函数确实是线性的且无<strong>噪声</strong>，这些估计值也不会使损失函数真正地达到最小值。 因为算法会使得损失向最小值缓慢收敛，但却不能在有限的步数内非常精确地达到最小值。</p><p>关于<strong>噪声</strong>的解释：深度神经网络的成功依赖于高质量标记的训练数据。训练数据中存在标记错误（标记噪声，即Noisy Labels）会大大降低模型在干净测试数据上的准确性。大型数据集几乎总是包含带有不正确或不准确的标签。这导致了一个悖论：一方面，大型数据集对于深度网络的训练是非常必要的，而另一方面，深度网络往往会记住训练标签噪声，从而在实践中导致较差的模型性能。</p><p>线性回归恰好是一个在整个域中只有一个最小值的学习问题。 但是对于像深度神经网络这样复杂的模型来说，损失平面上通常包含多个最小值。 深度学习实践者很少会去花费大力气寻找这样一组参数，使得在<em>训练集</em>上的损失达到最小。 事实上，更难做到的是找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失， 这一挑战被称为<em>泛化</em>（generalization）。</p><h4 id="1-1-5-用模型进行预测"><a href="#1-1-5-用模型进行预测" class="headerlink" title="1.1.5. 用模型进行预测"></a>1.1.5. 用模型进行预测</h4><p>给定“已学习”的线性回归模型$\hat{\mathbf{w}}^{\top} \mathbf{x}+\hat{b}$， 现在我们可以通过房屋面积和房龄来估计一个（未包含在训练数据中的）新房屋价格。 给定特征估计目标的过程通常称为<em>预测</em>（prediction）或<em>推断</em>（inference）。</p><h3 id="1-2-矢量化加速"><a href="#1-2-矢量化加速" class="headerlink" title="1.2. 矢量化加速"></a>1.2. 矢量化加速</h3><p>在训练我们的模型时，我们经常希望能够同时处理整个小批量的样本。 为了实现这一点，需要我们对计算进行矢量化， 从而利用线性代数库，而不是在Python中编写开销高昂的for循环。矢量化代码通常会带来数量级的加速。而且将更多的数学运算放到库中，而无须自己编写那么多的计算，从而减少了出错的可能性。</p><h3 id="1-3-正态分布与平方损失"><a href="#1-3-正态分布与平方损失" class="headerlink" title="1.3. 正态分布与平方损失"></a>1.3. 正态分布与平方损失</h3><p>这一部分通过对噪声分布的假设（假设符合正态分布）来解读平方损失目标函数（1.1.2决定的损失函数），也就是解释为什么均方误差可以用来作为线性回归的损失函数。<br>回顾一下，最大似然估计，就是利用已知的样本结果信息，反推具有最大概率导致这些样本结果出现的模型参数值。<br>正态分布和线性回归之间的关系很密切。 正态分布（normal distribution），也称为<em>高斯分布</em>（Gaussian distribution）。设随机变量x具有均值μ和方差 $\sigma^{2}$（标准差σ），概率密度函数如下。</p><script type="math/tex; mode=display">p(x)=\frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left(-\frac{1}{2 \sigma^{2}}(x-\mu)^{2}\right)</script><p>从正态分布的图像上看，改变均值会产生沿x轴的偏移，增加方差将会分散分布、降低其峰值。<br>均方误差损失函数（简称均方损失）可以用于线性回归的一个原因是： 我们假设了观测中包含噪声，其中噪声服从正态分布。噪声正态分布如：$y=\mathbf{w}^{\top} \mathbf{x}+b+\epsilon$,其中，$\epsilon \sim \mathcal{N}\left(0, \sigma^{2}\right)$。</p><p>通过把 $\epsilon$ 以外的项移到式子右侧可以得出 $y-\mathbf{w}^{\top} \mathbf{x}-b=\epsilon$，所以可以写出通过给定的x观测到特定y的<em>似然</em>（likelihood）：</p><script type="math/tex; mode=display">P(y \mid \mathbf{x})=\frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left(-\frac{1}{2 \sigma^{2}}\left(y-\mathbf{w}^{\top} \mathbf{x}-b\right)^{2}\right)</script><p>然后根据极大似然估计法，参数w和b的最优值是使整个数据集的<em>似然</em>最大的值：</p><script type="math/tex; mode=display">P(\mathbf{y} \mid \mathbf{X})=\prod_{i=1}^{n} p\left(y^{(i)} \mid \mathbf{x}^{(i)}\right)</script><p>根据极大似然估计法选择的估计量称为<em>极大似然估计量</em>。 通过最大化似然对数来简化使许多指数函数的乘积最大化的问题。 优化通常是说最小化而不是最大化，我们可以改为<em>最小化负对数似然</em>$-\log P(\mathbf{y} \mid \mathbf{X})$，由此得到：</p><script type="math/tex; mode=display">-\log P(\mathbf{y} \mid \mathbf{X})=\sum_{i=1}^{n} \frac{1}{2} \log \left(2 \pi \sigma^{2}\right)+\frac{1}{2 \sigma^{2}}\left(y^{(i)}-\mathbf{w}^{\top} \mathbf{x}^{(i)}-b\right)^{2}</script><p>只需要假设 $\sigma$ 是某个固定常数就可以忽略第一项， 因为第一项不依赖于w和b，改变参数不会对负对数似然的大小造成影响。 现在第二项除了常数 $\frac{1}{\sigma^{2}}$ 外，其余部分和前面介绍的均方误差是一样的。 可以看到标准差作为系数也不影响结果的大小，也就是说上面式子的解并不依赖于 $\sigma$。 因此，在高斯噪声的假设下，最小化均方误差等价于对线性模型的极大似然估计。</p><h3 id="1-4-从线性回归到深度网络"><a href="#1-4-从线性回归到深度网络" class="headerlink" title="1.4. 从线性回归到深度网络"></a>1.4. 从线性回归到深度网络</h3><p>用描述神经网络的方式来描述线性模型， 从而把线性模型看作一个神经网络。 首先，我们用“层”符号来重写这个模型。线性回归是单层神经网络（通常我们在计算层数时不考虑输入层），只有输入层和输出层构成，其中输入层有d个节点，输出层有1个节点。</p><p>如输入为x1,…,xd， 因此输入层中的<em>输入数</em>（或称为<em>特征维度</em>，feature dimensionality）为d。 网络的输出为o1，因此输出层中的<em>输出数</em>是1。 需要注意的是，输入值都是已经给定的，并且只有一个<em>计算</em>神经元。 我们可以将线性回归模型视为仅由单个人工神经元（输出层的一个）组成的神经网络，或称为单层神经网络。<br><img src="/assets/post_img/article37/线性回归神经网络.svg" alt="0x03pic1"><br>对于线性回归，每个输入都与每个输出（在本例中只有一个输出）相连， 我们将这种变换（图中的输出层） 称为<em>全连接层</em>（fully-connected layer）或称为<em>稠密层</em>（dense layer）。</p><h3 id="1-5-线性回归的从零实现与框架实现"><a href="#1-5-线性回归的从零实现与框架实现" class="headerlink" title="1.5. 线性回归的从零实现与框架实现"></a>1.5. 线性回归的从零实现与框架实现</h3><p><strong>从零实现</strong>：从零开始实现包括数据流水线、模型、损失函数和小批量随机梯度下降优化器。 虽然现代的深度学习框架几乎可以自动化地进行所有这些工作，但从零开始实现可以确保你真正知道自己在做什么。 同时，了解更细致的工作原理将方便我们自定义模型、自定义层或自定义损失函数。 在这一部分中将只使用张量和自动求导。</p><p><strong>框架实现</strong>：成熟的开源框架可以自动化基于梯度的学习算法中重复性的工作。除了张量和自动求导外，数据迭代器、损失函数、优化器和神经网络层很常用， 现代深度学习库为我们实现了这些组件。</p><p>细节代码放在实践notebook中，关于框架实现中采用求和方式后如何修改学习率的问题还是没有答案，希望以后可以搞明白。</p><h2 id="2-softmax回归"><a href="#2-softmax回归" class="headerlink" title="2. softmax回归"></a>2. softmax回归</h2><p>回归可以用于预测<em>多少</em>的问题。 比如预测房屋被售出价格，或者棒球队可能获得的胜场数，又或者患者住院的天数。<br>事实上，我们也对<em>分类</em>问题感兴趣：不是问“多少”，而是问“哪一个”：</p><ul><li>某个电子邮件是否属于垃圾邮件文件夹？</li><li>某个图像描绘的是驴、狗、猫、还是鸡？</li><li>某人接下来最有可能看哪部电影？</li></ul><p>通常，机器学习实践者用<em>分类</em>这个词来描述两个有微妙差别的问题： 1. 我们只对样本的“硬性”类别感兴趣，即属于哪个类别； 2. 我们希望得到“软性”类别，即得到属于每个类别的概率。 这两者的界限往往很模糊。其中的一个原因是：即使我们只关心硬类别，我们仍然使用软类别的模型。</p><h3 id="2-1-分类问题"><a href="#2-1-分类问题" class="headerlink" title="2.1. 分类问题"></a>2.1. 分类问题</h3><p>从一个图像分类问题开始。 假设每次输入是一个的2X2灰度图像。灰度图像是每个像素只有一个采样颜色的图像。这类图像通常显示为从最暗黑色到最亮的白色的灰度，尽管理论上这个采样可以任何颜色的不同深浅，甚至可以是不同亮度上的不同颜色。（注意：与黑白图像不同）<br>我们可以用一个标量表示每个像素值，每个图像对应四个特征x1,x2,x3,x4。此外，假设每个图像属于类别“猫”，“鸡”和“狗”中的一个。</p><p>接下来，我们要选择如何表示标签。 我们有两个明显的选择：最直接的想法是选择y∈{1,2,3}， 其中整数分别代表猫、鸡、狗。 这是在计算机上存储此类信息的有效方法。 如果类别间有一些自然顺序， 比如说我们试图预测{婴儿,儿童,青少年,青年人,中年人,老年人}， 那么将这个问题转变为回归问题，并且保留这种格式是有意义的。一般的分类问题并不与类别之间的自然顺序有关。</p><p>表示分类数据的简单方法：<em>独热编码（one-hot encoding）</em>。<br>独热编码是一个向量，它的分量和类别一样多。 类别对应的分量设置为1，其他所有分量设置为0。 在我们的例子中，标签y将是一个三维向量， 其中(1,0,0)对应于“猫”、(0,1,0)对应于“鸡”、(0,0,1)对应于“狗”：</p><script type="math/tex; mode=display">y \in\{(1,0,0),(0,1,0),(0,0,1)\}</script><h3 id="2-2-网络架构"><a href="#2-2-网络架构" class="headerlink" title="2.2. 网络架构"></a>2.2. 网络架构</h3><p>为了估计所有可能类别的条件概率，我们需要一个有多个输出的模型，每个类别对应一个输出。 为了解决线性模型的分类问题，我们需要和输出一样多的<strong>仿射函数（affine function）</strong>。 每个输出对应于它自己的仿射函数。 在我们的例子中，由于我们有4个特征和3个可能的输出类别， 我们将需要12个标量来表示权重（带下标的）， 3个标量来表示偏置（带下标的）。 下面我们为每个输入计算三个未规范化的预测（logit）：o1,o2和o3。</p><script type="math/tex; mode=display">\begin{aligned}&o_{1}=x_{1} w_{11}+x_{2} w_{12}+x_{3} w_{13}+x_{4} w_{14}+b_{1} \\&o_{2}=x_{1} w_{21}+x_{2} w_{22}+x_{3} w_{23}+x_{4} w_{24}+b_{2} \\&o_{3}=x_{1} w_{31}+x_{2} w_{32}+x_{3} w_{33}+x_{4} w_{34}+b_{3}\end{aligned}</script><p>可以用下面的单层神经网络图来描述计算过程，与线性回归一样，softmax回归也是一个单层神经网络。 由于计算每个输出(o1,o2,o3)取决于所有输入(x1,x2,x3,x4)，所以softmax回归的输出层也是全连接层。<br><img src="/assets/post_img/article37/softmax网络.png" alt="0x03pic2"></p><p>模型通过向量形式表达为：$\mathbf{o}=\mathbf{W} \mathbf{x}+\mathbf{b}$<br>这是一种更适合数学和编写代码的形式。 到此，我们已经将所有权重放到一个3X4矩阵中。 对于给定数据样本的特征x， 我们的输出是由权重与输入特征进行矩阵-向量乘法再加上偏置b得到的。</p><h3 id="2-3-全连接层的参数开销"><a href="#2-3-全连接层的参数开销" class="headerlink" title="2.3. 全连接层的参数开销"></a>2.3. 全连接层的参数开销</h3><p>深度学习中，全连接层无处不在。全连接层是“完全”连接的，可能有很多可学习的参数。 具体来说，对于任何具有d个输入和q个输出的全连接层， 参数开销为O(dq)，这个数字在实践中可能高得令人望而却步。 幸运的是，将d个输入转换为q个输出的成本可以减少到O$\left(\frac{d q}{n}\right)$， 其中超参数n可以由我们灵活指定，以在实际应用中平衡参数节约和模型有效性。</p><h3 id="2-4-softmax运算"><a href="#2-4-softmax运算" class="headerlink" title="2.4. softmax运算"></a>2.4. softmax运算</h3><p>现在我们将优化参数以最大化观测数据的概率。 为了得到预测结果，我们将设置一个阈值，如选择具有最大概率的标签。<br>我们希望模型的输出 $\hat{y}_{j}$ 可以视为属于类 $j$ 的概率， 然后选择具有最大输出值的类别argmax $\boldsymbol{}_{j} y_{j}$ 作为我们的预测。例如, 如果 $\hat{y}_{1}$ 、 $\hat{y}_{2}$ 和 $\hat{y}_{3}$ 分别为 $0.1$ 、 $0.8$ 和 $0.1$, 那么我们预测的类别是2，在我们的例子中代表“鸡”。</p><p>然而我们不能将未规范化的预测o直接视作我们感兴趣的输出。因为将线性层的输出直接视为概率时存在一些问题： 一方面，我们没有限制这些输出数字的总和为1。 另一方面，根据输入的不同，它们可以为负值。<br>要将输出视为概率，我们必须保证在任何数据上的输出都是非负的且总和为1。 此外，我们需要一个训练目标，来鼓励模型精准地估计概率。 在分类器输出0.5的所有样本中，我们希望这些样本有一半实际上属于预测的类。 这个属性叫做校准（calibration）。</p><p>softmax函数正是这样做的： softmax函数将未规范化的预测变换为非负并且总和为1，同时要求模型保持可导。 我们首先对每个未规范化的预测求幂（就是把预测作为e的指数），这样可以确保输出非负。 为了确保最终输出的总和为1，我们再对每个求幂后的结果除以它们的总和。如下式：</p><script type="math/tex; mode=display">\hat{\mathbf{y}}=\operatorname{softmax}(\mathbf{o}) \quad \text { 其中 } \quad \hat{y}_{j}=\frac{\exp \left(o_{j}\right)}{\sum_{k} \exp \left(o_{k}\right)}</script><p>这里, 对于所有的 $j$ 总有 $0 \leq \hat{y}_{j} \leq 1_{\circ}$ 因此, $\hat{\mathbf{y}}$ 可以视为一个正确的概率分布。 softmax运算不会改变末规范化的预测o之间的顺序, 只会确定分配给每个类别的概率。因此，在预测过程中, 我们仍然可以用下式来选择最有可能的类别。</p><script type="math/tex; mode=display">\underset{j}{\operatorname{argmax}} \hat{y}_{j}=\underset{j}{\operatorname{argmax}} o_{j}</script><p>softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。 因此，softmax回归是一个线性模型（linear model）。</p><h3 id="2-5-小批量样本的矢量化"><a href="#2-5-小批量样本的矢量化" class="headerlink" title="2.5. 小批量样本的矢量化"></a>2.5. 小批量样本的矢量化</h3><p>为了提高计算效率并且充分利用GPU，我们通常会针对小批量数据执行矢量计算。假设我们读取了一个批量的样本X, 其中特征维度 (输入数量) 为 $d$, 批量大小为 $n$。 此外, 假设我们在输出中有 $q$ 个类别。那么小批量特征为 $\mathrm{X} \in \mathbb{R}^{n \times d}$ ， 权重为 $\mathbf{W} \in \mathbb{R}^{d \times q}$ ，偏置为 $\mathbf{b} \in \mathbb{R}^{1 \times q}$ 。 softmax回归的矢量计算表达式为：</p><script type="math/tex; mode=display">\begin{aligned}&\mathbf{O}=\mathbf{X W}+\mathbf{b}, \\&\hat{\mathbf{Y}}=\operatorname{softmax}(\mathbf{O}) .\end{aligned}</script><p>相对于一次处理一个样本, 小批量样本的矢量化加快了 $\mathbf{X}$ 和 $\mathbf{W}$ 的矩阵-向量乘法。由于 $\mathbf{X}$ 中的每一行代表一个数据样本, 那么softmax运算可以按行 (rowwise) 执行：对于 $\mathrm{O}$ 的每一行, 我们先对所有项进行幂运算, 然后通过求和对它们进行标准化（规范化，对应线性代数中的正交规范化比较好理解）。在上式中, $\mathbf{X W}+\mathbf{b}$ 的求和会使用<em>广播</em>（每行都加）, 小批量的末规范化预测 $\mathbf{O}$ 和输出概率 $\hat{\mathbf{Y}}$ 都是形状为 $n \times q$ 的矩阵。</p><h3 id="2-6-损失函数"><a href="#2-6-损失函数" class="headerlink" title="2.6. 损失函数"></a>2.6. 损失函数</h3><p>损失函数用来度量预测的效果。使用与线性回归中相同的最大似然估计。</p><h4 id="2-6-1-对数似然"><a href="#2-6-1-对数似然" class="headerlink" title="2.6.1. 对数似然"></a>2.6.1. 对数似然</h4><p>softmax函数给出了一个向量 $\hat{\mathbf{y}}$, 我们可以将其视为“对给定任意输入 $\mathbf{x}$ 的每个类的<strong>条件概率</strong>”。例如, $\hat{y}_{1}=$ $P(y=$ 猫 $\mid \mathbf{x})$ ，因为猫1鸡2狗3嘛。 假设整个数据集 ${\mathbf{X}, \mathbf{Y}}$ （也就是n个（特征，标签）对）具有 $n$ 个样本, 其中索引 $i$ 的样本由特征向量 $\mathbf{x}^{(i)}$ 和独热标签向量 $\mathbf{y}^{(i)}$ 组成。 我们可以将估计值与实际值进行比较：</p><script type="math/tex; mode=display">P(\mathbf{Y} \mid \mathbf{X})=\prod_{i=1}^{n} P\left(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)}\right)</script><p>根据最大似然估计，我们最大化 $P(\mathbf{Y} \mid \mathbf{X})$ ，相当于最小化负对数似然：</p><script type="math/tex; mode=display">-\log P(\mathbf{Y} \mid \mathbf{X})=\sum_{i=1}^{n}-\log P\left(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)}\right)=\sum_{i=1}^{n} l\left(\mathbf{y}^{(i)}, \hat{\mathbf{y}}^{(i)}\right)</script><p>其中, 对于任何标签 $\mathbf{y}$ 和模型预测 $\hat{\mathbf{y}}$ ，损失函数为：</p><script type="math/tex; mode=display">l(\mathbf{y}, \hat{\mathbf{y}})=-\sum_{j=1}^{q} \boldsymbol{y}_{j} \log \hat{y}_{j}</script><p>上面式子中的损失函数 $l$ 被称为交叉熵损失（cross-entropy loss）。由于y是一个长度为q的独热编码向量， 所以除了一个项以外的所有项j都消失了。 由于所有 $\hat{y}_{j}$ 都是预测的概率，所以它们的对数永远不会大于0（因为预测概率小于1）。 因此，如果正确地预测实际标签，即如果实际标签 $P(\mathbf{y} \mid \mathbf{x})=1$， 则损失函数不能进一步最小化，虽然这往往是不可能的，因为数据集中可能存在标签噪声（比如某些样本可能被误标）， 或输入特征没有足够的信息来完美地对每一个样本分类。</p><h4 id="2-6-2-softmax及其导数"><a href="#2-6-2-softmax及其导数" class="headerlink" title="2.6.2. softmax及其导数"></a>2.6.2. softmax及其导数</h4><p>将softmax函数代入到损失函数中：</p><script type="math/tex; mode=display">\begin{aligned}l(\mathbf{y}, \hat{\mathbf{y}}) &=-\sum_{j=1}^{q} y_{j} \log \frac{\exp \left(o_{j}\right)}{\sum_{k=1}^{q} \exp \left(o_{k}\right)} \\&=\sum_{j=1}^{q} y_{j} \log \sum_{k=1}^{q} \exp \left(o_{k}\right)-\sum_{j=1}^{q} y_{j} o_{j} \\&=\log \sum_{k=1}^{q} \exp \left(o_{k}\right)-\sum_{j=1}^{q} y_{j} o_{j}\end{aligned}</script><p>代入后的损失函数相对于任何末规范化的预测 $o_{j}$ 求导：</p><script type="math/tex; mode=display">\partial_{o_{j}} l(\mathbf{y}, \hat{\mathbf{y}})=\frac{\exp \left(o_{j}\right)}{\sum_{k=1}^{q} \exp \left(o_{k}\right)}-y_{j}=\operatorname{softmax}(\mathbf{o})_{j}-y_{j}</script><p>可以看出，该导数是我们softmax模型分配的概率与实际发生的情况（由独热标签向量表示）之间的差异。 从这个意义上讲，这与我们在回归中看到的非常相似（哪里？）， 其中梯度是观测值y和估计值 $\hat{\mathbf{y}}$ 之间的差异。<br>这不是巧合，在任何指数族分布模型中，对数似然的梯度正是由此得出的。这使梯度计算在实践中变得容易很多。</p><h4 id="2-6-3-交叉熵损失"><a href="#2-6-3-交叉熵损失" class="headerlink" title="2.6.3. 交叉熵损失"></a>2.6.3. 交叉熵损失</h4><p>如果考虑整个结果分布的情况，即观察到的不仅仅是一个结果。 对于标签y，我们可以使用与以前相同的表示形式。 唯一的区别是，我们现在用一个概率向量表示，如（0.1，0.2，0.7）， 而不是独热编码的向量，如（0，0，1）。<br>使用 $l(\mathbf{y}, \hat{\mathbf{y}})=-\sum_{j=1}^{q} y_{j} \log \hat{y}_{j}$来定义损失$l$，它是所有标签分布的预期损失值。 此损失称为交叉熵损失（cross-entropy loss），是分类问题最常用的损失之一。</p><h3 id="2-7-信息论基础"><a href="#2-7-信息论基础" class="headerlink" title="2.7. 信息论基础"></a>2.7. 信息论基础</h3><p>信息论（information theory）涉及编码、解码、发送以及尽可能简洁地处理信息或数据。</p><h4 id="2-7-1-熵"><a href="#2-7-1-熵" class="headerlink" title="2.7.1. 熵"></a>2.7.1. 熵</h4><p>信息论的核心思想是量化数据中的信息内容。 在信息论中，该数值被称为分布的熵（entropy）。可以通过以下方程得到：</p><script type="math/tex; mode=display">H[P]=\sum_{j}-P(j) \log P(j)</script><p>信息论的基本定理之一指出，为了对从分布 p 中随机抽取的数据进行编码， 我们至少需要 $H[P]$ “纳特（nat）”对其进行编码。 “纳特”相当于比特（bit），但是对数底为 $e$ 而不是2。因此，一个纳特约为1.44比特。</p><h4 id="2-7-2-信息量"><a href="#2-7-2-信息量" class="headerlink" title="2.7.2. 信息量"></a>2.7.2. 信息量</h4><p>信息量是指信息多少的量度。信息量用 $\log \frac{1}{P(j)}即-\log P(j)$ 表示，在观察一个事件 $j$ 时，并赋予它（主观）概率 $P(j)$。<br>当我们赋予一个事件较低的概率时，该事件的信息量就更大。熵就是当分配的概率真正匹配数据生成过程时的信息量的期望。（这个地方比较晦涩）</p><h4 id="2-7-3-重新审视交叉熵"><a href="#2-7-3-重新审视交叉熵" class="headerlink" title="2.7.3. 重新审视交叉熵"></a>2.7.3. 重新审视交叉熵</h4><p>熵 $H[P]$，交叉熵从P到Q，记为 $H(P,Q)$。可以把交叉熵想象为“主观概率为Q的观察者在看到根据概率P生成的数据时的预期‘惊讶’程度”。 当P=Q时，交叉熵达到最低。 在这种情况下，从P到Q的交叉熵是 $H(P, P)=H(P)$。<br>可以从两方面来考虑交叉熵分类目标： </p><ol><li>最大化观测数据的似然</li><li>最小化传达标签所需的信息量。</li></ol><h3 id="2-8-模型预测和评估"><a href="#2-8-模型预测和评估" class="headerlink" title="2.8. 模型预测和评估"></a>2.8. 模型预测和评估</h3><p>在训练softmax回归模型后，给出任何样本特征，我们可以预测每个输出类别的概率。 通常我们使用预测概率最高的类别作为输出类别。 如果预测与实际类别（标签）一致，则预测是正确的。<br>在后续的实验中将使用<strong>精度（accuracy）</strong>来评估模型的性能。 精度等于正确预测数与预测总数之间的比率。</p><h3 id="2-9-图像分类数据集"><a href="#2-9-图像分类数据集" class="headerlink" title="2.9. 图像分类数据集"></a>2.9. 图像分类数据集</h3><p>MNIST数据集（一个手写数字集）是图像分类中广泛使用的数据集之一，但作为基准数据集过于简单。因为很多算法在测试集上的性能已经达到 99.6%！一个算法即便在这个数据集上work在其他数据集上也很可能不怎么样。<br>Fashion-MNIST 是一个替代 MNIST 手写数字集的图像数据集。 它是由 Zalando（一家德国的时尚科技公司）旗下的研究部门提供。其涵盖了来自 10 种类别的共 7 万个不同商品的正面图片。Fashion-MNIST 的大小、格式和训练集/测试集划分与原始的 MNIST 完全一致。60000/10000 的训练测试数据划分，<em>通道</em>数为1，28x28（28像素）的灰度图片。<br>关于<em>通道（channels）</em>：描述一个像素点，如果是灰度，那么只需要一个数值来描述它，就是单通道。如果一个像素点，有RGB三种颜色来描述它，就是三通道。常用图片形式还有四通道的，即再加一个透明度。<br>该数据集样子大致如下（每个类别占三行）：<br><img src="/assets/post_img/article37/FasionMNIST.jpeg" alt="0x03pic3"><br>数据集的存储方式如下：<br><img src="/assets/post_img/article37/FasionMNIST存储方式.jpeg" alt="0x03pic4"><br>其中样本标注如下，十个类别：<br><img src="/assets/post_img/article37/FasionMNIST样本标注方式.jpeg" alt="0x03pic5"></p><p>深度学习框架中的内置函数可以将Fashion-MNIST数据集下载并读取到内存中。具体操作在本笔记对应实践中。</p><h3 id="2-10-softmax回归的从零实现与框架实现"><a href="#2-10-softmax回归的从零实现与框架实现" class="headerlink" title="2.10. softmax回归的从零实现与框架实现"></a>2.10. softmax回归的从零实现与框架实现</h3><p><strong>从零实现</strong>：softmax回归也是重要的基础，因此应该知道实现softmax回归的细节，从零实现他。每个样本都将用固定长度的向量表示。 原始数据集中的每个样本都是28x28的图像。 在这里通过展平每个图像，把它们看作长度为784的向量，即暂时只把每个像素位置看作一个特征。引入的交叉熵损失函数可能是深度学习中最常见的损失函数，因为目前分类问题的数量远远超过回归问题的数量。</p><p><strong>框架实现</strong>：通过深度学习框架的高级API也能更方便地实现softmax回归模型。 框架实现中继续使用Fashion-MNIST数据集，并保持批量大小为256。同时在softmax函数的实现上进行了优化（具体如何优化翻书看），解决了数值可能上、下溢出的问题。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;p&gt;从经典算法————&lt;em&gt;线性&lt;/em&gt;神经网络开始，介绍神经网络的基础知识。 经典统计学习技术中的线性回归和softmax回归可以视为线性神经网络。&lt;/p&gt;
&lt;p&gt;对应实践：&lt;a href=&quot;https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x03.ipynb&quot;&gt;https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x03.ipynb&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://silencezheng.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>开源协议简单了解</title>
    <link href="http://silencezheng.top/2022/05/14/article36/"/>
    <id>http://silencezheng.top/2022/05/14/article36/</id>
    <published>2022-05-14T14:55:13.000Z</published>
    <updated>2022-05-27T14:14:58.426Z</updated>
    
    <content type="html"><![CDATA[<p>最近看到B站有人用pyqt5被发律师函，于是对开源协议产生了一些兴趣，对网络相关热门帖子结合协议官网整理归纳一下，简单了解一下开源协议，如有错误请评论指正，感谢。<br>提及的协议有：GPL、LGPL、BSD、MIT、Apache License 2.0。<br><span id="more"></span></p><h2 id="0-基本概念"><a href="#0-基本概念" class="headerlink" title="0. 基本概念"></a>0. 基本概念</h2><h3 id="0-1-Copyleft和Copyright"><a href="#0-1-Copyleft和Copyright" class="headerlink" title="0.1. Copyleft和Copyright"></a>0.1. Copyleft和Copyright</h3><p>Copyleft和Copyright是两种截然相反的版权所有方式。copyright的英文本意是版权©️。copyleft现在被译为公共版权或非盈利版权。<br><img src="/assets/post_img/article36/clcr.jpg" alt="pic1"></p><p><strong>Copyright</strong>：版权所有，即软件的一切权利归软件作者私有。<br>软件的版权和其它一切权利归软件作者所私有。用户只有使用权，没有其它权利，包括没有复制软件的权利。<br><strong>Copyleft</strong>：是一种让程序或其它作品保持自由的通用方法，它要求所有对 Copyleft 程序的修改和扩展都保持自由。但该规则仅与GNU相关。<br>Copyleft 是指任何人都可以重新分发软件，不管有没有进行修改，但必须同时保留软件所具有的自由特性。Copyleft是为了保证所有用户都拥有自由的权利。<br>一个程序遵循 Copyleft，我们首先声明它是有版权的；然后，我们给它加上发布的规则，这个规则就是一个法律声明，它赋予所有人有使用、修改和重新发布程序的代码 及其衍生作品 的权利，但要求在这个过程中发布规则是不可以改变的。这样的话，代码和自由权利在法律上就不可分割了。</p><h3 id="0-2-其他"><a href="#0-2-其他" class="headerlink" title="0.2. 其他"></a>0.2. 其他</h3><p><strong>复制（Copy）</strong>：将软件复制到你的电脑，你客户的电脑，或者任何地方。<br><strong>分发（Distribution）</strong>：在网站供他人下载，拷贝到U盘送人。<br><strong>商用（Commercial use）</strong>：盈利，在分发软件的时候收费。<br><strong>修改（Modification）</strong>：添加或删除某个功能，在别的项目中使用部分代码等。<br><strong>专利使用（Patent use）</strong>：用于申请专利。<br><strong>个人使用（Private use）</strong>：供私人使用，基本都支持。<br>下面的介绍中权利部分严格按照以上概念进行，但通常带“自由”字眼的并非完全自由，有相应的规则加以限制，本文也会略微提到。<br><strong>开源</strong>：开源软件最大的特点是开放而不是免费，也就是任何人都可以得到软件的源代码，加以修改学习，甚至重新发放，但需要在版权限制范围之内。</p><h2 id="1-GPL（GNU-General-Public-License）"><a href="#1-GPL（GNU-General-Public-License）" class="headerlink" title="1. GPL（GNU General Public License）"></a>1. GPL（GNU General Public License）</h2><p>GNU 通用公共许可证 (GPL)，目前共有3个版本，分别为GPLv1、GPLv2、GPLv3（最新）。<br>GNU 通用公共许可证有以下几种格式：HTML、纯文本、ODF、Docbook v4或者v5、Texinfo、LaTeX、Markdown和RTF。这些文档的格式不是为了单独发布使用，它们的目的是嵌入其他文档。</p><p>GPL提供的权利主要有：自由复制、自由分发、“自由”修改、盈利、专利使用。<br>注意：</p><ol><li>盈利收费前向你的客户提供该软件的GPL许可协议，以便让他们知道，他们可以从别的渠道免费得到这份软件，以及你收费的理由。</li><li>修改时，使用了这段代码的项目也必须使用 GPL 协议。</li><li>分发时，软件自始至终都以开放源代码形式发布，无论软件以何种形式发布，都必须同时附上源代码。</li><li>GPLv3允许专利使用。</li></ol><p>总结：<br>GPL 大致就是一个Copyleft的体现。你可以去掉所有原作的版权信息，只要你保持开源，并且随源代码、二进制版附上GPL的许可证就行。开发或维护遵循 GPL 协议开发的软件的公司或个人，可以对使用者收取一定的服务费用。但必须无偿提供软件的完整源代码，不得将源代码与服务做捆绑或任何变相捆绑销售。</p><h2 id="2-LGPL（GNU-Lesser-General-Public-License）"><a href="#2-LGPL（GNU-Lesser-General-Public-License）" class="headerlink" title="2. LGPL（GNU Lesser General Public License）"></a>2. LGPL（GNU Lesser General Public License）</h2><p>GNU宽通用公共许可证 (GNU LGPL)，其最新版本号是3。它对产品所保留的权利比 GPL 少。<br>GNU 宽通用公共许可证文本有以下格式：HTML、纯文本、Docbook、Texinfo、Markdown、ODF和RTF。这些文档的格式不是为了单独发布使用，它们的目的是嵌入其他文档。</p><p>LGPL提供的权利：自由复制、自由分发、自由修改、盈利。<br>与GPL大致相同，但条件上有差异：</p><ol><li>GNU不建议使用LGPL，因为LGPL许可证允许专有软件使用该函数库；而普通的GNU GPL只允许在自由软件中使用该函数库。</li><li>LGPL不要求其它使用LGPL授权代码的软件以LGPL方式发布。</li><li>LGPL软件可以被转换成GPL，这种特性对于在GPL库或应用程序中直接使用LGPL程序有一定程度的帮助。</li></ol><p>总结：<br>使用普通GPL的函数库给予自由软件开发者一个超越专有软件开发者的优势：能够使用按照普通GPL发布的函数库，而专有软件的开发者不能使用这些库。<br>使用LGPL的常见情况是当专有软件可以通过其他函数库来实现使用自由软件函数库的功能时，该函数库便对自由软件没有任何独有优势，此时采用LGPL。</p><h2 id="3-BSD-Berkeley-Software-Distribution"><a href="#3-BSD-Berkeley-Software-Distribution" class="headerlink" title="3. BSD (Berkeley Software Distribution)"></a>3. BSD (Berkeley Software Distribution)</h2><p>BSD，即伯克利软件分发许可协议。BSD协议的出发点是鼓励代码重用，可以将公开的资源纳入私人软件并使用闭源的形式进行出售，但是需要尊重代码作者的著作权。通常有BSD 3-Clause （又称”New BSD License”或”Modified BSD License”）, BSD 2-Clause（又称”Simplified BSD License”或”FreeBSD License”。<br>新 BSD 协议在软件分发方面，除需要包含一份版权提示和免责声明之外，没有任何限制。另外，该协议还禁止拿开发者的名义为衍生产品背书，但简单 BSD 协议删除了这一条款。</p><p>BSD提供的权利：自由复制、自由分发、自由修改、盈利。<br>注意：</p><ol><li>BSD不允许专利使用。</li><li>如果再发布的产品中包含源代码，则在源代码中必须带有原来代码中的 BSD 协议。如果再发布的只是二进制类库 / 软件，则需要在类库 / 软件的文档和版权声明中包含原来代码中的 BSD 协议。</li><li>BSD 3-Clause不可以用开源代码的作者 / 机构名字和原来产品的名字做市场推广。</li></ol><p>总结：<br>BSD 代码鼓励代码共享，但需要尊重代码作者的著作权。BSD由于允许使用者修改和重新发布代码，也允许使用或在 BSD 代码上开发商业软件发布和销售，因此是对商业集成很友好的协议。很多的公司企业在选用开源产品的时候都首选 BSD 协议，因为可以完全控制这些第三方的代码，在必要的时候可以修改或者二次开发。</p><h2 id="4-MIT-（Massachusetts-Institute-of-Technology）"><a href="#4-MIT-（Massachusetts-Institute-of-Technology）" class="headerlink" title="4. MIT （Massachusetts Institute of Technology）"></a>4. MIT （Massachusetts Institute of Technology）</h2><p>MIT 许可证之名源自麻省理工学院（Massachusetts Institute of Technology, MIT），又称「X 条款」（X License）或「X11 条款」（X11 License）<br>MIT 内容与三条款 BSD 许可证（3-clause BSD license）内容颇为近似，但是赋予软体被授权人更大的权利与更少的限制。<br>MIT 条款可与其他授权条款并存。另外，MIT 条款也是自由软体基金会（FSF）所认可的自由软体授权条款，与 GPL 相容。</p><p>MIT提供的权利：自由复制、自由分发、自由修改、盈利。<br>注意：</p><ol><li>该软件及其相关文档对所有人免费，可以任意处置，包括使用，复制，修改，合并，发表，分发，再授权，或者销售。唯一的限制是，软件中必须包含上述版 权和许可提示。</li><li>此授权条款并非属 copyleft 的自由软体授权条款，允许在自由 / 开放源码软件或非自由软件（proprietary software）所使用。</li><li>不允许专利使用。</li></ol><p>总结：<br>MIT 协议是所有开源许可中最宽松的一个，除了必须包含许可声明外，再无任何限制。</p><h2 id="5-Apache-License-2-0"><a href="#5-Apache-License-2-0" class="headerlink" title="5. Apache License 2.0"></a>5. Apache License 2.0</h2><p>Apache License 2.0是对商业应用友好的许可。使用者也可以在需要的时候修改代码来满足需要并作为开源或商业产品发布/销售。</p><p>Apache License 2.0提供的权利：自由复制、自由分发、自由修改、盈利、专利使用。<br>注意：</p><ol><li>一旦被授权，永久拥有。</li><li>在一个国家获得授权，适用于所有国家。</li><li>授权免费，且无版税。</li><li>任何人都可以获得授权。</li><li>一旦获得授权，没有任何人可以取消。比如，你基于该产品代码开发了衍生产品，你不用担心会在某一天被禁止使用该代码。</li><li>分发代码方面包含一些要求，主要是，要在声明中对参与开发的人给予认可并包含一份许可协议原文。</li><li>如果修改了代码，需要在被修改的文件中说明。并且在延伸的代码中（修改和有源代码衍生的代码中）需要带有原来代码中的协议，商标，专利声明和其他原来作者规定需要包含的说明。</li><li>如果再发布的产品中包含一个 Notice 文件，则在 Notice 文件中需要带有 Apache Licence。你可以在 Notice 中增加自己的许可，但不可以表现为对 Apache Licence 构成更改。</li></ol><p>总结：<br>Apache 协议 2.0 和除了为用户提供版权许可之外，还有专利许可，对于那些涉及专利内容的开发者而言，该协议最适合。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近看到B站有人用pyqt5被发律师函，于是对开源协议产生了一些兴趣，对网络相关热门帖子结合协议官网整理归纳一下，简单了解一下开源协议，如有错误请评论指正，感谢。&lt;br&gt;提及的协议有：GPL、LGPL、BSD、MIT、Apache License 2.0。&lt;br&gt;</summary>
    
    
    
    
    <category term="开源协议" scheme="http://silencezheng.top/tags/%E5%BC%80%E6%BA%90%E5%8D%8F%E8%AE%AE/"/>
    
  </entry>
  
  <entry>
    <title>Spring boot学习笔记0x01</title>
    <link href="http://silencezheng.top/2022/05/10/article35/"/>
    <id>http://silencezheng.top/2022/05/10/article35/</id>
    <published>2022-05-10T04:16:39.000Z</published>
    <updated>2022-05-27T14:15:07.615Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-前言："><a href="#1-前言：" class="headerlink" title="1. 前言："></a>1. 前言：</h2><p>继续学习Spring boot～<br><span id="more"></span></p><h2 id="2-概念"><a href="#2-概念" class="headerlink" title="2. 概念"></a>2. 概念</h2><p>对Spring boot的宏观理解，它的诞生是为了简化 Spring 应用的搭建和开发过程，具有 Spring 一切优秀特性，而且使用更加简单，功能更加丰富，性能更加稳定而健壮。在Spring boot中我们使用基于注解和JAVA的配置方式，抛弃XML！</p><p>学习Spring boot最好还是对Spring、Servlet有基本的理解。</p><h3 id="2-1-三层架构"><a href="#2-1-三层架构" class="headerlink" title="2.1. 三层架构"></a>2.1. 三层架构</h3><p>注意区别MVC架构即可，MVC是表现层（Web层）的一个设计模式，真正意义的三层架构如下：</p><ul><li>A 表现层—Web层</li><li>B 业务层—Service层</li><li>C 持久层—Dao层</li></ul><h3 id="2-2-控制反转—IoC"><a href="#2-2-控制反转—IoC" class="headerlink" title="2.2. 控制反转—IoC"></a>2.2. 控制反转—IoC</h3><p>IoC——Inversion of Control，指的是将对象的创建权交给 Spring 去创建。使用 Spring 之前，对象的创建都是由我们自己在代码中new创建。而使用 Spring 之后。对象的创建都是给了 Spring 框架。控制反转可以用许多方式表达，依赖注入是其中一种方式。</p><h3 id="2-3-依赖注入—DI"><a href="#2-3-依赖注入—DI" class="headerlink" title="2.3. 依赖注入—DI"></a>2.3. 依赖注入—DI</h3><p>Dependency Injection，当某个角色(可能是一个Java实例，调用者)需要另一个角色(另一个Java实例，被调用者)的协助时，在传统的程序设计过程中，通常由调用者来创建被调用者的实例。但在Spring里，创建被调用者的工作不再由调用者来完成，因此称为控制反转;创建被调用者实例的工作通常由Spring容器来完成，然后注入调用者。</p><p>Spring框架的核心功能之一就是通过依赖注入的方式来管理Bean之间的依赖关系。DI 主要有两种变体，即通过构造函数参数形式和通过setter方法形式：</p><ol><li>Constructor-based dependency injection，当容器调用带有多个参数的构造函数类时，实现基于构造函数的 DI，每个代表在其他类中的一个依赖关系。</li><li>Setter-based dependency injection，基于 setter 方法的 DI 是通过在调用无参数的构造函数或无参数的静态工厂方法实例化 bean 之后容器调用 beans 的 setter 方法来实现的。</li></ol><h3 id="2-4-面向切面编程—AOP"><a href="#2-4-面向切面编程—AOP" class="headerlink" title="2.4.  面向切面编程—AOP"></a>2.4.  面向切面编程—AOP</h3><p>先理解什么是切面。用刀把一个西瓜分成两瓣，切开的切口就是切面；炒菜，锅与炉子共同来完成炒菜，锅与炉子就是切面。web层级设计中，web层->网关层->服务层->数据层，每一层之间也是一个切面。编程中，对象与对象之间，方法与方法之间，模块与模块之间都是一个个切面。</p><p>其他的先忽略，需要仔细研究。</p><p>Spring 框架的 AOP 模块提供了面向方面的程序设计实现，可以定义诸如方法拦截器和切入点等，从而使实现功能的代码彻底的解耦出来。</p><h3 id="2-5-容器—Container"><a href="#2-5-容器—Container" class="headerlink" title="2.5.  容器—Container"></a>2.5.  容器—Container</h3><p>Spring 是一个容器，因为它包含并且管理应用对象的生命周期。Spring 容器是 Spring 框架的核心。容器将创建对象，把它们连接在一起，配置它们，并管理他们的整个生命周期从创建到销毁。Spring 容器使用依赖注入（DI）来管理组成一个应用程序的组件。这些对象被称为 Spring Beans。在Spring中BeanFactory是IOC容器的实际代表者。</p><h3 id="2-6-“对象”—Bean"><a href="#2-6-“对象”—Bean" class="headerlink" title="2.6.  “对象”—Bean"></a>2.6.  “对象”—Bean</h3><p>Bean是一个被实例化，组装，并通过 Spring IoC 容器所管理的对象。这些 bean 是由用容器提供的配置元数据创建的。把配置元数据提供给 Spring 容器有基于XML、注解和JAVA三种方式。Bean的重要属性有作用域（scope）、初始化与销毁和与依赖注入相关的constructor-arg、properties、autowiring mode等。</p><h3 id="2-7-自动装配—Autowire"><a href="#2-7-自动装配—Autowire" class="headerlink" title="2.7.  自动装配—Autowire"></a>2.7.  自动装配—Autowire</h3><p>Spring 容器可以在不使用<code>&lt;constructor-arg&gt;</code>和<code>&lt;property&gt;</code>元素的情况下<strong>自动装配</strong>相互协作的 bean 之间的关系。Spring中可以使用<code>&lt;bean&gt;</code>元素的<strong>autowire</strong>属性为一个 bean 定义指定自动装配模式，选择由属性名或由属性数据类型自动装配。</p><p>以由属性名自动装配举例，在 XML 配置文件中 beans 的<em>auto-wire</em>属性设置为<em>byName</em>。然后尝试将它的属性与配置文件中定义为相同名称的 beans 进行匹配和连接。如果找到匹配项，它将注入这些 beans，否则，它将抛出异常。总的来说，如果在一个类A中使用了另一个类B作为属性，若不想显式的对B进行绑定，就可以使用自动装配。</p><p>Spring采用基于注解的方式配置时，@Autowired是最常用的注解之一，这个注解的功能就是为我们注入一个定义好的 bean。</p><h3 id="2-8-Spring-boot-starter"><a href="#2-8-Spring-boot-starter" class="headerlink" title="2.8. Spring boot starter"></a>2.8. Spring boot starter</h3><p>Spring Boot 将日常企业应用研发中的各种场景都抽取出来，做成一个个的 starter（启动器），starter 中整合了该场景下各种可能用到的依赖，用户只需要在 Maven 中引入 starter 依赖，SpringBoot 就能自动扫描到要加载的信息并启动相应的默认配置。starter 提供了大量的自动配置，让用户摆脱了处理各种依赖和配置的困扰。所有这些 starter 都遵循着约定成俗的默认配置，并允许用户调整这些配置，即遵循“约定大于配置”的原则。</p><p>以 spring-boot-starter-web 为例，它能够为提供 Web 开发场景所需要的几乎所有依赖，因此在使用 Spring Boot 开发 Web 项目时，只需要引入该 Starter 即可，而不需要额外导入 Web 服务器和其他的 Web 依赖。</p><p>spring-boot-starter-parent 是所有 Spring Boot 项目的父级依赖，它被称为 Spring Boot 的版本仲裁中心，可以对项目内的部分常用依赖进行统一管理。Spring Boot 项目可以通过继承 spring-boot-starter-parent 来获得一些合理的默认配置如默认 JDK 版本、默认字符集、依赖管理功能、资源过滤、默认插件配置、识别 application.properties（或yml）类型的配置文件。</p><h3 id="2-9-配置文件—Spring-boot-profile"><a href="#2-9-配置文件—Spring-boot-profile" class="headerlink" title="2.9. 配置文件—Spring boot profile"></a>2.9. 配置文件—Spring boot profile</h3><p>在实际的项目开发中，一个项目通常会存在多个环境，例如，开发环境、测试环境和生产环境等，不同环境的配置也不尽相同。</p><p>Spring Boot 的配置文件共有两种形式：.properties  文件和 .yml 文件，不管哪种形式，它们都能通过文件名的命名形式区分出不同的环境的配置，文件命名格式为：</p><p>application-{profile}.properties/yml</p><p>其中，{profile} 一般为各个环境的名称或简称，例如 dev、test 和 prod 等等。</p><h4 id="2-9-1-默认配置文件"><a href="#2-9-1-默认配置文件" class="headerlink" title="2.9.1. 默认配置文件"></a>2.9.1. 默认配置文件</h4><p>通常情况下，Spring Boot 在启动时会将 resources 目录下的 application.properties 或 apllication.yml 作为其默认配置文件，我们可以在该配置文件中对项目进行配置，但这并不意味着 Spring Boot 项目中只能存在一个 application.properties 或 application.yml。</p><h4 id="2-9-2-外部配置文件"><a href="#2-9-2-外部配置文件" class="headerlink" title="2.9.2. 外部配置文件"></a>2.9.2. 外部配置文件</h4><p>除了默认配置文件，Spring Boot 还可以加载一些位于项目外部的配置文件。我们可以通过如下 2 个参数，指定外部配置文件的路径：</p><ul><li>spring.config.location</li><li>spring.config.additional-location</li></ul><p>我们可以先将 Spring Boot 项目打包成 JAR 文件，然后在命令行启动命令中，使用命令行参数 —spring.config.location，指定外部配置文件的路径。</p><p><code>java -jar &#123;JAR&#125;  --spring.config.location=&#123;外部配置文件全路径&#125;</code></p><p>需要注意的是，使用该参数指定配置文件后，会使项目默认配置文件（application.properties 或 application.yml ）失效，Spring Boot 将只加载指定的外部配置文件。</p><h3 id="2-10-Spring-boot-配置"><a href="#2-10-Spring-boot-配置" class="headerlink" title="2.10. Spring boot 配置"></a>2.10. Spring boot 配置</h3><p>Spring Boot 不仅可以通过配置文件进行配置，还可以通过环境变量、命令行参数等多种形式进行配置。这些配置都可以让开发人员在不修改任何代码的前提下，直接将一套 Spring Boot  应用程序在不同的环境中运行。</p><h4 id="2-10-1-配置加载顺序"><a href="#2-10-1-配置加载顺序" class="headerlink" title="2.10.1. 配置加载顺序"></a>2.10.1. 配置加载顺序</h4><ol><li>命令行参数</li><li>来自 java:comp/env 的 JNDI 属性</li><li>Java 系统属性（System.getProperties()）</li><li>操作系统环境变量</li><li>RandomValuePropertySource 配置的 random.* 属性值</li><li>配置文件（YAML 文件、Properties 文件）</li><li>@Configuration 注解类上的 @PropertySource 指定的配置文件</li><li>通过 SpringApplication.setDefaultProperties 指定的默认属性</li></ol><p>以上所有形式的配置都会被加载，当存在相同配置内容时，高优先级的配置会覆盖低优先级的配置；存在不同的配置内容时，高优先级和低优先级的配置内容取并集，共同生效，形成互补配置。同一位置下，Properties 文件优先级高于 YAML 文件。</p><h4 id="2-10-2-自动配置"><a href="#2-10-2-自动配置" class="headerlink" title="2.10.2. 自动配置"></a>2.10.2. 自动配置</h4><p>Spring Boot 的<strong>自动配置</strong>是基于 Spring Factories 机制实现的。</p><p>Spring Factories 机制是 Spring Boot 中的一种服务发现机制，这种扩展机制与 Java SPI 机制十分相似。Spring Boot 会自动扫描所有 Jar 包类路径下 META-INF/spring.factories 文件，并读取其中的内容，进行实例化，这种机制也是 Spring Boot Starter 的基础。具体来说，spring-core 包里定义了 SpringFactoriesLoader 类，这个类会扫描所有 Jar 包类路径下的 META-INF/spring.factories 文件，并获取指定接口的配置。</p><p>spring.factories 文件本质上与 properties 文件相似，其中包含一组或多组键值对（key=vlaue），其中，key 的取值为接口的完全限定名；value 的取值为接口实现类的完全限定名，一个接口可以设置多个实现类，不同实现类之间使用“，”隔开。</p><p>基于以上，Spring boot的<strong>自动配置</strong>也是通过同样方式实现的，在 spring-boot-autoconfigure-xxx.jar 类路径下的 META-INF/spring.factories 中设置了 Spring Boot 自动配置的内容。</p><h3 id="2-11-拦截器"><a href="#2-11-拦截器" class="headerlink" title="2.11. 拦截器"></a>2.11. 拦截器</h3><p>拦截器可以根据 URL 对请求进行拦截，主要应用于登陆校验、权限验证、乱码解决、性能监控和异常处理等功能上。</p><p>在 Spring Boot 项目中，使用拦截器功能通常需要以下 3 步：</p><ol><li>定义拦截器；</li><li>注册拦截器；</li><li>指定拦截规则（如果是拦截所有，静态资源也会被拦截）。</li></ol><h4 id="2-11-1-定义拦截器"><a href="#2-11-1-定义拦截器" class="headerlink" title="2.11.1. 定义拦截器"></a>2.11.1. 定义拦截器</h4><p>只需要创建一个拦截器类，并实现 HandlerInterceptor 接口即可，该接口中定义以下 3 个方法（按需重写即可）：</p><ol><li>boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler)，该方法在控制器处理请求方法前执行，其返回值表示是否中断后续操作，返回 true 表示继续向下执行，返回 false 表示中断后续操作。</li><li>void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView)，该方法在控制器处理请求方法调用之后、解析视图之前执行，可以通过此方法对请求域中的模型和视图做进一步修改。</li><li>void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex)，该方法在视图渲染结束后执行，可以通过此方法实现资源清理、记录日志信息等工作。</li></ol><h4 id="2-11-2-注册拦截器"><a href="#2-11-2-注册拦截器" class="headerlink" title="2.11.2. 注册拦截器"></a>2.11.2. 注册拦截器</h4><p>创建一个实现了 WebMvcConfigurer 接口的配置类（使用了 @Configuration 注解的类），重写 addInterceptors() 方法，并在该方法中调用 registry.addInterceptor() 方法将自定义的拦截器注册到容器中。示例：</p><pre><code>@ConfigurationpublicclassMyMvcConfigimplementsWebMvcConfigurer&#123;    ......    @Override    public void addInterceptors(InterceptorRegistryregistry)&#123;      registry.addInterceptor(newLoginInterceptor());    &#125;&#125;</code></pre><h4 id="2-11-3-指定拦截规则"><a href="#2-11-3-指定拦截规则" class="headerlink" title="2.11.3. 指定拦截规则"></a>2.11.3. 指定拦截规则</h4><p>在指定拦截器拦截规则时，可以调用两个方法，说明如下：</p><ul><li>addPathPatterns：该方法用于指定拦截路径，例如拦截路径为<code>/**</code>，表示拦截所有请求，包括对静态资源的请求。</li><li>excludePathPatterns：该方法用于排除拦截路径，即指定不需要被拦截器拦截的请求。</li></ul><p>示例：</p><p><code>registry.addInterceptor(newLoginInterceptor()).addPathPatterns(&quot;/**&quot;)</code></p><p>可以链式调用。</p><h2 id="3-常见注解介绍"><a href="#3-常见注解介绍" class="headerlink" title="3. 常见注解介绍"></a>3. 常见注解介绍</h2><h3 id="3-1-Autowired"><a href="#3-1-Autowired" class="headerlink" title="3.1. @Autowired"></a>3.1. @Autowired</h3><p>这个注解是属于 Spring 的容器配置的一个注解，与它同属容器配置的注解还有：@Required,@Primary, @Qualifier 等等。</p><p>在 Spring 的世界当中，自动装配指的就是使用将 Spring 容器中的 bean 自动的和我们需要这个 bean 的类组装在一起，注入一个定义好的 bean。</p><p>用法：</p><ol><li>应用于字段</li><li>应用于构造函数</li><li>应用于 setter 方法</li><li>应用于具有任意名称和多个参数的方法</li><li>添加到需要该类型数组（或容器）的字段或方法，则 Spring 会从 ApplicationContext 中搜寻符合指定类型的所有 bean</li></ol><h3 id="3-2-RestController"><a href="#3-2-RestController" class="headerlink" title="3.2. @RestController"></a>3.2. @RestController</h3><p>用于标注控制层组件。在 Spring Boot 中，@Controller 注解是专门用于处理 Http 请求处理的，是以 MVC 为核心的设计思想的控制层。@RestController 则是 @Controller 的衍生注解，都是用来表示Spring某个类的是否可以接收HTTP请求。</p><p>@RestController是@Controller和@ResponseBody的结合体，两个标注合并起来的作用。@Controller类中的方法可以直接通过返回String跳转到jsp、ftl、html等模版页面。在方法上加@ResponseBody注解，也可以返回实体对象。@RestController类中的所有方法只能返回String、Object、Json等实体对象，不能跳转到模版页面。</p><p>用法（感觉有点问题）：</p><p>@Controller: 一般应用在有返回界面的应用场景下.例如，管理后台使用了 thymeleaf 作为模板开发，需要从后台直接返回 Model 对象到前台，那么这时候就需要使用 @Controller 来注解。</p><p>@RestController: 如果只是接口，那么就用 RestController 来注解.如前端页面全部使用了 Html、Jquery来开发，通过 Ajax 请求服务端接口，那么接口就使用 @RestController 统一注解。</p><h3 id="3-3-…Mapping"><a href="#3-3-…Mapping" class="headerlink" title="3.3. @…Mapping"></a>3.3. @…Mapping</h3><p>表示路由请求，可以设置各种操作方法。最基本的是@RequestMapping，@GetMapping、@PostMapping、@PutMapping、@DeleteMapping 是 @RequestMapping 的子集，分别表示用不同的请求方式的对应路由。</p><p>举例：</p><p><code>@RequestMapping(value=&quot;/add&quot;,method = RequestMethod.POST),params=&quot;myParam=xyz&quot;</code> 表示路由为 /add 的 POST 请求，但仅仅处理头部包括 myParam=xyz 的请求。</p><p><code>@GetMapping(&quot;/add&quot;)等价于@RequestMapping(method = RequestMethod.GET,value = &quot;/add&quot;)</code></p><h3 id="3-4-Configuration-和-Bean"><a href="#3-4-Configuration-和-Bean" class="headerlink" title="3.4. @Configuration 和 @Bean"></a>3.4. @Configuration 和 @Bean</h3><p>带有<strong>@Configuration</strong>的注解类表示这个类可以使用 Spring IoC 容器作为 bean 定义的来源。<strong>@Bean</strong>注解告诉 Spring，一个带有 @Bean 的注解方法将返回一个对象，该对象应该被注册为在 Spring 应用程序上下文中的 bean。例如：</p><pre><code>@Configurationpublic class HelloWorldConfig&#123;    @Bean     public HelloWorld helloWorld()&#123;        return new HelloWorld();       &#125;&#125;</code></pre><h3 id="3-5-SpringBootApplication"><a href="#3-5-SpringBootApplication" class="headerlink" title="3.5. @SpringBootApplication"></a>3.5. @SpringBootApplication</h3><p>所有 Spring Boot 项目的主启动程序类上都使用了一个 @SpringBootApplication 注解，该注解是 Spring Boot 中最重要的注解之一 ，也是 Spring Boot 实现自动化配置的关键。</p><p>@SpringBootApplication 是一个组合元注解，其主要包含两个注解：<strong>@SpringBootConfiguration</strong> 和 <strong>@EnableAutoConfiguration</strong>，其中 @EnableAutoConfiguration 注解是 SpringBoot 自动化配置的核心所在。</p><h4 id="3-5-1-EnableAutoConfiguration"><a href="#3-5-1-EnableAutoConfiguration" class="headerlink" title="3.5.1. @EnableAutoConfiguration"></a>3.5.1. @EnableAutoConfiguration</h4><p>@EnableAutoConfiguration 注解用于开启 Spring Boot 的自动配置功能， 它使用 Spring 框架提供的 @Import 注解通过 AutoConfigurationImportSelector类（选择器）给容器中导入自动配置组件。</p><h4 id="3-5-2-SpringBootConfiguration"><a href="#3-5-2-SpringBootConfiguration" class="headerlink" title="3.5.2. @SpringBootConfiguration"></a>3.5.2. @SpringBootConfiguration</h4><p>@SpringBootConfiguration继承自@Configuration，二者功能也一致，标注当前类是配置类，并会将当前类内声明的一个或多个以@Bean注解标记的方法的实例纳入到spring容器中，并且实例名就是方法名。</p><h3 id="3-6-RequestBody"><a href="#3-6-RequestBody" class="headerlink" title="3.6. @RequestBody"></a>3.6. @RequestBody</h3><p>@RequestBody主要用来接收前端传递给后端的json字符串中的数据(请求体中的数据)；而最常用的使用请求体传参的无疑是POST请求了，所以使用@RequestBody接收数据时，一般都用POST方式进行提交。<br>在后端的同一个接收方法里，@RequestBody与@RequestParam()可以同时使用，@RequestBody最多只能有一个，而@RequestParam()可以有多个。</p><h3 id="3-7-Service"><a href="#3-7-Service" class="headerlink" title="3.7. @Service"></a>3.7. @Service</h3><p>对于业务层（service层）的类，在类上用 @Service 注解声明，表示是业务层组件。SpringBoot会将标注类自动注册到 Spring 容器中，可以通过指定value参数更改名称。<br>SpringBoot的企业级开发中经常采用Service+ServiceImpl的结构，一开始大多数项目都是直接在业务处理层的Service类中嵌入JDBC代码，这就使得这个Service类与数据库紧耦合，在换一种数据库后，就要修改Service类中的sql。于是就有了Controller+Service+ServiceImpl，Service类设计成一个接口，使控制层只依赖这个接口，这样，当某天这个应用要跑在其它数据库上时，就而只需要增加一个serviceImpl类。</p><h3 id="3-8-CrossOrigin"><a href="#3-8-CrossOrigin" class="headerlink" title="3.8. @CrossOrigin"></a>3.8. @CrossOrigin</h3><p>该注解用于解决跨域问题，可以有以下使用方法：</p><ol><li>对@Controller中的方法使用</li><li>对@Controller使用</li><li>同时对Controller和其中的方法使用</li></ol><p>@CrossOrigin中的2个参数：</p><ul><li>origins： 允许可访问的域列表</li><li>maxAge:准备响应前的缓存持续的最大时间（以秒为单位）</li></ul><p>除了这种细粒度的注解配置方式外，SpringBoot还提供全局配置的方式。<br><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">@Configuration</span><br><span class="line">public <span class="keyword">class</span> CorsConfig &#123;</span><br><span class="line">    @Bean</span><br><span class="line">    public CorsFilter cors<span class="constructor">Filter()</span> &#123;</span><br><span class="line">        UrlBasedCorsConfigurationSource source = <span class="keyword">new</span> <span class="constructor">UrlBasedCorsConfigurationSource()</span>;</span><br><span class="line">        CorsConfiguration corsConfiguration = <span class="keyword">new</span> <span class="constructor">CorsConfiguration()</span>;</span><br><span class="line">        <span class="comment">//允许所有源</span></span><br><span class="line">        corsConfiguration.add<span class="constructor">AllowedOrigin(<span class="string">&quot;*&quot;</span>)</span>;</span><br><span class="line">        <span class="comment">//允许所有请求头</span></span><br><span class="line">        corsConfiguration.add<span class="constructor">AllowedHeader(<span class="string">&quot;*&quot;</span>)</span>;</span><br><span class="line">        <span class="comment">//允许所有方法</span></span><br><span class="line">        corsConfiguration.add<span class="constructor">AllowedMethod(<span class="string">&quot;*&quot;</span>)</span>;</span><br><span class="line">        <span class="comment">//允许跨域cookies</span></span><br><span class="line">        corsConfiguration.set<span class="constructor">AllowCredentials(<span class="params">true</span>)</span>;</span><br><span class="line">        source.register<span class="constructor">CorsConfiguration(<span class="string">&quot;/**&quot;</span>, <span class="params">corsConfiguration</span>)</span>;</span><br><span class="line">        return <span class="keyword">new</span> <span class="constructor">CorsFilter(<span class="params">source</span>)</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-前言：&quot;&gt;&lt;a href=&quot;#1-前言：&quot; class=&quot;headerlink&quot; title=&quot;1. 前言：&quot;&gt;&lt;/a&gt;1. 前言：&lt;/h2&gt;&lt;p&gt;继续学习Spring boot～&lt;br&gt;</summary>
    
    
    
    
    <category term="Java" scheme="http://silencezheng.top/tags/Java/"/>
    
    <category term="Spring" scheme="http://silencezheng.top/tags/Spring/"/>
    
  </entry>
  
  <entry>
    <title>数据操作及数学基础--《动手学深度学习》笔记0x02</title>
    <link href="http://silencezheng.top/2022/05/05/article34/"/>
    <id>http://silencezheng.top/2022/05/05/article34/</id>
    <published>2022-05-04T16:49:29.000Z</published>
    <updated>2022-05-27T14:15:16.255Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><p>这一笔记对应预备知识章节，包括数据操作、数据预处理、线性代数、微积分、概率论等。其中很多知识在学数一的时候都更深入理解过了，但是现在发现忘的差不多了，哎～</p><p>对应实践：<a href="https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x02.ipynb">https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x02.ipynb</a></p><p>在M1芯片的设备上使用miniforge安装pytorch：<code>conda install -c pytorch pytorch</code><br><span id="more"></span></p><h2 id="1-数据操作"><a href="#1-数据操作" class="headerlink" title="1. 数据操作"></a>1. 数据操作</h2><h3 id="1-1-概念"><a href="#1-1-概念" class="headerlink" title="1.1. 概念"></a>1.1. 概念</h3><h4 id="1-1-1-张量（tensor）"><a href="#1-1-1-张量（tensor）" class="headerlink" title="1.1.1 张量（tensor）"></a>1.1.1 张量（tensor）</h4><p>即n维数组，无论使用哪个深度学习框架，它的<em>张量类</em>（在MXNet中为<code>ndarray</code>， 在PyTorch和TensorFlow中为<code>Tensor</code>）都与Numpy的<code>ndarray</code>类似。 但深度学习框架又比Numpy的<code>ndarray</code>多一些重要功能： 首先，GPU很好地支持加速计算，而NumPy仅支持CPU计算； 其次，张量类支持自动微分。 这些功能使得张量类更适合深度学习。</p><p>张量表示由一个数值组成的数组，这个数组可能有多个维度。 具有一个轴的张量对应数学上的<em>向量</em>（vector）； 具有两个轴的张量对应数学上的<em>矩阵</em>（matrix）； 具有两个轴以上的张量没有特殊的数学名称。张量中的每个值都称为张量的<em>元素</em>（element）。</p><h4 id="1-1-2-运算符"><a href="#1-1-2-运算符" class="headerlink" title="1.1.2. 运算符"></a>1.1.2. 运算符</h4><p>我们的兴趣不仅限于读取数据和写入数据。 我们想在这些数据上执行数学运算，其中最简单且最有用的操作是<em>按元素</em>（elementwise）运算。 它们将标准标量运算符应用于数组的<strong>每个元素</strong>。 对于将两个数组作为输入的函数，按元素运算将二元运算符应用于两个数组中的<strong>每对位置对应的元素</strong>。 我们可以基于任何从标量到标量的函数来创建按元素函数。</p><p>对于任意具有相同形状的张量， 常见的标准算术运算符（<code>+</code>、<code>-</code>、<code>*</code>、<code>/</code>和<code>**</code>）都可以被升级为按元素运算。 我们可以在同一形状的任意两个张量上调用按元素操作。</p><h4 id="1-1-3-广播机制"><a href="#1-1-3-广播机制" class="headerlink" title="1.1.3. 广播机制"></a>1.1.3. 广播机制</h4><p>在上面的部分中，我们看到了如何在相同形状的两个张量上执行按元素操作。 在某些情况下，即使形状不同，我们仍然可以通过调用<em>广播机制</em>（broadcasting mechanism）来执行按元素操作。 这种机制的工作方式如下：首先，通过适当复制元素来扩展一个或两个数组， 以便在转换之后，两个张量具有相同的形状。 其次，对生成的数组执行按元素操作。</p><h4 id="1-1-4-索引和切片"><a href="#1-1-4-索引和切片" class="headerlink" title="1.1.4. 索引和切片"></a>1.1.4. 索引和切片</h4><p>就像在任何其他Python数组中一样，张量中的元素可以通过索引访问。 与任何Python数组一样：第一个元素的索引是0，最后一个元素索引是-1； 可以指定范围以包含第一个元素和最后一个之前的元素。例如：我们可以用[-1]选择最后一个元素，可以用[1:3]选择第二个和第三个元素。</p><h4 id="1-1-5-内存变动"><a href="#1-1-5-内存变动" class="headerlink" title="1.1.5. 内存变动"></a>1.1.5. 内存变动</h4><p>运行一些操作可能会导致为新结果分配内存。 例如，如果我们用Y=X+Y，我们将取消引用Y指向的张量，而是指向新分配的内存处的张量。Python的id()函数给我们提供了内存中引用对象的确切地址。 运行Y=Y+X后，我们会发现id(Y)指向另一个位置。 这是因为Python首先计算Y+X，为结果分配新的内存，然后使Y指向内存中的这个新位置。</p><p>我们不希望内存在不必要时发生重新分配的情况，原因有两个：首先，我们不想总是不必要地分配内存。 在机器学习中，我们可能有数百兆的参数，并且在一秒内多次更新所有参数。 通常情况下，我们希望原地执行这些更新。 其次，如果我们不原地更新，其他引用仍然会指向旧的内存位置， 这样我们的某些代码可能会无意中引用旧的参数。</p><p>在这种情况下，我们可以使用切片表示法将操作的结果分配给先前分配的数组，例如<code>Y[:]=&lt;expression&gt;</code>。</p><p>在节省内存开销方面，如果在后续计算中没有重复使用X， 我们也可以使用<code>X[:]=X+Y</code>或<code>X+=Y</code>来减少操作的内存开销。</p><h4 id="1-1-6-对象转换"><a href="#1-1-6-对象转换" class="headerlink" title="1.1.6. 对象转换"></a>1.1.6. 对象转换</h4><p>将深度学习框架定义的张量转换为NumPy张量（ndarray）很容易，反之也同样容易。 注意torch张量和numpy数组将共享它们的底层内存，就地操作更改一个张量也会同时更改另一个张量。</p><p>要将大小为1的张量转换为Python标量，可以调用item函数或Python的内置函数。</p><h3 id="1-2-以PyTorch为例，列举常见操作"><a href="#1-2-以PyTorch为例，列举常见操作" class="headerlink" title="1.2. 以PyTorch为例，列举常见操作"></a>1.2. 以PyTorch为例，列举常见操作</h3><p>虽然它被称为PyTorch，但是代码中使用torch而不是pytorch。</p><p><code>import torch</code></p><p>首先，我们可以使用arange创建一个行向量x。这个行向量包含以0开始的前12个整数，它们默认创建为整数。也可指定创建类型为浮点数。例如，张量x中有 12 个元素。除非额外指定，新的张量将存储在内存中，并采用基于CPU的计算。</p><p><code>x=torch.arange(12)</code></p><p>可以通过张量的shape属性来访问张量（沿每个轴的长度）的<em>形状</em>。</p><p><code>x.shape</code></p><p>如果只想知道张量中元素的总数，即形状的所有元素乘积，可以检查它的大小（size）。 因为这里在处理的是一个向量，所以它的shape与它的size相同。</p><p><code>x.numel()</code></p><p>要想改变一个张量的形状而不改变元素数量和元素值，可以调用reshape函数。 例如，可以把张量x从形状为（12,）的行向量转换为形状为（3,4）的矩阵。 这个新的张量包含与转换前相同的值，但是它被看成一个3行4列的矩阵。 要重点说明一下，虽然张量的形状发生了改变，但其元素值并没有变。 注意，通过改变张量的形状，张量的大小（size）不会改变。我们可以通过-1来调用此自动计算出维度的功能。 即我们可以用<code>x.reshape(-1,4)</code>或<code>x.reshape(3,-1)</code>来取代<code>x.reshape(3,4)</code>。</p><p><code>X=x.reshape(3,4)</code></p><p>有时，我们希望使用全0、全1、其他常量，或者从特定分布中随机采样的数字来初始化矩阵。 我们可以创建一个形状为（2,3,4）的张量，其中所有元素都设置为0。<code>torch.ones((2,3,4))</code>则创建元素全为1的张量，但默认数据类型为float。</p><pre><code>torch.zeros((2,3,4))#结果：tensor([[[0.,0.,0.,0.],         [0.,0.,0.,0.],         [0.,0.,0.,0.]],        [[0.,0.,0.,0.],         [0.,0.,0.,0.],         [0.,0.,0.,0.]]])</code></pre><p>有时我们想通过从某个特定的概率分布中随机采样来得到张量中每个元素的值。 例如，当我们构造数组来作为神经网络中的参数时，我们通常会随机初始化参数的值。 以下代码创建一个形状为（3,4）的张量。 其中的每个元素都从均值为0、标准差为1的标准高斯分布（正态分布）中随机采样。</p><p><code>torch.randn(3,4)</code></p><p>我们还可以通过提供包含数值的Python列表（或嵌套列表），来为所需张量中的每个元素赋予确定值。 在这里，最外层的列表对应于轴0，内层的列表对应于轴1。</p><p><code>torch.tensor([[2,1,4,3],[1,2,3,4],[4,3,2,1]])</code></p><p>在下面的例子中，我们使用逗号来表示一个具有5个元素的元组，其中每个元素都是按元素操作的结果。</p><pre><code>x=torch.tensor([1.0,2,4,8])y=torch.tensor([2,2,2,2])x+y,x-y,x*y,x/y,x**y# **运算符是求幂运算#结果：(tensor([3.,4.,6.,10.]),tensor([-1.,0.,2.,6.]),tensor([2.,4.,8.,16.]),tensor([0.5000,1.0000,2.0000,4.0000]),tensor([1.,4.,16.,64.]))</code></pre><p>“按元素”方式可以应用更多的计算，包括像求幂这样的一元运算符。</p><p><code>torch.exp(x)</code></p><p>我们也可以把多个张量<em>连结</em>（concatenate）在一起， 把它们端对端地叠起来形成一个更大的张量。 我们只需要提供张量列表，并给出沿哪个轴连结。 下面的例子分别演示了当我们沿行（轴-0，形状的第一个元素） 和按列（轴-1，形状的第二个元素）连结两个矩阵时，会发生什么情况。 我们可以看到，第一个输出张量的轴-0长度（6）是两个输入张量轴-0长度的总和（3+3）； 第二个输出张量的轴-1长度（8）是两个输入张量轴-1长度的总和（4+4）。</p><pre><code>X=torch.arange(12,dtype=torch.float32).reshape((3,4))Y=torch.tensor([[2.0,1,4,3],[1,2,3,4],[4,3,2,1]])torch.cat((X,Y),dim=0),torch.cat((X,Y),dim=1)#结果：(tensor([[0.,1.,2.,3.],[4.,5.,6.,7.],[8.,9.,10.,11.],[2.,1.,4.,3.],[1.,2.,3.,4.],[4.,3.,2.,1.]]),tensor([[0.,1.,2.,3.,2.,1.,4.,3.],[4.,5.,6.,7.,1.,2.,3.,4.],[8.,9.,10.,11.,4.,3.,2.,1.]]))</code></pre><p>有时，我们想通过<em>逻辑运算符</em>构建二元张量。 以<code>X==Y</code>为例： 对于每个位置，如果X和Y在该位置相等，则新张量中相应项的值为1。 这意味着逻辑语句X==Y在该位置处为真，否则该位置为0。</p><pre><code>X==Y#结果：tensor([[False,True,False,True],[False,False,False,False],[False,False,False,False]])</code></pre><p>对张量中的所有元素进行求和，会产生一个单元素张量。</p><pre><code>X.sum()#结果：tensor(66.)</code></pre><p>在大多数情况下，我们将沿着数组中长度为1的轴进行广播，如下例子：</p><pre><code>a=torch.arange(3).reshape((3,1))b=torch.arange(2).reshape((1,2))a,b#结果：(tensor([[0],[1],[2]]),tensor([[0,1]]))</code></pre><p>由于a和b分别是3×1和1×2矩阵，如果让它们相加，它们的形状不匹配。 我们将两个矩阵<em>广播</em>为一个更大的3×2矩阵，如下所示：矩阵a将复制列， 矩阵b将复制行，然后再按元素相加。</p><pre><code>a+b#结果：tensor([[0,1],[1,2],[2,3]])</code></pre><p>如下所示，我们可以用[-1]选择最后一个元素，可以用[1:3]选择第二个和第三个元素：</p><p><code>X[-1],X[1:3]</code></p><p>除读取外，我们还可以通过指定索引来将元素写入矩阵。</p><p><code>X[1,2]=9</code></p><p>如果我们想为多个元素赋值相同的值，我们只需要索引所有元素，然后为它们赋值。 例如，[0:2,:]访问第1行和第2行，其中“:”代表沿轴1（列）的所有元素。虽然我们讨论的是矩阵的索引，但这也适用于向量和超过2个维度的张量。</p><p><code>X[0:2,:]=12</code></p><p>将深度学习框架定义的张量转换为NumPy张量（ndarray）很容易，反之也同样容易。 torch张量和numpy数组将共享它们的底层内存，就地操作更改一个张量也会同时更改另一个张量。</p><pre><code>A=X.numpy()B=torch.tensor(A)type(A),type(B)#结果：(numpy.ndarray,torch.Tensor)</code></pre><p>要将大小为1的张量转换为Python标量，我们可以调用<code>item</code>函数或Python的内置函数。</p><pre><code>a=torch.tensor([3.5])a,a.item(),float(a),int(a)#结果：(tensor([3.5000]),3.5,3.5,3)</code></pre><h2 id="2-数据预处理"><a href="#2-数据预处理" class="headerlink" title="2. 数据预处理"></a>2. 数据预处理</h2><p>在Python中常用的数据分析工具中，我们通常使用pandas软件包。 像庞大的Python生态系统中的许多其他扩展包一样，pandas可以与张量兼容。 本节我们将简要介绍使用pandas预处理原始数据，并将原始数据转换为张量格式的步骤。</p><h3 id="2-1-读取数据集"><a href="#2-1-读取数据集" class="headerlink" title="2.1. 读取数据集"></a>2.1. 读取数据集</h3><p>要从创建的CSV文件中加载原始数据集，我们导入pandas包并调用read_csv函数。</p><pre><code>data=pd.read_csv(data_file)print(data)#结果：   NumRooms  Alley0   NaN      Pave1   2.0      NaN2   4.0      NaN3   NaN      NaN</code></pre><h3 id="2-2-处理缺失值"><a href="#2-2-处理缺失值" class="headerlink" title="2.2. 处理缺失值"></a>2.2. 处理缺失值</h3><p>注意，“NaN”项代表缺失值。 为了处理缺失的数据，典型的方法包括<em>插值法</em>和<em>删除法</em>， 其中插值法用一个替代值弥补缺失值，而删除法则直接忽略缺失值。 </p><p>在对上面例子的处理中，我们将考虑插值法。通过位置索引iloc，我们将data分成inputs和outputs， 其中前者为data的前两列，而后者为data的最后一列。 对于inputs中缺少的数值，我们用同一列的均值替换“NaN”项。</p><pre><code>inputs,outputs=data.iloc[:,0:2],data.iloc[:,2]inputs=inputs.fillna(inputs.mean())print(inputs)#结果：  NumRooms  Alley0   3.0      Pave1   2.0      NaN2   4.0      NaN3   3.0      NaN</code></pre><p>对于inputs中的类别值或离散值，我们将“NaN”视为一个类别。 由于“巷子类型”（“Alley”）列只接受两种类型的类别值“Pave”和“NaN”，pandas可以自动将此列转换为两列“Alley_Pave”和“Alley_nan”。 巷子类型为“Pave”的行会将“Alley_Pave”的值设置为1，“Alley_nan”的值设置为0。 缺少巷子类型的行会将“Alley_Pave”和“Alley_nan”分别设置为0和1。Pandas中的get_dummies方法主要用于对类别型特征做<strong>One-Hot</strong>编码。dummy_na参数默认为False，增加一列表示空缺值，如果为False就忽略空缺值。</p><pre><code>inputs=pd.get_dummies(inputs,dummy_na=True)print(inputs)#结果：  NumRooms  Alley_Pave  Alley_nan0   3.0              1          01   2.0              0          12   4.0              0          13   3.0              0          1</code></pre><h3 id="2-3-转换为张量"><a href="#2-3-转换为张量" class="headerlink" title="2.3. 转换为张量"></a>2.3. 转换为张量</h3><p>现在inputs和outputs中的所有条目都是数值类型，它们需要转换为张量格式进行下一步操作。</p><pre><code>import torchX,y=torch.tensor(inputs.values),torch.tensor(outputs.values)</code></pre><h2 id="3-线性代数"><a href="#3-线性代数" class="headerlink" title="3. 线性代数"></a>3. 线性代数</h2><h3 id="3-1-基础概念"><a href="#3-1-基础概念" class="headerlink" title="3.1. 基础概念"></a>3.1. 基础概念</h3><p><strong>标量（scalar）</strong>：称仅包含一个数值的叫<em>标量</em>。标量由只有一个元素的张量表示时如<code>x=torch.tensor(3.0)</code>。</p><p><strong>变量（variable）</strong>：符号（x、y等）称为<em>变量</em>，它们表示未知的标量值。</p><p><strong>向量（vector）</strong>：可以将向量视为标量值组成的列表。 我们将这些标量值称为向量的<em>元素</em>（element）或<em>分量</em>（component）。向量的长度通常称为向量的<em>维度</em>。 当向量表示数据集中的样本时，它们的值具有一定的现实意义。例如，如果我们正在研究医院患者可能面临的心脏病发作风险，我们可能会用一个向量来表示每个患者， 其分量为最近的生命体征、胆固醇水平、每天运动时间等。我们通过一维张量处理向量，其长度任意，通过张量的索引来访问向量中任一元素。</p><p><strong>维度（dimension）</strong>：<em>向量</em>或<em>轴</em>的维度被用来表示<em>向量</em>或<em>轴</em>的长度，即向量或轴的元素数量。 然而，张量的维度用来表示张量具有的轴数。 在这个意义上，张量的某个轴的维数就是这个轴的长度。</p><p><strong>矩阵（matrix）</strong>：正如向量将标量从零阶推广到一阶，矩阵将向量从一阶推广到二阶，在代码中表示为具有两个轴的张量。当调用函数来实例化张量时， 我们可以通过指定两个分量m和n来创建一个形状为m×n的矩阵，如<code>A=torch.arange(20).reshape(5,4)</code>。在代码中访问矩阵的转置<code>A.T</code>。</p><p><strong>张量（tensor）</strong>：在线性代数中指代数对象，为我们提供了描述具有任意数量轴的n维数组的通用方法。向量是一阶张量，矩阵是二阶张量。它们的索引机制与矩阵类似。</p><p><strong>点积（dot product）</strong>：也就是内积，给定两个向量x,y∈Rd， 它们的<em>点积</em>（dot product）x⊤y（或⟨x,y⟩） 是相同位置的按元素乘积的和。点积在很多场合都很有用。 例如，给定一组由向量x∈Rd表示的值， 和一组由w∈Rd表示的权重。x中的值根据权重w的加权和， 可以表示为点积x⊤w。 当权重为非负数且和为1（即(∑i=1dwi=1)）时， 点积表示<em>加权平均</em>（weighted average）。 将两个向量规范化得到单位长度后，点积表示它们夹角的余弦。 在代码中使用<code>torch.dot(x,y)</code>表示，注意点积中只接收向量（1维）。</p><p><strong>矩阵-向量积（matrix-vector product）</strong>：将矩阵看作一个列向量，其中每一个元素为一个行向量，则矩阵-向量积为将该列向量的每个元素替换为对应行向量与向量的点积。在代码中使用张量表示矩阵-向量积，我们使用mv函数，接收一个矩阵（2维）和一个向量（1维）。 当我们为矩阵<code>A</code>和向量<code>x</code>调用<code>torch.mv(A,x)</code>时，会执行矩阵-向量积。 注意，<code>A</code>的列维数（沿轴1的长度）必须与<code>x</code>的维数（其长度）相同。</p><p><strong>矩阵乘法（matrix-matrix multiplication）</strong>：设A为n×k矩阵、B为k×m矩阵，可以将矩阵-矩阵乘法AB看作是简单地执行m次矩阵-向量积，并将结果拼接在一起，形成一个n×m矩阵。在代码中：<code>torch.mm(A,B)</code></p><p><strong>表示法</strong>：在《动手学深度学习》中，标量变量由普通小写字母表示（例如，x、y和z），用R表示所有（连续）<em>实数</em>标量的空间，将向量记为粗体、小写的符号 （例如，x、y和z)，认为列向量是向量的默认方向，通常用粗体、大写字母来表示矩阵 （例如，X、Y和Z），张量用特殊字体的大写字母表示（例如，X、Y和Z）。</p><h3 id="3-2-张量算法"><a href="#3-2-张量算法" class="headerlink" title="3.2. 张量算法"></a>3.2. 张量算法</h3><p>当我们开始处理图像时，张量将变得更加重要，图像以n维数组形式出现， 其中3个轴对应于高度、宽度，以及一个<em>通道</em>（channel）轴， 用于表示颜色通道（红色、绿色和蓝色）。</p><p>给定具有相同形状的任意两个张量，任何按元素二元运算的结果都将是相同形状的张量。例如，将两个相同形状的矩阵相加，会在这两个矩阵上执行元素加法。</p><pre><code>A=torch.arange(20,dtype=torch.float32).reshape(5,4)B=A.clone()# 通过分配新内存，将A的一个副本分配给BA, A+B</code></pre><p>两个矩阵的按元素乘法称为<em>Hadamard积</em>（Hadamard product）（数学符号⊙）。</p><p><code>A*B</code></p><p>将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘。</p><p><code>A*2, A+2</code></p><h3 id="3-3-降维"><a href="#3-3-降维" class="headerlink" title="3.3. 降维"></a>3.3. 降维</h3><p>我们可以对任意张量进行的一个有用的操作是计算其元素的和。 在数学表示法中，我们使用∑符号表示求和。在代码中，我们可以调用计算求和的函数<code>x.sum()</code>。</p><p>默认情况下，调用求和函数会沿所有的轴<strong>降低张量的维度</strong>，使它变为一个标量。 我们还可以指定张量沿哪一个轴来通过求和降低维度。 以矩阵为例，为了通过求和所有行的元素来降维（轴0），我们可以在调用函数时指定axis=0。 由于输入矩阵沿0轴降维以生成输出向量，因此输入轴0的维数在输出形状中消失。</p><pre><code>A_sum_axis0=A.sum(axis=0)A_sum_axis0,A_sum_axis0.shape#结果：(tensor([40.,45.,50.,55.]),torch.Size([4]))</code></pre><p>指定axis=1将通过汇总所有列的元素降维（轴1）。因此，输入轴1的维数在输出形状中消失。</p><pre><code>A_sum_axis1=A.sum(axis=1)A_sum_axis1,A_sum_axis1.shape#结果：(tensor([6.,22.,38.,54.,70.]),torch.Size([5]))</code></pre><p>沿着行和列对矩阵求和，等价于对矩阵的所有元素进行求和。如<code>A.sum(axis=[0,1])</code>。</p><p>一个与求和相关的量是<em>平均值</em>（mean或average）。 我们通过将总和除以元素总数来计算平均值。 在代码中，我们可以调用函数来计算任意形状张量的平均值。</p><pre><code>A.mean(),A.sum()/A.numel() #两者等价</code></pre><p>同样，计算平均值的函数也可以沿指定轴降低张量的维度。</p><pre><code>A.mean(axis=0),A.sum(axis=0)/A.shape[0]#结果：(tensor([8.,9.,10.,11.]),tensor([8.,9.,10.,11.]))</code></pre><h4 id="3-3-1-非降维求和"><a href="#3-3-1-非降维求和" class="headerlink" title="3.3.1. 非降维求和"></a>3.3.1. 非降维求和</h4><p>有时在调用函数来计算总和或均值时保持轴数不变会很有用。</p><pre><code>sum_A=A.sum(axis=1,keepdims=True)sum_A#结果：tensor([[6.],[22.],[38.],[54.],[70.]])</code></pre><p>例如，由于<code>sum_A</code>在对每行进行求和后仍保持两个轴，我们可以通过广播将A除以<code>sum_A</code>。如：<code>A/sum_A</code></p><p>如果我们想沿某个轴计算<code>A</code>元素的累积总和， 比如<code>axis=0</code>（按行计算），我们可以调用<code>cumsum</code>函数。 此函数不会沿任何轴降低输入张量的维度。</p><pre><code>A.cumsum(axis=0)#原本A：tensor([[0.,1.,2.,3.],        [4.,5.,6.,7.],        [8.,9.,10.,11.],        [12.,13.,14.,15.],        [16.,17.,18.,19.]])#结果：tensor([[0.,1.,2.,3.],        [4.,6.,8.,10.],        [12.,15.,18.,21.],        [24.,28.,32.,36.],        [40.,45.,50.,55.]])</code></pre><h3 id="3-4-范数"><a href="#3-4-范数" class="headerlink" title="3.4 范数"></a>3.4 范数</h3><p>线性代数中最有用的一些运算符是<em>范数</em>（norm）。 非正式地说，一个向量的<em>范数</em>告诉我们一个向量有多大。 这里考虑的<em>大小</em>（size）概念不涉及维度，而是分量的大小。</p><p>在线性代数中，向量范数是将向量映射到标量的函数f。 给定任意向量x，向量范数要满足一些属性。 第一个性质是如果我们按常数因子α缩放向量的所有元素， 其范数也会按相同常数因子的<em>绝对值</em>缩放。第二个性质是我们熟悉的三角不等式f(x+y)\&lt;=f(x)+f(y)。第三个性质简单地说范数必须是非负的，这是有道理的。因为在大多数情况下，任何东西的最小的<em>大小</em>是0。 最后一个性质要求范数最小为0，当且仅当向量全由0组成。</p><p>假设n维向量x中的元素是x1,…,xn，其L2<em>范数</em>是向量元素平方和的平方根。在代码中，我们可以按如下方式计算向量的L2范数。在深度学习中，我们更经常地使用L2范数的平方。</p><pre><code>u=torch.tensor([3.0,-4.0])torch.norm(u)#结果：tensor(5.)</code></pre><p>L1范数，表示为向量元素的绝对值之和。与L2范数相比，L1范数受异常值的影响较小。 为了计算L1范数，我们将绝对值函数和按元素求和组合起来。</p><pre><code>torch.abs(u).sum()#结果：tensor(7.)</code></pre><p>L2范数和L1范数都是更一般的Lp范数的特例。</p><p>类似于向量的L2范数，矩阵X∈Rm×n的<em>Frobenius范数</em>（Frobenius norm）是矩阵元素平方和的平方根。Frobenius范数满足向量范数的所有性质，它就像是矩阵形向量的L2范数。 调用以下函数将计算矩阵的Frobenius范数。</p><pre><code>torch.norm(torch.ones((4,9)))#结果：tensor(6.)</code></pre><p>在深度学习中，我们经常试图解决优化问题：<em>最大化</em>分配给观测数据的概率;<em>最小化</em>预测和真实观测之间的距离。 用向量表示物品（如单词、产品或新闻文章），以便最小化相似项目之间的距离，最大化不同项目之间的距离。 <strong>目标</strong>，或许是深度学习算法最重要的组成部分（除了数据），通常被表达为范数。</p><h2 id="4-微积分和自动微分"><a href="#4-微积分和自动微分" class="headerlink" title="4. 微积分和自动微分"></a>4. 微积分和自动微分</h2><p>在深度学习中，我们“训练”模型，不断更新它们，使它们在看到越来越多的数据时变得越来越好。 通常情况下，变得更好意味着最小化一个<em>损失函数</em>（loss function）， 即一个衡量“我们的模型有多糟糕”这个问题的分数。 最终，我们真正关心的是生成一个模型，它能够在从未见过的数据上表现良好。 但“训练”模型只能将模型与我们实际能看到的数据相拟合。 因此，我们可以将拟合模型的任务分解为两个关键问题：</p><ul><li><em>优化</em>（optimization）：用模型拟合观测数据的过程；</li><li><em>泛化</em>（generalization）：数学原理和实践者的智慧，能够指导我们生成出有效性超出用于训练的数据集本身的模型。</li></ul><p>总结：</p><ul><li>微分和积分是微积分的两个分支，前者可以应用于深度学习中的优化问题。</li><li>导数可以被解释为函数相对于其变量的瞬时变化率，它也是函数曲线的切线的斜率。</li><li>梯度是一个向量，其分量是多变量函数相对于其所有变量的偏导数。</li><li>链式法则使我们能够微分复合函数。</li><li>深度学习框架可以自动计算导数：我们首先将梯度附加到想要对其计算偏导数的变量上。然后我们记录目标值的计算，执行它的反向传播函数，并访问得到的梯度。</li></ul><h3 id="4-1-导数、微分、偏导"><a href="#4-1-导数、微分、偏导" class="headerlink" title="4.1. 导数、微分、偏导"></a>4.1. 导数、微分、偏导</h3><p>我们首先讨论导数的计算，这是几乎所有深度学习优化算法的关键步骤。 在深度学习中，我们通常选择对于模型参数可微的损失函数。 简而言之，对于每个参数， 如果我们把这个参数<em>增加</em>或<em>减少</em>一个无穷小的量，我们可以知道损失会以多快的速度增加或减少。</p><p>假设我们有一个函数f，其输入和输出都是标量。 导数定义不用多说，如果f′(a)存在，则称f在a处是<em>可微</em>（differentiable）的。如果f在一个区间内的每个数上都是可微的，则此函数在此区间中是可微的。可以将导数定义中的导数f′(x)解释为f(x)相对于x的<em>瞬时</em>（instantaneous）变化率。 所谓的瞬时变化率是基于x中的变化h，且h接近0。</p><p>要了解微分，以及对函数微分的法则。例如Dx^n = nx^(n-1)，其中D为微分运算符。还要了解求偏导，这些都是基础。</p><h3 id="4-2-梯度"><a href="#4-2-梯度" class="headerlink" title="4.2. 梯度"></a>4.2. 梯度</h3><p>我们可以连结一个多元函数对其所有变量的偏导数，以得到该函数的<em>梯度</em>（gradient）向量。 具体而言，设函数f:Rn→R的输入是一个n维向量x=[x1,x2,…,xn]⊤，并且输出是一个标量。 函数f(x)相对于x的梯度是一个包含n个偏导数的向量</p><script type="math/tex; mode=display">\nabla_{\mathbf{x}} f(\mathbf{x})=\left[\frac{\partial f(\mathbf{x})}{\partial x_{1}}, \frac{\partial f(\mathbf{x})}{\partial x_{2}}, \ldots, \frac{\partial f(\mathbf{x})}{\partial x_{n}}\right]^{\top}</script><p>,其中∇xf(x)通常在没有歧义时被∇f(x)取代。梯度对于设计深度学习中的优化算法有很大用处。</p><p>假设x为n维向量，在微分多元函数时经常使用以下规则:</p><ul><li>对于所有 $\mathbf{A} \in \mathbb{R}^{m \times n}$, 都有 $\nabla_{\mathbf{x}} \mathbf{A} \mathbf{x}=\mathbf{A}^{\top}$</li><li>对于所有 $\mathbf{A} \in \mathbb{R}^{n \times m}$, 都有 $\nabla_{\mathbf{x}} \mathbf{x}^{\top} \mathbf{A}=\mathbf{A}$<br>-对于所有 $\mathbf{A} \in \mathbb{R}^{n \times n}$, 都有 $\nabla_{\mathbf{x}} \mathbf{x}^{\top} \mathbf{A} \mathbf{x}=\left(\mathbf{A}+\mathbf{A}^{\top}\right) \mathbf{x}$</li><li>$\nabla_{\mathbf{x}}|\mathbf{x}|^{2}=\nabla_{\mathbf{x}} \mathbf{x}^{\top} \mathbf{x}=2 \mathbf{x}$</li></ul><p>同样，对于任何矩阵X，都有 $\nabla \mathbf{X}|\mathbf{X}|_{F}^{2}=2 \mathbf{X}$。</p><h3 id="4-3-链式法则"><a href="#4-3-链式法则" class="headerlink" title="4.3. 链式法则"></a>4.3. 链式法则</h3><p>然而，上面方法可能很难找到梯度。 这是因为在深度学习中，多元函数通常是<em>复合</em>（composite）的， 所以我们可能没法应用上述任何规则来微分这些函数。 幸运的是，链式法则使我们能够微分复合函数。</p><p>让我们先考虑单变量函数。假设函数y=f(u)和u=g(x)都是可微的，根据链式法则有</p><script type="math/tex; mode=display">\frac{d y}{d x}=\frac{d y}{d u} \frac{d u}{d x}</script><p>当处于函数具有任意数量的变量的情况下。假设可微分函数y有变量u1,u2,…,um，其中每个可微分函数ui都有变量x1,x2,…,xn。 注意，y是x1,x2，…,xn的函数。 对于任意i=1,2,…,n，链式法则给出：</p><script type="math/tex; mode=display">\frac{d y}{d x_{i}}=\frac{d y}{d u_{1}} \frac{d u_{1}}{d x_{i}}+\frac{d y}{d u_{2}} \frac{d u_{2}}{d x_{i}}+\cdots+\frac{d y}{d u_{m}} \frac{d u_{m}}{d x_{i}}</script><h3 id="4-4-自动微分"><a href="#4-4-自动微分" class="headerlink" title="4.4. 自动微分"></a>4.4. 自动微分</h3><p>求导是几乎所有深度学习优化算法的关键步骤。 虽然求导的计算很简单，只需要一些基本的微积分。 但对于复杂的模型，手工进行更新是一件很痛苦的事情（而且经常容易出错）。</p><p>深度学习框架通过自动计算导数，即<em>自动微分</em>（automatic differentiation）来加快求导。 实际中，根据我们设计的模型，系统会构建一个<em>计算图</em>（computational graph）， 来跟踪计算是哪些数据通过哪些操作组合起来产生输出。 自动微分使系统能够随后反向传播梯度。 这里，<em>反向传播</em>（backpropagate）意味着跟踪整个计算图，填充关于每个参数的偏导数。设y为关于x的函数，调用<code>y.backward()</code>反向传播计算x的梯度<code>x.grad</code>。</p><p>例如对于：$e=(a+b) *(b+1)$<br>可以得到计算图如下：<br><img src="/assets/post_img/article34/BP.png" alt="0x02pic1"></p><p>反向传播参考：<a href="https://blog.csdn.net/Weary_PJ/article/details/105706318">https://blog.csdn.net/Weary_PJ/article/details/105706318</a></p><h4 id="4-4-1-非标量变量的反向传播"><a href="#4-4-1-非标量变量的反向传播" class="headerlink" title="4.4.1. 非标量变量的反向传播"></a>4.4.1. 非标量变量的反向传播</h4><p>当y不是标量时，向量y关于向量x的导数的最自然解释是一个矩阵。 对于高阶和高维的y和x，求导的结果可以是一个高阶张量。</p><p>然而，虽然这些更奇特的对象确实出现在高级机器学习中（包括深度学习中）， 但当我们调用向量的反向计算时，我们通常会试图计算一批训练样本中每个组成部分的损失函数的导数。 这里，我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和。</p><h4 id="4-4-2-分离计算"><a href="#4-4-2-分离计算" class="headerlink" title="4.4.2. 分离计算"></a>4.4.2. 分离计算</h4><p>有时，我们希望将某些计算移动到记录的计算图之外。 例如，假设y是作为x的函数计算的，而z则是作为y和x的函数计算的。 想象一下，我们想计算z关于x的梯度，但由于某种原因，我们希望将y视为一个常数， 并且只考虑到x在y被计算后发挥的作用。</p><p>在这里，我们可以分离y来返回一个新变量u，该变量与y具有相同的值， 但丢弃计算图中如何计算y的任何信息。 换句话说，梯度不会向后流经u到x。</p><p>为了实现自动微分，PyTorch跟踪所有涉及张量的操作，可能需要为其计算梯度（即require_grad为True）。 这些操作记录为有向图。在代码上，上述分离计算可以通过使用<strong>detach（）</strong>方法在张量上构造一个新视图，该张量声明为不需要梯度，即从进一步跟踪操作中将其排除在外，因此不记录涉及该视图的子图。即对应上述的情况，令<code>u=y.detach()</code>，使用<code>z=u*x</code>替代原本的<code>z=y*x</code>。</p><h4 id="4-4-3-Python控制流的梯度计算"><a href="#4-4-3-Python控制流的梯度计算" class="headerlink" title="4.4.3. Python控制流的梯度计算"></a>4.4.3. Python控制流的梯度计算</h4><p>使用自动微分的一个好处是：即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度。在下面的代码中，while循环的迭代次数和if语句的结果都取决于输入a的值。</p><pre><code>def f(a):  b=a*2  while b.norm()&lt;1000:    b=b*2  if b.sum()&gt;0:    c=b  else:    c=100*b  return c# 计算梯度a=torch.randn(size=(),requires_grad=True)d=f(a)d.backward()</code></pre><p>我们现在可以分析上面定义的f函数。 请注意，它在其输入a中是分段线性的。 换言之，对于任何a，存在某个常量标量k，使得f(a)=k*a，其中k的值取决于输入a。 因此，我们可以用d/a验证梯度是否正确。<code>a.grad==d/a</code>结果为真。</p><h2 id="5-概率"><a href="#5-概率" class="headerlink" title="5. 概率"></a>5. 概率</h2><p>简单地说，机器学习就是做出预测。</p><p>根据病人的临床病史，我们可能想预测他们在下一年心脏病发作的<em>概率</em>。 在飞机喷气发动机的异常检测中，我们想要评估一组发动机读数为正常运行情况的概率有多大。 在强化学习中，我们希望智能体（agent）能在一个环境中智能地行动。 这意味着我们需要考虑在每种可行的行为下获得高奖励的概率。 当我们建立推荐系统时，我们也需要考虑概率。 例如，假设我们为一家大型在线书店工作，我们可能希望估计某些用户购买特定图书的概率。 为此，我们需要使用概率学。 概率是一种灵活的语言，用于说明我们的确定程度，并且它可以有效地应用于广泛的领域中。</p><p>对于概率（probability）和统计（statistics）：概率是已知模型和参数，推数据。统计是已知数据，推模型和参数。</p><h3 id="5-1-基本概率论"><a href="#5-1-基本概率论" class="headerlink" title="5.1. 基本概率论"></a>5.1. 基本概率论</h3><p>以掷骰子为例，想知道看到1的几率有多大，而不是看到另一个数字。 如果骰子是公平的，那么所有六个结果{1,…,6}都有相同的可能发生， 因此我们可以说1发生的概率为6分之1。</p><p>然而现实生活中，对于我们从工厂收到的真实骰子，我们需要检查它是否有瑕疵。 检查骰子的唯一方法是多次投掷并记录结果。 对于每个骰子，我们将观察到{1,…,6}中的一个值。 对于每个值，一种自然的方法是将它出现的次数除以投掷的总次数， 即此<em>事件</em>（event）概率的<em>估计值</em>。<em>大数定律</em>（law of large numbers）告诉我们： 随着投掷次数的增加，这个估计值会越来越接近真实的潜在概率。</p><p>在统计学中，我们把从概率分布中抽取样本的过程称为<em>抽样</em>（sampling）。 笼统来说，可以把<em>分布</em>（distribution）看作是对事件的概率分配， 稍后我们将给出的更正式定义。 将概率分配给一些离散选择的分布称为<em>多项分布</em>（multinomial distribution）。</p><p>在代码中multinomial.Multinomial函数创建由 total_count 和 probs 或 logits（但不是两者）参数化的多项分布。 probs（概率）的最内层维度索引种类，所有其他维度索引批次。total_count表示总的实验次数。如：<code>multinomial.Multinomial(10,fair_probs).sample()</code>表示在fair_probs的概率下进行了10次实验得到的分布情况。在此基础上除10即可得到真实概率的估计。</p><h4 id="5-1-1-概率论公理"><a href="#5-1-1-概率论公理" class="headerlink" title="5.1.1. 概率论公理"></a>5.1.1. 概率论公理</h4><p>在处理骰子掷出时，我们将集合S={1,2,3,4,5,6}称为<em>样本空间</em>（sample space）或<em>结果空间</em>（outcome space）， 其中每个元素都是<em>结果</em>（outcome）。<em>事件</em>（event）是一组给定样本空间的随机结果。 例如，“看到5”（{5}）和“看到奇数”（{1,3,5}）都是掷出骰子的有效事件。 注意，如果一个随机实验的结果在A中，则事件A已经发生。 也就是说，如果投掷出3点，因为3∈{1,3,5}，我们可以说，“看到奇数”的事件发生了。<em>概率</em>（probability）可以被认为是将集合映射到真实值的函数。</p><p>性质：</p><ul><li>概率非负</li><li>全样本空间概率为1</li><li>对于<em>互斥</em>（mutually exclusive）事件（对于所有i≠j都有Ai∩Aj=∅）的任意一个可数序列A1,A2,…，序列中任意一个事件发生的概率等于它们各自发生的概率之和。</li></ul><h4 id="5-1-2-随机变量"><a href="#5-1-2-随机变量" class="headerlink" title="5.1.2. 随机变量"></a>5.1.2. 随机变量</h4><p>在我们掷骰子的随机实验中，我们引入了<em>随机变量</em>（random variable）的概念。 随机变量几乎可以是任何数量，并且它可以在随机实验的一组可能性中取一个值。 考虑一个随机变量X，其值在掷骰子的样本空间S={1,2,3,4,5,6}中。 我们可以将事件“看到一个5”表示为{X=5}或X=5， 其概率表示为P({X=5})或P(X=5)。 通过P(X=a)，我们区分了随机变量X和X可以采取的值（例如a）。</p><p>然而，这可能会导致繁琐的表示。 为了简化符号，一方面，我们可以将P(X)表示为随机变量X上的<em>分布</em>（distribution）： 分布告诉我们X获得某一值的概率。 另一方面，我们可以简单用P(a)表示随机变量取值a的概率。 由于概率论中的事件是来自样本空间的一组结果，因此我们可以为随机变量指定值的可取范围。 例如，P(1≤X≤3)表示事件{1≤X≤3}， 即{X=1,2,or,3}的概率。 等价地，P(1≤X≤3)表示随机变量X从{1,2,3}中取值的概率。这一节主要讨论离散型随机变量，连续型不咋考虑。</p><h3 id="5-2-处理多个随机变量"><a href="#5-2-处理多个随机变量" class="headerlink" title="5.2. 处理多个随机变量"></a>5.2. 处理多个随机变量</h3><p>例如图像包含数百万像素，因此有数百万个随机变量。 在许多情况下，图像会附带一个<em>标签</em>（label），标识图像中的对象。 我们也可以将标签视为一个随机变量。 我们甚至可以将所有元数据视为随机变量，例如位置、时间、光圈、焦距、ISO、对焦距离和相机类型。 所有这些都是联合发生的随机变量。 当我们处理多个随机变量时，会有若干个变量是我们感兴趣的。</p><h4 id="5-2-1-一些基本概念"><a href="#5-2-1-一些基本概念" class="headerlink" title="5.2.1. 一些基本概念"></a>5.2.1. 一些基本概念</h4><p><strong>联合概率（joint probability）</strong>：$P(A=a, B=b)$，联合概率可以回答：A=a和B=b同时满足的概率是多少的问题，但对于任何a和b的取值，P(A=a,B=b)≤P(A=a)。</p><p><strong>条件概率（conditional probability）</strong>：用P(B=b∣A=a)表示它，它是B=b的概率，前提是A=a已发生。</p><p><strong>贝叶斯定理（Bayes’ theorem）</strong>：根据<em>乘法法则</em>（multiplication rule）可得到 $P(A, B)=P(B \mid A) P(A)$ 。根据对称性, 可得到 $P(A, B)=P(A \mid B) P(B)$ 。根据两式假设P(B)>0，求解其中一个条件变量，我们得到：</p><script type="math/tex; mode=display">P(A \mid B)=\frac{P(B \mid A) P(A)}{P(B)}</script><p>注意这里使用紧凑的表示法： 其中 $P(A, B)$ 是一个联合分布（joint distribution）, $P(A \mid B)$ 是一个条件分布 (conditional distribution)。这种分布可以在给定值 $A=a, B=b$ 上进行求值。</p><p><strong>边际化（marginalization）</strong>：为了能进行事件概率求和，我们需要<em>加法法则</em>（sum rule）， 即B的概率相当于计算A的所有可能选择，并将所有选择的联合概率聚合在一起：</p><script type="math/tex; mode=display">P(B)=\sum_{A} P(A, B)</script><p>这也称为<em>边际化</em>（marginalization）。边际化结果的概率或分布称为<em>边缘概率</em>（marginal probability） 或<em>边缘分布</em>（marginal distribution）。</p><p><strong>依赖（dependence）与独立（independence）</strong>：如果两个随机变量A和B是独立的，意味着事件A的发生跟B事件的发生无关。 在这种情况下，通常将这一点表述为A⊥B。 根据贝叶斯定理，得到P(A∣B)=P(A)。 在所有其他情况下，我们称A和B依赖。 比如，两次连续抛出一个骰子的事件是相互独立的。 相比之下，灯开关的位置和房间的亮度并不是（因为可能存在灯泡坏掉、电源故障，或者开关故障）。两个随机变量是独立的，当且仅当两个随机变量的联合分布是其各自分布的乘积。</p><p><strong>期望（expectation，或平均值（average））</strong>：均值。</p><p><strong>方差（variance）</strong>：数据与平均数之差平方和的平均数，方差的平方根被称为<em>标准差</em>（standared deviation）。随机变量函数的方差衡量的是：当从该随机变量分布中采样不同值x时， 函数值偏离该函数的期望的程度。</p><p><strong>似然函数（Likelihood Function）</strong>：似然也就是可能性。在统计中，似然函数和概率函数是两个不同的概念。<br>对于这个函数： $p(x \mid \theta)$ 输入有两个: $\mathrm{x}$ 表示某一个具体的数据; $\theta$ 表示模型的参数。<br>如果 $\theta$ 是已知确定的, $x$ 是变量, 这个函数叫做概率函数(probability function), 它描述对于不同的样本点 $x$, 其出现概率是多少。<br>如果 $x$ 是已知确定的, $\theta$ 是变量, 这个函数叫做似然函数(likelihood function), 它描述对于不同的模型参数, 出现 $x$ 这个样本点的概率是多少。</p><p><strong>最大似然估计(Maximum Likelihood Estimation, MLE)</strong>：属于统计领域问题，利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值。其中似然即可能性。<br>极大似然估计中采样需满足一个重要的假设，就是所有的采样都是独立同分布的。<br>最大似然估计是怎么做的呢？个人理解：对于一个概率分布（或离散或连续），抽出一个具有n个值的采样（结果采样），确定一个似然函数：$\operatorname{lik}(\theta)=f_{D}\left(x_{1}, x_{2}, \ldots, x_{n} \mid \theta\right)$，在θ的所有取值上，使这个函数最大化。这个使可能性最大的值即被称为θ的最大似然估计。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;p&gt;这一笔记对应预备知识章节，包括数据操作、数据预处理、线性代数、微积分、概率论等。其中很多知识在学数一的时候都更深入理解过了，但是现在发现忘的差不多了，哎～&lt;/p&gt;
&lt;p&gt;对应实践：&lt;a href=&quot;https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x02.ipynb&quot;&gt;https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x02.ipynb&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在M1芯片的设备上使用miniforge安装pytorch：&lt;code&gt;conda install -c pytorch pytorch&lt;/code&gt;&lt;br&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://silencezheng.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>M1芯片VMfusion安装win11教程</title>
    <link href="http://silencezheng.top/2022/04/04/article33/"/>
    <id>http://silencezheng.top/2022/04/04/article33/</id>
    <published>2022-04-04T09:26:33.000Z</published>
    <updated>2022-04-04T09:32:34.875Z</updated>
    
    <content type="html"><![CDATA[<p>首先需要准备的有：VMfusion预览版（支持ARM架构）、Win11ARM版镜像（.iso）</p><span id="more"></span><p>下面开始安装：</p><ol><li>打开VMfusion创建自定虚拟机，操作系统选择Other 64-bit Arm</li><li>下一步虚拟磁盘那块点继续，不用设置</li><li>然后一定要注意选择 <strong>自定设置</strong> ，否则虚拟机会自动启动，就无法安装了。</li><li>自定设置时，cpu推荐4核以上，内存4g以上，硬盘容量52g以上（保险起见，实际30g也够用了）</li><li>dvd设置我们下载的win11镜像后，在启动磁盘处选择dvd启动（不选应该也会默认这个）</li><li>找到该虚拟机的安装位置，查看包内容，编辑vmx文件，把guestOS改为arm-windows11-64</li><li>启动虚拟机，点击任意按键进入安装界面</li><li>选择没有产品密钥，一直进入到选择操作系统界面</li><li>下面需要绕过TPM，首先按住fn+shift+f10打开cmd，输入regedit打开注册表编辑器</li><li>进到HKEY_LOCAL_MACHINE-&gt;SYSTEM-&gt;Setup下，新建项LabConfig</li><li>在该项创建两个DWORD值，key分别为BypassTPMCheck和BypassSecureBootCheck，value都为1</li><li>然后都关闭掉继续进行安装，到网络设置界面发现无法继续，打开cmd准备跳过</li><li>输入taskmgr打开任务管理器，详细信息中找到OOBE开头的进程关闭掉，然后全部关闭返回发现跳过了网络设置步骤</li><li>下面成功进入桌面后，需要配置好网络才能正常使用（装来玩扫雷当我没说），管理员身份打开cmd</li><li>输入命令<code>bcdedit /debug on</code> 启用默认启动项的内核调试</li><li>再输入 <code>bcdedit /dbgsettings net hostip:10.0.0.1 port:55555</code> 将目标计算机配置为使用以太网连接进行调试，并指定主计算机的 IP 地址和主机可用于连接到目标计算机的端口号。</li><li>重启系统后即可连接网络，配置完毕。</li></ol><p>解读：<br>10.X.X.X是私有地址（私有地址是互联网上不使用，而用在局域网络中的地址）<br><code>netstat -ano     查看所有端口使用情况</code><br><code>netstat -aon|findstr &quot;55555&quot; 查看55555端口pid</code><br><code>tasklist|findstr &quot;9088&quot; 查看pid对应程序</code></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;首先需要准备的有：VMfusion预览版（支持ARM架构）、Win11ARM版镜像（.iso）&lt;/p&gt;</summary>
    
    
    
    
    <category term="MacOS" scheme="http://silencezheng.top/tags/MacOS/"/>
    
  </entry>
  
</feed>
