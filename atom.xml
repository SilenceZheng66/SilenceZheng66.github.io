<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>silenceZheng66-BLOG</title>
  
  <subtitle>log</subtitle>
  <link href="http://silencezheng.top/atom.xml" rel="self"/>
  
  <link href="http://silencezheng.top/"/>
  <updated>2022-09-12T13:08:39.711Z</updated>
  <id>http://silencezheng.top/</id>
  
  <author>
    <name>silenceZheng66</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>PycharmCV2无法自动提示解决</title>
    <link href="http://silencezheng.top/2022/09/12/article61/"/>
    <id>http://silencezheng.top/2022/09/12/article61/</id>
    <published>2022-09-12T13:06:15.000Z</published>
    <updated>2022-09-12T13:08:39.711Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>M1芯片 MacBook 上 <strong>Pycharm无法对cv2自动代码提示</strong> 问题解决。</p><span id="more"></span><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>1、找到环境中<code>/Users/YOURNAME/miniforge3/envs/YOURENV/lib/python3.9/site-packages/cv2</code></p><p>2、复制<code>cv2.abi3.so</code>文件，放置到上一级目录中。</p><p>3、最终效果：<code>/Users/YOURNAME/miniforge3/envs/YOURENV/lib/python3.9/site-packages/cv2.abi3.so</code></p><p>4、Boom，解决。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;M1芯片 MacBook 上 &lt;strong&gt;Pycharm无法对cv2自动代码提示&lt;/strong&gt; 问题解决。&lt;/p&gt;</summary>
    
    
    
    
    <category term="MacOS" scheme="http://silencezheng.top/tags/MacOS/"/>
    
    <category term="Pycharm" scheme="http://silencezheng.top/tags/Pycharm/"/>
    
  </entry>
  
  <entry>
    <title>普通人的反向传播理解</title>
    <link href="http://silencezheng.top/2022/09/11/article60/"/>
    <id>http://silencezheng.top/2022/09/11/article60/</id>
    <published>2022-09-11T08:14:14.000Z</published>
    <updated>2022-09-11T09:02:35.535Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>一个普通人尝试理解反向传播…从上至下阅读吧，看看有什么收获。<br><span id="more"></span> </p><p>先说结论：<strong>反向传播用于快速计算神经网络中节点的梯度</strong></p><h2 id="导数（Derivative）"><a href="#导数（Derivative）" class="headerlink" title="导数（Derivative）"></a>导数（Derivative）</h2><p>对函数$f: \mathbb{R} \rightarrow \mathbb{R}$，其输入和输出都是标量。 如果$f$的导数存在，这个极限被定义为：</p><script type="math/tex; mode=display">f'(x) = \lim_{h \rightarrow 0} \frac{f(x+h) - f(x)}{h}.</script><p>如果$f’(a)$存在，则称$f$在$a$处是<strong>可微</strong>（differentiable）的。 如果$f$在一个区间的每一个点上都是可微的，那么该函数在此区间上可微。导数$f’(x)$可以解释为$f(x)$相对于$x$的瞬时（instantaneous）变化率，该变化率基于变化$h$，$h$趋近于$0$。</p><h2 id="微分（Differential）"><a href="#微分（Differential）" class="headerlink" title="微分（Differential）"></a>微分（Differential）</h2><p>给定$y = f(x)$，其中$x$和$y$分别是函数$f$的自变量和因变量，则以下表达式等价：</p><script type="math/tex; mode=display">f'(x) = y' = \frac{dy}{dx} = \frac{df}{dx} = \frac{d}{dx} f(x) = Df(x) = D_x f(x),</script><p>其中符号$\frac{d}{dx}$和$D$是微分运算符。可以使用以下规则来对常见函数求微分：</p><ul><li>$DC = 0$（$C$为常数）</li><li>$Dx^n = nx^{n-1}$（n为任意实数）</li><li>$De^x = e^x$</li><li>$D\ln(x) = 1/x$</li></ul><p>假设函数$f$和$g$可微，$C$为常数，则有如下法则：</p><p>1、 $\frac{d}{dx} [Cf(x)] = C \frac{d}{dx} f(x)$</p><p>2、 $\frac{d}{dx} [f(x) + g(x)] = \frac{d}{dx} f(x) + \frac{d}{dx} g(x)$</p><p>3、 $\frac{d}{dx} [f(x)g(x)] = f(x) \frac{d}{dx} [g(x)] + g(x) \frac{d}{dx} [f(x)]$</p><p>4、 $\frac{d}{dx} \left[\frac{f(x)}{g(x)}\right] = \frac{g(x) \frac{d}{dx} [f(x)] - f(x) \frac{d}{dx} [g(x)]}{[g(x)]^2}$</p><p>这些法则可以便于计算由常见函数组成的函数的微分，注意，<em>复合函数无法通过这些法则微分</em>。</p><h2 id="偏导数（Partial-derivative）"><a href="#偏导数（Partial-derivative）" class="headerlink" title="偏导数（Partial derivative）"></a>偏导数（Partial derivative）</h2><p>就是将微分推广到多元函数，这个应该懂得都懂。</p><p>设$y = f(x_1, x_2, \ldots, x_n)$为具有$n$个变量的函数，$y$关于$x_i$的偏导数为：</p><script type="math/tex; mode=display">\frac{\partial y}{\partial x_i} = \lim_{h \rightarrow 0} \frac{f(x_1, \ldots, x_{i-1}, x_i+h, x_{i+1}, \ldots, x_n) - f(x_1, \ldots, x_i, \ldots, x_n)}{h}.</script><p>计算时，将$x_1, \ldots, x_{i-1}, x_{i+1}, \ldots, x_n$看作常数，直接计算$y$关于$x_i$的导数即可。</p><p>同时也有以下等价表示：</p><script type="math/tex; mode=display">\frac{\partial y}{\partial x_i} = \frac{\partial f}{\partial x_i} = f_{x_i} = f_i = D_i f = D_{x_i} f.</script><h2 id="梯度（Gradient）"><a href="#梯度（Gradient）" class="headerlink" title="梯度（Gradient）"></a>梯度（Gradient）</h2><p><strong>梯度</strong>向量是连结一个多元函数对其所有变量的偏导数后得到的。</p><p>设函数$f:\mathbb{R}^n\rightarrow\mathbb{R}$的输入是一个$n$维向量$\mathbf{x}=[x_1,x_2,\ldots,x_n]^\top$，且输出是一个标量。则函数$f(\mathbf{x})$相对于$x$的梯度为：</p><script type="math/tex; mode=display">\nabla_{\mathbf{x}} f(\mathbf{x}) = \bigg[\frac{\partial f(\mathbf{x})}{\partial x_1}, \frac{\partial f(\mathbf{x})}{\partial x_2}, \ldots, \frac{\partial f(\mathbf{x})}{\partial x_n}\bigg]^\top</script><p>$\nabla_{\mathbf{x}} f(\mathbf{x})$在没有歧义的时候可以被$\nabla f(\mathbf{x})$替换。</p><p>继续假设$x$为$n$维向量，在微分多元函数时有如下法则（注意n和m的位置）：</p><p>1、对所有$\mathbf{A} \in \mathbb{R}^{m \times n}$，有$\nabla_{\mathbf{x}} \mathbf{A} \mathbf{x} = \mathbf{A}^\top$ （用于转置矩阵）</p><p>2、 对所有$\mathbf{A} \in \mathbb{R}^{n \times m}$，有$\nabla_{\mathbf{x}} \mathbf{x}^\top \mathbf{A} = \mathbf{A}$ （抵消）</p><p>3、对所有$\mathbf{A} \in \mathbb{R}^{n \times n}$，有$\nabla_{\mathbf{x}} \mathbf{x}^\top \mathbf{A} \mathbf{x} = (\mathbf{A} + \mathbf{A}^\top)\mathbf{x}$ （方阵）</p><p>4、$\nabla_{\mathbf{x}} |\mathbf{x} |^2 = \nabla_{\mathbf{x}} \mathbf{x}^\top \mathbf{x} = 2\mathbf{x}$ （函数相对于向量$x$的梯度乘以向量$x$范数的平方等于$2x$）</p><p>接第四条，对于任何矩阵$\mathbf{X}$，都有$\nabla_{\mathbf{X}} |\mathbf{X} |_F^2 = 2\mathbf{X}$。</p><h2 id="链式法则（Chain-rule）"><a href="#链式法则（Chain-rule）" class="headerlink" title="链式法则（Chain rule）"></a>链式法则（Chain rule）</h2><p>计算梯度的关键是对多元函数求导，但深度学习中多元函数通常是<em>复合</em>（composite）的，即无法通过常见函数微分法则求导。</p><p><strong>链式法则可以帮助我们对复合多元函数求导。</strong></p><p>设可微分函数$y$有变量$u_1, u_2, \ldots, u_m$，其中每个可微分函数$u_i$都有变量$x_1, x_2, \ldots, x_n$，$y$是$x_1, x_2, \ldots, x_n$的函数。对于任意$i = 1, 2, \ldots, n$，链式法则给出：</p><script type="math/tex; mode=display">\frac{dy}{dx_i} = \frac{dy}{du_1} \frac{du_1}{dx_i} + \frac{dy}{du_2} \frac{du_2}{dx_i} + \cdots + \frac{dy}{du_m} \frac{du_m}{dx_i}</script><h2 id="反向传播（Back-Propagation）"><a href="#反向传播（Back-Propagation）" class="headerlink" title="反向传播（Back Propagation）"></a>反向传播（Back Propagation）</h2><p>现在，依据上方的基础知识，我们已经知道<strong>深度学习优化算法的关键在于梯度，梯度的关键在于求导，而这个导数通常不好求，需要链式法则的参与。</strong></p><p><strong>反向传播提供了一种基于链式法则快速计算任一偏导数的方法</strong>。在深度学习框架中，框架根据我们的模型构建<strong>计算图</strong>（computational graph），计算图用于跟踪数据、操作和组合顺序，正向传播（即由输入到输出进行计算）后框架通过<strong>自动微分</strong>（automatic differentiation）反向传播梯度，跟踪整个计算图，填充关于每个参数的偏导数。</p><p>在此我不准备罗列一堆复杂的数学公式来推导<strong>反向传播</strong>的计算细节（我也不会），而是想基于应用的角度，简单的理解一下反向传播。</p><p>设有一函数$y=2\mathbf{u} + 5$，其中$u = 3\mathbf{x}^{\top}\mathbf{x}$，$y$是关于$x$的函数。</p><p>下面我们来求$y$对$x$的梯度，求梯度本质上就是求偏导。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># requires_grad为x申请存放梯度的内存</span></span><br><span class="line">x = torch.arange(<span class="number">4.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># x的转置乘x == x和x的点积</span></span><br><span class="line">u = <span class="number">3</span> * torch.dot(x, x)</span><br><span class="line"><span class="built_in">print</span>(u)</span><br><span class="line"></span><br><span class="line">y = <span class="number">2</span> * u + <span class="number">5</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">tensor(<span class="number">42.</span>, grad_fn=&lt;MulBackward0&gt;)</span><br><span class="line">tensor(<span class="number">89.</span>, grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line"><span class="literal">None</span></span><br></pre></td></tr></table></figure><p>到这里$x$还没有梯度，因为还没有进行反向传播，下面来计算梯度。 原式$y$对$u$的导数为$2$，$u$对$x$的导数为$6x$，根据链式法则，$y$对$x$的导数为$12x$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">y.backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"><span class="built_in">print</span>(<span class="number">12</span> * x == x.grad)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([ <span class="number">0.</span>, <span class="number">12.</span>, <span class="number">24.</span>, <span class="number">36.</span>])</span><br><span class="line">tensor([<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>])</span><br></pre></td></tr></table></figure><p>验证得，梯度计算正确。 这里存在一个问题，为什么不去展示中间变量$u$的梯度呢？因为出于节省内存（显存）的考虑，pytorch在反向传播的过程中只保留了计算图中的叶子结点的梯度值，而未保留中间节点的梯度。但的确可以通过一些手段获取中间变量的梯度，只是没必要，这里也不做演示了。</p><h2 id="参考文献："><a href="#参考文献：" class="headerlink" title="参考文献："></a>参考文献：</h2><p>[1]<a href="https://zh-v2.d2l.ai/chapter_preliminaries/calculus.html">https://zh-v2.d2l.ai/chapter_preliminaries/calculus.html</a><br>[2]<a href="https://blog.csdn.net/Weary_PJ/article/details/105706318">https://blog.csdn.net/Weary_PJ/article/details/105706318</a><br>[3]<a href="https://blog.csdn.net/weixin_41799019/article/details/117353078">https://blog.csdn.net/weixin_41799019/article/details/117353078</a><br>[4]<a href="https://blog.csdn.net/xierhacker/article/details/53431207">https://blog.csdn.net/xierhacker/article/details/53431207</a></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;一个普通人尝试理解反向传播…从上至下阅读吧，看看有什么收获。&lt;br&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://silencezheng.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="机器学习" scheme="http://silencezheng.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>smartmontools安装使用</title>
    <link href="http://silencezheng.top/2022/09/08/article59/"/>
    <id>http://silencezheng.top/2022/09/08/article59/</id>
    <published>2022-09-08T10:58:22.000Z</published>
    <updated>2022-09-08T11:02:14.025Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>介绍安装和使用磁盘监控工具smartmontools，以Mac为例。<br><span id="more"></span><br>smartmontools软件包包含两个实用程序（<code>smartctl</code> 和 <code>smartd</code>），使用内置在大多数现代 ATA/SATA、SCSI/SAS 和 NVMe 磁盘中的自我监控、分析和报告技术系统 (SMART) 来控制和监控存储系统。在许多情况下，这些工具将提供磁盘降级和故障的高级警告。 Smartmontools 最初源自 Linux smartsuite 包，实际上支持 ATA/SATA、SCSI/SAS 和 NVMe 磁盘以及 SCSI/SAS 磁带设备。它应该可以在任何现代 Linux、FreeBSD、NetBSD、OpenBSD、Darwin (macOS)、Solaris、Windows、Cygwin、OS/2、eComStation 或 QNX 系统上运行。 Smartmontools 也可以从许多不同的 Live CD/DVD 之一运行。</p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p><code>brew install smartmontools</code></p><h2 id="检查硬盘"><a href="#检查硬盘" class="headerlink" title="检查硬盘"></a>检查硬盘</h2><p>方式一（通用）：<br>1、打开磁盘工具，找到设备名，如<code>disk3s1s1</code><br>2、<code>smartctl -a disk3s3s1</code></p><p>方式二（仅内置）：<br><code>smartctl -a disk0</code></p><h2 id="参数解析"><a href="#参数解析" class="headerlink" title="参数解析"></a>参数解析</h2><p><strong>ID1：Critical Warning警告状态</strong><br>RAW数值显示0为正常无警告，1为过热警告，2为闪存介质引起的内部错误导致可靠性降级，3为闪存进入只读状态，4为增强型断电保护功能失效（只针对有该特性的固态硬盘）。</p><p>正常情况下ID1的RAW属性值应为0，当显示为1时代表NVMe固态硬盘已经过热，需要改善散热条件或降低工作负载。属性值为2时应考虑返修或更换新硬盘，当属性值为3时硬盘已经进入只读状态，无法正常工作，应抓紧时间备份其中的数据。家用固态硬盘通常不会配备增强型断电保护（完整断电保护），所以通常该项目不会显示为4。</p><p><strong>ID2：Temperature当前温度（十进制显示）</strong></p><p><strong>ID3：Available Spare可用冗余空间（百分比显示）</strong><br>指示当前固态硬盘可用于替换坏块的保留备用块占出厂备用块总数量的百分比。该数值从出厂时的100%随使用过程降低，直至到零。ID3归零之前就有可能产生不可预料的故障，所以不要等到该项目彻底归零才考虑更换新硬盘。</p><p><strong>ID4：Available Spare Threshold备用空间阈值</strong><br>与ID3相关，当ID3的数值低于ID4所定义的阈值之后，固态硬盘被认为达到极限状态，此时系统可能会发出可靠性警告。该项数值由厂商定义，通常为10%或0%。</p><p><strong>ID5：Percentage Used已使用的写入耐久度（百分比显示）</strong><br>该项显示已产生的写入量占厂商定义总写入寿命的百分比。该项数值为动态显示，计算结果与写入量及固态硬盘的TBW总写入量指标有关。新盘状态下该项目为0%。</p><p><strong>ID6：Data Units Read读取扇区计数（1000）</strong><br>该项数值乘以1000后即为读取的扇区（512Byte）数量统计。</p><p><strong>ID7：Data Units Write写入扇区计数（1000）</strong><br>该项数值乘以1000后即为写入的扇区（512Byte）数量统计。</p><p><strong>ID8：Host Read Commands读取命令计数</strong><br>硬盘生命周期内累计接收到的读取命令数量统计。</p><p><strong>ID9：Host Write Commands写入命令计数</strong><br>硬盘生命周期内累计接收到的写入命令数量统计。</p><p><strong>ID10：Controller Busy Time主控繁忙时间计数</strong><br>该项统计的是主控忙于处理IO命令的时间总和（单位：分钟）。当IO队列有未完成的命令时，主控即处于“忙”的状态。</p><p><strong>ID11：Power Cycles通电次数</strong></p><p><strong>ID12：Power On Hours通电时间</strong></p><p><strong>ID13：Unsafe Shut downs不安全关机次数（异常断电计数）</strong></p><p><strong>ID14：Media and Data Integrity Errors闪存和数据完整性错误</strong></p><p>主控检测到未恢复的数据完整性错误的次数。正常情况下主控不应检测到数据完整性错误（纠错应该在此之前完成），当有不可校正的ECC、CRC校验失败或者LBA标签不匹配错误发生时，该数值会增加。正常情况下ID14应保持为零。</p><p><strong>ID15：Number of Error Information Log Entries错误日志条目计数</strong></p><p>控制器使用期限内，发生的错误信息日志条目的数量统计。正常情况该项目应为零。<br>有时该条目下会有<code>Read 1 entries from Error Information Log failed: GetLogPage failed: system=0x38, sub=0x0, code=745</code>之类的信息提示。</p><p><strong>以下项目</strong>为非标准项，并非所有NVMe SSD都支持显示。<br><strong>ID16：Warning Composite Temperature Time过热警告时间</strong><br><strong>ID17：Critical Composite Temerature Time过热临界温度时间</strong><br><strong>ID18-25：Temperature Sensor X：多个温度传感器（若存在）的读数</strong></p><p>参考：<a href="https://blog.csdn.net/qq_24343177/article/details/122521952">https://blog.csdn.net/qq_24343177/article/details/122521952</a></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;介绍安装和使用磁盘监控工具smartmontools，以Mac为例。&lt;br&gt;</summary>
    
    
    
    
    <category term="MacOS" scheme="http://silencezheng.top/tags/MacOS/"/>
    
  </entry>
  
  <entry>
    <title>现代循环神经网络--《动手学深度学习》笔记0x0A</title>
    <link href="http://silencezheng.top/2022/09/07/article58/"/>
    <id>http://silencezheng.top/2022/09/07/article58/</id>
    <published>2022-09-07T14:41:53.000Z</published>
    <updated>2022-09-07T15:20:50.097Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><p>前一章中介绍了循环神经网络的基础知识，这种网络可以更好地处理序列数据。但对于当今各种各样的序列学习问题，这些技术可能并不够用。</p><p>例如，循环神经网络在实践中一个常见问题是数值不稳定性。尽管我们已经应用了梯度裁剪等技巧来缓解这个问题，但是仍需要通过设计更复杂的序列模型可以进一步处理它。比如两个广泛使用的网络：<em>门控循环单元</em>（gated recurrent units，GRU）和<em>长短期记忆网络</em>（long short-term memory，LSTM）。然后本章将基于一个单向隐藏层来扩展循环神经网络架构，描述具有多个隐藏层的深层架构，并讨论基于前向和后向循环计算的双向设计。现代循环网络经常采用这种扩展。在解释这些循环神经网络的变体时将继续利用上一章中的语言建模问题。<br><span id="more"></span><br>事实上，语言建模只揭示了序列学习能力的冰山一角。在各种序列学习问题中，如自动语音识别、文本到语音转换和机器翻译，输入和输出都是任意长度的序列。为了阐述如何拟合这种类型的数据，我们将以机器翻译为例介绍基于循环神经网络的“编码器－解码器”架构和束搜索，并用它们来生成序列。</p><h3 id="0-1-小结"><a href="#0-1-小结" class="headerlink" title="0.1. 小结"></a>0.1. 小结</h3><ul><li>门控循环神经网络可以更好地捕获时间步距离很长的序列上的依赖关系。</li><li>重置门有助于捕获序列中的短期依赖关系。</li><li>更新门有助于捕获序列中的长期依赖关系。</li><li>重置门打开时，门控循环单元包含基本循环神经网络；更新门打开时，门控循环单元可以跳过子序列。</li><li>长短期记忆网络有三种类型的门：输入门、遗忘门和输出门。</li><li>长短期记忆网络的隐藏层输出包括“隐状态”和“记忆元”。只有隐状态会传递到输出层，而记忆元完全属于内部信息。</li><li>长短期记忆网络可以缓解梯度消失和梯度爆炸。</li><li>在深度循环神经网络中，隐状态的信息被传递到当前层的下一时间步和下一层的当前时间步。</li><li>有许多不同风格的深度循环神经网络， 如长短期记忆网络、门控循环单元、或经典循环神经网络。 这些模型在深度学习框架的高级API中都有涵盖。</li><li>总体而言，深度循环神经网络需要大量的调参（如学习率和修剪） 来确保合适的收敛，模型的初始化也需要谨慎。</li><li>在双向循环神经网络中，每个时间步的隐状态由当前时间步的前后数据同时决定。</li><li>双向循环神经网络与概率图模型中的“前向-后向”算法具有相似性。</li><li>双向循环神经网络主要用于序列编码和给定双向上下文的观测估计。</li><li>由于梯度链更长，因此双向循环神经网络的训练代价非常高</li><li>机器翻译指的是将文本序列从一种语言自动翻译成另一种语言。</li><li>使用单词级词元化时的词表大小，将明显大于使用字符级词元化时的词表大小。为了缓解这一问题，我们可以将低频词元视为相同的未知词元。</li><li>通过截断和填充文本序列，可以保证所有的文本序列都具有相同的长度，以便以小批量的方式加载。</li><li>“编码器－解码器”架构可以将长度可变的序列作为输入和输出，因此适用于机器翻译等序列转换问题。</li><li>编码器将长度可变的序列作为输入，并将其转换为具有固定形状的编码状态。</li><li>解码器将具有固定形状的编码状态映射为长度可变的序列。</li><li>根据“编码器-解码器”架构的设计， 我们可以使用两个循环神经网络来设计一个序列到序列学习的模型。</li><li>在实现编码器和解码器时，我们可以使用多层循环神经网络。</li><li>可以使用屏蔽（mask）来过滤不相关的计算，例如在计算损失时。</li><li>在“编码器－解码器”训练中，强制教学方法将原始输出序列（而非预测结果）输入解码器。</li><li>BLEU是一种常用的评估方法，它通过测量预测序列和标签序列之间的元语法的匹配度来评估预测。</li><li>序列搜索策略包括贪心搜索、穷举搜索和束搜索。</li><li>贪心搜索所选取序列的计算量最小，但精度相对较低。</li><li>穷举搜索所选取序列的精度最高，但计算量最大。</li><li>束搜索通过灵活选择束宽，在正确率和计算代价之间进行权衡。</li></ul><h2 id="1-门控循环单元（GRU）"><a href="#1-门控循环单元（GRU）" class="headerlink" title="1. 门控循环单元（GRU）"></a>1. 门控循环单元（GRU）</h2><p>上一章讨论了如何在循环神经网络中计算梯度，以及矩阵连续乘积可以导致梯度消失或梯度爆炸的问题。下面简单思考一下这种梯度异常在实践中的意义：</p><ul><li>可能会遇到这样的情况：早期观测值对预测所有未来观测值具有非常重要的意义。例如一个极端情况，其中第一个观测值包含一个校验和，目标是在序列的末尾辨别校验和是否正确。在这种情况下，第一个词元的影响至关重要。我们希望有某些机制能够在一个记忆元里存储重要的早期信息。如果没有这样的机制，我们将不得不给这个观测值指定一个非常大的梯度，因为它会影响所有后续的观测值。</li><li>可能会遇到这样的情况：一些词元没有相关的观测值。例如，在对网页内容进行情感分析时，可能有一些辅助HTML代码与网页传达的情绪无关。我们希望有一些机制来<em>跳过</em>隐状态表示中的此类词元。</li><li>可能会遇到这样的情况：序列的各个部分之间存在逻辑中断。例如，书的章节之间可能会有过渡存在，或者证券的熊市和牛市之间可能会有过渡存在。在这种情况下，最好有一种方法来<em>重置</em>内部状态表示。</li></ul><p>学术界已经提出了许多方法来解决这类问题。其中最早的方法是”长短期记忆”（long-short-term memory，LSTM）[<code>Hochreiter.Schmidhuber.1997</code>]。门控循环单元（gated recurrent unit，GRU）[<code>Cho.Van-Merrienboer.Bahdanau.ea.2014</code>]是一个稍微简化的变体，通常能够提供同等的效果，并且计算[<code>Chung.Gulcehre.Cho.ea.2014</code>]的速度明显更快。由于门控循环单元更简单，本章从它开始解读。</p><h3 id="1-1-门控隐状态"><a href="#1-1-门控隐状态" class="headerlink" title="1.1. 门控隐状态"></a>1.1. 门控隐状态</h3><p>门控循环单元与普通的循环神经网络之间的关键区别在于： 前者支持隐状态的门控。 这意味着模型有专门的机制来确定应该何时更新隐状态， 以及应该何时重置隐状态。 这些机制是可学习的，并且能够解决了上面列出的问题。 例如，如果第一个词元非常重要， 模型将学会在第一次观测之后不更新隐状态。 同样，模型也可以学会跳过不相关的临时观测。 最后，模型还将学会在需要的时候重置隐状态。 下面将详细讨论各类门控。</p><h4 id="1-1-1-重置门和更新门"><a href="#1-1-1-重置门和更新门" class="headerlink" title="1.1.1. 重置门和更新门"></a>1.1.1. 重置门和更新门</h4><p>首先介绍<em>重置门</em>（reset gate）和<em>更新门</em>（update gate）。它们被设计成$(0, 1)$区间中的向量，这样就可以进行凸组合。重置门允许我们控制“可能还想记住”的过去状态的数量；更新门将允许我们控制新状态中有多少个是旧状态的副本。</p><p>凸组合<br>: 设向量 $ { x_i }, i=1,2, \ldots, n $, 如有实数 $\lambda_i \geq 0$, 且 $\sum_{i=1}^n \lambda_i=1$, 则称 $\sum_{i=1}^n \lambda_i x_i$ 为向量 $ { x_i } $ 的一个凸组合(凸线性组合)。</p><p>下图描述了门控循环单元中的重置门和更新门的输入，输入是由当前时间步的输入和前一时间步的隐状态给出。两个门的输出是由使用sigmoid激活函数的两个全连接层给出。</p><p><img src="/assets/post_img/article58/gru-1.svg" alt="在门控循环单元模型中计算重置门和更新门"></p><p>来看一下门控循环单元的数学表达。对于给定的时间步$t$，假设输入是一个小批量$\mathbf{X}_t \in \mathbb{R}^{n \times d}$（样本个数$n$，输入个数$d$），上一个时间步的隐状态是$\mathbf{H}_{t-1} \in \mathbb{R}^{n \times h}$（隐藏单元个数$h$）。那么，重置门$\mathbf{R}_t \in \mathbb{R}^{n \times h}$和更新门$\mathbf{Z}_t \in \mathbb{R}^{n \times h}$的计算如下所示：</p><script type="math/tex; mode=display">\begin{aligned}\mathbf{R}_t = \sigma(\mathbf{X}_t \mathbf{W}_{xr} + \mathbf{H}_{t-1} \mathbf{W}_{hr} + \mathbf{b}_r),\\\mathbf{Z}_t = \sigma(\mathbf{X}_t \mathbf{W}_{xz} + \mathbf{H}_{t-1} \mathbf{W}_{hz} + \mathbf{b}_z),\end{aligned}</script><p>其中$\mathbf{W}_{xr}, \mathbf{W}_{xz} \in \mathbb{R}^{d \times h}$和$\mathbf{W}_{hr}, \mathbf{W}_{hz} \in \mathbb{R}^{h \times h}$是权重参数，$\mathbf{b}_r, \mathbf{b}_z \in \mathbb{R}^{1 \times h}$是偏置参数。注意，在求和过程中会触发广播机制。使用sigmoid函数的目的是将输入值转换到区间$(0, 1)$。</p><h4 id="1-1-2-候选隐状态"><a href="#1-1-2-候选隐状态" class="headerlink" title="1.1.2. 候选隐状态"></a>1.1.2. 候选隐状态</h4><p>接下来将重置门$\mathbf{R}_t$与前一章中的常规隐状态更新机制集成，得到在时间步$t$的<em>候选隐状态</em>（candidate hidden state）$\tilde{\mathbf{H}}_t \in \mathbb{R}^{n \times h}$。</p><script type="math/tex; mode=display">\tilde{\mathbf{H}}_t = \tanh(\mathbf{X}_t \mathbf{W}_{xh} + \left(\mathbf{R}_t \odot \mathbf{H}_{t-1}\right) \mathbf{W}_{hh} + \mathbf{b}_h),</script><p>其中$\mathbf{W}_{xh} \in \mathbb{R}^{d \times h}$和$\mathbf{W}_{hh} \in \mathbb{R}^{h \times h}$是权重参数，$\mathbf{b}_h \in \mathbb{R}^{1 \times h}$是偏置项，符号$\odot$是Hadamard积（按元素乘积）运算符。这里使用tanh非线性激活函数来确保候选隐状态中的值保持在区间$(-1, 1)$中。</p><p>与常规隐状态更新机制$\mathbf{H}_t = \phi(\mathbf{X}_t \mathbf{W}_{xh} + \mathbf{H}_{t-1} \mathbf{W}_{hh}  + \mathbf{b}_h)$相比，上式中的$\mathbf{R}_t$和$\mathbf{H}_{t-1}$的元素相乘可以减少以往状态的影响。每当重置门$\mathbf{R}_t$中的项接近$1$时，会恢复一个常规隐状态更新的普通循环神经网络。对于重置门$\mathbf{R}_t$中所有接近$0$的项，候选隐状态是以$\mathbf{X}_t$作为输入的多层感知机的结果。因此，任何预先存在的隐状态都会被<em>重置</em>为默认值。下图说明了应用重置门之后的计算流程。</p><p><img src="/assets/post_img/article58/gru-2.svg" alt="在门控循环单元模型中计算候选隐状态"></p><h4 id="1-1-3-隐状态"><a href="#1-1-3-隐状态" class="headerlink" title="1.1.3. 隐状态"></a>1.1.3. 隐状态</h4><p>上述的计算结果只是候选隐状态，之后仍然需要结合更新门$\mathbf{Z}_t$的效果。这一步确定新的隐状态$\mathbf{H}_t \in \mathbb{R}^{n \times h}$在多大程度上来自旧的状态$\mathbf{H}_{t-1}$和新的候选状态$\tilde{\mathbf{H}}_t$。更新门$\mathbf{Z}_t$仅需要在$\mathbf{H}_{t-1}$和$\tilde{\mathbf{H}}_t$之间进行按元素的凸组合就可以实现这个目标。这就得出了门控循环单元的最终更新公式：</p><script type="math/tex; mode=display">\mathbf{H}_t = \mathbf{Z}_t \odot \mathbf{H}_{t-1}  + (1 - \mathbf{Z}_t) \odot \tilde{\mathbf{H}}_t.</script><p>每当更新门$\mathbf{Z}_t$接近$1$时，模型就倾向只保留旧状态。此时，来自$\mathbf{X}_t$的信息基本上被忽略，从而有效地跳过了依赖链条中的时间步$t$。相反，当$\mathbf{Z}_t$接近$0$时，新的隐状态$\mathbf{H}_t$就会接近候选隐状态$\tilde{\mathbf{H}}_t$。这些设计可以帮助我们处理循环神经网络中的梯度消失问题，并更好地捕获时间步距离很长的序列的依赖关系。例如，如果整个子序列的所有时间步的更新门都接近于$1$，则无论序列的长度如何，在序列起始时间步的旧隐状态都将很容易保留并传递到序列结束。</p><p>下图说明了更新门起作用后的计算流。</p><p><img src="/assets/post_img/article58/gru-3.svg" alt="计算门控循环单元模型中的隐状态"></p><p>总之，门控循环单元具有以下两个显著特征：</p><ul><li>重置门有助于捕获序列中的短期依赖关系；</li><li>更新门有助于捕获序列中的长期依赖关系。</li></ul><h3 id="1-2-从零实现"><a href="#1-2-从零实现" class="headerlink" title="1.2. 从零实现"></a>1.2. 从零实现</h3><p>首先读取时间机器数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br></pre></td></tr></table></figure><h4 id="1-2-1-初始化模型参数"><a href="#1-2-1-初始化模型参数" class="headerlink" title="1.2.1. 初始化模型参数"></a>1.2.1. 初始化模型参数</h4><p>下一步是初始化模型参数。从标准差为$0.01$的高斯分布中提取权重，并将偏置项设为$0$，超参数<code>num_hiddens</code>定义隐藏单元的数量，实例化与更新门、重置门、候选隐状态和输出层相关的所有权重和偏置。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_params</span>(<span class="params">vocab_size, num_hiddens, device</span>):</span></span><br><span class="line">    num_inputs = num_outputs = vocab_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">normal</span>(<span class="params">shape</span>):</span></span><br><span class="line">        <span class="keyword">return</span> torch.randn(size=shape, device=device)*<span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">three</span>():</span></span><br><span class="line">        <span class="keyword">return</span> (normal((num_inputs, num_hiddens)),</span><br><span class="line">                normal((num_hiddens, num_hiddens)),</span><br><span class="line">                torch.zeros(num_hiddens, device=device))</span><br><span class="line"></span><br><span class="line">    W_xz, W_hz, b_z = three()  <span class="comment"># 更新门参数</span></span><br><span class="line">    W_xr, W_hr, b_r = three()  <span class="comment"># 重置门参数</span></span><br><span class="line">    W_xh, W_hh, b_h = three()  <span class="comment"># 候选隐状态参数</span></span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = normal((num_hiddens, num_outputs))</span><br><span class="line">    b_q = torch.zeros(num_outputs, device=device)</span><br><span class="line">    <span class="comment"># 附加梯度</span></span><br><span class="line">    params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q]</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> params</span><br></pre></td></tr></table></figure><h4 id="1-2-2-定义模型"><a href="#1-2-2-定义模型" class="headerlink" title="1.2.2. 定义模型"></a>1.2.2. 定义模型</h4><p>现在定义隐状态的初始化函数<code>init_gru_state</code>。与上一章中从零实现RNN中定义的<code>init_rnn_state</code>函数一样，此函数返回一个形状为（批量大小，隐藏单元个数）的张量，张量的值全部为零。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_gru_state</span>(<span class="params">batch_size, num_hiddens, device</span>):</span></span><br><span class="line">    <span class="keyword">return</span> (torch.zeros((batch_size, num_hiddens), device=device), )</span><br></pre></td></tr></table></figure><p>现在定义门控循环单元模型，模型的架构与基本的循环神经网络单元是相同的，只是权重更新公式更为复杂。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gru</span>(<span class="params">inputs, state, params</span>):</span></span><br><span class="line">    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class="line">    H, = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        Z = torch.sigmoid((X @ W_xz) + (H @ W_hz) + b_z)</span><br><span class="line">        R = torch.sigmoid((X @ W_xr) + (H @ W_hr) + b_r)</span><br><span class="line">        H_tilda = torch.tanh((X @ W_xh) + ((R * H) @ W_hh) + b_h)</span><br><span class="line">        H = Z * H + (<span class="number">1</span> - Z) * H_tilda</span><br><span class="line">        Y = H @ W_hq + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">0</span>), (H,)</span><br></pre></td></tr></table></figure><h4 id="1-2-3-训练与预测"><a href="#1-2-3-训练与预测" class="headerlink" title="1.2.3. 训练与预测"></a>1.2.3. 训练与预测</h4><p>训练和预测的工作方式与上一章完全相同。 训练结束后，分别打印输出训练集的困惑度， 以及前缀“time traveler”和“traveler”的预测序列上的困惑度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vocab_size, num_hiddens, device = <span class="built_in">len</span>(vocab), <span class="number">256</span>, d2l.try_gpu()</span><br><span class="line">num_epochs, lr = <span class="number">500</span>, <span class="number">1</span></span><br><span class="line">model = d2l.RNNModelScratch(<span class="built_in">len</span>(vocab), num_hiddens, device, get_params,</span><br><span class="line">                            init_gru_state, gru)</span><br><span class="line">d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br></pre></td></tr></table></figure><h3 id="1-3-框架实现"><a href="#1-3-框架实现" class="headerlink" title="1.3. 框架实现"></a>1.3. 框架实现</h3><p>高级API包含了前文介绍的所有配置细节， 所以可以直接实例化门控循环单元模型。 这段代码的运行速度要快得多， 因为它使用的是编译好的运算符而不是Python来处理之前阐述的许多细节。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">num_inputs = vocab_size</span><br><span class="line">gru_layer = nn.GRU(num_inputs, num_hiddens)</span><br><span class="line">model = d2l.RNNModel(gru_layer, <span class="built_in">len</span>(vocab))</span><br><span class="line">model = model.to(device)</span><br><span class="line">d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br></pre></td></tr></table></figure><h2 id="2-长短期记忆网络（LSTM）"><a href="#2-长短期记忆网络（LSTM）" class="headerlink" title="2. 长短期记忆网络（LSTM）"></a>2. 长短期记忆网络（LSTM）</h2><p>长期以来，隐变量模型存在着长期信息保存和短期输入缺失的问题。解决这一问题的最早方法之一是长短期存储器（long short-term memory，LSTM）[<code>Hochreiter.Schmidhuber.1997</code>]。它有许多与门控循环单元一样的属性。长短期记忆网络的设计比门控循环单元稍微复杂一些，却比门控循环单元早诞生了近20年。</p><h3 id="2-1-门控记忆元"><a href="#2-1-门控记忆元" class="headerlink" title="2.1. 门控记忆元"></a>2.1. 门控记忆元</h3><p>长短期记忆网络的设计灵感来自于计算机的逻辑门。长短期记忆网络引入了<em>记忆元</em>（memory cell），或简称为<em>单元</em>（cell）。有些文献认为记忆元是隐状态的一种特殊类型，它们与隐状态具有相同的形状，其设计目的是用于记录附加的信息。为了控制记忆元，我们需要许多门。其中一个门用来从单元中输出条目，称其为<em>输出门</em>（output gate）。另外一个门用来决定何时将数据读入单元，称其为<em>输入门</em>（input gate）。还需要一种机制来重置单元的内容，由<em>遗忘门</em>（forget gate）来管理，这种设计的动机与门控循环单元相同，能够通过专用机制决定什么时候记忆或忽略隐状态中的输入。下面看看这在实践中是如何运作的。</p><h4 id="2-1-1-输入门、忘记门和输出门"><a href="#2-1-1-输入门、忘记门和输出门" class="headerlink" title="2.1.1. 输入门、忘记门和输出门"></a>2.1.1. 输入门、忘记门和输出门</h4><p>就如在门控循环单元中一样，当前时间步的输入和前一个时间步的隐状态作为数据送入长短期记忆网络的门中，如下图所示。它们由三个具有sigmoid激活函数的全连接层处理，以计算输入门、遗忘门和输出门的值。因此，这三个门的值都在$(0, 1)$的范围内。</p><p><img src="/assets/post_img/article58/lstm-0.svg" alt="长短期记忆模型中的输入门、遗忘门和输出门"></p><p>详细了解一下长短期记忆网络的数学表达。假设有$h$个隐藏单元，批量大小为$n$，输入数为$d$。因此，输入为$\mathbf{X}_t \in \mathbb{R}^{n \times d}$，前一时间步的隐状态为$\mathbf{H}_{t-1} \in \mathbb{R}^{n \times h}$。相应地，时间步$t$的门被定义如下：输入门是$\mathbf{I}_t \in \mathbb{R}^{n \times h}$，遗忘门是$\mathbf{F}_t \in \mathbb{R}^{n \times h}$，输出门是$\mathbf{O}_t \in \mathbb{R}^{n \times h}$。它们的计算方法如下：</p><script type="math/tex; mode=display">\begin{aligned}\mathbf{I}_t &= \sigma(\mathbf{X}_t \mathbf{W}_{xi} + \mathbf{H}_{t-1} \mathbf{W}_{hi} + \mathbf{b}_i),\\\mathbf{F}_t &= \sigma(\mathbf{X}_t \mathbf{W}_{xf} + \mathbf{H}_{t-1} \mathbf{W}_{hf} + \mathbf{b}_f),\\\mathbf{O}_t &= \sigma(\mathbf{X}_t \mathbf{W}_{xo} + \mathbf{H}_{t-1} \mathbf{W}_{ho} + \mathbf{b}_o),\end{aligned}</script><p>其中$\mathbf{W}_{xi}, \mathbf{W}_{xf}, \mathbf{W}_{xo} \in \mathbb{R}^{d \times h}$和$\mathbf{W}_{hi}, \mathbf{W}_{hf}, \mathbf{W}_{ho} \in \mathbb{R}^{h \times h}$是权重参数，$\mathbf{b}_i, \mathbf{b}_f, \mathbf{b}_o \in \mathbb{R}^{1 \times h}$是偏置参数。</p><h4 id="2-1-2-候选记忆元"><a href="#2-1-2-候选记忆元" class="headerlink" title="2.1.2. 候选记忆元"></a>2.1.2. 候选记忆元</h4><p>由于还没有指定各种门的操作，所以先介绍<em>候选记忆元</em>（candidate memory cell）$\tilde{\mathbf{C}}_t \in \mathbb{R}^{n \times h}$。它的计算与上面描述的三个门的计算类似，但是使用$\tanh$函数作为激活函数，函数的值范围为$(-1, 1)$。下面导出在时间步$t$处的方程：</p><script type="math/tex; mode=display">\tilde{\mathbf{C}}_t = \text{tanh}(\mathbf{X}_t \mathbf{W}_{xc} + \mathbf{H}_{t-1} \mathbf{W}_{hc} + \mathbf{b}_c),</script><p>其中$\mathbf{W}_{xc} \in \mathbb{R}^{d \times h}$和<br>$\mathbf{W}_{hc} \in \mathbb{R}^{h \times h}$是权重参数，$\mathbf{b}_c \in \mathbb{R}^{1 \times h}$是偏置参数。如下图所示：</p><p><img src="/assets/post_img/article58/lstm-1.svg" alt="长短期记忆模型中的候选记忆元"></p><h4 id="2-1-3-记忆元"><a href="#2-1-3-记忆元" class="headerlink" title="2.1.3. 记忆元"></a>2.1.3. 记忆元</h4><p>在门控循环单元中，有一种机制来控制输入和遗忘（或称跳过）。类似地，在长短期记忆网络中，也有两个门用于这样的目的：输入门$\mathbf{I}_t$控制采用多少来自$\tilde{\mathbf{C}}_t$的新数据，而遗忘门$\mathbf{F}_t$控制保留多少过去的记忆元$\mathbf{C}_{t-1} \in \mathbb{R}^{n \times h}$的内容。使用按元素乘法，得出：</p><script type="math/tex; mode=display">\mathbf{C}_t = \mathbf{F}_t \odot \mathbf{C}_{t-1} + \mathbf{I}_t \odot \tilde{\mathbf{C}}_t.</script><p>如果遗忘门始终为$1$且输入门始终为$0$，则过去的记忆元$\mathbf{C}_{t-1}$将随时间被保存并传递到当前时间步。引入这种设计是为了缓解梯度消失问题，并更好地捕获序列中的长距离依赖关系。<br>这样就得到了计算记忆元的流程图，如下图。</p><p><img src="/assets/post_img/article58/lstm-2.svg" alt="在长短期记忆网络模型中计算记忆元"></p><h4 id="2-1-4-隐状态"><a href="#2-1-4-隐状态" class="headerlink" title="2.1.4. 隐状态"></a>2.1.4. 隐状态</h4><p>最后需要定义如何计算隐状态$\mathbf{H}_t \in \mathbb{R}^{n \times h}$，这就是输出门发挥作用的地方。在长短期记忆网络中，它仅仅是记忆元的$\tanh$的门控版本。这就确保了$\mathbf{H}_t$的值始终在区间$(-1, 1)$内（因为输出门在0和1之间）：</p><script type="math/tex; mode=display">\mathbf{H}_t = \mathbf{O}_t \odot \tanh(\mathbf{C}_t).</script><p>只要输出门接近$1$，就能够有效地将所有记忆信息传递给预测部分，而对于输出门接近$0$，则只保留记忆元内的所有信息，而不需要更新隐状态。<br>下图提供了数据流的图形化演示。</p><p><img src="/assets/post_img/article58/lstm-3.svg" alt="在长短期记忆模型中计算隐状态"></p><h3 id="2-2-从零实现"><a href="#2-2-从零实现" class="headerlink" title="2.2. 从零实现"></a>2.2. 从零实现</h3><p>首先加载时光机器数据集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br></pre></td></tr></table></figure><h4 id="2-2-1-初始化模型参数"><a href="#2-2-1-初始化模型参数" class="headerlink" title="2.2.1. 初始化模型参数"></a>2.2.1. 初始化模型参数</h4><p>接下来需要定义和初始化模型参数。如前所述，超参数<code>num_hiddens</code>定义隐藏单元的数量。按照标准差$0.01$的高斯分布初始化权重，并将偏置项设为$0$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_lstm_params</span>(<span class="params">vocab_size, num_hiddens, device</span>):</span></span><br><span class="line">    num_inputs = num_outputs = vocab_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">normal</span>(<span class="params">shape</span>):</span></span><br><span class="line">        <span class="keyword">return</span> torch.randn(size=shape, device=device)*<span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">three</span>():</span></span><br><span class="line">        <span class="keyword">return</span> (normal((num_inputs, num_hiddens)),</span><br><span class="line">                normal((num_hiddens, num_hiddens)),</span><br><span class="line">                torch.zeros(num_hiddens, device=device))</span><br><span class="line"></span><br><span class="line">    W_xi, W_hi, b_i = three()  <span class="comment"># 输入门参数</span></span><br><span class="line">    W_xf, W_hf, b_f = three()  <span class="comment"># 遗忘门参数</span></span><br><span class="line">    W_xo, W_ho, b_o = three()  <span class="comment"># 输出门参数</span></span><br><span class="line">    W_xc, W_hc, b_c = three()  <span class="comment"># 候选记忆元参数</span></span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = normal((num_hiddens, num_outputs))</span><br><span class="line">    b_q = torch.zeros(num_outputs, device=device)</span><br><span class="line">    <span class="comment"># 附加梯度</span></span><br><span class="line">    params = [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc,</span><br><span class="line">              b_c, W_hq, b_q]</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> params</span><br></pre></td></tr></table></figure><h4 id="2-2-2-定义模型"><a href="#2-2-2-定义模型" class="headerlink" title="2.2.2. 定义模型"></a>2.2.2. 定义模型</h4><p>在初始化函数中，长短期记忆网络的隐状态需要返回一个<em>额外</em>的记忆元，单元的值为0，形状为（批量大小，隐藏单元数）。因此得到以下的状态初始化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_lstm_state</span>(<span class="params">batch_size, num_hiddens, device</span>):</span></span><br><span class="line">    <span class="keyword">return</span> (torch.zeros((batch_size, num_hiddens), device=device),</span><br><span class="line">            torch.zeros((batch_size, num_hiddens), device=device))</span><br></pre></td></tr></table></figure><p>实际模型的定义与前面讨论的一样：提供三个门和一个额外的记忆元。注意只有隐状态才会传递到输出层，而记忆元$\mathbf{C}_t$不直接参与输出计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm</span>(<span class="params">inputs, state, params</span>):</span></span><br><span class="line">    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c,</span><br><span class="line">     W_hq, b_q] = params</span><br><span class="line">    (H, C) = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        I = torch.sigmoid((X @ W_xi) + (H @ W_hi) + b_i)</span><br><span class="line">        F = torch.sigmoid((X @ W_xf) + (H @ W_hf) + b_f)</span><br><span class="line">        O = torch.sigmoid((X @ W_xo) + (H @ W_ho) + b_o)</span><br><span class="line">        C_tilda = torch.tanh((X @ W_xc) + (H @ W_hc) + b_c)</span><br><span class="line">        C = F * C + I * C_tilda</span><br><span class="line">        H = O * torch.tanh(C)</span><br><span class="line">        Y = (H @ W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">0</span>), (H, C)</span><br></pre></td></tr></table></figure><h4 id="2-2-3-训练和预测"><a href="#2-2-3-训练和预测" class="headerlink" title="2.2.3. 训练和预测"></a>2.2.3. 训练和预测</h4><p>通过实例化上一章引入的RNNModelScratch类来训练一个长短期记忆网络，就如在上一节中所做的一样。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vocab_size, num_hiddens, device = <span class="built_in">len</span>(vocab), <span class="number">256</span>, d2l.try_gpu()</span><br><span class="line">num_epochs, lr = <span class="number">500</span>, <span class="number">1</span></span><br><span class="line">model = d2l.RNNModelScratch(<span class="built_in">len</span>(vocab), num_hiddens, device, get_lstm_params,</span><br><span class="line">                            init_lstm_state, lstm)</span><br><span class="line">d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br></pre></td></tr></table></figure><h3 id="2-3-框架实现"><a href="#2-3-框架实现" class="headerlink" title="2.3. 框架实现"></a>2.3. 框架实现</h3><p>使用高级API可以直接实例化LSTM模型。 高级API封装了前文介绍的所有配置细节。 这段代码的运行速度要快得多， 因为它使用的是编译好的运算符而不是Python来处理之前阐述的许多细节。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">num_inputs = vocab_size</span><br><span class="line">lstm_layer = nn.LSTM(num_inputs, num_hiddens)</span><br><span class="line">model = d2l.RNNModel(lstm_layer, <span class="built_in">len</span>(vocab))</span><br><span class="line">model = model.to(device)</span><br><span class="line">d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br></pre></td></tr></table></figure><p>长短期记忆网络是典型的具有重要状态控制的隐变量自回归模型。 多年来已经提出了其许多变体，例如，多层、残差连接、不同类型的正则化。 然而，由于序列的长距离依赖性，训练长短期记忆网络和其他序列模型（例如门控循环单元）的成本是相当高的。 后面的内容中将讲述更高级的替代模型，如transformer。</p><h2 id="3-深度循环神经网络"><a href="#3-深度循环神经网络" class="headerlink" title="3. 深度循环神经网络"></a>3. 深度循环神经网络</h2><p>到目前为止，我们一直专注于定义由序列输入、单个隐藏 RNN 层和输出层组成的网络。尽管在任何时间步长的输入和相应的输出之间只有一个隐藏层，但这些网络在某种意义上是很深的。第一个时间步的输入可以影响最后一个时间步$T$的输出 （通常是 100 或 1000 步之后）。这些输入通过长度为$T$的时间距离，在产生最终输出之前一直应用了<em>循环层</em>。但通常人们也希望保留在某时间步长上的输入与输出之间表达复杂关系的能力。因此，我们经常构建不仅在时间方向上而且在输入到输出方向上都很深的 RNN。这正是在开发 MLP 和深度 CNN 时已经遇到的深度概念。</p><p>构建这种深度 RNN 的标准方法非常简单：可以将多层循环神经网络堆叠在一起，给定一个长度序列$T$，第一个 RNN 产生一系列输出，长度也一样为$T$. 这些又构成了下一个 RNN 层的输入。通过对几个简单层的组合，产生了一个灵活的机制。特别的是数据可能与不同层的堆叠有关。例如人们可能希望保持有关金融市场状况（熊市或牛市）的宏观数据可用，而微观数据只记录较短期的时间动态。</p><p>下图描述了一个具有$L$个隐藏层的深度循环神经网络，每个隐状态都连续地传递到当前层的下一个时间步和下一层的当前时间步。</p><p><img src="/assets/post_img/article58/deep-rnn.svg" alt="深度循环神经网络结构"></p><h3 id="3-1-函数依赖关系"><a href="#3-1-函数依赖关系" class="headerlink" title="3.1. 函数依赖关系"></a>3.1. 函数依赖关系</h3><p>可以将深度架构中的函数依赖关系形式化，这个架构是由上图中描述的$L$个隐藏层构成。后续的讨论主要集中在经典的循环神经网络模型上，但是这些讨论也适应于其他序列模型。</p><p>假设在时间步$t$有一个小批量的输入数据$\mathbf{X}_t \in \mathbb{R}^{n \times d}$（样本数：$n$，每个样本中的输入数：$d$）。同时，将$l^\mathrm{th}$隐藏层（$l=1,\ldots,L$）的隐状态设为$\mathbf{H}_t^{(l)}  \in \mathbb{R}^{n \times h}$（隐藏单元数：$h$），输出层变量设为$\mathbf{O}_t \in \mathbb{R}^{n \times q}$（输出数：$q$）。设置$\mathbf{H}_t^{(0)} = \mathbf{X}_t$，第$l$个隐藏层的隐状态使用激活函数$\phi_l$，则：</p><script type="math/tex; mode=display">\mathbf{H}_t^{(l)} = \phi_l(\mathbf{H}_t^{(l-1)} \mathbf{W}_{xh}^{(l)} + \mathbf{H}_{t-1}^{(l)} \mathbf{W}_{hh}^{(l)}  + \mathbf{b}_h^{(l)}),</script><p>其中，权重$\mathbf{W}_{xh}^{(l)} \in \mathbb{R}^{h \times h}$，$\mathbf{W}_{hh}^{(l)} \in \mathbb{R}^{h \times h}$和偏置$\mathbf{b}_h^{(l)} \in \mathbb{R}^{1 \times h}$都是第$l$个隐藏层的模型参数。</p><p>最后，输出层的计算仅基于第$l$个隐藏层最终的隐状态：</p><script type="math/tex; mode=display">\mathbf{O}_t = \mathbf{H}_t^{(L)} \mathbf{W}_{hq} + \mathbf{b}_q,</script><p>其中，权重$\mathbf{W}_{hq} \in \mathbb{R}^{h \times q}$和偏置$\mathbf{b}_q \in \mathbb{R}^{1 \times q}$都是输出层的模型参数。</p><p>与多层感知机一样，隐藏层数目$L$和隐藏单元数目$h$都是超参数，它们可以被人为调整。另外，用门控循环单元或长短期记忆网络的隐状态来代替深度循环网络中的隐状态进行计算，可以得到深度门控循环神经网络或深度长短期记忆神经网络。</p><h3 id="3-2-框架实现"><a href="#3-2-框架实现" class="headerlink" title="3.2. 框架实现"></a>3.2. 框架实现</h3><p>实现多层循环神经网络所需的许多逻辑细节在高级API中都是现成的。 简单起见这里仅示范使用此类内置函数的使用方式。 以长短期记忆网络模型为例， 该代码与上节中使用的代码非常相似，唯一的区别是我们指定了层的数量，而不是使用单一层这个默认值。从加载数据集开始。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br></pre></td></tr></table></figure><p>像选择超参数这类架构决策也跟上节中的决策非常相似。因为我们有不同的词元，所以输入和输出都选择相同数量（why？），即<code>vocab_size</code>。隐藏单元的数量仍然是$256$。唯一的区别是现在通过<code>num_layers</code>的值来设定隐藏层数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vocab_size, num_hiddens, num_layers = <span class="built_in">len</span>(vocab), <span class="number">256</span>, <span class="number">2</span></span><br><span class="line">num_inputs = vocab_size</span><br><span class="line">device = d2l.try_gpu()</span><br><span class="line">lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers)</span><br><span class="line">model = d2l.RNNModel(lstm_layer, <span class="built_in">len</span>(vocab))</span><br><span class="line">model = model.to(device)</span><br></pre></td></tr></table></figure><h3 id="3-3-训练与预测"><a href="#3-3-训练与预测" class="headerlink" title="3.3. 训练与预测"></a>3.3. 训练与预测</h3><p>由于使用了长短期记忆网络模型来实例化两个层，因此训练速度被大大降低了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, lr = <span class="number">500</span>, <span class="number">2</span></span><br><span class="line">d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br></pre></td></tr></table></figure><h2 id="4-双向循环神经网络（Bidirectional-RNN）"><a href="#4-双向循环神经网络（Bidirectional-RNN）" class="headerlink" title="4. 双向循环神经网络（Bidirectional RNN）"></a>4. 双向循环神经网络（Bidirectional RNN）</h2><p>在序列学习中，以往假设的目标是：在给定观测的情况下（例如，在时间序列的上下文中或在语言模型的上下文中），对下一个输出进行建模。虽然这是一个典型情景，但不是唯一的。还可能发生什么其它的情况呢？考虑以下三个在文本序列中填空的任务。</p><ul><li>我<code>___</code>。</li><li>我<code>___</code>饿了。</li><li>我<code>___</code>饿了，我可以吃半头猪。</li></ul><p>根据可获得的信息量，可以用不同的词填空，如“很高兴”（”happy”）、“不”（”not”）和“非常”（”very”）。很明显，每个短语的“下文”传达了重要信息（如果有的话），而这些信息关乎到选择哪个词来填空，所以无法利用这一点的序列模型将在相关任务上表现不佳。例如，如果要做好命名实体识别（例如，识别“Green”指的是“格林先生”还是绿色），不同长度的上下文范围重要性是相同的。为了获得一些解决问题的灵感，让我们先迂回到概率图模型。</p><h3 id="4-1-隐马尔可夫模型中的动态规划"><a href="#4-1-隐马尔可夫模型中的动态规划" class="headerlink" title="4.1. 隐马尔可夫模型中的动态规划"></a>4.1. 隐马尔可夫模型中的动态规划</h3><p>这一小节是用来说明动态规划问题的，具体的技术细节对于理解深度学习模型并不重要，但有助于思考为什么要使用深度学习，以及为什么要选择特定的架构。</p><p>如果想用概率图模型来解决这个问题，可以设计一个隐变量模型：在任意时间步$t$，假设存在某个隐变量$h_t$，通过概率$P(x_t \mid h_t)$控制我们观测到的$x_t$。此外，任何$h_t \to h_{t+1}$转移都是由一些状态转移概率$P(h_{t+1} \mid h_{t})$给出。这个概率图模型就是一个<em>隐马尔可夫模型</em>（hidden Markov model，HMM），如下图所示。</p><p><img src="/assets/post_img/article58/hmm.svg" alt="隐马尔可夫模型"></p><p>因此，对于有$T$个观测值的序列，在观测状态和隐状态上具有以下联合概率分布：</p><script type="math/tex; mode=display">P(x_1, \ldots, x_T, h_1, \ldots, h_T) = \prod_{t=1}^T P(h_t \mid h_{t-1}) P(x_t \mid h_t), \text{ where } P(h_1 \mid h_0) = P(h_1).</script><p>现在假设观测到了所有的$x_i$，除了$x_j$，并且我们的目标是计算$P(x_j \mid x_{-j})$，其中$x_{-j} = (x_1, \ldots, x_{j-1}, x_{j+1}, \ldots, x_{T})$。由于$P(x_j \mid x_{-j})$中没有隐变量，因此我们考虑对$h_1, \ldots, h_T$选择构成的所有可能的组合进行求和。如果任何$h_i$可以接受$k$个不同的值（有限的状态数），则意味着需要对$k^T$个项求和，这个任务显然难于登天。幸运的是，有个巧妙的解决方案：<em>动态规划</em>（dynamic programming）。</p><p>为了解动态规划的工作方式，考虑对隐变量$h_1, \ldots, h_T$的依次求和。根据上式，将得出：</p><script type="math/tex; mode=display">\begin{aligned}    &P(x_1, \ldots, x_T) \\    =& \sum_{h_1, \ldots, h_T} P(x_1, \ldots, x_T, h_1, \ldots, h_T) \\    =& \sum_{h_1, \ldots, h_T} \prod_{t=1}^T P(h_t \mid h_{t-1}) P(x_t \mid h_t) \\    =& \sum_{h_2, \ldots, h_T} \underbrace{\left[\sum_{h_1} P(h_1) P(x_1 \mid h_1) P(h_2 \mid h_1)\right]}_{\pi_2(h_2) \stackrel{\mathrm{def}}{=}}    P(x_2 \mid h_2) \prod_{t=3}^T P(h_t \mid h_{t-1}) P(x_t \mid h_t) \\    =& \sum_{h_3, \ldots, h_T} \underbrace{\left[\sum_{h_2} \pi_2(h_2) P(x_2 \mid h_2) P(h_3 \mid h_2)\right]}_{\pi_3(h_3)\stackrel{\mathrm{def}}{=}}    P(x_3 \mid h_3) \prod_{t=4}^T P(h_t \mid h_{t-1}) P(x_t \mid h_t)\\    =& \dots \\    =& \sum_{h_T} \pi_T(h_T) P(x_T \mid h_T).\end{aligned}</script><p>通常我们将<em>前向递归</em>（forward recursion）写为：</p><script type="math/tex; mode=display">\pi_{t+1}(h_{t+1}) = \sum_{h_t} \pi_t(h_t) P(x_t \mid h_t) P(h_{t+1} \mid h_t).</script><p>递归被初始化为$\pi_1(h_1) = P(h_1)$。符号简化，也可以写成$\pi_{t+1} = f(\pi_t, x_t)$，其中$f$是一些可学习的函数。这看起来就像循环神经网络中讨论的隐变量模型中的更新方程。</p><p>与前向递归（或称正向）一样，也可以使用后向递归对同一组隐变量求和。这将得到：</p><script type="math/tex; mode=display">\begin{aligned}    & P(x_1, \ldots, x_T) \\     =& \sum_{h_1, \ldots, h_T} P(x_1, \ldots, x_T, h_1, \ldots, h_T) \\    =& \sum_{h_1, \ldots, h_T} \prod_{t=1}^{T-1} P(h_t \mid h_{t-1}) P(x_t \mid h_t) \cdot P(h_T \mid h_{T-1}) P(x_T \mid h_T) \\    =& \sum_{h_1, \ldots, h_{T-1}} \prod_{t=1}^{T-1} P(h_t \mid h_{t-1}) P(x_t \mid h_t) \cdot    \underbrace{\left[\sum_{h_T} P(h_T \mid h_{T-1}) P(x_T \mid h_T)\right]}_{\rho_{T-1}(h_{T-1})\stackrel{\mathrm{def}}{=}} \\    =& \sum_{h_1, \ldots, h_{T-2}} \prod_{t=1}^{T-2} P(h_t \mid h_{t-1}) P(x_t \mid h_t) \cdot    \underbrace{\left[\sum_{h_{T-1}} P(h_{T-1} \mid h_{T-2}) P(x_{T-1} \mid h_{T-1}) \rho_{T-1}(h_{T-1}) \right]}_{\rho_{T-2}(h_{T-2})\stackrel{\mathrm{def}}{=}} \\    =& \ldots \\    =& \sum_{h_1} P(h_1) P(x_1 \mid h_1)\rho_{1}(h_{1}).\end{aligned}</script><p>因此可以将<em>后向递归</em>（backward recursion）写为：</p><script type="math/tex; mode=display">\rho_{t-1}(h_{t-1})= \sum_{h_{t}} P(h_{t} \mid h_{t-1}) P(x_{t} \mid h_{t}) \rho_{t}(h_{t}),</script><p>初始化$\rho_T(h_T) = 1$。前向和后向递归都允许我们对$T$个隐变量在$\mathcal{O}(kT)$（线性而不是指数）时间内对$(h_1, \ldots, h_T)$的所有值求和。这是使用图模型进行概率推理的巨大好处之一。它也是通用消息传递算法[<code>Aji.McEliece.2000</code>]的一个非常特殊的例子。结合前向和后向递归，我们能够计算</p><script type="math/tex; mode=display">P(x_j \mid x_{-j}) \propto \sum_{h_j} \pi_j(h_j) \rho_j(h_j) P(x_j \mid h_j).</script><blockquote><p>∝，数学符号，表示与某个量成正比例。A∝B也可表示有一个从 𝐴 到 𝐵 的多项式变换，当A、B为集合或表示$\text { set }{(a, b) \in A \times B: a=k b} \text { for some constant } k$</p></blockquote><p>因为符号简化的需要，后向递归也可以写为$\rho_{t-1} = g(\rho_t, x_t)$，其中$g$是一个可以学习的函数。这同样看起来像一个更新方程，只是不像在循环神经网络中看到的那样前向运算，而是后向计算。事实上，知道未来数据何时可用对隐马尔可夫模型是有益的。信号处理学家将是否知道未来观测这两种情况区分为内插和外推，有关更多详细信息，请参阅 [<code>Doucet.De-Freitas.Gordon.2001</code>]。</p><h3 id="4-2-双向模型"><a href="#4-2-双向模型" class="headerlink" title="4.2. 双向模型"></a>4.2. 双向模型</h3><p>如果我们希望在循环神经网络中拥有一种机制，使之能够提供与隐马尔可夫模型类似的前瞻能力，就需要修改循环神经网络的设计。这在概念上很容易，只需要增加一个“从最后一个词元开始从后向前运行”的循环神经网络，而不是只有一个在前向模式下“从第一个词元开始运行”的循环神经网络。<em>双向循环神经网络</em>（bidirectional RNNs）添加了反向传递信息的隐藏层，以便更灵活地处理此类信息。下图描述了具有单个隐藏层的双向循环神经网络的架构。</p><p><img src="/assets/post_img/article58/birnn.svg" alt="双向循环神经网络架构"></p><p>事实上这与隐马尔可夫模型中的动态规划的前向和后向递归没有太大区别。主要区别是在隐马尔可夫模型中的方程具有特定的统计意义。双向循环神经网络没有这样容易理解的解释，我们只能把它们当作通用的、可学习的函数。这种转变集中体现了现代深度网络的设计原则：首先使用经典统计模型的函数依赖类型，然后将其参数化为通用形式。（就是模仿结构但是没有数学理论支持？）</p><h4 id="4-2-1-定义"><a href="#4-2-1-定义" class="headerlink" title="4.2.1. 定义"></a>4.2.1. 定义</h4><p>双向循环神经网络是由[<code>Schuster.Paliwal.1997</code>]提出的，关于各种架构的详细讨论请参阅[<code>Graves.Schmidhuber.2005</code>]。来看看这样一个网络的细节。</p><p>对于任意时间步$t$，给定一个小批量的输入数据$\mathbf{X}_t \in \mathbb{R}^{n \times d}$（样本数$n$，每个示例中的输入数$d$），并且令隐藏层激活函数为$\phi$。在双向架构中，设该时间步的前向和反向隐状态分别为$\overrightarrow{\mathbf{H}}_t  \in \mathbb{R}^{n \times h}$和$\overleftarrow{\mathbf{H}}_t  \in \mathbb{R}^{n \times h}$，其中$h$是隐藏单元的数目。前向和反向隐状态的更新如下：</p><script type="math/tex; mode=display">\begin{aligned}\overrightarrow{\mathbf{H}}_t &= \phi(\mathbf{X}_t \mathbf{W}_{xh}^{(f)} + \overrightarrow{\mathbf{H}}_{t-1} \mathbf{W}_{hh}^{(f)}  + \mathbf{b}_h^{(f)}),\\\overleftarrow{\mathbf{H}}_t &= \phi(\mathbf{X}_t \mathbf{W}_{xh}^{(b)} + \overleftarrow{\mathbf{H}}_{t+1} \mathbf{W}_{hh}^{(b)}  + \mathbf{b}_h^{(b)}),\end{aligned}</script><p>其中，权重$\mathbf{W}_{xh}^{(f)} \in \mathbb{R}^{d \times h}, \mathbf{W}_{hh}^{(f)} \in \mathbb{R}^{h \times h}, \mathbf{W}_{xh}^{(b)} \in \mathbb{R}^{d \times h}, \mathbf{W}_{hh}^{(b)} \in \mathbb{R}^{h \times h}$和偏置$\mathbf{b}_h^{(f)} \in \mathbb{R}^{1 \times h}, \mathbf{b}_h^{(b)} \in \mathbb{R}^{1 \times h}$都是模型参数。</p><p>接下来，将前向隐状态$\overrightarrow{\mathbf{H}}_t$和反向隐状态$\overleftarrow{\mathbf{H}}_t$连接起来，获得需要送入输出层的隐状态$\mathbf{H}_t \in \mathbb{R}^{n \times 2h}$。在具有多个隐藏层的深度双向循环神经网络中，该信息作为输入传递到下一个双向层。最后，输出层计算得到的输出为$\mathbf{O}_t \in \mathbb{R}^{n \times q}$（$q$是输出单元的数目）：</p><script type="math/tex; mode=display">\mathbf{O}_t = \mathbf{H}_t \mathbf{W}_{hq} + \mathbf{b}_q.</script><p>这里，权重矩阵$\mathbf{W}_{hq} \in \mathbb{R}^{2h \times q}$和偏置$\mathbf{b}_q \in \mathbb{R}^{1 \times q}$是输出层的模型参数。事实上，这两个方向可以拥有不同数量的隐藏单元。</p><h4 id="4-2-2-模型的计算代价及其应用"><a href="#4-2-2-模型的计算代价及其应用" class="headerlink" title="4.2.2. 模型的计算代价及其应用"></a>4.2.2. 模型的计算代价及其应用</h4><p>双向循环神经网络的一个关键特性是：使用来自序列两端的信息来估计输出。也就是说，我们使用来自过去和未来的观测信息来预测当前的观测。但是在对下一个词元进行预测的情况中，这样的模型并不是我们所需的。因为在预测下一个词元时我们无法知道下一个词元的下文是什么，所以将不会得到很好的精度。具体地说，在训练期间，我们能够利用过去和未来的数据来估计现在空缺的词；而在测试期间，我们只有过去的数据，因此精度将会很差。下面的实验将说明这一点。</p><p>另一个严重问题是，双向循环神经网络的计算速度非常慢。其主要原因是网络的前向传播需要在双向层中进行前向和后向递归，并且网络的反向传播还依赖于前向传播的结果。因此，梯度求解将有一个非常长的链。</p><p>双向层的使用在实践中非常少，并且仅仅应用于部分场合。例如，填充缺失的单词、词元注释（例如，用于命名实体识别）以及作为序列处理流水线中的一个步骤对序列进行编码（例如，用于机器翻译）。本书未来将介绍如何使用双向循环神经网络编码文本序列。</p><h3 id="4-3-双向循环神经网络的错误应用"><a href="#4-3-双向循环神经网络的错误应用" class="headerlink" title="4.3. 双向循环神经网络的错误应用"></a>4.3. 双向循环神经网络的错误应用</h3><p>由于双向循环神经网络使用了过去的和未来的数据，所以不能盲目地将这一语言模型应用于任何预测任务。 尽管模型产出的困惑度是合理的，该模型预测未来词元的能力却可能存在严重缺陷。 这里用下面的示例代码引以为戒，以防在错误的环境中使用它们。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line">batch_size, num_steps, device = <span class="number">32</span>, <span class="number">35</span>, d2l.try_gpu()</span><br><span class="line">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br><span class="line"><span class="comment"># 通过设置“bidirective=True”来定义双向LSTM模型</span></span><br><span class="line">vocab_size, num_hiddens, num_layers = <span class="built_in">len</span>(vocab), <span class="number">256</span>, <span class="number">2</span></span><br><span class="line">num_inputs = vocab_size</span><br><span class="line">lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers, bidirectional=<span class="literal">True</span>)</span><br><span class="line">model = d2l.RNNModel(lstm_layer, <span class="built_in">len</span>(vocab))</span><br><span class="line">model = model.to(device)</span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">num_epochs, lr = <span class="number">500</span>, <span class="number">1</span></span><br><span class="line">d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">perplexity <span class="number">1.1</span>, <span class="number">130005.5</span> tokens/sec on cuda:<span class="number">0</span></span><br><span class="line">time travellerererererererererererererererererererererererererer</span><br><span class="line">travellerererererererererererererererererererererererererer</span><br></pre></td></tr></table></figure><p>最终结果虽然困惑度降低了，但预测效果非常差。关于如何更有效地使用双向循环神经网络的讨论，请参阅情感分类应用。</p><h2 id="5-机器翻译与数据集"><a href="#5-机器翻译与数据集" class="headerlink" title="5. 机器翻译与数据集"></a>5. 机器翻译与数据集</h2><p>语言模型是自然语言处理的关键，而<em>机器翻译</em>是语言模型最成功的基准测试。因为机器翻译正是将输入序列转换成输出序列的<em>序列转换模型</em>（sequence transduction）的核心问题。序列转换模型在各类现代人工智能应用中发挥着至关重要的作用，因此将其做为本章剩余部分和下一章的重点。本节将介绍机器翻译问题及其后文需要使用的数据集。</p><p><em>机器翻译</em>（machine translation）指的是将序列从一种语言自动翻译成另一种语言。这个研究领域可以追溯到数字计算机发明后不久的20世纪40年代，特别是在第二次世界大战中使用计算机破解语言编码。几十年来，在使用神经网络进行端到端学习的兴起之前，统计学方法在这一领域一直占据主导地位。因为<em>统计机器翻译</em>（statisticalmachine translation）涉及了翻译模型和语言模型等组成部分的统计分析，因此基于神经网络的方法通常被称为<em>神经机器翻译</em>（neuralmachine translation），用于将两种翻译模型区分开来。</p><p>本书的关注点是神经网络机器翻译方法，强调的是端到端的学习。与上一章中语料库是单一语言的语言模型问题存在不同，机器翻译的数据集是由源语言和目标语言的文本序列对组成的。因此需要一种完全不同的方法来预处理机器翻译数据集，而不是复用语言模型的预处理程序。下面看一下如何将预处理后的数据加载到小批量中用于训练。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><h3 id="5-1-下载和预处理数据集"><a href="#5-1-下载和预处理数据集" class="headerlink" title="5.1. 下载和预处理数据集"></a>5.1. 下载和预处理数据集</h3><p>首先，下载一个由<a href="http://www.manythings.org/anki/">Tatoeba项目的双语句子对</a>组成的“英－法”数据集，数据集中的每一行都是制表符分隔的文本序列对，序列对由英文文本序列和翻译后的法语文本序列组成。请注意，每个文本序列可以是一个句子，也可以是包含多个句子的一个段落。在这个将英语翻译成法语的机器翻译问题中，英语是<em>源语言</em>（source language），法语是<em>目标语言</em>（target language）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">d2l.DATA_HUB[<span class="string">&#x27;fra-eng&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;fra-eng.zip&#x27;</span>,</span><br><span class="line">                           <span class="string">&#x27;94646ad1522d915e7b0f9296181140edcf86a4f5&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_data_nmt</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;载入“英语－法语”数据集&quot;&quot;&quot;</span></span><br><span class="line">    data_dir = d2l.download_extract(<span class="string">&#x27;fra-eng&#x27;</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(data_dir, <span class="string">&#x27;fra.txt&#x27;</span>), <span class="string">&#x27;r&#x27;</span>,</span><br><span class="line">             encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">return</span> f.read()</span><br><span class="line"></span><br><span class="line">raw_text = read_data_nmt()</span><br><span class="line"><span class="built_in">print</span>(raw_text[:<span class="number">75</span>])</span><br></pre></td></tr></table></figure><p>下载数据集后，原始文本数据需要经过几个预处理步骤。例如:用空格代替<em>不间断空格</em>（non-breaking space），使用小写字母替换大写字母，并在单词和标点符号之间插入空格。</p><p>关于空格的种类（带u的是unicode编码）：<br>1、半角空格：<code>\0x20</code>，占位符为一个半角字符，日常英文数学和代码编写使用。<br>2、全角空格：<code>\u3000</code>，中文输入空格，两个半角空格。<br>3、不间断空格：<code>\u00A0</code>或<code>\xa0</code>，在word、html等中大量使用。<br>4、零宽度空格：<code>\u200B</code>，不可见非打印字符，可以替换html中的<code>&lt;wbr/&gt;</code>标签。<br>5、零宽度非中断空格：<code>\u2060</code>，结合了 non-breaking space 和 零宽度空格的特点。既会自动换行，宽度又是0。<br>6、还有一些html宽度度量下的其他空格字符，如<code>\u202f</code>就是一种窄不间断空格，不同语种中不太一样。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_nmt</span>(<span class="params">text</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;预处理“英语－法语”数据集&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">no_space</span>(<span class="params">char, prev_char</span>):</span></span><br><span class="line">        <span class="keyword">return</span> char <span class="keyword">in</span> <span class="built_in">set</span>(<span class="string">&#x27;,.!?&#x27;</span>) <span class="keyword">and</span> prev_char != <span class="string">&#x27; &#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用空格替换不间断空格</span></span><br><span class="line">    <span class="comment"># 使用小写字母替换大写字母</span></span><br><span class="line">    text = text.replace(<span class="string">&#x27;\u202f&#x27;</span>, <span class="string">&#x27; &#x27;</span>).replace(<span class="string">&#x27;\xa0&#x27;</span>, <span class="string">&#x27; &#x27;</span>).lower()</span><br><span class="line">    <span class="comment"># 在单词和标点符号之间插入空格</span></span><br><span class="line">    out = [<span class="string">&#x27; &#x27;</span> + char <span class="keyword">if</span> i &gt; <span class="number">0</span> <span class="keyword">and</span> no_space(char, text[i - <span class="number">1</span>]) <span class="keyword">else</span> char</span><br><span class="line">           <span class="keyword">for</span> i, char <span class="keyword">in</span> <span class="built_in">enumerate</span>(text)]</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;&#x27;</span>.join(out)</span><br><span class="line"></span><br><span class="line">text = preprocess_nmt(raw_text)</span><br><span class="line"><span class="built_in">print</span>(text[:<span class="number">80</span>])</span><br></pre></td></tr></table></figure><h3 id="5-2-词元化"><a href="#5-2-词元化" class="headerlink" title="5.2. 词元化"></a>5.2. 词元化</h3><p>与上一章中的字符级词元化不同，在机器翻译中，更喜欢做单词级词元化（最先进的模型可能使用更高级的词元化技术）。下面的<code>tokenize_nmt</code>函数对前<code>num_examples</code>个文本序列对进行词元，其中每个词元要么是一个词，要么是一个标点符号。此函数返回两个词元列表：<code>source</code>和<code>target</code>：<code>source[i]</code>是源语言（这里是英语）第$i$个文本序列的词元列表，<code>target[i]</code>是目标语言（这里是法语）第$i$个文本序列的词元列表。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize_nmt</span>(<span class="params">text, num_examples=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;词元化“英语－法语”数据数据集&quot;&quot;&quot;</span></span><br><span class="line">    source, target = [], []</span><br><span class="line">    <span class="keyword">for</span> i, line <span class="keyword">in</span> <span class="built_in">enumerate</span>(text.split(<span class="string">&#x27;\n&#x27;</span>)):</span><br><span class="line">        <span class="keyword">if</span> num_examples <span class="keyword">and</span> i &gt; num_examples:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        parts = line.split(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(parts) == <span class="number">2</span>:</span><br><span class="line">            source.append(parts[<span class="number">0</span>].split(<span class="string">&#x27; &#x27;</span>))</span><br><span class="line">            target.append(parts[<span class="number">1</span>].split(<span class="string">&#x27; &#x27;</span>))</span><br><span class="line">    <span class="keyword">return</span> source, target</span><br><span class="line"></span><br><span class="line">source, target = tokenize_nmt(text)</span><br></pre></td></tr></table></figure><p>绘制每个文本序列所包含的词元数量的直方图。在这个简单的“英－法”数据集中，大多数文本序列的词元数量少于$20$个。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_list_len_pair_hist</span>(<span class="params">legend, xlabel, ylabel, xlist, ylist</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;绘制列表长度对的直方图&quot;&quot;&quot;</span></span><br><span class="line">    d2l.set_figsize()</span><br><span class="line">    _, _, patches = d2l.plt.hist(</span><br><span class="line">        [[<span class="built_in">len</span>(l) <span class="keyword">for</span> l <span class="keyword">in</span> xlist], [<span class="built_in">len</span>(l) <span class="keyword">for</span> l <span class="keyword">in</span> ylist]])</span><br><span class="line">    d2l.plt.xlabel(xlabel)</span><br><span class="line">    d2l.plt.ylabel(ylabel)</span><br><span class="line">    <span class="keyword">for</span> patch <span class="keyword">in</span> patches[<span class="number">1</span>].patches:</span><br><span class="line">        patch.set_hatch(<span class="string">&#x27;/&#x27;</span>)</span><br><span class="line">    d2l.plt.legend(legend)</span><br><span class="line"></span><br><span class="line">show_list_len_pair_hist([<span class="string">&#x27;source&#x27;</span>, <span class="string">&#x27;target&#x27;</span>], <span class="string">&#x27;# tokens per sequence&#x27;</span>,</span><br><span class="line">                        <span class="string">&#x27;count&#x27;</span>, source, target);</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article58/output_machine-translation-and-dataset_887557_54_0.svg" alt="tokens per sequence"></p><h3 id="5-3-词表"><a href="#5-3-词表" class="headerlink" title="5.3. 词表"></a>5.3. 词表</h3><p>由于机器翻译数据集由语言对组成，则可以分别为源语言和目标语言构建两个词表。使用单词级词元化时，词表大小将明显大于使用字符级词元化时的词表大小。为了缓解这一问题，这里将出现次数少于2次的低频率词元视为相同的未知（“&lt;unk&gt;”）词元。除此之外，还指定了额外的特定词元，例如在小批量时用于将序列填充到相同长度的填充词元（“&lt;pad&gt;”），以及序列的开始词元（“&lt;bos&gt;”）和结束词元（“&lt;eos&gt;”）。这些特殊词元在自然语言处理任务中比较常用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">src_vocab = d2l.Vocab(source, min_freq=<span class="number">2</span>,</span><br><span class="line">                      reserved_tokens=[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>, <span class="string">&#x27;&lt;bos&gt;&#x27;</span>, <span class="string">&#x27;&lt;eos&gt;&#x27;</span>])</span><br><span class="line"><span class="built_in">len</span>(src_vocab)</span><br></pre></td></tr></table></figure><h3 id="5-4-加载数据集"><a href="#5-4-加载数据集" class="headerlink" title="5.4. 加载数据集"></a>5.4. 加载数据集</h3><p>语言模型中的序列样本都有一个固定的长度，无论这个样本是一个句子的一部分还是跨越了多个句子的一个片断。这个固定长度是上一章中第3节中的<code>num_steps</code>（时间步数或词元数量）参数指定的。在机器翻译中，每个样本都是由源和目标组成的文本序列对，其中的每个文本序列可能具有不同的长度。</p><p>为了提高计算效率，我们仍然可以通过<em>截断</em>（truncation）和<em>填充</em>（padding）方式实现一次只处理一个小批量的文本序列。假设同一个小批量中的每个序列都应该具有相同的长度<code>num_steps</code>，那么如果文本序列的词元数目少于<code>num_steps</code>时，我们将继续在其末尾添加特定的“&lt;pad&gt;”词元，直到其长度达到<code>num_steps</code>；反之，我们将截断文本序列时，只取其前<code>num_steps</code> 个词元，并且丢弃剩余的词元。这样，每个文本序列将具有相同的长度，以便以相同形状的小批量进行加载。</p><p>下面的<code>truncate_pad</code>函数将截断或填充文本序列。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">truncate_pad</span>(<span class="params">line, num_steps, padding_token</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;截断或填充文本序列&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(line) &gt; num_steps:</span><br><span class="line">        <span class="keyword">return</span> line[:num_steps]  <span class="comment"># 截断</span></span><br><span class="line">    <span class="keyword">return</span> line + [padding_token] * (num_steps - <span class="built_in">len</span>(line))  <span class="comment"># 填充</span></span><br><span class="line"></span><br><span class="line">truncate_pad(src_vocab[source[<span class="number">0</span>]], <span class="number">10</span>, src_vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>])</span><br></pre></td></tr></table></figure><p>现在定义一个函数，可以将文本序列转换成小批量数据集用于训练。我们将特定的“&lt;eos&gt;”词元添加到所有序列的末尾，用于表示序列的结束。当模型通过一个词元接一个词元地生成序列进行预测时，生成的“&lt;eos&gt;”词元说明完成了序列输出工作。此外，我们还记录了每个文本序列的长度，统计长度时排除了填充词元，之后介绍的一些模型会需要这个长度信息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_array_nmt</span>(<span class="params">lines, vocab, num_steps</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;将机器翻译的文本序列转换成小批量&quot;&quot;&quot;</span></span><br><span class="line">    lines = [vocab[l] <span class="keyword">for</span> l <span class="keyword">in</span> lines]</span><br><span class="line">    lines = [l + [vocab[<span class="string">&#x27;&lt;eos&gt;&#x27;</span>]] <span class="keyword">for</span> l <span class="keyword">in</span> lines]</span><br><span class="line">    array = torch.tensor([truncate_pad(</span><br><span class="line">        l, num_steps, vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]) <span class="keyword">for</span> l <span class="keyword">in</span> lines])</span><br><span class="line">    valid_len = (array != vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]).<span class="built_in">type</span>(torch.int32).<span class="built_in">sum</span>(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> array, valid_len</span><br></pre></td></tr></table></figure><h3 id="5-5-训练模型"><a href="#5-5-训练模型" class="headerlink" title="5.5. 训练模型"></a>5.5. 训练模型</h3><p>最后定义<code>load_data_nmt</code>函数来返回数据迭代器，以及源语言和目标语言的两种词表。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_nmt</span>(<span class="params">batch_size, num_steps, num_examples=<span class="number">600</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回翻译数据集的迭代器和词表&quot;&quot;&quot;</span></span><br><span class="line">    text = preprocess_nmt(read_data_nmt())</span><br><span class="line">    source, target = tokenize_nmt(text, num_examples)</span><br><span class="line">    src_vocab = d2l.Vocab(source, min_freq=<span class="number">2</span>,</span><br><span class="line">                          reserved_tokens=[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>, <span class="string">&#x27;&lt;bos&gt;&#x27;</span>, <span class="string">&#x27;&lt;eos&gt;&#x27;</span>])</span><br><span class="line">    tgt_vocab = d2l.Vocab(target, min_freq=<span class="number">2</span>,</span><br><span class="line">                          reserved_tokens=[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>, <span class="string">&#x27;&lt;bos&gt;&#x27;</span>, <span class="string">&#x27;&lt;eos&gt;&#x27;</span>])</span><br><span class="line">    src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)</span><br><span class="line">    tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)</span><br><span class="line">    data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)</span><br><span class="line">    data_iter = d2l.load_array(data_arrays, batch_size)</span><br><span class="line">    <span class="keyword">return</span> data_iter, src_vocab, tgt_vocab</span><br></pre></td></tr></table></figure><p>下面读出“英语－法语”数据集中的第一个小批量数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">train_iter, src_vocab, tgt_vocab = load_data_nmt(batch_size=<span class="number">2</span>, num_steps=<span class="number">8</span>)</span><br><span class="line"><span class="keyword">for</span> X, X_valid_len, Y, Y_valid_len <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;X:&#x27;</span>, X.<span class="built_in">type</span>(torch.int32))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;X的有效长度:&#x27;</span>, X_valid_len)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Y:&#x27;</span>, Y.<span class="built_in">type</span>(torch.int32))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Y的有效长度:&#x27;</span>, Y_valid_len)</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">X: tensor([[ <span class="number">9</span>,  <span class="number">4</span>,  <span class="number">3</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>],</span><br><span class="line">        [<span class="number">87</span>, <span class="number">22</span>,  <span class="number">4</span>,  <span class="number">3</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>]], dtype=torch.int32)</span><br><span class="line">X的有效长度: tensor([<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">Y: tensor([[ <span class="number">16</span>,   <span class="number">5</span>,   <span class="number">3</span>,   <span class="number">1</span>,   <span class="number">1</span>,   <span class="number">1</span>,   <span class="number">1</span>,   <span class="number">1</span>],</span><br><span class="line">        [<span class="number">175</span>, <span class="number">176</span>,   <span class="number">4</span>,   <span class="number">3</span>,   <span class="number">1</span>,   <span class="number">1</span>,   <span class="number">1</span>,   <span class="number">1</span>]], dtype=torch.int32)</span><br><span class="line">Y的有效长度: tensor([<span class="number">3</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure><h2 id="6-编码器-解码器架构"><a href="#6-编码器-解码器架构" class="headerlink" title="6. 编码器-解码器架构"></a>6. 编码器-解码器架构</h2><p>机器翻译是序列转换模型的一个核心问题，其输入和输出都是长度可变的序列。为了处理这种类型的输入和输出，可以设计一个包含两个主要组件的架构：第一个组件是一个<em>编码器</em>（encoder）：它接受一个长度可变的序列作为输入，并将其转换为具有固定形状的编码状态。第二个组件是<em>解码器</em>（decoder）：它将固定形状的编码状态映射到长度可变的序列。这被称为<em>编码器-解码器</em>（encoder-decoder）架构，如下图所示。</p><p><img src="/assets/post_img/article58/encoder-decoder.svg" alt="编码器-解码器架构"></p><p>以英语到法语的机器翻译为例：给定一个英文的输入序列：“They”“are”“watching”“.”。首先，这种“编码器－解码器”架构将长度可变的输入序列编码成一个“状态”，然后对该状态进行解码，一个词元接着一个词元地生成翻译后的序列作为输出：“Ils”“regordent”“.”。由于“编码器－解码器”架构是形成后续章节中不同序列转换模型的基础，因此本节将把这个架构转换为接口方便后面的代码实现。</p><h3 id="6-1-编码器"><a href="#6-1-编码器" class="headerlink" title="6.1. 编码器"></a>6.1. 编码器</h3><p>在编码器接口中，我们只指定长度可变的序列作为编码器的输入<code>X</code>。<br>任何继承这个<code>Encoder</code>基类的模型将完成代码实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;编码器-解码器架构的基本编码器接口&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, *args</span>):</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure><p>这里解析一下<code>super(Encoder, self).__init__()</code>这个写法。self指的是实例（instance）本身，Python的类中规定：<strong>类中的方法的第一个参数一定要是self，而且不能省略。</strong> 所以构造函数也就是<code>__init__ ()</code>方法必须包含一个self参数，而且要是第一个参数。</p><p><code>super()</code>是Python的内置函数，用于调用父类。在Python3中我们通常使用<code>super().xxx</code>代替<code>super(Class, self).xxx</code>。<code>super(Encoder, self).__init__()</code>的工作原理是首先找到Encoder的父类nn.Module，然后把实例self转化为父类的对象，再去调用该实例的构造方法，实际上也就是调用了父类的构造方法。</p><h3 id="6-2-解码器"><a href="#6-2-解码器" class="headerlink" title="6.2. 解码器"></a>6.2. 解码器</h3><p>在下面的解码器接口中，新增一个<code>init_state</code>函数，用于将编码器的输出（<code>enc_outputs</code>）转换为编码后的状态。注意，此步骤可能需要额外的输入，例如：输入序列的有效长度。为了逐个地生成长度可变的词元序列，解码器在每个时间步都会将输入（例如：在前一时间步生成的词元）和编码后的状态映射成当前时间步的输出词元。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;编码器-解码器架构的基本解码器接口&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_state</span>(<span class="params">self, enc_outputs, *args</span>):</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, state</span>):</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure><h3 id="6-3-合并编码器和解码器"><a href="#6-3-合并编码器和解码器" class="headerlink" title="6.3. 合并编码器和解码器"></a>6.3. 合并编码器和解码器</h3><p>总而言之，“编码器-解码器”架构包含了一个编码器和一个解码器， 并且还拥有可选的额外的参数。 在前向传播中，编码器的输出用于生成编码状态， 这个状态又被解码器作为其输入的一部分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderDecoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;编码器-解码器架构的基类&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, encoder, decoder, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(EncoderDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, enc_X, dec_X, *args</span>):</span></span><br><span class="line">        enc_outputs = self.encoder(enc_X, *args)</span><br><span class="line">        dec_state = self.decoder.init_state(enc_outputs, *args)</span><br><span class="line">        <span class="keyword">return</span> self.decoder(dec_X, dec_state)</span><br></pre></td></tr></table></figure><p>“编码器－解码器”体系架构中的术语<em>状态</em>会启发人们使用具有状态的神经网络来实现该架构。下一节将学习如何应用循环神经网络，来设计基于“编码器－解码器”架构的序列转换模型。</p><h2 id="7-序列到序列学习（seq2seq）"><a href="#7-序列到序列学习（seq2seq）" class="headerlink" title="7. 序列到序列学习（seq2seq）"></a>7. 序列到序列学习（seq2seq）</h2><p>机器翻译中的输入序列和输出序列都是长度可变的。为了解决这类问题，我们设计了一个通用的”编码器－解码器“架构。本节将使用两个循环神经网络的编码器和解码器，并将其应用于<em>序列到序列</em>（sequence to sequence，seq2seq）类的学习任务[<code>Sutskever.Vinyals.Le.2014,Cho.Van-Merrienboer.Gulcehre.ea.2014</code>]。</p><p>遵循编码器－解码器架构的设计原则，循环神经网络编码器使用长度可变的序列作为输入，将其转换为固定形状的隐状态。换言之，输入序列的信息被<em>编码</em>到循环神经网络编码器的隐状态中。为了连续生成输出序列的词元，独立的循环神经网络解码器是基于输入序列的编码信息和输出序列已经看见的或者生成的词元来预测下一个词元。下图演示了如何在机器翻译中使用两个循环神经网络进行序列到序列学习。</p><p><img src="/assets/post_img/article58/seq2seq.svg" alt="使用循环神经网络编码器和循环神经网络解码器的序列到序列学习"></p><p>图中，特定的“&lt;eos&gt;”表示序列结束词元。一旦输出序列生成此词元，模型就会停止预测。在循环神经网络解码器的初始化时间步，有两个特殊的设计：第一，特定的“&lt;bos&gt;”表示序列开始词元，它是解码器的输入序列的第一个词元。第二，使用循环神经网络编码器最终的隐状态来初始化解码器的隐状态。例如，在[<code>Sutskever.Vinyals.Le.2014</code>]的设计中，正是基于这种设计将输入序列的编码信息送入到解码器中来生成输出序列的。在其他一些设计中[<code>Cho.Van-Merrienboer.Gulcehre.ea.2014</code>]，如上图所示，<em>编码器最终的隐状态在每一个时间步都作为解码器的输入序列的一部分</em>。类似于上一章中语言模型的训练，可以允许标签成为原始的输出序列，从源序列词元“&lt;bos&gt;”“Ils”“regardent”“.”到新序列词元“Ils”“regardent”“.”“&lt;eos&gt;”来移动预测的位置。</p><p>下面来动手构建以上的设计，并将基于“英－法”数据集来训练这个机器翻译模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><h3 id="7-1-编码器"><a href="#7-1-编码器" class="headerlink" title="7.1. 编码器"></a>7.1. 编码器</h3><p>从技术上讲，编码器将长度可变的输入序列转换成形状固定的上下文变量$\mathbf{c}$，并且将输入序列的信息在该上下文变量中进行编码。如前图所示，可以使用循环神经网络来设计编码器。</p><p>对于由一个序列组成的样本（批量大小是$1$）。假设输入序列是$x_1, \ldots, x_T$，其中$x_t$是输入文本序列中的第$t$个词元。在时间步$t$，循环神经网络将词元$x_t$的输入特征向量$\mathbf{x}_t$和$\mathbf{h} _{t-1}$（即上一时间步的隐状态）转换为$\mathbf{h}_t$（即当前步的隐状态）。使用一个函数$f$来描述循环神经网络的循环层所做的变换：</p><script type="math/tex; mode=display">\mathbf{h}_t = f(\mathbf{x}_t, \mathbf{h}_{t-1}).</script><p>而编码器通过选定的函数$q$，将所有时间步的隐状态转换为上下文变量：</p><script type="math/tex; mode=display">\mathbf{c} =  q(\mathbf{h}_1, \ldots, \mathbf{h}_T).</script><p>比如在上面的图中，指定$q(\mathbf{h}_1, \ldots, \mathbf{h}_T) = \mathbf{h}_T$后，上下文变量则仅是输入序列在最后时间步的隐状态$\mathbf{h}_T$。</p><p>目前为止，我们使用的是一个单向循环神经网络来设计编码器，其中隐状态只依赖于输入子序列，这个子序列是由输入序列的开始位置到隐状态所在的时间步的位置（包括隐状态所在的时间步）组成。当然也可以使用双向循环神经网络构造编码器，其中隐状态依赖于两个输入子序列，两个子序列是由隐状态所在的时间步的位置之前的序列 和 之后的序列（包括隐状态所在的时间步），因此隐状态对整个序列的信息都进行了编码。</p><p>现在来实现循环神经网络编码器。注意这里使用了<em>嵌入层</em>（embedding layer）来获得输入序列中每个词元的特征向量。嵌入层的权重是一个矩阵，其行数等于输入词表的大小（<code>vocab_size</code>），其列数等于特征向量的维度（<code>embed_size</code>）。对于任意输入词元的索引$i$，嵌入层获取权重矩阵的第$i$行（从$0$开始）以返回其特征向量。另外，本文选择了一个多层门控循环单元来实现编码器。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Seq2SeqEncoder</span>(<span class="params">Encoder</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;用于序列到序列学习的循环神经网络编码器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embed_size, num_hiddens, num_layers,</span></span></span><br><span class="line"><span class="params"><span class="function">                 dropout=<span class="number">0</span>, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Seq2SeqEncoder, self).__init__(**kwargs)</span><br><span class="line">        <span class="comment"># 嵌入层</span></span><br><span class="line">        <span class="comment"># Embedding的作用简单来说就是为单词编码，将单词编码成为向量。</span></span><br><span class="line">        <span class="comment"># 嵌入层比独热编码更节约空间，能方便运算。</span></span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        <span class="comment"># 嵌入层size等于特征向量维数作为input_size</span></span><br><span class="line">        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers,</span><br><span class="line">                          dropout=dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, *args</span>):</span></span><br><span class="line">        <span class="comment"># 输出&#x27;X&#x27;的形状：(batch_size,num_steps,embed_size)</span></span><br><span class="line">        <span class="comment"># batch_size,num_steps来自输入，经嵌入层后会增加一维，因为原本的元素（单词）被向量化。</span></span><br><span class="line">        X = self.embedding(X)</span><br><span class="line">        <span class="comment"># 在循环神经网络模型中，第一个轴对应于时间步</span></span><br><span class="line">        <span class="comment"># permute就是维度变换，其中0、1、2指第一维、第二维、第三维</span></span><br><span class="line">        <span class="comment"># 这里就是调换两个维度，把形状(batch_size,num_steps,embed_size)</span></span><br><span class="line">        <span class="comment"># 转换为(num_steps,batch_size,embed_size)</span></span><br><span class="line">        X = X.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 如果未提及状态，则默认为0</span></span><br><span class="line">        output, state = self.rnn(X)</span><br><span class="line">        <span class="comment"># output的形状:(num_steps,batch_size,num_hiddens)</span></span><br><span class="line">        <span class="comment"># state的形状:(num_layers,batch_size,num_hiddens)</span></span><br><span class="line">        <span class="keyword">return</span> output, state</span><br></pre></td></tr></table></figure><p>循环层返回变量的说明可以参考上一章“循环神经网络的框架实现”。</p><p>下面实例化上述编码器的实现：使用一个两层门控循环单元编码器，其隐藏单元数为$16$。给定一小批量的输入序列<code>X</code>（批量大小为$4$，时间步为$7$）。在完成所有时间步后，最后一层的隐状态的输出是一个张量（<code>output</code>由编码器的循环层返回），其形状为（时间步数，批量大小，隐藏单元数）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">encoder = Seq2SeqEncoder(vocab_size=<span class="number">10</span>, embed_size=<span class="number">8</span>, num_hiddens=<span class="number">16</span>,</span><br><span class="line">                         num_layers=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 评估模式，测试时一定要使用，对Dropout和BatchNorm等有作用。</span></span><br><span class="line">encoder.<span class="built_in">eval</span>()</span><br><span class="line">X = torch.zeros((<span class="number">4</span>, <span class="number">7</span>), dtype=torch.long)</span><br><span class="line">output, state = encoder(X)</span><br><span class="line">output.shape, state.shape</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">(torch.Size([<span class="number">7</span>, <span class="number">4</span>, <span class="number">16</span>]), torch.Size([<span class="number">2</span>, <span class="number">4</span>, <span class="number">16</span>]))</span><br></pre></td></tr></table></figure><p>由于这里使用的是门控循环单元，所以在最后一个时间步的多层隐状态的形状是 （隐藏层的数量，批量大小，隐藏单元的数量）。 如果使用长短期记忆网络，state中还将包含记忆单元信息。</p><h3 id="7-2-解码器"><a href="#7-2-解码器" class="headerlink" title="7.2. 解码器"></a>7.2. 解码器</h3><p>如上文所说，编码器输出的上下文变量$\mathbf{c}$对整个输入序列$x_1, \ldots, x_T$进行编码。来自训练数据集的输出序列$y_1, y_2, \ldots, y_{T’}$，对于每个时间步$t’$（与输入序列或编码器的时间步$t$不同），解码器输出$y_{t’}$的概率取决于先前的输出子序列$y_1, \ldots, y_{t’-1}$和上下文变量$\mathbf{c}$，即$P(y_{t’} \mid y_1, \ldots, y_{t’-1}, \mathbf{c})$。</p><p>为了在序列上模型化这种条件概率，可以使用另一个循环神经网络作为解码器。在输出序列上的任意时间步$t^\prime$，循环神经网络将来自上一时间步的输出$y_{t^\prime-1}$和上下文变量$\mathbf{c}$作为其输入，然后在当前时间步将它们和上一隐状态$\mathbf{s}_{t^\prime-1}$转换为隐状态$\mathbf{s}_{t^\prime}$。可以使用函数$g$来表示解码器的隐藏层的变换：</p><script type="math/tex; mode=display">\mathbf{s}_{t^\prime} = g(y_{t^\prime-1}, \mathbf{c}, \mathbf{s}_{t^\prime-1}).</script><p>在获得解码器的隐状态之后，我们可以使用输出层和softmax操作来计算在时间步$t^\prime$时输出$y_{t^\prime}$的条件概率分布$P(y_{t^\prime} \mid y_1, \ldots, y_{t^\prime-1}, \mathbf{c})$。</p><p>根据一开始给出的图，当实现解码器时，我们直接使用编码器最后一个时间步的隐状态来初始化解码器的隐状态。这就要求使用循环神经网络实现的编码器和解码器具有相同数量的层和隐藏单元（批量大小是一样的）。为了进一步包含经过编码的输入序列的信息，上下文变量在所有的时间步与解码器的输入进行拼接（concatenate）。为了预测输出词元的概率分布，在循环神经网络解码器的最后一层使用全连接层来变换隐状态。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Seq2SeqDecoder</span>(<span class="params">Decoder</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;用于序列到序列学习的循环神经网络解码器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embed_size, num_hiddens, num_layers,</span></span></span><br><span class="line"><span class="params"><span class="function">                 dropout=<span class="number">0</span>, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Seq2SeqDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers,</span><br><span class="line">                          dropout=dropout)</span><br><span class="line">        self.dense = nn.Linear(num_hiddens, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_state</span>(<span class="params">self, enc_outputs, *args</span>):</span></span><br><span class="line">        <span class="keyword">return</span> enc_outputs[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, state</span>):</span></span><br><span class="line">        <span class="comment"># 输出&#x27;X&#x27;的形状：(batch_size,num_steps,embed_size)</span></span><br><span class="line">        X = self.embedding(X).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 广播context，使其具有与X相同的num_steps</span></span><br><span class="line">        <span class="comment"># PyTorch中的repeat()函数可以对张量进行重复扩充。</span></span><br><span class="line">        <span class="comment"># 三个参数分别是：通道数的重复倍数，列的重复倍数，行的重复倍数。</span></span><br><span class="line">        <span class="comment"># 这里表示第三维扩充batch_size倍，其他不变</span></span><br><span class="line">        context = state[-<span class="number">1</span>].repeat(X.shape[<span class="number">0</span>], <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        X_and_context = torch.cat((X, context), <span class="number">2</span>)</span><br><span class="line">        output, state = self.rnn(X_and_context, state)</span><br><span class="line">        output = self.dense(output).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># output的形状:(batch_size,num_steps,vocab_size)</span></span><br><span class="line">        <span class="comment"># state的形状:(num_layers,batch_size,num_hiddens)</span></span><br><span class="line">        <span class="keyword">return</span> output, state</span><br></pre></td></tr></table></figure><p>下面用与前面提到的编码器中相同的超参数来实例化解码器。解码器的输出形状变为（批量大小，时间步数，词表大小）， 其中张量的最后一个维度存储预测的词元分布。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">decoder = Seq2SeqDecoder(vocab_size=<span class="number">10</span>, embed_size=<span class="number">8</span>, num_hiddens=<span class="number">16</span>,</span><br><span class="line">                         num_layers=<span class="number">2</span>)</span><br><span class="line">decoder.<span class="built_in">eval</span>()</span><br><span class="line">state = decoder.init_state(encoder(X))</span><br><span class="line">output, state = decoder(X, state)</span><br><span class="line">output.shape, state.shape</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">(torch.Size([<span class="number">4</span>, <span class="number">7</span>, <span class="number">10</span>]), torch.Size([<span class="number">2</span>, <span class="number">4</span>, <span class="number">16</span>]))</span><br></pre></td></tr></table></figure><p>上述循环神经网络“编码器－解码器”模型中的各层如下图所示：</p><p><img src="/assets/post_img/article58/seq2seq-details.svg" alt="循环神经网络编码器-解码器模型中的层"></p><h3 id="7-3-损失函数"><a href="#7-3-损失函数" class="headerlink" title="7.3. 损失函数"></a>7.3. 损失函数</h3><p>在每个时间步，解码器预测了输出词元的概率分布。类似于语言模型，可以使用softmax来获得分布，并通过计算交叉熵损失函数来进行优化。<a href="#5-机器翻译与数据集">第五节</a>中，特定的填充词元被添加到序列的末尾，因此不同长度的序列可以以相同形状的小批量加载。但是，我们应该将填充词元的预测排除在损失函数的计算之外。</p><p>为此可以使用下面的<code>sequence_mask</code>函数通过零值化屏蔽不相关的项，以便后面任何不相关预测的计算都是与零的乘积，结果都等于零。例如，如果两个序列的有效长度（不包括填充词元）分别为$1$和$2$，则第一个序列的第一项和第二个序列的前两项之后的剩余项将被清除为零。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sequence_mask</span>(<span class="params">X, valid_len, value=<span class="number">0</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;在序列中屏蔽不相关的项&quot;&quot;&quot;</span></span><br><span class="line">    maxlen = X.size(<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 在[]中加入None表示维度扩充，第二维用:表示在第二维放入全部元素</span></span><br><span class="line">    <span class="comment"># 例如原本的张量为：tensor([0., 1., 2.])</span></span><br><span class="line">    <span class="comment"># 则tensor([0., 1., 2.])[None, :] = tensor([[0., 1., 2.]])</span></span><br><span class="line">    <span class="comment"># 即第一维扩充了，第二维放入原本张量的所有元素</span></span><br><span class="line">    <span class="comment"># [:, None]则会使torch.tensor([1, 2]) 变为 tensor([[1], [2]])</span></span><br><span class="line">    <span class="comment"># 后面的 &lt; 比较符号应用广播机制，使得mask数组变为布尔值</span></span><br><span class="line">    mask = torch.arange((maxlen), dtype=torch.float32,</span><br><span class="line">                        device=X.device)[<span class="literal">None</span>, :] &lt; valid_len[:, <span class="literal">None</span>]</span><br><span class="line">    <span class="comment"># ～表示按位取反，对于布尔值就是T变F，F变T</span></span><br><span class="line">    <span class="comment"># 在一个张量的索引中放入另一个布尔值张量，会进行按位比对，将True对应的元素提取出来，对这些元素所做的更改会体现在原张量中。</span></span><br><span class="line">    X[~mask] = value</span><br><span class="line">    <span class="keyword">return</span> X</span><br><span class="line"></span><br><span class="line">X = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">sequence_mask(X, torch.tensor([<span class="number">1</span>, <span class="number">2</span>]))</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>, <span class="number">0</span>]])</span><br></pre></td></tr></table></figure><p>可以使用此函数屏蔽最后几个轴上的所有项。也可以通过指定<code>value</code>参数使用非零值来替换这些项。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">X = torch.ones(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">sequence_mask(X, torch.tensor([<span class="number">1</span>, <span class="number">2</span>]), value=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">输出:</span><br><span class="line">tensor([[[ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">         [-<span class="number">1.</span>, -<span class="number">1.</span>, -<span class="number">1.</span>, -<span class="number">1.</span>],</span><br><span class="line">         [-<span class="number">1.</span>, -<span class="number">1.</span>, -<span class="number">1.</span>, -<span class="number">1.</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">         [ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">         [-<span class="number">1.</span>, -<span class="number">1.</span>, -<span class="number">1.</span>, -<span class="number">1.</span>]]])</span><br></pre></td></tr></table></figure><p>现在可以通过扩展softmax交叉熵损失函数来遮蔽不相关的预测。最初，所有预测词元的掩码都设置为1。一旦给定了有效长度，与填充词元对应的掩码将被设置为0。最后，将所有词元的损失乘以掩码，以过滤掉损失中填充词元产生的不相关预测。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MaskedSoftmaxCELoss</span>(<span class="params">nn.CrossEntropyLoss</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;带屏蔽的softmax交叉熵损失函数&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># pred的形状：(batch_size,num_steps,vocab_size)</span></span><br><span class="line">    <span class="comment"># label的形状：(batch_size,num_steps)</span></span><br><span class="line">    <span class="comment"># valid_len的形状：(batch_size,)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, pred, label, valid_len</span>):</span></span><br><span class="line">        weights = torch.ones_like(label)</span><br><span class="line">        weights = sequence_mask(weights, valid_len)</span><br><span class="line">        self.reduction=<span class="string">&#x27;none&#x27;</span></span><br><span class="line">        <span class="comment"># 原本的交叉熵</span></span><br><span class="line">        unweighted_loss = <span class="built_in">super</span>(MaskedSoftmaxCELoss, self).forward(</span><br><span class="line">            pred.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>), label)</span><br><span class="line">        <span class="comment"># 除去屏蔽掉的填充词元</span></span><br><span class="line">        weighted_loss = (unweighted_loss * weights).mean(dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> weighted_loss</span><br></pre></td></tr></table></figure><p>可以创建三个相同的序列来进行代码健全性检查，然后分别指定这些序列的有效长度为$4$、$2$和$0$。结果就是，第一个序列的损失应为第二个序列的两倍，而第三个序列的损失应为零。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">loss = MaskedSoftmaxCELoss()</span><br><span class="line">loss(torch.ones(<span class="number">3</span>, <span class="number">4</span>, <span class="number">10</span>), torch.ones((<span class="number">3</span>, <span class="number">4</span>), dtype=torch.long),</span><br><span class="line">     torch.tensor([<span class="number">4</span>, <span class="number">2</span>, <span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([<span class="number">2.3026</span>, <span class="number">1.1513</span>, <span class="number">0.0000</span>])</span><br></pre></td></tr></table></figure><h3 id="7-4-训练"><a href="#7-4-训练" class="headerlink" title="7.4. 训练"></a>7.4. 训练</h3><p>在下面的循环训练过程中，如<a href="#7-序列到序列学习seq2seq">前图</a>所示，特定的序列开始词元（“&lt;bos&gt;”）和原始的输出序列（不包括序列结束词元“&lt;eos&gt;”）拼接在一起作为解码器的输入。这被称为<em>强制教学</em>（teacher forcing），因为原始的输出序列（词元的标签）被送入解码器。或者，将来自上一个时间步的<em>预测</em>得到的词元作为解码器的当前输入。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_seq2seq</span>(<span class="params">net, data_iter, lr, num_epochs, tgt_vocab, device</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练序列到序列模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">xavier_init_weights</span>(<span class="params">m</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">            nn.init.xavier_uniform_(m.weight)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.GRU:</span><br><span class="line">            <span class="keyword">for</span> param <span class="keyword">in</span> m._flat_weights_names:</span><br><span class="line">                <span class="keyword">if</span> <span class="string">&quot;weight&quot;</span> <span class="keyword">in</span> param:</span><br><span class="line">                    nn.init.xavier_uniform_(m._parameters[param])</span><br><span class="line"></span><br><span class="line">    net.apply(xavier_init_weights)</span><br><span class="line">    net.to(device)</span><br><span class="line">    optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">    loss = MaskedSoftmaxCELoss()</span><br><span class="line">    net.train()</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>,</span><br><span class="line">                     xlim=[<span class="number">10</span>, num_epochs])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        timer = d2l.Timer()</span><br><span class="line">        metric = d2l.Accumulator(<span class="number">2</span>)  <span class="comment"># 训练损失总和，词元数量</span></span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> data_iter:</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            X, X_valid_len, Y, Y_valid_len = [x.to(device) <span class="keyword">for</span> x <span class="keyword">in</span> batch]</span><br><span class="line">            bos = torch.tensor([tgt_vocab[<span class="string">&#x27;&lt;bos&gt;&#x27;</span>]] * Y.shape[<span class="number">0</span>],</span><br><span class="line">                          device=device).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            dec_input = torch.cat([bos, Y[:, :-<span class="number">1</span>]], <span class="number">1</span>)  <span class="comment"># 强制教学</span></span><br><span class="line">            Y_hat, _ = net(X, dec_input, X_valid_len)</span><br><span class="line">            l = loss(Y_hat, Y, Y_valid_len)</span><br><span class="line">            l.<span class="built_in">sum</span>().backward()      <span class="comment"># 损失函数的标量进行“反向传播”</span></span><br><span class="line">            d2l.grad_clipping(net, <span class="number">1</span>) <span class="comment"># 梯度裁剪</span></span><br><span class="line">            num_tokens = Y_valid_len.<span class="built_in">sum</span>()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                metric.add(l.<span class="built_in">sum</span>(), num_tokens)</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>, (metric[<span class="number">0</span>] / metric[<span class="number">1</span>],))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;loss <span class="subst">&#123;metric[<span class="number">0</span>] / metric[<span class="number">1</span>]:<span class="number">.3</span>f&#125;</span>, <span class="subst">&#123;metric[<span class="number">1</span>] / timer.stop():<span class="number">.1</span>f&#125;</span> &#x27;</span></span><br><span class="line">        <span class="string">f&#x27;tokens/sec on <span class="subst">&#123;<span class="built_in">str</span>(device)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>现在可以在 机器翻译数据集 上创建和训练一个循环神经网络“编码器－解码器”模型用于序列到序列的学习。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">embed_size, num_hiddens, num_layers, dropout = <span class="number">32</span>, <span class="number">32</span>, <span class="number">2</span>, <span class="number">0.1</span></span><br><span class="line">batch_size, num_steps = <span class="number">64</span>, <span class="number">10</span></span><br><span class="line">lr, num_epochs, device = <span class="number">0.005</span>, <span class="number">300</span>, d2l.try_gpu()</span><br><span class="line"></span><br><span class="line">train_iter, src_vocab, tgt_vocab = load_data_nmt(batch_size, num_steps)</span><br><span class="line">encoder = Seq2SeqEncoder(<span class="built_in">len</span>(src_vocab), embed_size, num_hiddens, num_layers,</span><br><span class="line">                        dropout)</span><br><span class="line">decoder = Seq2SeqDecoder(<span class="built_in">len</span>(tgt_vocab), embed_size, num_hiddens, num_layers,</span><br><span class="line">                        dropout)</span><br><span class="line">net = EncoderDecoder(encoder, decoder)</span><br><span class="line">train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)</span><br></pre></td></tr></table></figure><h3 id="7-5-预测"><a href="#7-5-预测" class="headerlink" title="7.5. 预测"></a>7.5. 预测</h3><p>为了采用一个接着一个词元的方式预测输出序列，每个解码器当前时间步的输入都将来自于前一时间步的预测词元。与训练类似，序列开始词元（“&lt;bos&gt;”）在初始时间步被输入到解码器中。该预测过程如下图所示，当输出序列的预测遇到序列结束词元（“&lt;eos&gt;”）时，预测就结束了。</p><p><img src="/assets/post_img/article58/seq2seq-predict.svg" alt="使用循环神经网络编码器-解码器逐词元地预测输出序列。"></p><p>下一节中将介绍不同的序列生成策略。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_seq2seq</span>(<span class="params">net, src_sentence, src_vocab, tgt_vocab, num_steps,</span></span></span><br><span class="line"><span class="params"><span class="function">                    device, save_attention_weights=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;序列到序列模型的预测&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 在预测时将net设置为评估模式</span></span><br><span class="line">    net.<span class="built_in">eval</span>()</span><br><span class="line">    src_tokens = src_vocab[src_sentence.lower().split(<span class="string">&#x27; &#x27;</span>)] + [src_vocab[<span class="string">&#x27;&lt;eos&gt;&#x27;</span>]]</span><br><span class="line">    enc_valid_len = torch.tensor([<span class="built_in">len</span>(src_tokens)], device=device)</span><br><span class="line">    src_tokens = truncate_pad(src_tokens, num_steps, src_vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>])</span><br><span class="line">    <span class="comment"># 添加批量轴</span></span><br><span class="line">    <span class="comment"># unsqueeze()函数起升维的作用,参数表示在哪个地方加一个维度。</span></span><br><span class="line">    enc_X = torch.unsqueeze(torch.tensor(src_tokens, dtype=torch.long, device=device), dim=<span class="number">0</span>)</span><br><span class="line">    enc_outputs = net.encoder(enc_X, enc_valid_len)</span><br><span class="line">    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)</span><br><span class="line">    <span class="comment"># 添加批量轴</span></span><br><span class="line">    dec_X = torch.unsqueeze(torch.tensor([tgt_vocab[<span class="string">&#x27;&lt;bos&gt;&#x27;</span>]], dtype=torch.long, device=device), dim=<span class="number">0</span>)</span><br><span class="line">    output_seq, attention_weight_seq = [], []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_steps):</span><br><span class="line">        Y, dec_state = net.decoder(dec_X, dec_state)</span><br><span class="line">        <span class="comment"># 使用具有预测最高可能性的词元，作为解码器在下一时间步的输入</span></span><br><span class="line">        dec_X = Y.argmax(dim=<span class="number">2</span>)</span><br><span class="line">        pred = dec_X.squeeze(dim=<span class="number">0</span>).<span class="built_in">type</span>(torch.int32).item()</span><br><span class="line">        <span class="comment"># 保存注意力权重（稍后讨论）</span></span><br><span class="line">        <span class="keyword">if</span> save_attention_weights:</span><br><span class="line">            attention_weight_seq.append(net.decoder.attention_weights)</span><br><span class="line">        <span class="comment"># 一旦序列结束词元被预测，输出序列的生成就完成了</span></span><br><span class="line">        <span class="keyword">if</span> pred == tgt_vocab[<span class="string">&#x27;&lt;eos&gt;&#x27;</span>]:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        output_seq.append(pred)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27; &#x27;</span>.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq</span><br></pre></td></tr></table></figure><h3 id="7-6-预测序列的评估"><a href="#7-6-预测序列的评估" class="headerlink" title="7.6. 预测序列的评估"></a>7.6. 预测序列的评估</h3><p>我们可以通过与真实的标签序列进行比较来评估预测序列。虽然[<code>Papineni.Roukos.Ward.ea.2002</code>]提出的BLEU（bilingual evaluation understudy）最先是用于评估机器翻译的结果，但现在它已经被广泛用于测量许多应用的输出序列的质量。原则上说，对于预测序列中的任意$n$元语法（n-grams），BLEU的评估都是这个$n$元语法是否出现在标签序列中。</p><p>我们将BLEU定义为：</p><script type="math/tex; mode=display">\exp\left(\min\left(0, 1 - \frac{\mathrm{len}_{\text{label}}}{\mathrm{len}_{\text{pred}}}\right)\right) \prod_{n=1}^k p_n^{1/2^n},</script><p>其中$\mathrm{len}_{\text{label}}$表示标签序列中的词元数和$\mathrm{len}_{\text{pred}}$表示预测序列中的词元数，$k$是用于匹配的最长的$n$元语法。另外，用$p_n$表示$n$元语法的精确度，它是两个数量的比值：第一个是预测序列与标签序列中匹配的$n$元语法的数量，第二个是预测序列中$n$元语法的数量的比率。具体地说，给定标签序列$A$、$B$、$C$、$D$、$E$、$F$和预测序列$A$、$B$、$B$、$C$、$D$，我们有$p_1 = 4/5$、$p_2 = 3/4$、$p_3 = 1/3$和$p_4 = 0$。</p><p>这里解释一下，先说$p_1$，首先明确一点，序列是有方向有顺序的，我们把序列都从左向右看，那么预测序列中的1元语法分别为：$P(A)$、$P(B)$、$P(B)$、$P(C)$、$P(D)$，标签序列同理。可知预测标签与标签序列中匹配的1元语法数量为4，分别为：$P(A)$、$P(B)$、$P(C)$、$P(D)$，注意这里是一一对应，所以要去重。而预测序列中1元语法的数量为5，故有$p_1 = 4/5$。对于$p_2$，预测序列中的2元语法分别为：$P(B \mid A)$、$P(B \mid B)$、$P(C \mid B)$、$P(D \mid C)$，后面同理。</p><p>根据上述BLEU的定义，当预测序列与标签序列完全相同时，BLEU为$1$。此外，由于$n$元语法越长则匹配难度越大，所以BLEU为更长的$n$元语法的精确度分配更大的权重。具体来说，当$p_n$固定时，$p_n^{1/2^n}$会随着$n$的增长而增加（原始论文使用$p_n^{1/n}$）。而且由于预测的序列越短获得的$p_n$值越高，所以上式中乘法项之前的系数 $\exp\left(\min\left(0, 1 - \frac{\mathrm{len}_{\text{label}}}{\mathrm{len}_{\text{pred}}}\right)\right)$ 用于惩罚较短的预测序列。例如，当$k=2$时，给定标签序列$A$、$B$、$C$、$D$、$E$、$F$和预测序列$A$、$B$，尽管$p_1 = p_2 = 1$，惩罚因子$\exp(1-6/2) \approx 0.14$会降低BLEU。BLEU的代码实现如下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bleu</span>(<span class="params">pred_seq, label_seq, k</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算BLEU&quot;&quot;&quot;</span></span><br><span class="line">    pred_tokens, label_tokens = pred_seq.split(<span class="string">&#x27; &#x27;</span>), label_seq.split(<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">    len_pred, len_label = <span class="built_in">len</span>(pred_tokens), <span class="built_in">len</span>(label_tokens)</span><br><span class="line">    score = math.exp(<span class="built_in">min</span>(<span class="number">0</span>, <span class="number">1</span> - len_label / len_pred))</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, k + <span class="number">1</span>):</span><br><span class="line">        num_matches, label_subs = <span class="number">0</span>, collections.defaultdict(<span class="built_in">int</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(len_label - n + <span class="number">1</span>):</span><br><span class="line">            label_subs[<span class="string">&#x27; &#x27;</span>.join(label_tokens[i: i + n])] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(len_pred - n + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> label_subs[<span class="string">&#x27; &#x27;</span>.join(pred_tokens[i: i + n])] &gt; <span class="number">0</span>:</span><br><span class="line">                num_matches += <span class="number">1</span></span><br><span class="line">                label_subs[<span class="string">&#x27; &#x27;</span>.join(pred_tokens[i: i + n])] -= <span class="number">1</span></span><br><span class="line">        score *= math.<span class="built_in">pow</span>(num_matches / (len_pred - n + <span class="number">1</span>), math.<span class="built_in">pow</span>(<span class="number">0.5</span>, n))</span><br><span class="line">    <span class="keyword">return</span> score</span><br></pre></td></tr></table></figure><p>最后，利用训练好的循环神经网络“编码器－解码器”模型， 将几个英语句子翻译成法语，并计算BLEU的最终结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">engs = [<span class="string">&#x27;go .&#x27;</span>, <span class="string">&quot;i lost .&quot;</span>, <span class="string">&#x27;he\&#x27;s calm .&#x27;</span>, <span class="string">&#x27;i\&#x27;m home .&#x27;</span>]</span><br><span class="line">fras = [<span class="string">&#x27;va !&#x27;</span>, <span class="string">&#x27;j\&#x27;ai perdu .&#x27;</span>, <span class="string">&#x27;il est calme .&#x27;</span>, <span class="string">&#x27;je suis chez moi .&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> eng, fra <span class="keyword">in</span> <span class="built_in">zip</span>(engs, fras):</span><br><span class="line">    translation, attention_weight_seq = predict_seq2seq(</span><br><span class="line">        net, eng, src_vocab, tgt_vocab, num_steps, device)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;eng&#125;</span> =&gt; <span class="subst">&#123;translation&#125;</span>, bleu <span class="subst">&#123;bleu(translation, fra, k=<span class="number">2</span>):<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">go . =&gt; va !, bleu <span class="number">1.000</span></span><br><span class="line">i lost . =&gt; j<span class="string">&#x27;ai perdu ?, bleu 0.687</span></span><br><span class="line"><span class="string">he&#x27;</span>s calm . =&gt; soyez &lt;unk&gt; !, bleu <span class="number">0.000</span></span><br><span class="line">i<span class="string">&#x27;m home . =&gt; je suis chez moi qui l&#x27;</span>ai vu ., bleu <span class="number">0.640</span></span><br></pre></td></tr></table></figure><p><strong>第一次阅读，只对我注意到的细节部分做一些解释，实际上在更大的层面上，我完全是一知半解，甚至一窍不通，由于没有决定是否选择NLP方向，所以并没有追求一次性完全弄懂训练、预测过程中的所有细节，留待以后回看吧。 — SilenceZheng于22.09.07</strong></p><h2 id="8-束搜索"><a href="#8-束搜索" class="headerlink" title="8. 束搜索"></a>8. 束搜索</h2><p>上节中，我们逐个预测输出序列，直到预测序列中出现特定的序列结束词元“&lt;eos&gt;”。本节将首先介绍<em>贪心搜索</em>（greedy search）策略，并探讨其存在的问题，然后对比其他替代策略：<em>穷举搜索</em>（exhaustive search）和<em>束搜索</em>（beam search）。</p><p>在正式介绍贪心搜索之前，使用与上节中相同的数学符号定义搜索问题。在任意时间步$t’$，解码器输出$y_{t’}$的概率取决于时间步$t’$之前的输出子序列$y_1, \ldots, y_{t’-1}$和对输入序列的信息进行编码得到的上下文变量$\mathbf{c}$。为了量化计算代价，用$\mathcal{Y}$表示输出词表，其中包含“&lt;eos&gt;”，所以这个词汇集合的基数$\left|\mathcal{Y}\right|$就是词表的大小。再将输出序列的最大词元数指定为$T’$。则我们的目标是从所有$\mathcal{O}(\left|\mathcal{Y}\right|^{T’})$个可能的输出序列中寻找理想的输出。这种计算方式略微高估了可能输出的数量，因为对于所有输出序列，在“&lt;eos&gt;”之后的部分将在实际输出中丢弃。但大体上这个数字反应了搜索空间的大小。</p><h3 id="8-1-贪心搜索"><a href="#8-1-贪心搜索" class="headerlink" title="8.1. 贪心搜索"></a>8.1. 贪心搜索</h3><p>首先看一个简单的策略：<em>贪心搜索</em>，该策略已用于<a href="#76-预测序列的评估">上节</a>的序列预测。对于输出序列的每一时间步$t’$，我们都将基于贪心搜索从$\mathcal{Y}$中找到具有最高条件概率的词元，即：</p><script type="math/tex; mode=display">y_{t'} = \operatorname*{argmax}_{y \in \mathcal{Y}} P(y \mid y_1, \ldots, y_{t'-1}, \mathbf{c})</script><p>一旦输出序列包含了“&lt;eos&gt;”或者达到其最大长度$T’$，则输出完成。</p><p><img src="/assets/post_img/article58/s2s-prob1.svg" alt="在每个时间步，贪心搜索选择具有最高条件概率的词元"></p><p>如图，假设输出中有四个词元“A”“B”“C”和“&lt;eos&gt;”。每个时间步下的四个数字分别表示在该时间步生成“A”“B”“C”和“&lt;eos&gt;”的条件概率。在每个时间步，贪心搜索选择具有最高条件概率的词元。因此图中预测输出序列为“A”“B”“C”和“&lt;eos&gt;”。这个输出序列的条件概率是$0.5\times0.4\times0.4\times0.6 = 0.048$。</p><p>那么贪心搜索存在的问题是什么呢？现实中，<em>最优序列</em>（optimal sequence）应该是最大化$\prod_{t’=1}^{T’} P(y_{t’} \mid y_1, \ldots, y_{t’-1}, \mathbf{c})$值的输出序列，这是基于输入序列生成输出序列的条件概率。贪心搜索无法保证得到最优序列。</p><p><img src="/assets/post_img/article58/s2s-prob2.svg" alt="在时间步2，选择具有第二高条件概率的词元“C”（而非最高条件概率的词元）"></p><p>上图中的另一个例子阐述了这个问题。与第一种情况不同，在时间步$2$中，我们选择词元“C”，它具有<em>第二</em>高的条件概率。由于时间步$3$所基于的时间步$1$和$2$处的输出子序列已从 第一种情况中的“A”和“B”改变为上图中的“A”和“C”，因此时间步$3$处的每个词元的条件概率也在上图中改变。假设我们在时间步$3$选择词元“B”，于是当前的时间步$4$基于前三个时间步的输出子序列“A”“C”和“B”为条件，这与第一种情况中的“A”“B”和“C”不同。此时上图中的时间步$4$生成每个词元的条件概率也不同于第一种情况中的条件概率。结果，上图中的输出序列“A”“C”“B”和“&lt;eos&gt;”的条件概率为$0.5\times0.3 \times0.6\times0.6=0.054$，大于第一种情况中的贪心搜索的条件概率。这个例子说明：贪心搜索获得的输出序列“A”“B”“C”和“&lt;eos&gt;”不一定是最佳序列。</p><h3 id="8-2-穷举搜索"><a href="#8-2-穷举搜索" class="headerlink" title="8.2. 穷举搜索"></a>8.2. 穷举搜索</h3><p>如果目标是获得最优序列，可以考虑使用<em>穷举搜索</em>（exhaustive search）：穷举地列举所有可能的输出序列及其条件概率，然后计算输出条件概率最高的一个。</p><p>虽然我们可以使用穷举搜索来获得最优序列，但其计算量$\mathcal{O}(\left|\mathcal{Y}\right|^{T’})$高的惊人。例如，当$|\mathcal{Y}|=10000$和$T’=10$时，我们需要评估$10000^{10} = 10^{40}$序列，这是一个极大的数，现有的计算机几乎不可能计算它。然而贪心搜索的计算量$\mathcal{O}(\left|\mathcal{Y}\right|T’)$要显著地小于穷举搜索。例如，当$|\mathcal{Y}|=10000$和$T’=10$时，我们只需要评估$10000\times10=10^5$个序列。</p><h3 id="8-3-束搜索"><a href="#8-3-束搜索" class="headerlink" title="8.3. 束搜索"></a>8.3. 束搜索</h3><p>那么该选取哪种序列搜索策略呢？如果精度最重要，则显然是穷举搜索。如果计算成本最重要，则显然是贪心搜索。而束搜索的实际应用则介于这两个极端之间。</p><p><em>束搜索</em>（beam search）是贪心搜索的一个改进版本。它有一个超参数，名为<em>束宽</em>（beam size）$k$。在时间步$1$，我们选择具有最高条件概率的$k$个词元。这$k$个词元将分别是$k$个候选输出序列的第一个词元。在随后的每个时间步，基于上一时间步的$k$个候选输出序列，我们将继续从$k\left|\mathcal{Y}\right|$个可能的选择中挑出具有最高条件概率的$k$个候选输出序列。</p><p><img src="/assets/post_img/article58/beam-search.svg" alt="束搜索过程（束宽：2，输出序列的最大长度：3）。候选输出序列是$A$、$C$、$AB$、$CE$、$ABD$和$CED$"></p><p>上图演示了束搜索的过程。假设输出的词表只包含五个元素：$\mathcal{Y} = {A, B, C, D, E}$，其中有一个是“&lt;eos&gt;”。设置束宽为$2$，输出序列的最大长度为$3$。在时间步$1$，假设具有最高条件概率$P(y_1 \mid \mathbf{c})$的词元是$A$和$C$。在时间步$2$，我们计算所有$y_2 \in \mathcal{Y}$为：</p><script type="math/tex; mode=display">\begin{aligned}P(A, y_2 \mid \mathbf{c}) = P(A \mid \mathbf{c})P(y_2 \mid A, \mathbf{c}),\\ P(C, y_2 \mid \mathbf{c}) = P(C \mid \mathbf{c})P(y_2 \mid C, \mathbf{c}),\end{aligned}</script><p>从这十个值中选择最大的两个，比如$P(A, B \mid \mathbf{c})$和$P(C, E \mid \mathbf{c})$。然后在时间步$3$，我们计算所有$y_3 \in \mathcal{Y}$为：</p><script type="math/tex; mode=display">\begin{aligned}P(A, B, y_3 \mid \mathbf{c}) = P(A, B \mid \mathbf{c})P(y_3 \mid A, B, \mathbf{c}),\\P(C, E, y_3 \mid \mathbf{c}) = P(C, E \mid \mathbf{c})P(y_3 \mid C, E, \mathbf{c}),\end{aligned}</script><p>从这十个值中选择最大的两个，即$P(A, B, D \mid \mathbf{c})$和$P(C, E, D \mid  \mathbf{c})$，我们会得到六个候选输出序列：</p><p>（1）$A$；（2）$C$；（3）$A,B$；（4）$C,E$；（5）$A,B,D$；（6）$C,E,D$。</p><p>最后，基于这六个序列（例如，丢弃包括“&lt;eos&gt;”和之后的部分），我们获得最终候选输出序列集合。然后我们选择其中条件概率乘积最高的序列作为输出序列：</p><script type="math/tex; mode=display">\frac{1}{L^\alpha} \log P(y_1, \ldots, y_{L}\mid \mathbf{c}) = \frac{1}{L^\alpha} \sum_{t'=1}^L \log P(y_{t'} \mid y_1, \ldots, y_{t'-1}, \mathbf{c}),</script><p>其中$L$是最终候选序列的长度，$\alpha$通常设置为$0.75$。因为一个较长的序列在上式的求和中会有更多的对数项，因此分母中的$L^\alpha$用于惩罚长序列。</p><p>束搜索的计算量为$\mathcal{O}(k\left|\mathcal{Y}\right|T’)$，这个结果介于贪心搜索和穷举搜索之间。实际上，贪心搜索可以看作一种束宽为$1$的特殊类型的束搜索。通过灵活地选择束宽，束搜索可以在正确率和计算代价之间进行权衡。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;p&gt;前一章中介绍了循环神经网络的基础知识，这种网络可以更好地处理序列数据。但对于当今各种各样的序列学习问题，这些技术可能并不够用。&lt;/p&gt;
&lt;p&gt;例如，循环神经网络在实践中一个常见问题是数值不稳定性。尽管我们已经应用了梯度裁剪等技巧来缓解这个问题，但是仍需要通过设计更复杂的序列模型可以进一步处理它。比如两个广泛使用的网络：&lt;em&gt;门控循环单元&lt;/em&gt;（gated recurrent units，GRU）和&lt;em&gt;长短期记忆网络&lt;/em&gt;（long short-term memory，LSTM）。然后本章将基于一个单向隐藏层来扩展循环神经网络架构，描述具有多个隐藏层的深层架构，并讨论基于前向和后向循环计算的双向设计。现代循环网络经常采用这种扩展。在解释这些循环神经网络的变体时将继续利用上一章中的语言建模问题。&lt;br&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://silencezheng.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>循环神经网络--《动手学深度学习》笔记0x09</title>
    <link href="http://silencezheng.top/2022/08/31/article57/"/>
    <id>http://silencezheng.top/2022/08/31/article57/</id>
    <published>2022-08-30T17:40:50.000Z</published>
    <updated>2022-08-30T17:46:56.151Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><p>目前为止仅接触到两种类型的数据：表格数据和图像数据。 对于图像数据，可以设计专门的卷积神经网络架构来为这类特殊的数据结构建模。 对于一张图像，我们需要有效地利用其像素位置，假若对图像中的像素位置进行重排，就会对图像中内容的推断造成极大的困难。</p><p>最重要的是，到目前为止我们默认数据都来自于某种分布， 并且所有样本都是独立同分布的 （independently and identically distributed，i.i.d.）。 然而，大多数的数据并非如此。 例如，文章中的单词是按顺序写的，如果顺序被随机地重排，就很难理解文章原始的意思。 同样，视频中的图像帧、对话中的音频信号以及网站上的浏览行为都是有顺序的。 因此，针对此类数据而设计特定模型，可能效果会更好。<br><span id="more"></span><br>另一个问题是：我们不仅可以接收一个序列作为输入，而且还可能期望继续猜测这个序列的后续。 例如，一个任务可以是继续预测$2, 4, 6, 8, 10, \ldots$。 这在时间序列分析中是相当常见的，可以用来预测股市的波动、 患者的体温曲线或者赛车所需的加速度。 我们需要能够处理这些数据的特定模型。</p><p>简言之，如果说卷积神经网络可以有效地处理空间信息， 那么本章介绍的<em>循环神经网络</em>（recurrent neural network，RNN）则可以更好地处理序列信息。 循环神经网络通过引入状态变量存储过去的信息和当前的输入，从而可以确定当前的输出。</p><p>许多使用循环网络的例子都是基于文本数据的，因此本章重点介绍语言模型。 主要内容包括对序列数据的详细探讨，文本预处理的实用技术，语言模型的基本概念，循环神经网络的设计方法。 最后会解析循环神经网络的梯度计算方法，以探讨训练此类网络时可能遇到的问题。</p><p>对应实践：<a href="https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x09.ipynb">https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x09.ipynb</a></p><h3 id="0-1-结论"><a href="#0-1-结论" class="headerlink" title="0.1. 结论"></a>0.1. 结论</h3><ul><li>内插法（在现有观测值之间进行估计）和外推法（对超出已知观测范围进行预测）在实践的难度上差别很大。因此对于所拥有的序列数据，在训练时始终要尊重其时间顺序，即最好不要基于未来的数据进行训练。</li><li>序列模型的估计需要专门的统计工具，两种较流行的选择是自回归模型和隐变量自回归模型。</li><li>对于时间是向前推进的因果模型，正向估计通常比反向估计更容易。</li><li>对于直到时间步$t$的观测序列，其在时间步$t+k$的预测输出是“$k$步预测”。随着对预测时间$k$值的增加，会造成误差的快速累积和预测质量的极速下降。</li><li>文本是序列数据的一种最常见的形式之一。</li><li>为了对文本进行预处理，通常将文本拆分为词元，构建词表将词元字符串映射为数字索引，并将文本数据转换为词元索引以供模型操作。</li><li>语言模型是自然语言处理的关键。</li><li>$n$元语法通过截断相关性，为处理长序列提供了一种实用的模型。</li><li>长序列存在一个问题：很少出现或者从不出现。</li><li>齐普夫定律支配着单词的分布，这个分布不仅适用于一元语法，还适用于其他元语法。</li><li>通过拉普拉斯平滑法可以有效地处理结构丰富而频率不足的低频词词组。</li><li>读取长序列的主要方式是随机采样和顺序分区。在迭代过程中，后者可以保证来自两个相邻的小批量中的子序列在原始序列上也是相邻的。</li><li>对隐状态使用循环计算的神经网络称为循环神经网络（RNN）。</li><li>循环神经网络的隐状态可以捕获直到当前时间步序列的历史信息。</li><li>循环神经网络模型的参数数量不会随着时间步的增加而增加。</li><li>可以使用循环神经网络创建字符级语言模型。</li><li>可以使用困惑度来评价语言模型的质量。</li><li>可以训练一个基于循环神经网络的字符级语言模型，根据用户提供的文本的前缀生成后续文本。</li><li>一个简单的循环神经网络语言模型包括输入编码、循环神经网络模型和输出生成。</li><li>循环神经网络模型在训练以前需要初始化状态，不过随机抽样和顺序采样使用初始化方法不同。</li><li>当使用顺序采样时，我们需要分离梯度以减少计算量。</li><li>在进行任何预测之前，模型通过预热期进行自我更新（例如，获得比初始值更好的隐状态）。</li><li>梯度裁剪可以防止梯度爆炸，但不能应对梯度消失。</li><li>深度学习框架的高级API提供了循环神经网络层的实现。</li><li>高级API的循环神经网络层返回一个输出和一个更新后的隐状态，我们还需要计算整个模型的输出层。</li><li>相比从零实现的循环神经网络，使用高级API实现可以加速训练。</li><li>“通过时间反向传播”仅仅适用于反向传播在具有隐状态的序列模型。</li><li>截断是计算方便性和数值稳定性的需要。截断包括：规则截断和随机截断。</li><li>矩阵的高次幂可能导致神经网络特征值的发散或消失，将以梯度爆炸或梯度消失的形式表现。</li><li>为了计算的效率，“通过时间反向传播”在计算期间会缓存中间值。</li></ul><h2 id="1-序列模型"><a href="#1-序列模型" class="headerlink" title="1. 序列模型"></a>1. 序列模型</h2><p>想象一下有人正在看网飞（Netflix）上的电影。一名忠实的用户会对每一部电影都给出评价，毕竟一部好电影需要更多的支持和认可。然而事实上随着时间的推移，人们对电影的看法会发生很大的变化。心理学家对这些现象起了名字：</p><ul><li><em>锚定</em>（anchoring）效应：基于其他人的意见做出评价。例如，奥斯卡颁奖后，受到关注的电影的评分会上升，尽管它还是原来那部电影。这种影响将持续几个月，直到人们忘记了这部电影曾经获得的奖项。结果表明，这种效应会使评分提高半个百分点以上。</li><li><em>享乐适应</em>（hedonic adaption）：人们迅速接受并且适应一种更好或者更坏的情况作为新的常态。例如，在看了很多好电影之后，人们会强烈期望下部电影会更好。因此在看过许多精彩电影后，一部普通的电影也可能被认为是糟糕的。</li><li><em>季节性</em>（seasonality）：少有观众喜欢在八月看圣诞老人的电影。</li><li>有时，电影会由于导演或演员在制作中的不当行为变得不受欢迎。</li><li>有些电影因为其极度糟糕只能成为小众电影。</li></ul><p>简而言之，电影评分决不是固定不变的。因此，使用时间动力学可以得到更准确的电影推荐。当然，序列数据不仅仅是关于电影评分的。下面给出了更多的场景。</p><ul><li>在使用程序时，许多用户都有很强的特定习惯。例如，在学生放学后社交媒体应用更受欢迎。在市场开放时股市交易软件更常用。</li><li>预测明天的股价要比过去的股价更困难，尽管两者都只是估计一个数字。在统计学中，前者（对超出已知观测范围进行预测）称为<em>外推法</em>（extrapolation），而后者（在现有观测值之间进行估计）称为<em>内插法</em>（interpolation）。</li><li>在本质上，音乐、语音、文本和视频都是连续的。如果它们的序列被重排，那么就会失去原有的意义。比如，一个文本标题“狗咬人”远没有“人咬狗”那么令人惊讶，尽管组成两句话的字完全相同。</li><li>地震具有很强的相关性，即大地震发生后，很可能会有几次小余震，这些余震的强度比非大地震后的余震要大得多。事实上，地震是时空相关的，即余震通常发生在很短的时间跨度和很近的距离内。</li><li>人类之间的互动也是连续的，这可以从微博上的争吵和辩论中看出。</li></ul><h3 id="1-1-统计工具"><a href="#1-1-统计工具" class="headerlink" title="1.1. 统计工具"></a>1.1. 统计工具</h3><p>处理序列数据需要统计工具和新的深度神经网络架构。 为了简单起见，以下图所示的股票价格（富时100指数）为例。</p><p><img src="/assets/post_img/article57/数据分析.png" alt="近30年的富时100指数"></p><p>其中，用$x_t$表示价格，即在<em>时间步</em>（time step）$t \in \mathbb{Z}^+$时，观察到的价格$x_t$。注意$t$对于本文中的序列通常是离散的，并在整数或其子集上变化。假设一个交易员想在$t$日的股市中表现良好，于是通过以下途径预测$x_t$：</p><script type="math/tex; mode=display">x_t \sim P(x_t \mid x_{t-1}, \ldots, x_1)</script><h4 id="1-1-1-自回归模型"><a href="#1-1-1-自回归模型" class="headerlink" title="1.1.1. 自回归模型"></a>1.1.1. 自回归模型</h4><p>为了实现这个预测，交易员可以使用回归模型(例如最简单的线性回归）。这里仅有一个主要问题：输入数据的数量，输入$x_{t-1}, \ldots, x_1$本身因$t$而异。也就是说，输入数据的数量这个数字将会随着我们遇到的数据量的增加而增加，因此需要一个近似方法来使这个计算变得容易处理。本章后面的大部分内容将围绕着如何有效估计$P(x_t \mid x_{t-1}, \ldots, x_1)$展开。简单地说，它归结为以下两种策略。</p><p>第一种策略，假设在现实情况下相当长的序列$x_{t-1}, \ldots, x_1$可能是不必要的，则只需要满足某个长度为$\tau$的时间跨度，即使用观测序列$x_{t-1}, \ldots, x_{t-\tau}$。当下获得的最直接的好处就是参数的数量总是不变的，至少在$t &gt; \tau$时如此，这就使我们能够训练一个上述的深度网络。这种模型被称为<em>自回归模型</em>（autoregressive models），因为它们是对自己执行回归。</p><p>第二种策略，如下图所示，是保留一些对过去观测的总结$h_t$，并且同时更新预测$\hat{x}_t$和总结$h_t$。这就产生了基于$\hat{x}_t = P(x_t \mid h_{t})$估计$x_t$，以及公式$h_t = g(h_{t-1}, x_{t-1})$更新的模型。由于$h_t$从未被观测到，这类模型也被称为<em>隐变量自回归模型</em>（latent autoregressive models）。</p><p><img src="/assets/post_img/article57/sequence-model.svg" alt="隐变量自回归模型"></p><p>这两种策略有一个显而易见的问题：如何生成训练数据？一个经典方法是使用历史观测来预测下一个未来观测。我们并不指望时间会停滞不前，但一个常见的假设是虽然特定值$x_t$可能会改变，但是序列本身的动力学不会改变。这样的假设是合理的，因为新的动力学一定受新的数据影响，而人们不可能用目前所掌握的数据来预测新的动力学。统计学家称不变的动力学为<em>静止的</em>（stationary）。因此，整个序列的估计值都将通过以下的方式获得：</p><script type="math/tex; mode=display">P(x_1, \ldots, x_T) = \prod_{t=1}^T P(x_t \mid x_{t-1}, \ldots, x_1).</script><p>注意，如果我们处理的是离散的对象（如单词），而不是连续的数字，则上述的考虑仍然有效。唯一的差别是，对于离散的对象，需要使用分类器而不是回归模型来估计条件概率$P(x_t \mid  x_{t-1}, \ldots, x_1)$。</p><h4 id="1-1-2-马尔可夫模型"><a href="#1-1-2-马尔可夫模型" class="headerlink" title="1.1.2. 马尔可夫模型"></a>1.1.2. 马尔可夫模型</h4><p>在自回归模型的近似法中使用$x_{t-1}, \ldots, x_{t-\tau}$而不是$x_{t-1}, \ldots, x_1$来估计$x_t$。只要这种是近似精确的，我们就说序列满足<em>马尔可夫条件</em>（Markov condition）。特别是，如果$\tau = 1$，得到一个<em>一阶马尔可夫模型</em>（first-order Markov model），$P(x)$由下式给出：</p><script type="math/tex; mode=display">P(x_1, \ldots, x_T) = \prod_{t=1}^T P(x_t \mid x_{t-1}) \text{ 当 } P(x_1 \mid x_0) = P(x_1).</script><p>当假设$x_t$仅是离散值时，这样的模型特别棒，因为在这种情况下，使用动态规划可以沿着马尔可夫链精确地计算结果。例如可以高效地计算$P(x_{t+1} \mid x_{t-1})$：</p><script type="math/tex; mode=display">\begin{aligned}P(x_{t+1} \mid x_{t-1})&= \frac{\sum_{x_t} P(x_{t+1}, x_t, x_{t-1})}{P(x_{t-1})}\\&= \frac{\sum_{x_t} P(x_{t+1} \mid x_t, x_{t-1}) P(x_t, x_{t-1})}{P(x_{t-1})}\\&= \sum_{x_t} P(x_{t+1} \mid x_t) P(x_t \mid x_{t-1})\end{aligned}</script><p>利用这一事实，我们只需要考虑过去观察中的一个非常短的历史片段：$P(x_{t+1} \mid x_t, x_{t-1}) = P(x_{t+1} \mid x_t)$。隐马尔可夫模型中的动态规划超出了本节的范围，而动态规划这些计算工具已经在控制算法和强化学习算法广泛使用。</p><h4 id="1-1-3-因果关系"><a href="#1-1-3-因果关系" class="headerlink" title="1.1.3. 因果关系"></a>1.1.3. 因果关系</h4><p>原则上，将$P(x_1, \ldots, x_T)$倒序展开也没什么问题。毕竟，基于条件概率公式总是可以写出：</p><script type="math/tex; mode=display">P(x_1, \ldots, x_T) = \prod_{t=T}^1 P(x_t \mid x_{t+1}, \ldots, x_T).</script><p>事实上，如果基于一个马尔可夫模型，我们还可以得到一个反向的条件概率分布。但在许多情况下，数据存在一个自然的方向，即在时间上是前进的。未来的事件不能影响过去。因此，如果我们改变$x_t$，可能会影响未来发生的事情$x_{t+1}$，但不能反过来。也就是说，如果我们改变$x_t$，基于过去事件得到的分布不会改变。因此，解释$P(x_{t+1} \mid x_t)$应该比解释$P(x_t \mid x_{t+1})$更容易。例如在某些情况下，对于某些可加性噪声$\epsilon$，我们可以找到$x_{t+1} = f(x_t) + \epsilon$，而反之则不行。这个向前推进的方向恰好也是比较有用的方向。彼得斯等人对该主题的更多内容做了详尽的解释。</p><h3 id="1-2-训练"><a href="#1-2-训练" class="headerlink" title="1.2. 训练"></a>1.2. 训练</h3><p>在实践中尝试一下上述统计工具。首先生成一些数据：使用正弦函数和一些可加性噪声来生成序列数据，时间步为$1, 2, \ldots, 1000$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">T = <span class="number">1000</span>  <span class="comment"># 总共产生1000个点</span></span><br><span class="line">time = torch.arange(<span class="number">1</span>, T + <span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">x = torch.sin(<span class="number">0.01</span> * time) + torch.normal(<span class="number">0</span>, <span class="number">0.2</span>, (T,))</span><br><span class="line"><span class="comment"># d2l.plot</span></span><br><span class="line">plot(time, [x], <span class="string">&#x27;time&#x27;</span>, <span class="string">&#x27;x&#x27;</span>, xlim=[<span class="number">1</span>, <span class="number">1000</span>], figsize=(<span class="number">6</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article57/output_sequence.svg" alt="正弦函数和一些可加性噪声"></p><p>接下来将这个序列转换为模型的<em>特征－标签</em>（feature-label）对。基于嵌入维度$\tau$将数据映射为数据对$y_t = x_t$和$\mathbf{x}_t = [x_{t-\tau}, \ldots, x_{t-1}]$。这比我们提供的数据样本少了$\tau$个，因为我们没有足够的历史记录来描述前$\tau$个数据样本。一个简单的解决办法是：如果拥有足够长的序列就丢弃这几项；另一个方法是用零填充序列。这里我们仅使用前600个“特征－标签”对进行训练。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 嵌入维度 τ，决定了特征向量的维度（特征数和样本个数）。</span></span><br><span class="line">tau = <span class="number">4</span></span><br><span class="line">features = torch.zeros((T - tau, tau))</span><br><span class="line"><span class="comment"># 对每个特征（每一列），用x中的数据填入，每次循环x向后推进一位</span></span><br><span class="line"><span class="comment"># 最终特征向量中的最后一列样本为x中的后996个值，样本数为996</span></span><br><span class="line"><span class="comment"># 这样整理后，样本的起始时刻为5，前四个时刻的数据作为第一个样本的特征，以此类推</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(tau):</span><br><span class="line">    features[:, i] = x[i: T - tau + i]</span><br><span class="line"><span class="comment"># 标签也整理为（996，1）</span></span><br><span class="line">labels = x[tau:].reshape((-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">batch_size, n_train = <span class="number">16</span>, <span class="number">600</span></span><br><span class="line"><span class="comment"># 只有前n_train个样本用于训练</span></span><br><span class="line"><span class="comment"># d2l.load_array</span></span><br><span class="line">train_iter = load_array((features[:n_train], labels[:n_train]),</span><br><span class="line">                            batch_size, is_train=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>这里使用一个相当简单的架构训练模型： 一个拥有两个全连接层的多层感知机，ReLU激活函数和平方损失。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化网络权重的函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_weights</span>(<span class="params">m</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.xavier_uniform_(m.weight)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 一个简单的多层感知机</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_net</span>():</span></span><br><span class="line">    net = nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">10</span>),</span><br><span class="line">                        nn.ReLU(),</span><br><span class="line">                        nn.Linear(<span class="number">10</span>, <span class="number">1</span>))</span><br><span class="line">    net.apply(init_weights)</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line"><span class="comment"># 平方损失。注意：MSELoss计算平方误差时不带系数1/2</span></span><br><span class="line">loss = nn.MSELoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br></pre></td></tr></table></figure><p>现在训练模型，实现下面的训练代码的方式与前面几节中的循环训练基本相同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">net, train_iter, loss, epochs, lr</span>):</span></span><br><span class="line">    trainer = torch.optim.Adam(net.parameters(), lr)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            trainer.zero_grad()</span><br><span class="line">            l = loss(net(X), y)</span><br><span class="line">            l.<span class="built_in">sum</span>().backward()</span><br><span class="line">            trainer.step()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, &#x27;</span></span><br><span class="line">        <span class="comment"># d2l.evaluate_loss</span></span><br><span class="line">              <span class="string">f&#x27;loss: <span class="subst">&#123;evaluate_loss(net, train_iter, loss):f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">net = get_net()</span><br><span class="line">train(net, train_iter, loss, <span class="number">5</span>, <span class="number">0.01</span>)</span><br></pre></td></tr></table></figure><h3 id="1-3-预测"><a href="#1-3-预测" class="headerlink" title="1.3. 预测"></a>1.3. 预测</h3><p>前面训练的损失很小，则可以期望模型有很好的工作效果。首先是检查模型预测下一个时间步的能力， 也就是单步预测（one-step-ahead prediction）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">onestep_preds = net(features)</span><br><span class="line"><span class="comment"># d2l.plot</span></span><br><span class="line">plot([time, time[tau:]],</span><br><span class="line">         [x.detach().numpy(), onestep_preds.detach().numpy()], <span class="string">&#x27;time&#x27;</span>,</span><br><span class="line">         <span class="string">&#x27;x&#x27;</span>, legend=[<span class="string">&#x27;data&#x27;</span>, <span class="string">&#x27;1-step preds&#x27;</span>], xlim=[<span class="number">1</span>, <span class="number">1000</span>],</span><br><span class="line">         figsize=(<span class="number">6</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article57/output_sequence_predict.svg" alt="predict_output"></p><p>可以看到确实单步预测效果不错。即使这些预测的时间步超过了$600+4$（<code>n_train + tau</code>），其结果看起来仍然是可信的。然而有一个小问题：如果数据观察序列的时间步只到$604$，我们需要一步一步地向前迈进：</p><script type="math/tex; mode=display">\hat{x}_{605} = f(x_{601}, x_{602}, x_{603}, x_{604}), \\\hat{x}_{606} = f(x_{602}, x_{603}, x_{604}, \hat{x}_{605}), \\\hat{x}_{607} = f(x_{603}, x_{604}, \hat{x}_{605}, \hat{x}_{606}),\\\hat{x}_{608} = f(x_{604}, \hat{x}_{605}, \hat{x}_{606}, \hat{x}_{607}),\\\hat{x}_{609} = f(\hat{x}_{605}, \hat{x}_{606}, \hat{x}_{607}, \hat{x}_{608}),\\\ldots</script><p>通常，对于直到$x_t$的观测序列，其在时间步$t+k$处的预测输出$\hat{x}_{t+k}$称为$k$<em>步预测</em>（$k$-step-ahead-prediction）。由于观察已经到了$x_{604}$，它的$k$步预测是$\hat{x}_{604+k}$。则我们必须使用自己的预测（而不是原始数据）来进行多步预测，看效果如何：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">multistep_preds = torch.zeros(T)</span><br><span class="line"><span class="comment"># 把向量前面的数值替换为x中604前的数值</span></span><br><span class="line">multistep_preds[: n_train + tau] = x[: n_train + tau]</span><br><span class="line"><span class="comment"># 用自己的预测结果预测后面的时间步</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_train + tau, T):</span><br><span class="line">    multistep_preds[i] = net(</span><br><span class="line">        multistep_preds[i - tau:i].reshape((<span class="number">1</span>, -<span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># d2l.plot</span></span><br><span class="line">plot([time, time[tau:], time[n_train + tau:]],</span><br><span class="line">         [x.detach().numpy(), onestep_preds.detach().numpy(),</span><br><span class="line">          multistep_preds[n_train + tau:].detach().numpy()], <span class="string">&#x27;time&#x27;</span>,</span><br><span class="line">         <span class="string">&#x27;x&#x27;</span>, legend=[<span class="string">&#x27;data&#x27;</span>, <span class="string">&#x27;1-step preds&#x27;</span>, <span class="string">&#x27;multistep preds&#x27;</span>],</span><br><span class="line">         xlim=[<span class="number">1</span>, <span class="number">1000</span>], figsize=(<span class="number">6</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article57/output_sequence_k.svg" alt="k——step"></p><p>如上图所示，绿线的预测显然并不理想。经过几个预测步骤之后，预测的结果很快就会衰减到一个常数。这个算法效果如此差是由于错误的累积：假设在步骤$1$之后，我们积累了一些错误$\epsilon_1 = \bar\epsilon$。于是，步骤$2$的输入被扰动了$\epsilon_1$，结果积累的误差是依照次序的$\epsilon_2 = \bar\epsilon + c \epsilon_1$，其中$c$为某个常数，后面的预测误差依此类推。因此误差可能会相当快地偏离真实的观测结果。例如，未来$24$小时的天气预报往往相当准确，但超过这一点，精度就会迅速下降。本章及后续章节中将讨论如何改进这一点。</p><p>基于$k = 1, 4, 16, 64$，通过对整个序列预测的计算，来更仔细地看一下$k$步预测的困难。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">max_steps = <span class="number">64</span></span><br><span class="line"></span><br><span class="line">features = torch.zeros((T - tau - max_steps + <span class="number">1</span>, tau + max_steps))</span><br><span class="line"><span class="comment"># 列i（i&lt;tau）是来自x的观测，其时间步从（i）到（i+T-tau-max_steps+1）</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(tau):</span><br><span class="line">    features[:, i] = x[i: i + T - tau - max_steps + <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列i（i&gt;=tau）是来自（i-tau+1）步的预测，其时间步从（i）到（i+T-tau-max_steps+1）</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(tau, tau + max_steps):</span><br><span class="line">    features[:, i] = net(features[:, i - tau:i]).reshape(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">steps = (<span class="number">1</span>, <span class="number">4</span>, <span class="number">16</span>, <span class="number">64</span>)</span><br><span class="line"><span class="comment"># d2l.plot</span></span><br><span class="line">plot([time[tau + i - <span class="number">1</span>: T - max_steps + i] <span class="keyword">for</span> i <span class="keyword">in</span> steps],</span><br><span class="line">         [features[:, (tau + i - <span class="number">1</span>)].detach().numpy() <span class="keyword">for</span> i <span class="keyword">in</span> steps], <span class="string">&#x27;time&#x27;</span>, <span class="string">&#x27;x&#x27;</span>,</span><br><span class="line">         legend=[<span class="string">f&#x27;<span class="subst">&#123;i&#125;</span>-step preds&#x27;</span> <span class="keyword">for</span> i <span class="keyword">in</span> steps], xlim=[<span class="number">5</span>, <span class="number">1000</span>],</span><br><span class="line">         figsize=(<span class="number">6</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article57/output_sequence_kstep.svg" alt="kstep"></p><p>上图清楚地说明了当试图预测更远的未来时，预测的质量是如何变化的。 虽然“4步预测”看起来仍然不错，但超过这个跨度的任何预测几乎都是无用的。</p><h2 id="2-文本预处理"><a href="#2-文本预处理" class="headerlink" title="2. 文本预处理"></a>2. 文本预处理</h2><p>对于序列数据处理问题，上节中评估了所需的统计工具和预测时面临的挑战。 这样的数据存在许多种形式，文本是最常见例子之一。 例如，一篇文章可以被简单地看作是一串单词序列，甚至是一串字符序列。 本节将解析文本的常见预处理步骤。 这些步骤通常包括：</p><ol><li>将文本作为字符串加载到内存中。</li><li>将字符串拆分为词元（如单词和字符）。</li><li>建立一个词表，将拆分的词元映射到数字索引。</li><li>将文本转换为数字索引序列，方便模型操作。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> re</span><br></pre></td></tr></table></figure><h3 id="2-1-读取数据集"><a href="#2-1-读取数据集" class="headerlink" title="2.1. 读取数据集"></a>2.1. 读取数据集</h3><p>首先从H.G.Well的<a href="https://www.gutenberg.org/ebooks/35">《时光机器》</a>中加载文本。 这是一个相当小的语料库，只有30000多个单词， 而现实中的文档集合可能会包含数十亿个单词。 下面的函数将数据集读取到由多条文本行组成的列表中，其中每条文本行都是一个字符串。 为简单起见这里忽略了标点符号和字母大写。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># d2l.DATA_HUB、d2l.DATA_URL</span></span><br><span class="line">DATA_HUB[<span class="string">&#x27;time_machine&#x27;</span>] = (DATA_URL + <span class="string">&#x27;timemachine.txt&#x27;</span>,</span><br><span class="line">                                <span class="string">&#x27;090b5e7e70c295757f55df93cb0a180b9691891a&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_time_machine</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;将时间机器数据集加载到文本行的列表中&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># d2l.download</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(download(<span class="string">&#x27;time_machine&#x27;</span>), <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        lines = f.readlines()</span><br><span class="line">    <span class="comment"># regular expression substitute（替换）</span></span><br><span class="line">    <span class="comment"># 这里[^A-Za-z]+的含义是匹配除了字母A-Z和a-z外的字符，一次到多次，替换为空格</span></span><br><span class="line">    <span class="comment"># 然后strip移除字符串头尾的空格，化为小写后完毕。</span></span><br><span class="line">    <span class="keyword">return</span> [re.sub(<span class="string">&#x27;[^A-Za-z]+&#x27;</span>, <span class="string">&#x27; &#x27;</span>, line).strip().lower() <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 无法获取数据集可能是DNS问题导致</span></span><br><span class="line">lines = read_time_machine()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;# 文本总行数: <span class="subst">&#123;<span class="built_in">len</span>(lines)&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(lines[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(lines[<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line"><span class="comment"># 文本总行数: 3221</span></span><br><span class="line">the time machine by h g wells</span><br><span class="line">twinkled <span class="keyword">and</span> his usually pale face was flushed <span class="keyword">and</span> animated the</span><br></pre></td></tr></table></figure><h3 id="2-2-词元化"><a href="#2-2-词元化" class="headerlink" title="2.2. 词元化"></a>2.2. 词元化</h3><p><code>tokenize</code>函数将文本行列表（<code>lines</code>）作为输入，列表中的每个元素是一个文本序列（如一条文本行）。每个文本序列又被拆分成一个词元列表，<em>词元</em>（token）是文本的基本单位（单词或字母）。最后，返回一个由词元列表组成的列表，其中的每个词元都是一个字符串（string）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize</span>(<span class="params">lines, token=<span class="string">&#x27;word&#x27;</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;将文本行拆分为单词或字符词元&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> token == <span class="string">&#x27;word&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> [line.split() <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">    <span class="keyword">elif</span> token == <span class="string">&#x27;char&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> [<span class="built_in">list</span>(line) <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;错误：未知词元类型：&#x27;</span> + token)</span><br><span class="line"></span><br><span class="line">tokens = tokenize(lines)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">11</span>):</span><br><span class="line">    <span class="built_in">print</span>(tokens[i])</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">[<span class="string">&#x27;the&#x27;</span>, <span class="string">&#x27;time&#x27;</span>, <span class="string">&#x27;machine&#x27;</span>, <span class="string">&#x27;by&#x27;</span>, <span class="string">&#x27;h&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;wells&#x27;</span>]</span><br><span class="line">[]</span><br><span class="line">[]</span><br><span class="line">[]</span><br><span class="line">[]</span><br><span class="line">[<span class="string">&#x27;i&#x27;</span>]</span><br><span class="line">[]</span><br><span class="line">[]</span><br><span class="line">[<span class="string">&#x27;the&#x27;</span>, <span class="string">&#x27;time&#x27;</span>, <span class="string">&#x27;traveller&#x27;</span>, <span class="string">&#x27;for&#x27;</span>, <span class="string">&#x27;so&#x27;</span>, <span class="string">&#x27;it&#x27;</span>, <span class="string">&#x27;will&#x27;</span>, <span class="string">&#x27;be&#x27;</span>, <span class="string">&#x27;convenient&#x27;</span>, <span class="string">&#x27;to&#x27;</span>, <span class="string">&#x27;speak&#x27;</span>, <span class="string">&#x27;of&#x27;</span>, <span class="string">&#x27;him&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;was&#x27;</span>, <span class="string">&#x27;expounding&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;recondite&#x27;</span>, <span class="string">&#x27;matter&#x27;</span>, <span class="string">&#x27;to&#x27;</span>, <span class="string">&#x27;us&#x27;</span>, <span class="string">&#x27;his&#x27;</span>, <span class="string">&#x27;grey&#x27;</span>, <span class="string">&#x27;eyes&#x27;</span>, <span class="string">&#x27;shone&#x27;</span>, <span class="string">&#x27;and&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;twinkled&#x27;</span>, <span class="string">&#x27;and&#x27;</span>, <span class="string">&#x27;his&#x27;</span>, <span class="string">&#x27;usually&#x27;</span>, <span class="string">&#x27;pale&#x27;</span>, <span class="string">&#x27;face&#x27;</span>, <span class="string">&#x27;was&#x27;</span>, <span class="string">&#x27;flushed&#x27;</span>, <span class="string">&#x27;and&#x27;</span>, <span class="string">&#x27;animated&#x27;</span>, <span class="string">&#x27;the&#x27;</span>]</span><br></pre></td></tr></table></figure><h3 id="2-3-词表"><a href="#2-3-词表" class="headerlink" title="2.3. 词表"></a>2.3. 词表</h3><p>词元的类型是字符串，而模型需要的输入是数字，所以我们需要构建一个字典，通常也叫做<em>词表</em>（vocabulary），用来将字符串类型的词元映射到从$0$开始的数字索引中。首先将训练集中的所有文档合并在一起，对它们的唯一词元进行统计，得到的统计结果称之为<em>语料</em>（corpus）。然后根据每个唯一词元的出现频率，为其分配一个数字索引。很少出现的词元通常被移除，这可以降低复杂性。另外，语料库中不存在或已删除的任何词元都将映射到一个特定的未知词元“&lt;unk&gt;”。我们可以选择增加一个列表，用于保存那些被保留的词元，例如：填充词元（“&lt;pad&gt;”）；序列开始词元（“&lt;bos&gt;”）；序列结束词元（“&lt;eos&gt;”）。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Vocab</span>:</span> </span><br><span class="line">    <span class="string">&quot;&quot;&quot;文本词表&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, tokens=<span class="literal">None</span>, min_freq=<span class="number">0</span>, reserved_tokens=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> tokens <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            tokens = []</span><br><span class="line">        <span class="keyword">if</span> reserved_tokens <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            reserved_tokens = []</span><br><span class="line">        <span class="comment"># 获取词元展平后的Counter对象。</span></span><br><span class="line">        counter = count_corpus(tokens)</span><br><span class="line">        <span class="comment"># 私有变量，词元频率（事实上是伪私有）</span></span><br><span class="line">        <span class="comment"># 按出现频率排序，sorted() 函数用于对所有可迭代的对象进行排序操作。</span></span><br><span class="line">        <span class="comment"># counter.items()等同于字典的items()函数，返回一个可迭代的集合数据结构</span></span><br><span class="line">        <span class="comment"># 参数key是用来进行比较的元素，指定可迭代对象中的一个元素来进行排序，这里指的是每一个tuple中的第二个元素，即频率</span></span><br><span class="line">        <span class="comment"># reverse参数：True为降序，False为升序，默认False  </span></span><br><span class="line">        self._token_freqs = <span class="built_in">sorted</span>(counter.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>],</span><br><span class="line">                                   reverse=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 生成词元列表</span></span><br><span class="line">        <span class="comment"># 左加使未知词元的索引为0, 如：[&#x27;&lt;unk&gt;&#x27;, ...]</span></span><br><span class="line">        self.idx_to_token = [<span class="string">&#x27;&lt;unk&gt;&#x27;</span>] + reserved_tokens</span><br><span class="line">        <span class="comment"># 生成词元与索引对应的字典</span></span><br><span class="line">        self.token_to_idx = &#123;token: idx</span><br><span class="line">                             <span class="keyword">for</span> idx, token <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.idx_to_token)&#125;</span><br><span class="line">        <span class="keyword">for</span> token, freq <span class="keyword">in</span> self._token_freqs:</span><br><span class="line">            <span class="keyword">if</span> freq &lt; min_freq:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="comment"># 发现新词，在词元列表中加入该词，然后在词元-索引字典中添加该词及其索引</span></span><br><span class="line">            <span class="keyword">if</span> token <span class="keyword">not</span> <span class="keyword">in</span> self.token_to_idx:</span><br><span class="line">                self.idx_to_token.append(token)</span><br><span class="line">                self.token_to_idx[token] = <span class="built_in">len</span>(self.idx_to_token) - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.idx_to_token)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义该方法使得Vocab类可以以 p[key] 的方式取值</span></span><br><span class="line">    <span class="comment"># 此处‘key’的格式可以为单个词、list或tuple</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, tokens</span>):</span></span><br><span class="line">        <span class="comment"># 单个词</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(tokens, (<span class="built_in">list</span>, <span class="built_in">tuple</span>)):</span><br><span class="line">            <span class="comment"># 如果查找不到则返回频率0</span></span><br><span class="line">            <span class="keyword">return</span> self.token_to_idx.get(tokens, self.unk)</span><br><span class="line">        <span class="comment"># 可遍历对象，返回一个频率列表</span></span><br><span class="line">        <span class="keyword">return</span> [self.__getitem__(token) <span class="keyword">for</span> token <span class="keyword">in</span> tokens]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 接受索引返回词元</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">to_tokens</span>(<span class="params">self, indices</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(indices, (<span class="built_in">list</span>, <span class="built_in">tuple</span>)):</span><br><span class="line">            <span class="keyword">return</span> self.idx_to_token[indices]</span><br><span class="line">        <span class="keyword">return</span> [self.idx_to_token[index] <span class="keyword">for</span> index <span class="keyword">in</span> indices]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># property装饰器，可以直接通过方法名来访问方法，不需要在方法名后添加圆括号“()”</span></span><br><span class="line">    <span class="comment"># 如：vacab.unk、vacab.token_freqs</span></span><br><span class="line">    <span class="comment"># 相当于getter方法，访问私有成员的接口</span></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">unk</span>(<span class="params">self</span>):</span>  <span class="comment"># 未知词元的索引为0</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">token_freqs</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self._token_freqs</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_corpus</span>(<span class="params">tokens</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;统计词元的频率&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 这里的tokens是1D列表或2D列表</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(tokens) == <span class="number">0</span> <span class="keyword">or</span> <span class="built_in">isinstance</span>(tokens[<span class="number">0</span>], <span class="built_in">list</span>):</span><br><span class="line">        <span class="comment"># 将词元列表展平成一个列表</span></span><br><span class="line">        tokens = [token <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line">    <span class="comment"># 返回一个Counter对象，该对象是一个高性能的容器数据类型，有许多作用</span></span><br><span class="line">    <span class="comment"># 对于取频率这件事，直接dict（counter）就可以得到频率字典。</span></span><br><span class="line">    <span class="keyword">return</span> collections.Counter(tokens)</span><br></pre></td></tr></table></figure></p><p>使用时光机器数据集作为语料库来构建词表，然后打印前几个高频词元及其索引：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vocab = Vocab(tokens)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(vocab.token_to_idx.items())[:<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">[(<span class="string">&#x27;&lt;unk&gt;&#x27;</span>, <span class="number">0</span>), (<span class="string">&#x27;the&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;i&#x27;</span>, <span class="number">2</span>), (<span class="string">&#x27;and&#x27;</span>, <span class="number">3</span>), (<span class="string">&#x27;of&#x27;</span>, <span class="number">4</span>), (<span class="string">&#x27;a&#x27;</span>, <span class="number">5</span>), (<span class="string">&#x27;to&#x27;</span>, <span class="number">6</span>), (<span class="string">&#x27;was&#x27;</span>, <span class="number">7</span>), (<span class="string">&#x27;in&#x27;</span>, <span class="number">8</span>), (<span class="string">&#x27;that&#x27;</span>, <span class="number">9</span>)]</span><br></pre></td></tr></table></figure></p><p>可以将每一条文本行转换成一个数字索引列表：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">0</span>, <span class="number">10</span>]:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;文本:&#x27;</span>, tokens[i])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;索引:&#x27;</span>, vocab[tokens[i]])</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">文本: [<span class="string">&#x27;the&#x27;</span>, <span class="string">&#x27;time&#x27;</span>, <span class="string">&#x27;machine&#x27;</span>, <span class="string">&#x27;by&#x27;</span>, <span class="string">&#x27;h&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;wells&#x27;</span>]</span><br><span class="line">索引: [<span class="number">1</span>, <span class="number">19</span>, <span class="number">50</span>, <span class="number">40</span>, <span class="number">2183</span>, <span class="number">2184</span>, <span class="number">400</span>]</span><br><span class="line">文本: [<span class="string">&#x27;twinkled&#x27;</span>, <span class="string">&#x27;and&#x27;</span>, <span class="string">&#x27;his&#x27;</span>, <span class="string">&#x27;usually&#x27;</span>, <span class="string">&#x27;pale&#x27;</span>, <span class="string">&#x27;face&#x27;</span>, <span class="string">&#x27;was&#x27;</span>, <span class="string">&#x27;flushed&#x27;</span>, <span class="string">&#x27;and&#x27;</span>, <span class="string">&#x27;animated&#x27;</span>, <span class="string">&#x27;the&#x27;</span>]</span><br><span class="line">索引: [<span class="number">2186</span>, <span class="number">3</span>, <span class="number">25</span>, <span class="number">1044</span>, <span class="number">362</span>, <span class="number">113</span>, <span class="number">7</span>, <span class="number">1421</span>, <span class="number">3</span>, <span class="number">1045</span>, <span class="number">1</span>]</span><br></pre></td></tr></table></figure><h3 id="2-4-功能整合"><a href="#2-4-功能整合" class="headerlink" title="2.4. 功能整合"></a>2.4. 功能整合</h3><p>将所有功能打包到load_corpus_time_machine函数中， 该函数返回corpus（词元索引列表）和vocab（时光机器语料库的词表）。有两点需要注意：</p><ol><li>为了简化后面章节中的训练，使用字符（而不是单词）实现文本词元化；</li><li>时光机器数据集中的每个文本行不一定是一个句子或一个段落，还可能是一个单词，因此返回的corpus展平为一维列表，而不是由多词元列表构成的一个列表。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_corpus_time_machine</span>(<span class="params">max_tokens=-<span class="number">1</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回时光机器数据集的词元索引列表和词表&quot;&quot;&quot;</span></span><br><span class="line">    lines = read_time_machine()</span><br><span class="line">    tokens = tokenize(lines, <span class="string">&#x27;char&#x27;</span>)</span><br><span class="line">    vocab = Vocab(tokens)</span><br><span class="line">    <span class="comment"># 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，</span></span><br><span class="line">    <span class="comment"># 所以将所有文本行展平到一个列表中</span></span><br><span class="line">    corpus = [vocab[token] <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line">    <span class="keyword">if</span> max_tokens &gt; <span class="number">0</span>:</span><br><span class="line">        corpus = corpus[:max_tokens]</span><br><span class="line">    <span class="keyword">return</span> corpus, vocab</span><br><span class="line"></span><br><span class="line">corpus, vocab = load_corpus_time_machine()</span><br><span class="line"><span class="built_in">len</span>(corpus), <span class="built_in">len</span>(vocab)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">(<span class="number">170580</span>, <span class="number">28</span>)</span><br></pre></td></tr></table></figure><h2 id="3-语言模型和数据集"><a href="#3-语言模型和数据集" class="headerlink" title="3. 语言模型和数据集"></a>3. 语言模型和数据集</h2><p>假设长度为$T$的文本序列中的词元依次为$x_1, x_2, \ldots, x_T$。则$x_t$（$1 \leq t \leq T$）可以被认为是文本序列在时间步$t$处的观测或标签。在给定这样的文本序列时，<em>语言模型</em>（language model）的目标是估计序列的联合概率：</p><script type="math/tex; mode=display">P(x_1, x_2, \ldots, x_T)</script><p>例如，一个理想的语言模型只需一次抽取一个词元$x_t \sim P(x_t \mid x_{t-1}, \ldots, x_1)$就能够基于模型本身生成自然文本。从这样的模型中提取的文本都将作为自然语言来传递。只需要基于前面的对话片断中的文本，就足以生成一个有意义的对话。显然，我们离设计出这样的系统还很遥远，因为它需要“理解”文本，而不仅仅是生成语法合理的内容。</p><p>尽管如此，语言模型依然是非常有用的。例如，短语“to recognize speech”和“to wreck a nice beach”读音上听起来非常相似。这种相似性会导致语音识别中的歧义，但是这很容易通过语言模型来解决，因为第二句的语义很奇怪。同样，在文档摘要生成算法中，“狗咬人”比“人咬狗”出现的频率要高得多，或者“我想吃奶奶”是一个相当匪夷所思的语句，而“我想吃，奶奶”则要正常得多。</p><h3 id="3-1-学习语言模型"><a href="#3-1-学习语言模型" class="headerlink" title="3.1. 学习语言模型"></a>3.1. 学习语言模型</h3><p>我们面对的问题是如何对一个文档，甚至是一个词元序列进行建模。假设在单词级别对文本数据进行词元化，可以依靠<a href="#1-序列模型">之前</a>对序列模型的分析，从基本概率规则开始：</p><script type="math/tex; mode=display">P(x_1, x_2, \ldots, x_T) = \prod_{t=1}^T P(x_t  \mid  x_1, \ldots, x_{t-1})</script><p>例如，包含了四个单词的一个文本序列的概率是：</p><script type="math/tex; mode=display">P(\text{deep}, \text{learning}, \text{is}, \text{fun}) =  P(\text{deep}) P(\text{learning}  \mid  \text{deep}) P(\text{is}  \mid  \text{deep}, \text{learning}) P(\text{fun}  \mid  \text{deep}, \text{learning}, \text{is})</script><p>为训练语言模型，需要计算单词的概率，以及给定前面几个单词后出现某个单词的条件概率。这些概率本质上就是语言模型的参数。</p><p>这里假设训练数据集是一个大型的文本语料库。比如维基百科的所有条目或者所有发布在网络上的文本。训练数据集中<em>词的概率</em>可以根据给定词的相对词频来计算。比如可以将估计值$\hat{P}(\text{deep})$计算为任何以单词“deep”开头的句子的概率。另一种（不太精确的）方法是统计单词“deep”在数据集中的出现次数，然后将其除以整个语料库中的单词总数。这种方法效果不错，特别是对于频繁出现的单词。<br>接下来可以尝试估计：</p><script type="math/tex; mode=display">\hat{P}(\text{learning} \mid \text{deep}) = \frac{n(\text{deep, learning})}{n(\text{deep})},</script><p>其中$n(x)$和$n(x, x’)$分别是单个单词和连续单词对的出现次数。由于连续单词对“deep learning”的出现频率要低得多，所以估计这类单词正确的概率要困难得多。特别是对于一些不常见的单词组合，要想找到足够的出现次数来获得准确的估计并不容易。而对于三个或者更多的单词组合，情况会变得更糟。许多合理的三个单词组合可能是存在的，但是在数据集中却找不到。除非有某种策略，来将这些单词组合指定为非零计数，否则将无法在语言模型中使用它们。如果数据集很小，或者单词非常罕见，那么这类单词出现一次的机会可能都找不到。</p><p>一种常见的策略是执行某种形式的<em>拉普拉斯平滑</em>（Laplace smoothing），具体方法是在所有计数中添加一个小常量。用$n$表示训练集中的单词总数，用$m$表示唯一单词的数量。此解决方案有助于处理单元素问题，例如通过：</p><script type="math/tex; mode=display">\begin{aligned}    \hat{P}(x) & = \frac{n(x) + \epsilon_1/m}{n + \epsilon_1}, \\    \hat{P}(x' \mid x) & = \frac{n(x, x') + \epsilon_2 \hat{P}(x')}{n(x) + \epsilon_2}, \\    \hat{P}(x'' \mid x,x') & = \frac{n(x, x',x'') + \epsilon_3 \hat{P}(x'')}{n(x, x') + \epsilon_3}.\end{aligned}</script><p>其中，$\epsilon_1,\epsilon_2$和$\epsilon_3$是超参数。以$\epsilon_1$为例：当$\epsilon_1 = 0$时，不应用平滑；当$\epsilon_1$接近正无穷大时，$\hat{P}(x)$接近均匀概率分布$1/m$（可能是常数都忽略，然后上下消掉超参数）。上面的公式是<a href="https://zh-v2.d2l.ai/chapter_references/zreferences.html#id185">文章</a>的一个相当原始的变形。</p><p>但这样的模型很容易变得无效，原因如下：<br>1、模型需要存储所有的计数；<br>2、模型完全忽略了单词的意思。例如，“猫”（cat）和“猫科动物”（feline）可能出现在相关的上下文中，但是想根据上下文调整这类模型其实是相当困难的。<br>3、长单词序列大部分是没出现过的，因此一个模型如果只是简单地统计先前“看到”的单词序列频率，那么模型面对这种问题肯定是表现不佳的。</p><h3 id="3-2-马尔可夫模型与-n-元语法"><a href="#3-2-马尔可夫模型与-n-元语法" class="headerlink" title="3.2. 马尔可夫模型与$n$元语法"></a>3.2. 马尔可夫模型与$n$元语法</h3><p>回想一下马尔可夫模型，并且将其应用于语言建模。如果$P(x_{t+1} \mid x_t, \ldots, x_1) = P(x_{t+1} \mid x_t)$，则序列上的分布满足一阶马尔可夫性质。阶数越高，对应的依赖关系就越长。这种性质推导出了许多可以应用于序列建模的近似公式：</p><script type="math/tex; mode=display">\begin{aligned}P(x_1, x_2, x_3, x_4) &=  P(x_1) P(x_2) P(x_3) P(x_4),\\P(x_1, x_2, x_3, x_4) &=  P(x_1) P(x_2  \mid  x_1) P(x_3  \mid  x_2) P(x_4  \mid  x_3),\\P(x_1, x_2, x_3, x_4) &=  P(x_1) P(x_2  \mid  x_1) P(x_3  \mid  x_1, x_2) P(x_4  \mid  x_2, x_3).\end{aligned}</script><p>这里说一下我个人的理解，第一行的含义为“各个时刻的数据独立，与之前发生的事无关”，这与通过计数统计和平滑来建模单词的想法是一致的；第二行满足一阶马尔可夫性质，含义为“t时刻发生的概率或许可仅用t前一个时刻发生的事来断定”；第三行依赖关系更长，也就是满足二阶马尔可夫性质，含义为“t时刻发生的概率或许可仅用t前两个时刻发生的事来断定”。</p><p>事实上，上面的三个式子分别对应一、二、三元语法，涉及一个、两个和三个变量的概率公式分别被称为<em>一元语法</em>（unigram）、<em>二元语法</em>（bigram）和<em>三元语法</em>（trigram）模型。下面将学习如何去设计更好的模型。</p><h3 id="3-3-自然语言统计"><a href="#3-3-自然语言统计" class="headerlink" title="3.3. 自然语言统计"></a>3.3. 自然语言统计</h3><p>学习在真实数据上如何进行自然语言统计。根据时光机器数据集构建词表，并打印前$10$个最常用的（频率最高的）单词。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">tokens = tokenize(read_time_machine())</span><br><span class="line"><span class="comment"># 因为每个文本行不一定是一个句子或一个段落，因此把所有文本行拼接到一起</span></span><br><span class="line">corpus = [token <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line">vocab = Vocab(corpus)</span><br><span class="line">vocab.token_freqs[:<span class="number">10</span>]</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">[(<span class="string">&#x27;the&#x27;</span>, <span class="number">2261</span>),</span><br><span class="line"> (<span class="string">&#x27;i&#x27;</span>, <span class="number">1267</span>),</span><br><span class="line"> (<span class="string">&#x27;and&#x27;</span>, <span class="number">1245</span>),</span><br><span class="line"> (<span class="string">&#x27;of&#x27;</span>, <span class="number">1155</span>),</span><br><span class="line"> (<span class="string">&#x27;a&#x27;</span>, <span class="number">816</span>),</span><br><span class="line"> (<span class="string">&#x27;to&#x27;</span>, <span class="number">695</span>),</span><br><span class="line"> (<span class="string">&#x27;was&#x27;</span>, <span class="number">552</span>),</span><br><span class="line"> (<span class="string">&#x27;in&#x27;</span>, <span class="number">541</span>),</span><br><span class="line"> (<span class="string">&#x27;that&#x27;</span>, <span class="number">443</span>),</span><br><span class="line"> (<span class="string">&#x27;my&#x27;</span>, <span class="number">440</span>)]</span><br></pre></td></tr></table></figure><p>最流行的词看起来很无聊，这些词被称为<strong>停用词</strong>（stop words），因此可以被过滤掉。但它们本身仍然是有意义的，我们仍然会在模型中使用它们。另一个明显的问题是词频衰减的速度非常快，第$10$个还不到第$1$个的$1/5$。为了更好地理解，可以画出的词频图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">freqs = [freq <span class="keyword">for</span> token, freq <span class="keyword">in</span> vocab.token_freqs]</span><br><span class="line"><span class="comment"># d2l. </span></span><br><span class="line">plot(freqs, xlabel=<span class="string">&#x27;token: x&#x27;</span>, ylabel=<span class="string">&#x27;frequency: n(x)&#x27;</span>,</span><br><span class="line">         xscale=<span class="string">&#x27;log&#x27;</span>, yscale=<span class="string">&#x27;log&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article57/output_language-models-and-dataset_789d14_18_0.svg" alt="output"></p><p>可以发现：词频以一种明确的方式迅速衰减。将前几个单词作为例外消除后，剩余的所有单词大致遵循双对数坐标图上的一条直线。这意味着单词的频率满足<em>齐普夫定律</em>（Zipf’s law），即第$i$个最常用单词的频率$n_i$为：</p><script type="math/tex; mode=display">n_i \propto \frac{1}{i^\alpha},</script><p>等价于</p><script type="math/tex; mode=display">\log n_i = -\alpha \log i + c,</script><p>其中$\alpha$是刻画分布的指数，$c$是常数。这说明想要通过计数统计和平滑来建模单词是不可行的，这样建模的结果会大大高估尾部单词的频率，也就是所谓的不常用单词。那么其他的词元组合，比如二元语法、三元语法等等，又会如何呢？来看看二元语法的频率是否与一元语法的频率表现出相同的行为方式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一个去尾，一个掐头，然后zip在一起，读tuple出来</span></span><br><span class="line"><span class="comment"># 这样以后，每个tuple都是t时刻和t-1时刻的组合</span></span><br><span class="line">bigram_tokens = [pair <span class="keyword">for</span> pair <span class="keyword">in</span> <span class="built_in">zip</span>(corpus[:-<span class="number">1</span>], corpus[<span class="number">1</span>:])]</span><br><span class="line">bigram_vocab = Vocab(bigram_tokens)</span><br><span class="line">bigram_vocab.token_freqs[:<span class="number">10</span>]</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">[((<span class="string">&#x27;of&#x27;</span>, <span class="string">&#x27;the&#x27;</span>), <span class="number">309</span>),</span><br><span class="line"> ((<span class="string">&#x27;in&#x27;</span>, <span class="string">&#x27;the&#x27;</span>), <span class="number">169</span>),</span><br><span class="line"> ((<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;had&#x27;</span>), <span class="number">130</span>),</span><br><span class="line"> ((<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;was&#x27;</span>), <span class="number">112</span>),</span><br><span class="line"> ((<span class="string">&#x27;and&#x27;</span>, <span class="string">&#x27;the&#x27;</span>), <span class="number">109</span>),</span><br><span class="line"> ((<span class="string">&#x27;the&#x27;</span>, <span class="string">&#x27;time&#x27;</span>), <span class="number">102</span>),</span><br><span class="line"> ((<span class="string">&#x27;it&#x27;</span>, <span class="string">&#x27;was&#x27;</span>), <span class="number">99</span>),</span><br><span class="line"> ((<span class="string">&#x27;to&#x27;</span>, <span class="string">&#x27;the&#x27;</span>), <span class="number">85</span>),</span><br><span class="line"> ((<span class="string">&#x27;as&#x27;</span>, <span class="string">&#x27;i&#x27;</span>), <span class="number">78</span>),</span><br><span class="line"> ((<span class="string">&#x27;of&#x27;</span>, <span class="string">&#x27;a&#x27;</span>), <span class="number">73</span>)]</span><br></pre></td></tr></table></figure><p>这里值得注意：在十个最频繁的词对中，有九个是由两个停用词组成的， 只有一个与“the time”有关。 下面再进一步看看三元语法的频率是否表现出相同的行为方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 去尾两个、掐头去尾、掐头两个，然后zip在一起</span></span><br><span class="line"><span class="comment"># 对应t、t-1、t-2</span></span><br><span class="line">trigram_tokens = [triple <span class="keyword">for</span> triple <span class="keyword">in</span> <span class="built_in">zip</span>(</span><br><span class="line">    corpus[:-<span class="number">2</span>], corpus[<span class="number">1</span>:-<span class="number">1</span>], corpus[<span class="number">2</span>:])]</span><br><span class="line">trigram_vocab = Vocab(trigram_tokens)</span><br><span class="line">trigram_vocab.token_freqs[:<span class="number">10</span>]</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">[((<span class="string">&#x27;the&#x27;</span>, <span class="string">&#x27;time&#x27;</span>, <span class="string">&#x27;traveller&#x27;</span>), <span class="number">59</span>),</span><br><span class="line"> ((<span class="string">&#x27;the&#x27;</span>, <span class="string">&#x27;time&#x27;</span>, <span class="string">&#x27;machine&#x27;</span>), <span class="number">30</span>),</span><br><span class="line"> ((<span class="string">&#x27;the&#x27;</span>, <span class="string">&#x27;medical&#x27;</span>, <span class="string">&#x27;man&#x27;</span>), <span class="number">24</span>),</span><br><span class="line"> ((<span class="string">&#x27;it&#x27;</span>, <span class="string">&#x27;seemed&#x27;</span>, <span class="string">&#x27;to&#x27;</span>), <span class="number">16</span>),</span><br><span class="line"> ((<span class="string">&#x27;it&#x27;</span>, <span class="string">&#x27;was&#x27;</span>, <span class="string">&#x27;a&#x27;</span>), <span class="number">15</span>),</span><br><span class="line"> ((<span class="string">&#x27;here&#x27;</span>, <span class="string">&#x27;and&#x27;</span>, <span class="string">&#x27;there&#x27;</span>), <span class="number">15</span>),</span><br><span class="line"> ((<span class="string">&#x27;seemed&#x27;</span>, <span class="string">&#x27;to&#x27;</span>, <span class="string">&#x27;me&#x27;</span>), <span class="number">14</span>),</span><br><span class="line"> ((<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;did&#x27;</span>, <span class="string">&#x27;not&#x27;</span>), <span class="number">14</span>),</span><br><span class="line"> ((<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;saw&#x27;</span>, <span class="string">&#x27;the&#x27;</span>), <span class="number">13</span>),</span><br><span class="line"> ((<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;began&#x27;</span>, <span class="string">&#x27;to&#x27;</span>), <span class="number">13</span>)]</span><br></pre></td></tr></table></figure><p>直观地对比三种模型中的词元频率：一元语法、二元语法和三元语法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bigram_freqs = [freq <span class="keyword">for</span> token, freq <span class="keyword">in</span> bigram_vocab.token_freqs]</span><br><span class="line">trigram_freqs = [freq <span class="keyword">for</span> token, freq <span class="keyword">in</span> trigram_vocab.token_freqs]</span><br><span class="line"><span class="comment"># d2l. </span></span><br><span class="line">plot([freqs, bigram_freqs, trigram_freqs], xlabel=<span class="string">&#x27;token: x&#x27;</span>,</span><br><span class="line">         ylabel=<span class="string">&#x27;frequency: n(x)&#x27;</span>, xscale=<span class="string">&#x27;log&#x27;</span>, yscale=<span class="string">&#x27;log&#x27;</span>,</span><br><span class="line">         legend=[<span class="string">&#x27;unigram&#x27;</span>, <span class="string">&#x27;bigram&#x27;</span>, <span class="string">&#x27;trigram&#x27;</span>])</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article57/u-b-t-output.svg" alt="output"></p><p>这张图非常令人振奋！可以得到以下结论：<br>1、除了一元语法词，单词序列似乎也遵循齐普夫定律，尽管其中的指数$\alpha$更小（指数的大小受序列长度的影响）；<br>2、词表中$n$元组的数量并没有那么大，这说明语言中存在相当多的结构，这些结构给了我们应用模型的希望；<br>3、很多$n$元组很少出现，这使得拉普拉斯平滑非常不适合语言建模。作为代替，我们将使用<em>基于深度学习的模型</em>。</p><h3 id="3-4-读取长序列数据"><a href="#3-4-读取长序列数据" class="headerlink" title="3.4. 读取长序列数据"></a>3.4. 读取长序列数据</h3><p>由于序列数据本质上是连续的，在处理数据时需要解决这个问题。在<a href="#1-序列模型">文章的开头</a>我们以一种相当特别的方式做到了这一点：当序列变得太长而不能被模型一次性全部处理时，可能去拆分这样的序列方便模型读取。</p><p>介绍模型前说一下总体策略：假设使用神经网络来训练语言模型，模型中的网络一次处理具有预定义长度（例如$n$个时间步）的一个小批量序列。现在的问题是<em>如何随机生成一个小批量数据的特征和标签以供读取</em>。</p><p>首先，由于文本序列可以是任意长的（如整本《时光机器》），则任意长的序列可以被划分为具有相同时间步数的子序列。当训练神经网络时，这样的小批量子序列将被输入到模型中。假设网络一次只处理具有$n$个时间步的子序列。 下图画出了从原始文本序列获得子序列的所有不同的方式，其中$n=5$，并且每个时间步的词元对应一个字符。我们可以选择任意偏移量来指示初始位置，所以有相当大的自由度。</p><p><img src="/assets/post_img/article57/timemachine-5gram.svg" alt="分割文本时，不同的偏移量会导致不同的子序列"></p><p>那应该从图中选择哪一个呢？事实上他们都一样好。但如果我们只选择一个偏移量，那么用于训练网络的、所有可能的子序列的覆盖范围将是有限的。因此可以从随机偏移量开始划分序列，以同时获得<em>覆盖性</em>（coverage）和<em>随机性</em>（randomness）。下面将描述如何实现<em>随机采样</em>（random sampling）和<em>顺序分区</em>（sequential partitioning）策略。</p><h4 id="3-4-1-随机采样"><a href="#3-4-1-随机采样" class="headerlink" title="3.4.1. 随机采样"></a>3.4.1. 随机采样</h4><p>在随机采样中，每个样本都是在原始的长序列上任意捕获的子序列。 在迭代过程中，来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻。 对于语言建模，目标是基于到目前为止我们看到的词元来预测下一个词元， 因此标签是移位了一个词元的原始序列（右移，或者说后移）。</p><p>下面的代码每次可以从数据中随机生成一个小批量，参数batch_size指定了每个小批量中子序列样本的数目， 参数num_steps是每个子序列中预定义的时间步数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">seq_data_iter_random</span>(<span class="params">corpus, batch_size, num_steps</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;从数据中随机生成一个小批量&quot;&quot;&quot;</span> </span><br><span class="line">    <span class="comment"># 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1</span></span><br><span class="line">    corpus = corpus[random.randint(<span class="number">0</span>, num_steps - <span class="number">1</span>):]</span><br><span class="line">    <span class="comment"># 减去1，是因为我们需要考虑标签</span></span><br><span class="line">    <span class="comment"># 词数除去时间步长得到子序列的总数量</span></span><br><span class="line">    num_subseqs = (<span class="built_in">len</span>(corpus) - <span class="number">1</span>) // num_steps</span><br><span class="line">    <span class="comment"># 长度为num_steps的各个子序列的起始索引</span></span><br><span class="line">    initial_indices = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">0</span>, num_subseqs * num_steps, num_steps))</span><br><span class="line">    <span class="comment"># 在随机抽样的迭代过程中，</span></span><br><span class="line">    <span class="comment"># 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻</span></span><br><span class="line">    <span class="comment"># 也就是打乱起始索引列表</span></span><br><span class="line">    random.shuffle(initial_indices)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">data</span>(<span class="params">pos</span>):</span></span><br><span class="line">        <span class="comment"># 返回从pos位置开始的长度为num_steps的序列</span></span><br><span class="line">        <span class="comment"># 注意corpus是存储词对应索引的列表，并非字符串</span></span><br><span class="line">        <span class="keyword">return</span> corpus[pos: pos + num_steps]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对子序列分批，子列数量除去批量大小得到批次数量</span></span><br><span class="line">    num_batches = num_subseqs // batch_size</span><br><span class="line">    <span class="comment"># 这里的range方式与之前相同，就是获取批次的起始索引</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, batch_size * num_batches, batch_size):</span><br><span class="line">        <span class="comment"># initial_indices包含子序列的随机起始索引</span></span><br><span class="line">        <span class="comment"># 获取当前批次对应的子序列起始索引</span></span><br><span class="line">        initial_indices_per_batch = initial_indices[i: i + batch_size]</span><br><span class="line">        <span class="comment"># 训练数据</span></span><br><span class="line">        X = [data(j) <span class="keyword">for</span> j <span class="keyword">in</span> initial_indices_per_batch]</span><br><span class="line">        <span class="comment"># 训练数据后移一个词元得到标签</span></span><br><span class="line">        Y = [data(j + <span class="number">1</span>) <span class="keyword">for</span> j <span class="keyword">in</span> initial_indices_per_batch]</span><br><span class="line">        <span class="comment"># 生成器</span></span><br><span class="line">        <span class="keyword">yield</span> torch.tensor(X), torch.tensor(Y)</span><br></pre></td></tr></table></figure><p>下面生成一个从$0$到$34$的序列。假设批量大小为$2$，时间步数为$5$，则可以生成$\lfloor (35 - 1) / 5 \rfloor= 6$个“特征－标签”子序列对。如果设置小批量大小为$2$，就只能得到$3$个小批量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">my_seq = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">35</span>))</span><br><span class="line"><span class="keyword">for</span> X, Y <span class="keyword">in</span> seq_data_iter_random(my_seq, batch_size=<span class="number">2</span>, num_steps=<span class="number">5</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;X: &#x27;</span>, X, <span class="string">&#x27;\nY:&#x27;</span>, Y)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">X:  tensor([[<span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>],</span><br><span class="line">        [ <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>]])</span><br><span class="line">Y: tensor([[<span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>],</span><br><span class="line">        [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>]])</span><br><span class="line">X:  tensor([[<span class="number">19</span>, <span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>],</span><br><span class="line">        [<span class="number">29</span>, <span class="number">30</span>, <span class="number">31</span>, <span class="number">32</span>, <span class="number">33</span>]])</span><br><span class="line">Y: tensor([[<span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>, <span class="number">24</span>],</span><br><span class="line">        [<span class="number">30</span>, <span class="number">31</span>, <span class="number">32</span>, <span class="number">33</span>, <span class="number">34</span>]])</span><br><span class="line">X:  tensor([[ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>],</span><br><span class="line">        [<span class="number">24</span>, <span class="number">25</span>, <span class="number">26</span>, <span class="number">27</span>, <span class="number">28</span>]])</span><br><span class="line">Y: tensor([[ <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>],</span><br><span class="line">        [<span class="number">25</span>, <span class="number">26</span>, <span class="number">27</span>, <span class="number">28</span>, <span class="number">29</span>]])</span><br></pre></td></tr></table></figure><h4 id="3-4-2-顺序分区"><a href="#3-4-2-顺序分区" class="headerlink" title="3.4.2. 顺序分区"></a>3.4.2. 顺序分区</h4><p>在迭代过程中，除了对原始序列可以随机抽样外，还可以保证两个相邻的小批量中的子序列在原始序列上也是相邻的。 这种策略在基于小批量的迭代过程中保留了拆分的子序列的顺序，因此称为顺序分区。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">seq_data_iter_sequential</span>(<span class="params">corpus, batch_size, num_steps</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用顺序分区生成一个小批量子序列&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 从随机偏移量开始划分序列</span></span><br><span class="line">    offset = random.randint(<span class="number">0</span>, num_steps)</span><br><span class="line">    <span class="comment"># 可用词元的数量</span></span><br><span class="line">    num_tokens = ((<span class="built_in">len</span>(corpus) - offset - <span class="number">1</span>) // batch_size) * batch_size</span><br><span class="line">    <span class="comment"># 所有特征</span></span><br><span class="line">    Xs = torch.tensor(corpus[offset: offset + num_tokens])</span><br><span class="line">    <span class="comment"># 所有标签</span></span><br><span class="line">    Ys = torch.tensor(corpus[offset + <span class="number">1</span>: offset + <span class="number">1</span> + num_tokens])</span><br><span class="line">    <span class="comment"># 分批，第二维中对应各批次的数据（其实是索引）</span></span><br><span class="line">    Xs, Ys = Xs.reshape(batch_size, -<span class="number">1</span>), Ys.reshape(batch_size, -<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 这里其实写的有些奇怪，批次数量num_batches和批次大小事实上都是</span></span><br><span class="line">    <span class="comment"># 相对于子列数量而言的，而上一步直接在原始数据和批次大小间建立联系</span></span><br><span class="line">    <span class="comment"># 不过无所谓，最终只是要获取批次数量的值而已，该值的公式：</span></span><br><span class="line">    <span class="comment"># 批次数量 = 子列数量//批次大小</span></span><br><span class="line">    <span class="comment"># 子列数量 = 原始数据//时间步长</span></span><br><span class="line">    num_batches = Xs.shape[<span class="number">1</span>] // num_steps</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_steps * num_batches, num_steps):</span><br><span class="line">        X = Xs[:, i: i + num_steps]</span><br><span class="line">        Y = Ys[:, i: i + num_steps]</span><br><span class="line">        <span class="keyword">yield</span> X, Y</span><br></pre></td></tr></table></figure><p>基于相同的设置，通过顺序分区读取每个小批量的子序列的特征X和标签Y。 可以看到迭代期间来自两个相邻的小批量中的子序列在原始序列中确实是相邻的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> X, Y <span class="keyword">in</span> seq_data_iter_sequential(my_seq, batch_size=<span class="number">2</span>, num_steps=<span class="number">5</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;X: &#x27;</span>, X, <span class="string">&#x27;\nY:&#x27;</span>, Y)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">X:  tensor([[ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>],</span><br><span class="line">        [<span class="number">19</span>, <span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>]])</span><br><span class="line">Y: tensor([[ <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>],</span><br><span class="line">        [<span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>, <span class="number">24</span>]])</span><br><span class="line">X:  tensor([[ <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>],</span><br><span class="line">        [<span class="number">24</span>, <span class="number">25</span>, <span class="number">26</span>, <span class="number">27</span>, <span class="number">28</span>]])</span><br><span class="line">Y: tensor([[<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>],</span><br><span class="line">        [<span class="number">25</span>, <span class="number">26</span>, <span class="number">27</span>, <span class="number">28</span>, <span class="number">29</span>]])</span><br><span class="line">X:  tensor([[<span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>],</span><br><span class="line">        [<span class="number">29</span>, <span class="number">30</span>, <span class="number">31</span>, <span class="number">32</span>, <span class="number">33</span>]])</span><br><span class="line">Y: tensor([[<span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>],</span><br><span class="line">        [<span class="number">30</span>, <span class="number">31</span>, <span class="number">32</span>, <span class="number">33</span>, <span class="number">34</span>]])</span><br></pre></td></tr></table></figure><p>将上面的两个采样函数包装到一个类中， 以便稍后可以将其用作数据迭代器：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SeqDataLoader</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;加载序列数据的迭代器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, batch_size, num_steps, use_random_iter, max_tokens</span>):</span></span><br><span class="line">        <span class="keyword">if</span> use_random_iter:</span><br><span class="line">            self.data_iter_fn = seq_data_iter_random</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.data_iter_fn = seq_data_iter_sequential</span><br><span class="line">        self.corpus, self.vocab = load_corpus_time_machine(max_tokens)</span><br><span class="line">        self.batch_size, self.num_steps = batch_size, num_steps</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义 __iter__ 表示这个类是一个迭代器，只在迭代开始的时候运行一次</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)</span><br></pre></td></tr></table></figure><p>最后定义一个函数load_data_time_machine， 它同时返回数据迭代器和词表， 因此可以与其他带有load_data前缀的函数 （如d2l.load_data_fashion_mnist）类似地使用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_time_machine</span>(<span class="params">batch_size, num_steps, </span></span></span><br><span class="line"><span class="params"><span class="function">                           use_random_iter=<span class="literal">False</span>, max_tokens=<span class="number">10000</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回时光机器数据集的迭代器和词表&quot;&quot;&quot;</span></span><br><span class="line">    data_iter = SeqDataLoader(</span><br><span class="line">        batch_size, num_steps, use_random_iter, max_tokens)</span><br><span class="line">    <span class="keyword">return</span> data_iter, data_iter.vocab</span><br></pre></td></tr></table></figure><h2 id="4-循环神经网络"><a href="#4-循环神经网络" class="headerlink" title="4. 循环神经网络"></a>4. 循环神经网络</h2><p>上节中介绍了$n$元语法模型，其中单词$x_t$在时间步$t$的条件概率仅取决于前面$n-1$个单词。对于时间步$t-(n-1)$之前的单词，如果想将其可能产生的影响合并到$x_t$上，需要增加$n$的值，此时模型参数的数量也会随之呈指数增长，因为词表$\mathcal{V}$需要存储$|\mathcal{V}|^n$个数字，因此与其将$P(x_t \mid x_{t-1}, \ldots, x_{t-n+1})$模型化，不如使用隐变量模型：</p><script type="math/tex; mode=display">P(x_t \mid x_{t-1}, \ldots, x_1) \approx P(x_t \mid h_{t-1}),</script><p>其中$h_{t-1}$是<em>隐状态</em>（hidden state），也称为<em>隐藏变量</em>（hidden variable），它存储了到时间步$t-1$的序列信息。通常我们可以基于当前输入$x_{t}$和先前隐状态$h_{t-1}$来计算时间步$t$处的任何时间的隐状态：</p><script type="math/tex; mode=display">h_t = f(x_{t}, h_{t-1})</script><p>对于函数$f$，隐变量模型不是近似值（这句没看懂）。$h_t$可以存储到目前为止观察到的所有数据，但这样的操作可能会使计算和存储的代价提高。</p><p>值得注意的是，具有隐藏单元的隐藏层和隐状态是两个截然不同的概念。隐藏层是在从输入到输出的路径上（主要以观测角度来理解，实际上也是层）的隐藏的层，而隐状态则是在给定步骤所做的任何事情（是以技术角度来定义，存在精心设计）的<em>输入</em>，并且这些状态只能通过先前时间步的数据来计算。</p><p><em>循环神经网络</em>（recurrent neural networks，RNNs）是具有隐状态的神经网络。在介绍循环神经网络模型之前，先回顾一下多层感知机模型。</p><h3 id="4-1-无隐状态的神经网络"><a href="#4-1-无隐状态的神经网络" class="headerlink" title="4.1. 无隐状态的神经网络"></a>4.1. 无隐状态的神经网络</h3><p>对单隐藏层的多层感知机。设隐藏层的激活函数为$\phi$，给定一个小批量样本$\mathbf{X} \in \mathbb{R}^{n \times d}$，其中批量大小为$n$，输入维度为$d$，则隐藏层的输出$\mathbf{H} \in \mathbb{R}^{n \times h}$通过下式计算：</p><script type="math/tex; mode=display">\mathbf{H} = \phi(\mathbf{X} \mathbf{W}_{xh} + \mathbf{b}_h)</script><p>上式中，隐藏层权重参数为$\mathbf{W}_{xh} \in \mathbb{R}^{d \times h}$，偏置参数为$\mathbf{b}_h \in \mathbb{R}^{1 \times h}$，以及隐藏单元的数目为$h$。因此求和时可以应用广播机制。接下来，将隐藏变量$\mathbf{H}$用作输出层的输入。输出层由下式给出：</p><script type="math/tex; mode=display">\mathbf{O} = \mathbf{H} \mathbf{W}_{hq} + \mathbf{b}_q,</script><p>其中，$\mathbf{O} \in \mathbb{R}^{n \times q}$是输出变量，$\mathbf{W}_{hq} \in \mathbb{R}^{h \times q}$是权重参数，$\mathbf{b}_q \in \mathbb{R}^{1 \times q}$是输出层的偏置参数。如果是分类问题，可以用$\text{softmax}(\mathbf{O})$来计算输出类别的概率分布。</p><p>对于这种网络，只要可以随机选择“特征-标签”对，并且通过自动微分和随机梯度下降能够学习网络参数就可以了。</p><h3 id="4-2-有隐状态的循环神经网络"><a href="#4-2-有隐状态的循环神经网络" class="headerlink" title="4.2. 有隐状态的循环神经网络"></a>4.2. 有隐状态的循环神经网络</h3><p>有了隐状态后，情况就完全不同了。假设在时间步$t$有小批量输入$\mathbf{X}_t \in \mathbb{R}^{n \times d}$。也就是对$n$个序列样本的小批量，$\mathbf{X}_t$的每一行对应于来自该序列的时间步$t$处的一个样本（$n \times d$)。接下来，用$\mathbf{H}_t  \in \mathbb{R}^{n \times h}$表示时间步$t$的隐藏变量。与多层感知机不同的是，这里保存了前一个时间步的隐藏变量$\mathbf{H}_{t-1}$，并引入了一个新的权重参数$\mathbf{W}_{hh} \in \mathbb{R}^{h \times h}$，来描述如何在当前时间步中使用前一个时间步的隐藏变量。当前时间步隐藏变量由 当前时间步的输入 与 前一个时间步的隐藏变量 一起计算得出：</p><script type="math/tex; mode=display">\mathbf{H}_t = \phi(\mathbf{X}_t \mathbf{W}_{xh} + \mathbf{H}_{t-1} \mathbf{W}_{hh}  + \mathbf{b}_h)</script><p>与无隐状态的情况相比，上式多添加了一项$\mathbf{H}_{t-1} \mathbf{W}_{hh}$，从而实例化了开头提到的公式：$h_t = f(x_{t}, h_{t-1})$。从相邻时间步的隐藏变量$\mathbf{H}_t$和$\mathbf{H}_{t-1}$之间的关系可知，这些变量捕获并保留了（通过作为参数参与下一次计算的方式）序列直到其当前时间步的历史信息，如同当前时间步下神经网络的状态或记忆，因此这些隐藏变量被称为<em>隐状态</em>（hidden state）。由于在当前时间步中，隐状态使用的定义与前一个时间步中使用的定义相同，所以上式中的计算是<em>循环的</em>（recurrent）。基于循环计算的隐状态神经网络被命名为<em>循环神经网络</em>（recurrent neural network）。在循环神经网络中执行上式隐状态计算的层称为<em>循环层</em>（recurrent layer）。</p><p>有许多不同的方法可以构建循环神经网络，上式定义的隐状态的循环神经网络是其中常见的一种。对于时间步$t$，输出层的输出类似于多层感知机中的计算：</p><script type="math/tex; mode=display">\mathbf{O}_t = \mathbf{H}_t \mathbf{W}_{hq} + \mathbf{b}_q</script><p>循环神经网络的参数包括隐藏层的权重$\mathbf{W}_{xh} \in \mathbb{R}^{d \times h}, \mathbf{W}_{hh} \in \mathbb{R}^{h \times h}$和偏置$\mathbf{b}_h \in \mathbb{R}^{1 \times h}$，以及输出层的权重$\mathbf{W}_{hq} \in \mathbb{R}^{h \times q}$和偏置$\mathbf{b}_q \in \mathbb{R}^{1 \times q}$。在不同的时间步上，循环神经网络也使用这些相同模型参数。因此，<strong>循环神经网络的参数开销不会随着时间步的增加而增加</strong>。</p><p>下图展示了循环神经网络在三个相邻时间步的计算逻辑。在任意时间步$t$，隐状态的计算可以被视为：</p><ol><li>拼接当前时间步$t$的输入$\mathbf{X}_t$和前一时间步$t-1$的隐状态$\mathbf{H}_{t-1}$；</li><li>将拼接的结果送入带有激活函数$\phi$的全连接层。全连接层的输出是当前时间步$t$的隐状态$\mathbf{H}_t$。</li></ol><p>图中，模型参数是$\mathbf{W}_{xh}$和$\mathbf{W}_{hh}$的拼接（concat），以及$\mathbf{b}_h$的偏置。当前时间步$t$的隐状态$\mathbf{H}_t$将参与计算下一时间步$t+1$的隐状态$\mathbf{H}_{t+1}$。而且$\mathbf{H}_t$还将送入全连接输出层，用于计算当前时间步$t$的输出$\mathbf{O}_t$。</p><p><img src="/assets/post_img/article57/rnn.svg" alt="具有隐状态的循环神经网络"></p><p>刚才提到，隐状态中$\mathbf{X}_t \mathbf{W}_{xh} + \mathbf{H}_{t-1} \mathbf{W}_{hh}$的计算，相当于$\mathbf{X}_t$和$\mathbf{H}_{t-1}$的拼接与$\mathbf{W}_{xh}$和$\mathbf{W}_{hh}$的拼接的矩阵乘法。这个性质可以通过数学证明，下面使用一个简单的代码来说明一下。</p><p>首先定义矩阵<code>X</code>、<code>W_xh</code>、<code>H</code>和<code>W_hh</code>，它们的形状分别为$(3，1)$、$(1，4)$、$(3，4)$和$(4，4)$。分别将<code>X</code>乘以<code>W_xh</code>，将<code>H</code>乘以<code>W_hh</code>，然后将这两个乘法相加，得到一个形状为$(3，4)$的矩阵。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">X, W_xh = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">3</span>, <span class="number">1</span>)), torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">1</span>, <span class="number">4</span>))</span><br><span class="line">H, W_hh = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">3</span>, <span class="number">4</span>)), torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">torch.matmul(X, W_xh) + torch.matmul(H, W_hh)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[-<span class="number">1.4258</span>,  <span class="number">1.8849</span>, -<span class="number">1.2227</span>, -<span class="number">3.2763</span>],</span><br><span class="line">        [ <span class="number">0.5912</span>, -<span class="number">0.8081</span>,  <span class="number">0.6962</span>, -<span class="number">0.0819</span>],</span><br><span class="line">        [-<span class="number">3.9654</span>,  <span class="number">1.2145</span>, -<span class="number">4.2720</span>, -<span class="number">0.6869</span>]])</span><br></pre></td></tr></table></figure><p>现在沿列（轴1）拼接矩阵<code>X</code>和<code>H</code>，沿行（轴0）拼接矩阵<code>W_xh</code>和<code>W_hh</code>。这两个拼接分别产生形状$(3, 5)$和形状$(5, 4)$的矩阵。再将这两个拼接的矩阵相乘，可得到与上面相同形状$(3, 4)$的输出矩阵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">torch.matmul(torch.cat((X, H), <span class="number">1</span>), torch.cat((W_xh, W_hh), <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[-<span class="number">1.4258</span>,  <span class="number">1.8849</span>, -<span class="number">1.2227</span>, -<span class="number">3.2763</span>],</span><br><span class="line">        [ <span class="number">0.5912</span>, -<span class="number">0.8081</span>,  <span class="number">0.6962</span>, -<span class="number">0.0819</span>],</span><br><span class="line">        [-<span class="number">3.9654</span>,  <span class="number">1.2145</span>, -<span class="number">4.2720</span>, -<span class="number">0.6869</span>]])</span><br></pre></td></tr></table></figure><h3 id="4-3-基于循环神经网络的字符级语言模型"><a href="#4-3-基于循环神经网络的字符级语言模型" class="headerlink" title="4.3. 基于循环神经网络的字符级语言模型"></a>4.3. 基于循环神经网络的字符级语言模型</h3><p>回想一下<a href="#3-语言模型和数据集">3</a>中的语言模型，我们的目标是根据过去的和当前的词元预测下一个词元，因此将原始序列移位一个词元作为标签。Bengio等人首先提出使用神经网络进行语言建模。下面看一下如何使用循环神经网络来构建语言模型。设小批量大小为1，批量中的文本序列为“machine”。为简化后续的训练，此处使用<em>字符级语言模型</em>（character-level language model），将文本词元化为字符而不是单词。 下图演示了如何通过基于字符级语言建模的循环神经网络，使用当前的和先前的字符预测下一个字符。</p><p><img src="/assets/post_img/article57/rnn-train.svg" alt="基于循环神经网络的字符级语言模型：输入序列和标签序列分别为“machin”和“achine”"></p><p>在训练过程中，我们对每个时间步的输出层的输出进行softmax操作，然后利用交叉熵损失计算模型输出和标签之间的误差。由于隐藏层中隐状态的循环计算，上图中的第$3$个时间步的输出$\mathbf{O}_3$由文本序列“m”“a”和“c”确定。由于训练数据中这个文本序列的下一个字符是“h”，因此第$3$个时间步的损失将取决于下一个字符的概率分布，而下一个字符是基于特征序列“m”“a”“c”和这个时间步的标签“h”生成的。<br>（Personal Statement：这里的意思应该是损失是由与标签对比后计算得到的，而损失会影响模型参数的更新，进一步影响模型的预测结果，而不是说标签会直接影响，因为模型的根本目的是预测下一字符是什么。正确的说法应该是“下一个字符是基于特征序列‘m’‘a’‘c’和损失值生成的”）</p><p>在实践中使用的批量大小$n&gt;1$，每个词元都由一个$d$维向量表示。因此，在时间步$t$输入$\mathbf X_t$将是一个$n\times d$矩阵，这与在<a href="#42-有隐状态的循环神经网络">4.2</a>中的讨论相同，也与<a href="#12-训练">1.2</a>中近似。</p><h3 id="4-4-困惑度（Perplexity）"><a href="#4-4-困惑度（Perplexity）" class="headerlink" title="4.4. 困惑度（Perplexity）"></a>4.4. 困惑度（Perplexity）</h3><p>现在来讨论如何度量语言模型的质量，这将用于评估基于循环神经网络的模型。一个好的语言模型能够用高度准确的词元来预测我们接下来会看到什么。请看下列由不同的语言模型给出的对“It is raining …”的续写：</p><ol><li>“It is raining outside”（外面下雨了）；</li><li>“It is raining banana tree”（香蕉树下雨了）；</li><li>“It is raining piouw;kcj pwepoiut”。</li></ol><p>就质量而言，例$1$显然是最合乎情理、在逻辑上最连贯的。虽然这个模型可能没有很准确地反映出后续词的语义，比如，“It is raining in San Francisco”（旧金山下雨了）和“It is raining in winter”（冬天下雨了）可能才是更完美的合理扩展，但该模型已经能够捕捉到跟在后面的是哪类单词。例$2$则要糟糕得多，因为其产生了一个无意义的续写。尽管如此，至少该模型已经学会了如何拼写单词，以及单词之间的某种程度的相关性。最后，例$3$表明了训练不足的模型是无法正确地拟合数据的。</p><p>或许可以通过计算序列的似然概率来度量模型的质量，但这是一个难以理解、难以比较的数字。较短的序列比较长的序列更有可能出现，因此评估模型产生长篇巨著的可能性会比产生中篇小说的要小得多。</p><p>信息论这时可以派上用场了。前文在引入softmax回归时定义了熵、交叉熵，并在<a href="https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/information-theory.html">信息论在线附录</a>中讨论了更多的信息论知识（还没看）。如果想要压缩文本，我们可以根据当前词元集预测的下一个词元。一个更好的语言模型应该能更准确地预测下一个词元。因此，它应该允许我们在压缩序列时花费更少的比特。所以可以通过一个序列中所有的$n$个词元的交叉熵损失的平均值来衡量：</p><script type="math/tex; mode=display">\frac{1}{n} \sum_{t=1}^n -\log P(x_t \mid x_{t-1}, \ldots, x_1)</script><p>其中$P$由语言模型给出，$x_t$是在时间步$t$从该序列中观察到的实际词元。这使得不同长度的文档的性能具有了可比性。由于历史原因，自然语言处理的科学家更喜欢使用<em>困惑度</em>（perplexity）来表达，它是上式的指数：</p><script type="math/tex; mode=display">\exp\left(-\frac{1}{n} \sum_{t=1}^n \log P(x_t \mid x_{t-1}, \ldots, x_1)\right)</script><p>困惑度（perplexity）的基本思想是：给测试集的句子赋予较高概率值的语言模型较好,当语言模型训练完之后，测试集中的句子都是正常的句子，那么训练好的模型就是在测试集上的概率越高越好，此概率越高，困惑度越小，越“收敛”。</p><p>困惑度是“下一个词元的实际选择数的调和平均数”，请看下方案例。</p><ul><li>在最好的情况下，模型总是完美地估计标签词元的概率为1。在这种情况下，模型的困惑度为1。</li><li>在最坏的情况下，模型总是预测标签词元的概率为0。在这种情况下，困惑度是正无穷大。</li><li>在基准线情况下，模型的预测是词表的所有可用词元上的均匀分布。这种情况下，困惑度等于词表中唯一词元的数量。如果在没有任何压缩的情况下存储序列，这将是我们能做的最好的编码方式。因此这种方式提供了一个重要的下限，任何实际模型都必须超越这个下限。</li></ul><p>调和平均数<br>: 调和平均数（harmonic mean）又称倒数平均数，是总体各统计变量倒数的算术平均数的倒数。简单调和平均数的公式为：<script type="math/tex">H_{n}=\frac{1}{\frac{1}{n} \sum_{i=1}^{n} \frac{1}{x_{i}}}=\frac{n}{\sum_{i=1}^{n} \frac{1}{x_{i}}}</script></p><p>关于基准线情况的补充，有种说法是这样的:</p><blockquote><p>在看到一个语言模型报告其perplexity是109时，我们可以直观的理解为，平均情况下，这个语言模型预测下一个词时，其认为有109个词等可能地可以作为下一个词的合理选择。</p></blockquote><h2 id="5-循环神经网络的从零实现"><a href="#5-循环神经网络的从零实现" class="headerlink" title="5. 循环神经网络的从零实现"></a>5. 循环神经网络的从零实现</h2><p>从头开始基于循环神经网络实现字符级语言模型，将在H.G.Wells的时光机器数据集上训练。详细代码实现参见对应实践。</p><h3 id="5-1-独热编码"><a href="#5-1-独热编码" class="headerlink" title="5.1. 独热编码"></a>5.1. 独热编码</h3><p>在train_iter中，每个词元都表示为一个数字索引， 将这些索引直接输入神经网络可能会使学习变得困难。 通常将每个词元表示为更具表现力的特征向量。 最简单的表示为独热编码（one-hot encoding）。</p><p>简言之，将每个索引映射为相互不同的单位向量：假设词表中不同词元的数目为$N$（即<code>len(vocab)</code>），词元索引的范围为$0$到$N-1$。如果词元的索引是整数$i$，那么我们将创建一个长度为$N$的全$0$向量，并将第$i$处的元素设置为$1$。此向量是原始词元的一个独热向量。</p><p>每次采样的小批量数据形状是二维张量： （批量大小，时间步数）。 one_hot函数将这样一个小批量数据转换成三维张量， 张量的最后一个维度等于词表大小（len(vocab)）。 通常会转换输入的维度以获得形状为（时间步数，批量大小，词表大小）的输出。 这将使我们能够更方便地通过最外层的维度， 一步一步地更新小批量数据的隐状态。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">X = torch.arange(<span class="number">10</span>).reshape((<span class="number">2</span>, <span class="number">5</span>))</span><br><span class="line">F.one_hot(X.T, <span class="number">28</span>).shape</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">torch.Size([<span class="number">5</span>, <span class="number">2</span>, <span class="number">28</span>])</span><br></pre></td></tr></table></figure><h3 id="5-2-初始化模型参数"><a href="#5-2-初始化模型参数" class="headerlink" title="5.2. 初始化模型参数"></a>5.2. 初始化模型参数</h3><p>初始化循环神经网络模型的模型参数。 隐藏单元数num_hiddens是一个可调的超参数。 当训练语言模型时，输入和输出来自相同的词表。 因此，它们具有相同的维度，即词表的大小。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_params</span>(<span class="params">vocab_size, num_hiddens, device</span>):</span></span><br><span class="line">    num_inputs = num_outputs = vocab_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">normal</span>(<span class="params">shape</span>):</span></span><br><span class="line">        <span class="keyword">return</span> torch.randn(size=shape, device=device) * <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 隐藏层参数</span></span><br><span class="line">    W_xh = normal((num_inputs, num_hiddens))</span><br><span class="line">    W_hh = normal((num_hiddens, num_hiddens))</span><br><span class="line">    b_h = torch.zeros(num_hiddens, device=device)</span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = normal((num_hiddens, num_outputs))</span><br><span class="line">    b_q = torch.zeros(num_outputs, device=device)</span><br><span class="line">    <span class="comment"># 附加梯度</span></span><br><span class="line">    params = [W_xh, W_hh, b_h, W_hq, b_q]</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> params</span><br></pre></td></tr></table></figure><h3 id="5-3-循环神经网络模型"><a href="#5-3-循环神经网络模型" class="headerlink" title="5.3. 循环神经网络模型"></a>5.3. 循环神经网络模型</h3><p>定义循环神经网络模型， 首先需要一个init_rnn_state函数在初始化时返回隐状态。 这个函数的返回是一个张量，张量全用0填充，形状为（批量大小，隐藏单元数）。 后面的章节中会遇到隐状态包含多个变量的情况，使用元组可以更容易地处理些。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_rnn_state</span>(<span class="params">batch_size, num_hiddens, device</span>):</span></span><br><span class="line">    <span class="keyword">return</span> (torch.zeros((batch_size, num_hiddens), device=device), )</span><br></pre></td></tr></table></figure><p>下面的<code>rnn</code>函数定义了如何在一个时间步内计算隐状态和输出。循环神经网络模型通过<code>inputs</code>最外层的维度实现循环，以便逐时间步更新小批量数据的隐状态<code>H</code>。此外，这里使用$\tanh$函数作为激活函数。当元素在实数上满足均匀分布时，$\tanh$函数的平均值为0。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn</span>(<span class="params">inputs, state, params</span>):</span></span><br><span class="line">    <span class="comment"># inputs的形状：(时间步数量，批量大小，词表大小)</span></span><br><span class="line">    W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class="line">    H, = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="comment"># X的形状：(批量大小，词表大小)</span></span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)</span><br><span class="line">        Y = torch.mm(H, W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">0</span>), (H,)</span><br></pre></td></tr></table></figure><p>创建一个类来包装这些函数， 并存储从零开始实现的循环神经网络模型的参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNNModelScratch</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;从零开始实现的循环神经网络模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, num_hiddens, device,</span></span></span><br><span class="line"><span class="params"><span class="function">                 get_params, init_state, forward_fn</span>):</span></span><br><span class="line">        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens</span><br><span class="line">        self.params = get_params(vocab_size, num_hiddens, device)</span><br><span class="line">        self.init_state, self.forward_fn = init_state, forward_fn</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, X, state</span>):</span></span><br><span class="line">        X = F.one_hot(X.T, self.vocab_size).<span class="built_in">type</span>(torch.float32)</span><br><span class="line">        <span class="keyword">return</span> self.forward_fn(X, state, self.params)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">begin_state</span>(<span class="params">self, batch_size, device</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.init_state(batch_size, self.num_hiddens, device)</span><br></pre></td></tr></table></figure><h3 id="5-4-预测"><a href="#5-4-预测" class="headerlink" title="5.4. 预测"></a>5.4. 预测</h3><p>首先定义预测函数来生成<code>prefix</code>之后的新字符，<code>prefix</code>是一个用户提供的包含多个字符的字符串。在循环遍历<code>prefix</code>中的开始字符时，我们不断地将隐状态传递到下一个时间步，但是不生成任何输出。这被称为<em>预热</em>（warm-up）期，因为在此期间模型会自我更新（例如，更新隐状态），但不会进行预测。预热结束后，隐状态的值通常比刚开始的初始值更适合预测，从而预测字符并输出它们。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_ch8</span>(<span class="params">prefix, num_preds, net, vocab, device</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;在prefix后面生成新字符&quot;&quot;&quot;</span></span><br><span class="line">    state = net.begin_state(batch_size=<span class="number">1</span>, device=device)</span><br><span class="line">    outputs = [vocab[prefix[<span class="number">0</span>]]]</span><br><span class="line">    get_input = <span class="keyword">lambda</span>: torch.tensor([outputs[-<span class="number">1</span>]], device=device).reshape((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> prefix[<span class="number">1</span>:]:  <span class="comment"># 预热期</span></span><br><span class="line">        _, state = net(get_input(), state)</span><br><span class="line">        outputs.append(vocab[y])</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_preds):  <span class="comment"># 预测num_preds步</span></span><br><span class="line">        y, state = net(get_input(), state)</span><br><span class="line">        outputs.append(<span class="built_in">int</span>(y.argmax(dim=<span class="number">1</span>).reshape(<span class="number">1</span>)))</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;&#x27;</span>.join([vocab.idx_to_token[i] <span class="keyword">for</span> i <span class="keyword">in</span> outputs])</span><br></pre></td></tr></table></figure><h3 id="5-5-梯度裁剪"><a href="#5-5-梯度裁剪" class="headerlink" title="5.5. 梯度裁剪"></a>5.5. 梯度裁剪</h3><p>对于上述的循环神经网络，不仅有纵向的深度（输入到输出），还有横向的“深度”（从第一个时间步到末时间步），对于长度为$T$的序列，在迭代中计算这$T$个时间步上的梯度，将会在反向传播过程中产生长度为$\mathcal{O}(T)$的矩阵乘法链。当$T$较大时，它可能导致数值不稳定，例如梯度爆炸或梯度消失。因此，循环神经网络模型往往需要额外的方式来支持稳定训练。</p><p>一般在通过梯度下降解决优化问题（优化某个目标）时，采用迭代方式更新模型参数，更新操作是对参数向量$\mathbf{x}$，将其推向负梯度$\mathbf{g}$的方向上（在随机梯度下降中该梯度在随机抽样的小批量中计算）。例如，使用$\eta &gt; 0$作为学习率时，在一次迭代中，我们将$\mathbf{x}$更新为$\mathbf{x} - \eta \mathbf{g}$。此时进一步假设目标函数$f$表现良好，即函数$f$在常数$L$下是<em>利普希茨连续的</em>（Lipschitz continuous）。也就是说，对于任意$\mathbf{x}$和$\mathbf{y}$有：</p><script type="math/tex; mode=display">|f(\mathbf{x}) - f(\mathbf{y})| \leq L \|\mathbf{x} - \mathbf{y}\|</script><p>如果我们通过$\eta \mathbf{g}$更新参数向量，则目标值的变化取决于学习率、梯度的范数和$L$：</p><script type="math/tex; mode=display">|f(\mathbf{x}) - f(\mathbf{x} - \eta\mathbf{g})| \leq L \eta\|\mathbf{g}\|</script><p>这意味着目标的变化不会超过$L \eta |\mathbf{g}|$。这个上限的值较小既是坏事也是好事。坏的方面，它限制了取得进展的速度；好的方面，它限制了事情变糟的程度，尤其当我们朝着错误的方向前进时。</p><p>有时梯度可能很大（梯度爆炸），优化算法可能无法收敛，可以通过降低$\eta$的学习率来解决这个问题。但如果很少得到大的梯度时，不可能对所有情况采取降低学习率的方式，这个做法会减缓我们在所有步骤中的进展，只是为了处理罕见的梯度爆炸事件。一个流行的替代方案是通过将梯度$\mathbf{g}$投影回给定半径（例如$\theta$）的球来<strong>裁剪梯度</strong>$\mathbf{g}$。如下式：</p><script type="math/tex; mode=display">\mathbf{g} \leftarrow \min\left(1, \frac{\theta}{\|\mathbf{g}\|}\right) \mathbf{g}</script><p>这样做后，梯度范数永远不会超过$\theta$，并且更新后的梯度完全与$\mathbf{g}$的原始方向对齐。它还有一个值得拥有的副作用，即限制任何给定的小批量数据（以及其中任何给定的样本）对参数向量的影响，这赋予了模型一定程度的稳定性。梯度裁剪提供了一个快速修复梯度爆炸的方法，虽然它并不能完全解决问题，但它是众多有效的技术之一。</p><p>下面定义一个函数来裁剪模型的梯度，在此计算了所有模型参数的梯度的范数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad_clipping</span>(<span class="params">net, theta</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;裁剪梯度&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module):</span><br><span class="line">        params = [p <span class="keyword">for</span> p <span class="keyword">in</span> net.parameters() <span class="keyword">if</span> p.requires_grad]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        params = net.params</span><br><span class="line">    <span class="comment"># 范数，平方和</span></span><br><span class="line">    norm = torch.sqrt(<span class="built_in">sum</span>(torch.<span class="built_in">sum</span>((p.grad ** <span class="number">2</span>)) <span class="keyword">for</span> p <span class="keyword">in</span> params))</span><br><span class="line">    <span class="keyword">if</span> norm &gt; theta:</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            param.grad[:] *= theta / norm</span><br></pre></td></tr></table></figure><h3 id="5-6-训练"><a href="#5-6-训练" class="headerlink" title="5.6. 训练"></a>5.6. 训练</h3><p>在训练模型之前，定义一个函数在一个迭代周期内训练模型。这与之前讲到的训练模型的方式有三个不同之处。</p><ol><li>序列数据的不同采样方法（随机采样和顺序分区）将导致隐状态初始化的差异。</li><li>在更新模型参数之前会裁剪梯度，这样操作的目的是，即使训练过程中某个点上发生了梯度爆炸，也能保证模型不会发散。</li><li>用困惑度来评价模型，这样的度量确保了不同长度的序列具有可比性。</li></ol><p>当使用顺序采样（顺序分区）时，我们只在每个迭代周期的开始位置初始化隐状态。由于下一个小批量数据中的第$i$个子序列样本与当前第$i$个子序列样本相邻，因此当前小批量数据最后一个样本的隐状态，将用于初始化下一个小批量数据第一个样本的隐状态。这样，存储在隐状态中的序列的历史信息可以在一个迭代周期内流经相邻的子序列。然而，在任何一点隐状态的计算，都依赖于同一迭代周期中前面所有的小批量数据，这使得梯度计算变得复杂。为了降低计算量，在处理任何一个小批量数据之前，通常先分离梯度，使得隐状态的梯度计算总是限制在一个小批量数据的时间步内。</p><p>当使用随机抽样时，因为每个样本都是在一个随机位置抽样的，因此需要为每个迭代周期重新初始化隐状态。与之前章节中的<code>train_epoch_ch3</code>函数相同，<code>updater</code>是更新模型参数的常用函数。它既可以是从头开始实现的<code>d2l.sgd</code>函数，也可以是深度学习框架中内置的优化函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_epoch_ch8</span>(<span class="params">net, train_iter, loss, updater, device, use_random_iter</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练网络一个迭代周期（定义见第8章）&quot;&quot;&quot;</span></span><br><span class="line">    state, timer = <span class="literal">None</span>, Timer()</span><br><span class="line">    metric = Accumulator(<span class="number">2</span>)  <span class="comment"># 训练损失之和,词元数量</span></span><br><span class="line">    <span class="keyword">for</span> X, Y <span class="keyword">in</span> train_iter:</span><br><span class="line">        <span class="keyword">if</span> state <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> use_random_iter:</span><br><span class="line">            <span class="comment"># 在第一次迭代或使用随机抽样时初始化state</span></span><br><span class="line">            state = net.begin_state(batch_size=X.shape[<span class="number">0</span>], device=device)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 已有隐状态，在处理小批量前先分离梯度。</span></span><br><span class="line">            <span class="comment"># tensor.detach_()将一个tensor从创建它的图中分离，并把它设置成叶子tensor。是对tensor本身的更改。</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module) <span class="keyword">and</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(state, <span class="built_in">tuple</span>):</span><br><span class="line">                <span class="comment"># state对于nn.GRU是个张量</span></span><br><span class="line">                state.detach_()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># state对于nn.LSTM或对于我们从零开始实现的模型是个张量</span></span><br><span class="line">                <span class="keyword">for</span> s <span class="keyword">in</span> state:</span><br><span class="line">                    s.detach_()</span><br><span class="line">        y = Y.T.reshape(-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 加载到显存</span></span><br><span class="line">        X, y = X.to(device), y.to(device)</span><br><span class="line">        y_hat, state = net(X, state)</span><br><span class="line">        l = loss(y_hat, y.long()).mean()</span><br><span class="line">        <span class="comment"># grad_clipping-梯度裁剪</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(updater, torch.optim.Optimizer):</span><br><span class="line">            updater.zero_grad()</span><br><span class="line">            l.backward()</span><br><span class="line">            grad_clipping(net, <span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 开始优化一次</span></span><br><span class="line">            updater.step()</span><br><span class="line">        <span class="comment"># 如果优化函数是非框架的实现</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            l.backward()</span><br><span class="line">            grad_clipping(net, <span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 因为已经调用了mean函数</span></span><br><span class="line">            updater(batch_size=<span class="number">1</span>)</span><br><span class="line">        metric.add(l * y.numel(), y.numel())</span><br><span class="line">    <span class="comment"># 返回 困惑度 和 速度</span></span><br><span class="line">    <span class="keyword">return</span> math.exp(metric[<span class="number">0</span>] / metric[<span class="number">1</span>]), metric[<span class="number">1</span>] / timer.stop()</span><br></pre></td></tr></table></figure><p>训练函数，既支持从零开始实现， 也可以使用高级API来实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_ch8</span>(<span class="params">net, train_iter, vocab, lr, num_epochs, device,</span></span></span><br><span class="line"><span class="params"><span class="function">              use_random_iter=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练模型（定义见第8章）&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 定义损失函数，交叉熵</span></span><br><span class="line">    loss = nn.CrossEntropyLoss()</span><br><span class="line">    animator = Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;perplexity&#x27;</span>,</span><br><span class="line">                            legend=[<span class="string">&#x27;train&#x27;</span>], xlim=[<span class="number">10</span>, num_epochs])</span><br><span class="line">    <span class="comment"># 初始化，设定参数优化器（小批量随机梯度下降），指定参数表和学习率。</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module):</span><br><span class="line">        updater = torch.optim.SGD(net.parameters(), lr)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        updater = <span class="keyword">lambda</span> batch_size: sgd(net.params, lr, batch_size)</span><br><span class="line">    predict = <span class="keyword">lambda</span> prefix: predict_ch8(prefix, <span class="number">50</span>, net, vocab, device)</span><br><span class="line">    <span class="comment"># 训练和预测</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        ppl, speed = train_epoch_ch8(</span><br><span class="line">            net, train_iter, loss, updater, device, use_random_iter)</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(predict(<span class="string">&#x27;time traveller&#x27;</span>))</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>, [ppl])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;困惑度 <span class="subst">&#123;ppl:<span class="number">.1</span>f&#125;</span>, <span class="subst">&#123;speed:<span class="number">.1</span>f&#125;</span> 词元/秒 <span class="subst">&#123;<span class="built_in">str</span>(device)&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(predict(<span class="string">&#x27;time traveller&#x27;</span>))</span><br><span class="line">    <span class="built_in">print</span>(predict(<span class="string">&#x27;traveller&#x27;</span>))</span><br></pre></td></tr></table></figure><p>然后开始训练即可，因为在数据集中只使用了10000个词元， 所以模型需要更多的迭代周期来更好地收敛。</p><h2 id="6-循环神经网络的框架实现"><a href="#6-循环神经网络的框架实现" class="headerlink" title="6. 循环神经网络的框架实现"></a>6. 循环神经网络的框架实现</h2><p>使用深度学习框架的高级API提供的函数更有效地实现相同的语言模型。</p><h3 id="6-1-读取数据集"><a href="#6-1-读取数据集" class="headerlink" title="6.1. 读取数据集"></a>6.1. 读取数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">train_iter, vocab = load_data_time_machine(batch_size, num_steps)</span><br></pre></td></tr></table></figure><h3 id="6-2-定义模型"><a href="#6-2-定义模型" class="headerlink" title="6.2. 定义模型"></a>6.2. 定义模型</h3><p>构造一个具有256个隐藏单元的单隐藏层的循环神经网络层rnn_layer。虽然目前还没有讨论多层循环神经网络的意义。目前仅需要将多层理解为一层循环神经网络的输出被用作下一层循环神经网络的输入就足够了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens = <span class="number">256</span></span><br><span class="line">rnn_layer = nn.RNN(<span class="built_in">len</span>(vocab), num_hiddens)</span><br></pre></td></tr></table></figure><p>使用张量来初始化隐状态，它的形状是（隐藏层数，批量大小，隐藏单元数）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">state = torch.zeros((<span class="number">1</span>, batch_size, num_hiddens))</span><br><span class="line">state.shape</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">32</span>, <span class="number">256</span>])</span><br></pre></td></tr></table></figure><p>通过一个隐状态和一个输入，就可以用更新后的隐状态计算输出。 需要强调的是，rnn_layer的“输出”（Y）不涉及输出层的计算： 它是指每个时间步的隐状态，这些隐状态可以用作后续输出层的输入。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(size=(num_steps, batch_size, <span class="built_in">len</span>(vocab)))</span><br><span class="line">Y, state_new = rnn_layer(X, state)</span><br><span class="line">Y.shape, state_new.shape</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">(torch.Size([<span class="number">35</span>, <span class="number">32</span>, <span class="number">256</span>]), torch.Size([<span class="number">1</span>, <span class="number">32</span>, <span class="number">256</span>]))</span><br></pre></td></tr></table></figure><p>为一个完整的循环神经网络模型定义了一个RNNModel类。 由于rnn_layer只包含隐藏的循环层，我们还需要创建一个单独的输出层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNNModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;循环神经网络模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, rnn_layer, vocab_size, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(RNNModel, self).__init__(**kwargs)</span><br><span class="line">        self.rnn = rnn_layer</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.num_hiddens = self.rnn.hidden_size</span><br><span class="line">        <span class="comment"># 如果RNN是双向的，num_directions应该是2，否则应该是1</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.rnn.bidirectional:</span><br><span class="line">            self.num_directions = <span class="number">1</span></span><br><span class="line">            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.num_directions = <span class="number">2</span></span><br><span class="line">            self.linear = nn.Linear(self.num_hiddens * <span class="number">2</span>, self.vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs, state</span>):</span></span><br><span class="line">        X = F.one_hot(inputs.T.long(), self.vocab_size)</span><br><span class="line">        X = X.to(torch.float32)</span><br><span class="line">        Y, state = self.rnn(X, state)</span><br><span class="line">        <span class="comment"># 全连接层首先将Y的形状改为(时间步数*批量大小,隐藏单元数)</span></span><br><span class="line">        <span class="comment"># 它的输出形状是(时间步数*批量大小,词表大小)。</span></span><br><span class="line">        output = self.linear(Y.reshape((-<span class="number">1</span>, Y.shape[-<span class="number">1</span>])))</span><br><span class="line">        <span class="keyword">return</span> output, state</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">begin_state</span>(<span class="params">self, device, batch_size=<span class="number">1</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(self.rnn, nn.LSTM):</span><br><span class="line">            <span class="comment"># nn.GRU以张量作为隐状态</span></span><br><span class="line">            <span class="keyword">return</span>  torch.zeros((self.num_directions * self.rnn.num_layers,</span><br><span class="line">                                batch_size, self.num_hiddens),</span><br><span class="line">                                device=device)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># nn.LSTM以元组作为隐状态</span></span><br><span class="line">            <span class="keyword">return</span> (torch.zeros((</span><br><span class="line">                self.num_directions * self.rnn.num_layers,</span><br><span class="line">                batch_size, self.num_hiddens), device=device),</span><br><span class="line">                    torch.zeros((</span><br><span class="line">                        self.num_directions * self.rnn.num_layers,</span><br><span class="line">                        batch_size, self.num_hiddens), device=device))</span><br></pre></td></tr></table></figure><h3 id="6-3-训练与预测"><a href="#6-3-训练与预测" class="headerlink" title="6.3. 训练与预测"></a>6.3. 训练与预测</h3><p>在训练模型之前，基于一个具有随机权重的模型进行预测:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&#x27;mps&#x27;</span>)</span><br><span class="line">net = RNNModel(rnn_layer, vocab_size=<span class="built_in">len</span>(vocab))</span><br><span class="line">net = net.to(device)</span><br><span class="line">predict_ch8(<span class="string">&#x27;time traveller&#x27;</span>, <span class="number">10</span>, net, vocab, device)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line"><span class="string">&#x27;time travellerskhsskkkhs&#x27;</span></span><br></pre></td></tr></table></figure><p>很明显，这种模型根本不能输出好的结果。<br>下面开始使用预备好的超参数进行训练，结果会好的多。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, lr = <span class="number">500</span>, <span class="number">1</span></span><br><span class="line">train_ch8(net, train_iter, vocab, lr, num_epochs, device)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">perplexity <span class="number">1.3</span>, <span class="number">288014.3</span> tokens/sec on cuda:<span class="number">0</span></span><br><span class="line">time travellerit s against reason <span class="keyword">and</span> of the inghero mad alove a</span><br><span class="line">travellereat so largattematiche ture copethi thele we the i</span><br></pre></td></tr></table></figure><p>由于深度学习框架的高级API对代码进行了更多的优化，该模型相比从零实现在较短的时间内达到了较低的困惑度。</p><p>在M1芯片的机器上训练如果使用GPU可能会训练不出结果，估计是显存问题。</p><h2 id="7-通过时间反向传播"><a href="#7-通过时间反向传播" class="headerlink" title="7. 通过时间反向传播"></a>7. 通过时间反向传播</h2><p>本节会更深入地探讨序列模型反向传播的细节， 以及相关的数学原理。之前实践中遇到的“梯度爆炸”、“梯度消失”、对循环神经网络“分离梯度”等概念也会得到充分的解释。</p><p>通过时间反向传播（backpropagation through time，BPTT）实际上是循环神经网络中反向传播技术的一个特定应用。它要求我们将循环神经网络的计算图一次展开一个时间步， 以获得模型变量和参数之间的依赖关系。 然后，基于链式法则，应用反向传播来计算和存储梯度。 由于序列可能相当长，因此依赖关系也可能相当长。 例如，某个1000个字符的序列， 其第一个词元可能会对最后位置的词元产生重大影响。 这在计算上是不可行的（它需要的时间和内存都太多了）， 并且还需要超过1000个矩阵的乘积才能得到非常难以捉摸的梯度。 这个过程充满了计算与统计的不确定性。 下面将阐述过程中会发生什么以及如何在实践中解决它们。</p><h3 id="7-1-循环神经网络的梯度分析"><a href="#7-1-循环神经网络的梯度分析" class="headerlink" title="7.1. 循环神经网络的梯度分析"></a>7.1. 循环神经网络的梯度分析</h3><p>从一个描述循环神经网络工作原理的简化模型开始，此模型忽略了隐状态的特性及其更新方式的细节。这里的数学表示没有明确地区分标量、向量和矩阵，因为这些细节对于分析并不重要，反而只会使本小节中的符号变得混乱。</p><p>这个简化模型中将时间步$t$的隐状态表示为$h_t$，输入表示为$x_t$，输出表示为$o_t$。前面提到过，输入和隐状态可以拼接后与隐藏层中的一个权重变量相乘。这里分别使用$w_h$和$w_o$来表示隐藏层和输出层的权重。每个时间步的隐状态和输出可以写为：</p><script type="math/tex; mode=display">\begin{aligned}h_t &= f(x_t, h_{t-1}, w_h),\\o_t &= g(h_t, w_o),\end{aligned}</script><p>其中$f$和$g$分别是隐藏层和输出层的变换。则可以获得一个链${\ldots, (x_{t-1}, h_{t-1}, o_{t-1}), (x_{t}, h_{t}, o_t), \ldots}$，它们通过循环计算彼此依赖。在这个序列上正向传播相当简单，一次一个时间步的遍历三元组$(x_t, h_t, o_t)$，然后通过一个目标函数在所有$T$个时间步内评估输出$o_t$和对应的标签$y_t$之间的差异：</p><script type="math/tex; mode=display">L(x_1, \ldots, x_T, y_1, \ldots, y_T, w_h, w_o) = \frac{1}{T}\sum_{t=1}^T l(y_t, o_t)</script><p>而反向传播则相对棘手，特别是在计算目标函数$L$关于参数$w_h$的梯度时。按照链式法则：</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial L}{\partial w_h}  & = \frac{1}{T}\sum_{t=1}^T \frac{\partial l(y_t, o_t)}{\partial w_h}  \\& = \frac{1}{T}\sum_{t=1}^T \frac{\partial l(y_t, o_t)}{\partial o_t} \frac{\partial g(h_t, w_o)}{\partial h_t}  \frac{\partial h_t}{\partial w_h}.\end{aligned}</script><p>上式中乘积的第一项和第二项很容易计算，而第三项$\partial h_t/\partial w_h$是使事情变得棘手的地方，因为这一项需要循环地计算参数$w_h$对$h_t$的影响。根据$ h_t = f(x_t, h_{t-1}, w_h) $产生的递归计算，$h_t$既依赖于$h_{t-1}$又依赖于$w_h$，其中$h_{t-1}$的计算也依赖于$w_h$。因此，使用链式法则产生：</p><script type="math/tex; mode=display">\frac{\partial h_t}{\partial w_h}= \frac{\partial f(x_{t},h_{t-1},w_h)}{\partial w_h} +\frac{\partial f(x_{t},h_{t-1},w_h)}{\partial h_{t-1}} \frac{\partial h_{t-1}}{\partial w_h}</script><p>为导出梯度$\frac{\partial h_t}{\partial w_h}$的计算通式，设有三个序列${a_{t}},{b_{t}},{c_{t}}$，当$t=1,2,\ldots$时，序列满足$a_{0}=0$且$a_{t}=b_{t}+c_{t}a_{t-1}$。对于$t\geq 1$，很容易得出：</p><script type="math/tex; mode=display">a_{t}=b_{t}+\sum_{i=1}^{t-1}\left(\prod_{j=i+1}^{t}c_{j}\right)b_{i}</script><p>对应替换$a_t$、$b_t$和$c_t$：</p><script type="math/tex; mode=display">\begin{aligned}a_t &= \frac{\partial h_t}{\partial w_h},\\b_t &= \frac{\partial f(x_{t},h_{t-1},w_h)}{\partial w_h}, \\c_t &= \frac{\partial f(x_{t},h_{t-1},w_h)}{\partial h_{t-1}},\end{aligned}</script><p>由于前面计算公式中的梯度计算满足$a_{t}=b_{t}+c_{t}a_{t-1}$，则可以得到：</p><script type="math/tex; mode=display">\frac{\partial h_t}{\partial w_h}=\frac{\partial f(x_{t},h_{t-1},w_h)}{\partial w_h}+\sum_{i=1}^{t-1}\left(\prod_{j=i+1}^{t} \frac{\partial f(x_{j},h_{j-1},w_h)}{\partial h_{j-1}} \right) \frac{\partial f(x_{i},h_{i-1},w_h)}{\partial w_h}</script><p>虽然可以使用链式法则递归地计算$\partial h_t/\partial w_h$，但当$t$很大时这个链就会变得很长，我们需要想办法来处理这一问题。</p><h4 id="7-1-1-完全计算"><a href="#7-1-1-完全计算" class="headerlink" title="7.1.1. 完全计算"></a>7.1.1. 完全计算</h4><p>我们可以简单的递归计算上式中的全部总和，但这样的计算非常缓慢，并且可能会发生梯度爆炸， 因为初始条件的微小变化就可能会对结果产生巨大的影响，这类似于蝴蝶效应，即初始条件的很小变化就会导致结果发生不成比例的变化。 这对于我们追求的 能很好泛化高稳定性模型的预测器 是背道而驰的。 因此实践中这种方法几乎从未使用过。</p><h4 id="7-1-2-截断时间步"><a href="#7-1-2-截断时间步" class="headerlink" title="7.1.2. 截断时间步"></a>7.1.2. 截断时间步</h4><p>然后我们想到或许可以在$\tau$步后截断上式中的求和计算。这会带来真实梯度的<em>近似</em>，只需将求和终止为$\partial h_{t-\tau}/\partial w_h$。在实践中这种方法很凑效，它通常被称为截断的通过时间反向传播。这样做会导致该模型主要侧重于短期影响，而不是长期影响。这种截断是可取的，因为它会将估计值偏向更简单和更稳定的模型。</p><h4 id="7-1-3-随机截断"><a href="#7-1-3-随机截断" class="headerlink" title="7.1.3. 随机截断"></a>7.1.3. 随机截断</h4><p>在普通的“固定截断”上继续发展，则或许可以用一个随机变量替换$\partial h_t/\partial w_h$，该随机变量在预期中是正确的，但是会截断序列。这个随机变量是通过使用序列$\xi_t$来实现的，该序列预定义了$0 \leq \pi_t \leq 1$，其中$P(\xi_t = 0) = 1-\pi_t$且$P(\xi_t = \pi_t^{-1}) = \pi_t$，则有$E[\xi_t] = 1$。使用它来替换前式中的梯度$\partial h_t/\partial w_h$得到：</p><script type="math/tex; mode=display">z_t= \frac{\partial f(x_{t},h_{t-1},w_h)}{\partial w_h} +\xi_t \frac{\partial f(x_{t},h_{t-1},w_h)}{\partial h_{t-1}} \frac{\partial h_{t-1}}{\partial w_h}</script><p>从$\xi_t$的定义中推导出来$E[z_t] = \partial h_t/\partial w_h$。每当$\xi_t = 0$时，递归计算终止在这个$t$时间步。这导致了不同长度序列的加权和，其中长序列出现的很少，所以将适当地加大权重。这个想法是由塔莱克和奥利维尔提出的。</p><h4 id="7-1-4-比较上述计算策略"><a href="#7-1-4-比较上述计算策略" class="headerlink" title="7.1.4. 比较上述计算策略"></a>7.1.4. 比较上述计算策略</h4><p><img src="/assets/post_img/article57/truncated-bptt.svg" alt="比较RNN中计算梯度的策略，3行自上而下分别为：随机截断、常规截断、完整计算"><br>上图说明了基于循环神经网络使用通过时间反向传播分析《时间机器》书中前几个字符的三种策略：</p><ul><li>第一行采用随机截断，方法是将文本划分为不同长度的片断；</li><li>第二行采用常规截断，方法是将文本分解为相同长度的子序列。这也是我们在循环神经网络实验中一直在做的；</li><li>第三行采用通过时间的完全反向传播，结果是产生了在计算上不可行的表达式。</li></ul><p>遗憾的是，虽然随机截断在理论上具有吸引力，但很可能是由于多种因素在实践中并不比常规截断更好。首先，在对过去若干个时间步经过反向传播后，观测结果足以捕获实际的依赖关系。其次，增加的方差抵消了时间步数越多梯度越精确的事实。第三，我们真正想要的是只有短范围交互的模型。因此，模型需要的正是截断的通过时间反向传播方法所具备的轻度正则化效果。</p><h3 id="7-2-通过时间反向传播的细节"><a href="#7-2-通过时间反向传播的细节" class="headerlink" title="7.2. 通过时间反向传播的细节"></a>7.2. 通过时间反向传播的细节</h3><p>在讨论一般性原则之后，来看一下通过时间反向传播问题的细节。与上一小节中的分析不同，本小节将展示如何计算目标函数相对于所有分解模型参数的梯度。为保持简单，设有一个没有偏置参数的循环神经网络，其在隐藏层中的激活函数使用恒等映射（$\phi(x)=x$）。对于时间步$t$，设单个样本的输入及其对应的标签分别为$\mathbf{x}_t \in \mathbb{R}^d$和$y_t$。计算隐状态$\mathbf{h}_t \in \mathbb{R}^h$和输出$\mathbf{o}_t \in \mathbb{R}^q$的方式为：</p><script type="math/tex; mode=display">\begin{aligned}\mathbf{h}_t &= \mathbf{W}_{hx} \mathbf{x}_t + \mathbf{W}_{hh} \mathbf{h}_{t-1},\\\mathbf{o}_t &= \mathbf{W}_{qh} \mathbf{h}_{t},\end{aligned}</script><p>其中权重参数为$\mathbf{W}_{hx} \in \mathbb{R}^{h \times d}$、$\mathbf{W}_{hh} \in \mathbb{R}^{h \times h}$和$\mathbf{W}_{qh} \in \mathbb{R}^{q \times h}$。用$l(\mathbf{o}_t, y_t)$表示时间步$t$处（即从序列开始起的超过$T$个时间步）的损失函数，则目标函数的总体损失是：</p><script type="math/tex; mode=display">L = \frac{1}{T} \sum_{t=1}^T l(\mathbf{o}_t, y_t)</script><p>为了在循环神经网络的计算过程中可视化模型变量和参数之间的依赖关系，可以为模型绘制一个计算图，如下图所示。利用计算图可以看到很多东西，比如时间步3的隐状态$\mathbf{h}_3$的计算取决于模型参数$\mathbf{W}_{hx}$和$\mathbf{W}_{hh}$，以及最终时间步的隐状态$\mathbf{h}_2$以及当前时间步的输入$\mathbf{x}_3$。</p><p><img src="/assets/post_img/article57/rnn-bptt.svg" alt="上图表示具有三个时间步的循环神经网络模型依赖关系的计算图。未着色的方框表示变量，着色的方框表示参数，圆表示运算符"></p><p>上中的模型参数是$\mathbf{W}_{hx}$、$\mathbf{W}_{hh}$和$\mathbf{W}_{qh}$。通常，训练该模型需要对这些参数进行梯度计算：$\partial L/\partial \mathbf{W}_{hx}$、$\partial L/\partial \mathbf{W}_{hh}$和$\partial L/\partial \mathbf{W}_{qh}$。根据依赖关系，我们可以沿箭头的相反方向遍历计算图，依次计算和存储梯度。为灵活地表示链式法则中不同形状的矩阵、向量和标量的乘法，继续采用$\text{prod}$运算符表示参数乘法以及一些必要操作。</p><p>首先，在任意时间步$t$，目标函数关于模型输出的微分计算是相当简单的：</p><script type="math/tex; mode=display">\frac{\partial L}{\partial \mathbf{o}_t} =  \frac{\partial l (\mathbf{o}_t, y_t)}{T \cdot \partial \mathbf{o}_t} \in \mathbb{R}^q</script><p>现在可以计算目标函数关于输出层中参数$\mathbf{W}_{qh}$的梯度：$\partial L/\partial \mathbf{W}_{qh} \in \mathbb{R}^{q \times h}$。基于计算图，目标函数$L$通过$\mathbf{o}_1, \ldots, \mathbf{o}_T$依赖于$\mathbf{W}_{qh}$。依据链式法则，得到</p><script type="math/tex; mode=display">\frac{\partial L}{\partial \mathbf{W}_{qh}}= \sum_{t=1}^T \text{prod}\left(\frac{\partial L}{\partial \mathbf{o}_t}, \frac{\partial \mathbf{o}_t}{\partial \mathbf{W}_{qh}}\right)= \sum_{t=1}^T \frac{\partial L}{\partial \mathbf{o}_t} \mathbf{h}_t^\top,</script><p>其中$\partial L/\partial \mathbf{o}_t$是由第一步给出的。</p><p>接下来，由计算图知在最后的时间步$T$，目标函数$L$仅通过$\mathbf{o}_T$依赖于隐状态$\mathbf{h}_T$。使用链式法可以很容易地得到梯度$\partial L/\partial \mathbf{h}_T \in \mathbb{R}^h$：</p><script type="math/tex; mode=display">\frac{\partial L}{\partial \mathbf{h}_T} = \text{prod}\left(\frac{\partial L}{\partial \mathbf{o}_T}, \frac{\partial \mathbf{o}_T}{\partial \mathbf{h}_T} \right) = \mathbf{W}_{qh}^\top \frac{\partial L}{\partial \mathbf{o}_T}</script><p>当目标函数$L$通过$\mathbf{h}_{t+1}$和$\mathbf{o}_t$依赖$\mathbf{h}_t$时，对任意时间步$t &lt; T$来说都变得更加棘手。根据链式法则，隐状态的梯度$\partial L/\partial \mathbf{h}_t \in \mathbb{R}^h$在任何时间步骤$t &lt; T$时都可以递归地计算为：</p><script type="math/tex; mode=display">\frac{\partial L}{\partial \mathbf{h}_t} = \text{prod}\left(\frac{\partial L}{\partial \mathbf{h}_{t+1}}, \frac{\partial \mathbf{h}_{t+1}}{\partial \mathbf{h}_t} \right) + \text{prod}\left(\frac{\partial L}{\partial \mathbf{o}_t}, \frac{\partial \mathbf{o}_t}{\partial \mathbf{h}_t} \right) = \mathbf{W}_{hh}^\top \frac{\partial L}{\partial \mathbf{h}_{t+1}} + \mathbf{W}_{qh}^\top \frac{\partial L}{\partial \mathbf{o}_t}</script><p>为了进行分析，对于任何时间步$1 \leq t \leq T$展开递归计算得：</p><script type="math/tex; mode=display">\frac{\partial L}{\partial \mathbf{h}_t}= \sum_{i=t}^T {\left(\mathbf{W}_{hh}^\top\right)}^{T-i} \mathbf{W}_{qh}^\top \frac{\partial L}{\partial \mathbf{o}_{T+t-i}}</script><p>我们可以从上式中看到，这个简单的线性例子已经展现了长序列模型的一些关键问题：它陷入到$\mathbf{W}_{hh}^\top$的潜在的非常大的幂。在这个幂中，小于1的特征值将会消失，大于1的特征值将会发散。这在数值上是不稳定的，表现形式为梯度消失或梯度爆炸。解决此问题的一种方法是按照计算方便的需要截断指定大小的时间步长。实践中这种截断是通过在给定数量的时间步之后分离梯度来实现的。后面将学习更复杂的序列模型（如长短期记忆模型）是如何进一步缓解这一问题的。</p><p>最后，计算图表明：目标函数$L$通过隐状态$\mathbf{h}_1, \ldots, \mathbf{h}_T$依赖于隐藏层中的模型参数$\mathbf{W}_{hx}$和$\mathbf{W}_{hh}$。为了计算有关这些参数的梯度$\partial L / \partial \mathbf{W}_{hx} \in \mathbb{R}^{h \times d}$和$\partial L / \partial \mathbf{W}_{hh} \in \mathbb{R}^{h \times h}$，可以应用链式规则得：</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial L}{\partial \mathbf{W}_{hx}}&= \sum_{t=1}^T \text{prod}\left(\frac{\partial L}{\partial \mathbf{h}_t}, \frac{\partial \mathbf{h}_t}{\partial \mathbf{W}_{hx}}\right)= \sum_{t=1}^T \frac{\partial L}{\partial \mathbf{h}_t} \mathbf{x}_t^\top,\\\frac{\partial L}{\partial \mathbf{W}_{hh}}&= \sum_{t=1}^T \text{prod}\left(\frac{\partial L}{\partial \mathbf{h}_t}, \frac{\partial \mathbf{h}_t}{\partial \mathbf{W}_{hh}}\right)= \sum_{t=1}^T \frac{\partial L}{\partial \mathbf{h}_t} \mathbf{h}_{t-1}^\top,\end{aligned}</script><p>其中$\partial L/\partial \mathbf{h}_t$是由$ \frac{\partial L}{\partial \mathbf{h}_T} = \text{prod}\left(\frac{\partial L}{\partial \mathbf{o}_T}, \frac{\partial \mathbf{o}_T}{\partial \mathbf{h}_T} \right) = \mathbf{W}_{qh}^\top \frac{\partial L}{\partial \mathbf{o}_T} $和$ \frac{\partial L}{\partial \mathbf{h}_t} = \text{prod}\left(\frac{\partial L}{\partial \mathbf{h}_{t+1}}, \frac{\partial \mathbf{h}_{t+1}}{\partial \mathbf{h}_t} \right) + \text{prod}\left(\frac{\partial L}{\partial \mathbf{o}_t}, \frac{\partial \mathbf{o}_t}{\partial \mathbf{h}_t} \right) = \mathbf{W}_{hh}^\top \frac{\partial L}{\partial \mathbf{h}_{t+1}} + \mathbf{W}_{qh}^\top \frac{\partial L}{\partial \mathbf{o}_t} $递归计算得到的，是影响数值稳定性的关键量。</p><p>由于BPTT是反向传播在循环神经网络中的应用方式，所以训练循环神经网络交替使用正向传播和BPTT。BPTT依次计算并存储上述梯度。具体而言，存储的中间值会被重复使用，以避免重复计算，例如存储$\partial L/\partial \mathbf{h}_t$，以便在计算$\partial L / \partial \mathbf{W}_{hx}$和$\partial L / \partial \mathbf{W}_{hh}$时使用。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;p&gt;目前为止仅接触到两种类型的数据：表格数据和图像数据。 对于图像数据，可以设计专门的卷积神经网络架构来为这类特殊的数据结构建模。 对于一张图像，我们需要有效地利用其像素位置，假若对图像中的像素位置进行重排，就会对图像中内容的推断造成极大的困难。&lt;/p&gt;
&lt;p&gt;最重要的是，到目前为止我们默认数据都来自于某种分布， 并且所有样本都是独立同分布的 （independently and identically distributed，i.i.d.）。 然而，大多数的数据并非如此。 例如，文章中的单词是按顺序写的，如果顺序被随机地重排，就很难理解文章原始的意思。 同样，视频中的图像帧、对话中的音频信号以及网站上的浏览行为都是有顺序的。 因此，针对此类数据而设计特定模型，可能效果会更好。&lt;br&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://silencezheng.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>什么是Mixin</title>
    <link href="http://silencezheng.top/2022/08/26/article56/"/>
    <id>http://silencezheng.top/2022/08/26/article56/</id>
    <published>2022-08-26T10:30:40.000Z</published>
    <updated>2022-08-26T10:32:25.266Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在开发程序的过程中，尤其是在使用各个成熟的工具时，经常会接触到<strong>Mixin</strong>的概念。初见时完全不理解是什么含义，在网上查找了一些资料，总结一下，如有错误请评论指正。<br><span id="more"></span></p><h2 id="什么是Mixin？"><a href="#什么是Mixin？" class="headerlink" title="什么是Mixin？"></a>什么是Mixin？</h2><p>Mixin即“混入”，也可以是MixIn（和迷信区分开）或Mix-in，在笔者当前的理解来看，Mixin的根本目的有三：解耦、灵活、简单。</p><p>从Python的角度看，Mixin和多继承是密不可分的。在讲述Mixin的逻辑前，先来明确两个主体：A类 和 B类，我们假定A类为<em>Mixin类</em>，B类为<em>需要使用A功能的类</em>。</p><p>注意这里说的多继承与多重继承还有些许区别，强调广度而不是深度。</p><p>设计Mixin类的目的首先是为需要功能的类（如B类）提供功能，获得功能的方法就是继承。例如在程序设计中需要设计C类时，先考虑通过多继承来组合多个Mixin类来实现C类的功能，而不是依赖传统的多重继承。这样做有很多好处，例如使类的组织结构扁平化，抛弃了多重继承链的复杂性，以及减少子类功能冗余等等。</p><p>在Vue中同样使用了Mixin的思想，官方的说法是这样的：</p><blockquote><p>Mixin（混入）提供了一种非常灵活的方式，来分发 Vue 组件中的可复用功能。一个混入对象可以包含任意组件选项。当组件使用混入对象时，所有混入对象的选项将被“混合”进入该组件本身的选项。</p></blockquote><p>通俗来讲，Mixin有些像组件的组件，将组件的公共逻辑或者配置提取出来，哪个组件需要用到时，直接将提取的这部分混入到组件内部。</p><p>不难看出，虽然在不同的语言中实现的方式略有不同，但Mixin的基本思想是一致的，Mixin类本质上是一种功能抽象。</p><h2 id="Mixin规范"><a href="#Mixin规范" class="headerlink" title="Mixin规范"></a>Mixin规范</h2><p>基于以上Mixin类的概念，可以发现Mixin类实际上还要遵守一些“隐性”规范，说“隐性”是因为Mixin并不是Python中的一种关键字，很多要求是语义上允许的，但当我们决定使用Mixin时，不得不去思考如何构建一个成功的Mixin模式，于是产生了这些规范。</p><p>引用一个很好的说法：</p><blockquote><p>从某种程度上来说，继承强调 I am，Mixin 强调 I can。</p></blockquote><p>这句话简单明了的说明了Mixin与传统继承的区别：Mixin类直接包含功能的默认实现，虽然使用继承的方式引入，但真正做到开箱即用。</p><p>笔者认为需要再次强调明确一下“继承”（指含义上的继承，并非语义上的继承）和“引入”的概念，假设有两种类：Mixin类和普通类，那么普通类继承普通类是“继承”，普通类继承Mixin类是“引入”，Mixin类继承Mixin类是“继承”，Mixin类继承普通类是不被允许的。</p><p>Mixin类需具备的特征（”隐性“要求）：<br>1、不能单独产生实例，只能作为“功能模块”被其他可产生实例的类引入，因为Mixin类是抽象类。<br>2、不能继承普通类，只能继承其他Mixin类。<br>3、能独立实现功能，仅写出方法内容和方法所需的变量名。<br>4、普通类引入Mixin类时，不覆盖所继承Mixin的属性和方法，不需要调用super()去取Mixin中的方法。<br>5、可被同时继承的Mixin类之间没有重复的方法或属性，因此不用关心继承的顺序，强调隔离性。</p><p>之所以称上述特征为”隐性“要求，还是因为语义上对这些规范都没有明令禁止，但这些特征却实际上是Mixin奏效的灵魂所在。</p><p>举一个Mixin类的例子（取自Flask-Login）：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UserMixin</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    This provides default implementations for the methods </span></span><br><span class="line"><span class="string">    that Flask-Login expects user objects to have.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Python 3 implicitly set __hash__ to None if we override __eq__</span></span><br><span class="line">    <span class="comment"># We set it back to its default implementation</span></span><br><span class="line">    __hash__ = <span class="built_in">object</span>.__hash__</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">is_active</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">is_authenticated</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.is_active</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">is_anonymous</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_id</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">str</span>(self.<span class="built_in">id</span>)</span><br><span class="line">        <span class="keyword">except</span> AttributeError:</span><br><span class="line">            <span class="keyword">raise</span> NotImplementedError(<span class="string">&quot;No `id` attribute - override `get_id`&quot;</span>) <span class="keyword">from</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__eq__</span>(<span class="params">self, other</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Checks the equality of two `UserMixin` objects using `get_id`.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(other, UserMixin):</span><br><span class="line">            <span class="keyword">return</span> self.get_id() == other.get_id()</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">NotImplemented</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__ne__</span>(<span class="params">self, other</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Checks the inequality of two `UserMixin` objects using `get_id`.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        equal = self.__eq__(other)</span><br><span class="line">        <span class="keyword">if</span> equal <span class="keyword">is</span> <span class="literal">NotImplemented</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">NotImplemented</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">not</span> equal</span><br></pre></td></tr></table></figure></p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>关于Mixin，网络上也是众说纷纭，相关的内容有组合模式（Composite Pattern），Duck Typing等等，但笔者认为仅依靠上述解释已经可以初步理解 Mixin的含义 及 Mixin的使用目的。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;在开发程序的过程中，尤其是在使用各个成熟的工具时，经常会接触到&lt;strong&gt;Mixin&lt;/strong&gt;的概念。初见时完全不理解是什么含义，在网上查找了一些资料，总结一下，如有错误请评论指正。&lt;br&gt;</summary>
    
    
    
    
    <category term="Python" scheme="http://silencezheng.top/tags/Python/"/>
    
    <category term="编程思想" scheme="http://silencezheng.top/tags/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/"/>
    
  </entry>
  
  <entry>
    <title>远程调试代码的几种方式</title>
    <link href="http://silencezheng.top/2022/08/16/article55/"/>
    <id>http://silencezheng.top/2022/08/16/article55/</id>
    <published>2022-08-16T14:45:20.000Z</published>
    <updated>2022-08-16T14:50:18.990Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>针对远程运行深度学习程序以及其他代码调试探索了几种方法，依次介绍一下。此次用个人电脑作为服务器实验，Windows系统作为服务器还是比较麻烦的。</p><p>服务器配置要求：SSH、Conda<br><span id="more"></span><br>方式主要有以下几种：<br>1、远程使用jupyter<br>2、Pycharm远程开发<br>3、VSCode远程开发</p><h2 id="远程使用jupyter"><a href="#远程使用jupyter" class="headerlink" title="远程使用jupyter"></a>远程使用jupyter</h2><p>原理：在服务器上运行jupyter，开发机用ssh进行端口映射，在本地浏览器上使用远端的jupyter进行开发。</p><h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><p>1、ssh连接服务器。<br>2、启动jupyter：<br><figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#</span> --<span class="comment">no</span><span class="literal">-</span><span class="comment">browser表示不需要寻找浏览器</span></span><br><span class="line"><span class="comment">jupyter</span> <span class="comment">notebook</span> --<span class="comment">no</span><span class="literal">-</span><span class="comment">browser</span> --<span class="comment">port=6666</span></span><br></pre></td></tr></table></figure><br>启动后会给一个token（口令），后面用的到。<br>3、ssh端口映射：<br><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># -N：ssh没有远程命令需要执行</span></span><br><span class="line"><span class="meta"># -f：ssh在后台执行</span></span><br><span class="line"><span class="meta"># -L：指定本地端口</span></span><br><span class="line"><span class="meta"># -g：允许服务器连接此端口，否则将只允许开发机使用该端口</span></span><br><span class="line">ssh -N -f -g -L <span class="number">6688</span>:localhost:<span class="number">6666</span> username@remote_ip</span><br></pre></td></tr></table></figure><br>输入密码后，本地6688端口会完成映射。<br>4、登录jupyter：<br>浏览器通过地址<code>localhost:6688</code>即可访问服务器jupyter，需要输入token或密码。<br>5、若要取消端口映射：<br>首先查看端口使用情况<code>lsof -i:6688</code>，然后<code>kill 进程ID</code>。<br>6、关闭jupyter：<br>直接在步骤1的连接中<code>ctrl+c</code>即可关闭。</p><h3 id="可能遇到的问题"><a href="#可能遇到的问题" class="headerlink" title="可能遇到的问题"></a>可能遇到的问题</h3><p>1、步骤1连接后，需要先使用conda激活指定的环境，然后在环境下启动jupyter，否则可能发生找不到包的问题。<br>2、当服务器为windows系统时，可以使用<code>Call conda.bat</code>代替<code>conda</code>。<br>3、步骤1连接后，可以先cd移动到想去的目录，再在该目录下启动jupyter。</p><h2 id="Pycharm远程开发"><a href="#Pycharm远程开发" class="headerlink" title="Pycharm远程开发"></a>Pycharm远程开发</h2><p>先说下结论：截至2022.08.16，Pycharm不支持在Windows服务器上进行远程开发。<br>原理：同样是基于SSH，只不过进行了封装。</p><h3 id="步骤："><a href="#步骤：" class="headerlink" title="步骤："></a>步骤：</h3><p>1、到<code>Preferences | Build, Execution, Deployment | Deployment</code> 中添加一个SFTP服务器。测试连接，该服务是可以工作的。<br>2、到<code>Preferences | Project: name | Python Interpreter</code> 中添加一个Python解释器，选择SSH解释器，添加一个SSH配置，下一步填入服务器Python解释器路径以及希望当前项目在服务器上的同步目录，示例如下：<br><figure class="highlight taggerscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">C:<span class="symbol">\u</span>ser<span class="symbol">\A</span>pps<span class="symbol">\A</span>naconda<span class="symbol">\e</span>nvs<span class="symbol">\D</span>eepLearning<span class="symbol">\p</span>ython.exe</span><br><span class="line"></span><br><span class="line">C:<span class="symbol">\u</span>ser<span class="symbol">\P</span>rojs<span class="symbol">\r</span>emote</span><br></pre></td></tr></table></figure></p><p>这里会遇到问题，无法显示服务器文件目录：<br><img src="/assets/post_img/article55/error1.jpg" alt="显示服务器文件目录报错"><br>该报错的官方issue在<a href="https://youtrack.jetbrains.com/issue/PY-38097">这里</a>，民间也有很多<a href="https://bbs.csdn.net/topics/397093590">反馈</a>，目前没有解决办法。<br>3、虽然无法显示文件树，但可以自行填入以上两个目录，这样以后会自动开始refresh skeletons。这里会遇到最致命的问题，报错<code>Couldn&#39;t refresh skeletons for remote interpreter failed to run generator3/__main__.py for sftp...</code>。尝试过删除服务器上的<code>.pycharm_helpers</code>目录，重新添加python解释器，也没办法解决。怀疑是上一个issue的延续，或者是文件编码问题，因为报错的后半段会有乱码。<br>4、Anyway，目前无法解决，Pycharm不能使用Windows服务器远程开发。</p><h2 id="VSCode远程开发"><a href="#VSCode远程开发" class="headerlink" title="VSCode远程开发"></a>VSCode远程开发</h2><p>原理：同上</p><h3 id="步骤-1"><a href="#步骤-1" class="headerlink" title="步骤"></a>步骤</h3><p>1、VSCode安装Remote-SSH、Remote-Containers插件。Remote-SSH负责建立SSH连接、Remote-Containers负责访问服务器目录。<br>2、安装完毕会左侧工具栏会出现图标，点击后如下：<br><img src="/assets/post_img/article55/remote-ssh.jpg" alt="remote-ssh"><br>在此新建一个SSH会话，第一次连接时的信息，会被写入根目录的<code>.ssh/config</code>文件下。<br>3、右击服务器图标即可连接，连接成功后左下角会显示：<br><img src="/assets/post_img/article55/left-corner.jpg" alt="lc"><br>第一次连接成功会先在用户目录下安装<code>vscode-server</code>，大概190MB，形成<code>.vscode-server目录</code>。<br>4、此时左侧文件目录处即为远端服务器目录，可以随心所欲的编辑代码了～<br>5、在服务器上跑代码的话，只需要在服务器上安装对应插件即可，通常训练模型需要Python和Jupyter这两个，安装过后可以自动识别服务器上的conda环境，右上角就可以直接选内核运行了，十分方便。<br>6、第一次连接跑代码时，可能出现绘图不显示的问题，关闭VSCode重新打开就解决了，动图也可以正常显示，效果比直接使用jupyter远程更加好。</p><p>下面是我使用的扩展：<br><img src="/assets/post_img/article55/extensions.jpg" alt="extensions"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇所有测试基于以下平台：<br>开发机：MacOS 12.5<br>服务器：Windows 10 Professional<br>VSCode：1.70.1<br>Pycharm Professional Edition：2021.3.3</p><p>总的来说，目前使用VSCode进行远程开发与训练效果最好。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;针对远程运行深度学习程序以及其他代码调试探索了几种方法，依次介绍一下。此次用个人电脑作为服务器实验，Windows系统作为服务器还是比较麻烦的。&lt;/p&gt;
&lt;p&gt;服务器配置要求：SSH、Conda&lt;br&gt;</summary>
    
    
    
    
    <category term="VSCode" scheme="http://silencezheng.top/tags/VSCode/"/>
    
  </entry>
  
  <entry>
    <title>现代卷积神经网络--《动手学深度学习》笔记0x08</title>
    <link href="http://silencezheng.top/2022/08/06/article54/"/>
    <id>http://silencezheng.top/2022/08/06/article54/</id>
    <published>2022-08-05T18:43:56.000Z</published>
    <updated>2022-08-11T05:45:55.323Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><p>介绍现代的卷积神经网络架构，本章中的每一个模型都曾一度占据主导地位，其中许多模型都是ImageNet竞赛的优胜者。ImageNet竞赛自2010年以来，一直是计算机视觉中监督学习进展的指向标。</p><span id="more"></span><p>这些模型包括：<br>AlexNet，第一个在大规模视觉竞赛中击败传统计算机视觉模型的大型神经网络；</p><p>使用重复块的网络（VGG），它利用许多重复的神经网络块；</p><p>网络中的网络（NiN），它重复使用由卷积层和$1 \times 1$卷积层（用来代替全连接层）来构建深层网络;</p><p>含并行连结的网络（GoogLeNet），它使用并行连结的网络，通过不同窗口大小的卷积层和最大池化层来并行抽取信息；</p><p>残差网络（ResNet），它通过残差块构建跨层的数据通道，是计算机视觉中最流行的体系架构；</p><p>稠密连接网络（DenseNet），它的计算成本很高，但带来了更好的效果。</p><p>虽然深度神经网络的概念非常简单——将神经网络堆叠在一起。但由于不同的网络架构和超参数选择，这些神经网络的性能会发生很大变化。 本章介绍的神经网络是将人类直觉和相关数学见解结合后，经过大量研究试错后的结晶。 按时间顺序介绍这些模型是很好的，能够加深对模型是如何被创造出来的理解。 例如，批量规范化（batch normalization）和残差网络（ResNet）为设计和训练深度神经网络提供了重要思想指导。</p><p>顺便一提，本章的训练用M1芯片算起来费劲😄。</p><p>对应实践：<a href="https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x08.ipynb">https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x08.ipynb</a></p><h3 id="0-1-结论"><a href="#0-1-结论" class="headerlink" title="0.1. 结论"></a>0.1. 结论</h3><ul><li>AlexNet的架构与LeNet相似，但使用了更多的卷积层和更多的参数来拟合大规模的ImageNet数据集。</li><li>今天，AlexNet已经被更有效的架构所超越，但它是从浅层网络到深层网络的关键一步。</li><li>尽管AlexNet的代码只比LeNet多出几行，但学术界花了很多年才接受深度学习这一概念，并应用其出色的实验结果。这也是由于缺乏有效的计算工具</li><li>Dropout、ReLU和预处理是提升计算机视觉任务性能的其他关键步骤。</li><li>VGG-11使用可复用的卷积块构造网络。不同的VGG模型可通过每个块中卷积层数量和输出通道数量的差异来定义。</li><li>块的使用导致网络定义的非常简洁。使用块可以有效地设计复杂的网络。</li><li>在VGG论文中，Simonyan和Ziserman尝试了各种架构。特别是他们发现深层且窄的卷积（即$3\times3$）比较浅层且宽的卷积更有效。</li><li>NiN使用由一个$1 \times 1$卷积层和多个卷积层组成的块。该块可以在卷积神经网络中使用，以允许更多的每像素非线性。</li><li>NiN去除了容易造成过拟合的全连接层，将它们替换为全局平均池化层（即在所有位置上进行求和）。该池化层通道数量为所需的输出数量（例如，Fashion-MNIST的输出为10）。</li><li>移除全连接层可减少过拟合，同时显著减少NiN的参数。</li><li>NiN的设计影响了许多后续卷积神经网络的设计。</li><li>Inception块相当于一个有4条路径的子网络。它通过不同窗口形状的卷积层和最大池化层来并行抽取信息，并使用$1 \times 1$卷积层减少每像素级别上的通道维数从而降低模型复杂度。</li><li>GoogLeNet将多个设计精细的Inception块与其他层（卷积层、全连接层）串联起来。其中Inception块的通道数分配之比是在ImageNet数据集上通过大量的实验得来的。</li><li>GoogLeNet和它的后继者们一度是ImageNet上最有效的模型之一：它以较低的计算复杂度提供了类似的测试精度。</li><li>在模型训练过程中，批量规范化利用小批量的均值和标准差，不断调整神经网络的中间输出，使整个神经网络各层的中间输出值更加稳定。</li><li>批量规范化在全连接层和卷积层的使用略有不同。</li><li>批量规范化层和暂退层一样，在训练模式和预测模式下计算不同。</li><li>批量规范化有许多有益的副作用，主要是正则化。另一方面，”减少内部协变量偏移“的原始动机似乎不是一个有效的解释。</li><li>学习嵌套函数（nested function）是训练神经网络的理想情况。在深层神经网络中，学习另一层作为恒等映射（identity function）较容易（尽管这是一个极端情况）。</li><li>残差映射可以更容易地学习同一函数，例如将权重层中的参数近似为零。</li><li>利用残差块（residual blocks）可以训练出一个有效的深层神经网络：输入可以通过层间的残余连接更快地向前传播。</li><li>残差网络（ResNet）对随后的深层神经网络设计产生了深远影响。</li><li>在跨层连接上，不同于ResNet中将输入与输出相加，稠密连接网络（DenseNet）在通道维上连结输入与输出。</li><li>DenseNet的主要构建模块是稠密块和过渡层。</li><li>在构建DenseNet时，需要通过添加过渡层来控制网络的维数，从而再次减少通道的数量。</li></ul><h2 id="1-深度卷积神经网络（AlexNet）"><a href="#1-深度卷积神经网络（AlexNet）" class="headerlink" title="1. 深度卷积神经网络（AlexNet）"></a>1. 深度卷积神经网络（AlexNet）</h2><p>事实上，在上世纪90年代初到2012年之间的大部分时间里，神经网络往往被其他机器学习方法超越，如支持向量机（support vector machines）。</p><p>在计算机视觉中，直接将神经网络与其他机器学习方法进行比较也许不公平。这是因为，卷积神经网络的输入是由原始像素值或是经过简单预处理（例如居中、缩放）的像素值组成的。但在使用传统机器学习方法时，从业者永远不会将原始像素作为输入。在传统机器学习方法中，计算机视觉流水线是由经过人的手工精心设计的特征流水线组成的。对于这些传统方法，大部分的进展都来自于对特征有了更聪明的想法，并且学习到的算法往往归于事后的解释。</p><p>虽然上世纪90年代就有了一些神经网络加速卡，但仅靠它们还不足以开发出有大量参数的深层多通道多层卷积神经网络。此外，当时的数据集仍然相对较小。除了这些障碍，训练神经网络的一些关键技巧仍然缺失，包括启发式参数初始化、随机梯度下降的变体、非挤压激活函数和有效的正则化技术。</p><p>因此，与训练<em>端到端</em>（从像素到分类结果）系统不同，经典机器学习的流水线看起来更像下面这样：</p><ol><li>获取一个有趣的数据集。</li><li>根据光学、几何学、其他知识以及偶然的发现，手工对特征数据集进行预处理。</li><li>通过标准的特征提取算法，如SIFT（尺度不变特征变换）和SURF（加速鲁棒特征）或其他手动调整的流水线来输入数据。</li><li>将提取的特征送入最喜欢的分类器中（例如线性模型或其它核方法），以训练分类器。</li></ol><p>机器学习研究人员相信机器学习既重要又美丽：优雅的理论去证明各种模型的性质。机器学习是一个正在蓬勃发展、严谨且非常有用的领域。然而计算机视觉研究人员会告诉你图像识别的诡异事实: <em>推动领域进步的是数据特征，而不是学习算法</em>。计算机视觉研究人员相信，从对最终模型精度的影响来说，更大或更干净的数据集、或是稍微改进的特征提取，比任何学习算法带来的进步要大得多。</p><h3 id="1-1-学习表征"><a href="#1-1-学习表征" class="headerlink" title="1.1. 学习表征"></a>1.1. 学习表征</h3><p>在2012年前，图像特征都是机械地计算出来的。事实上，设计一套新的特征函数、改进结果，并撰写论文是盛极一时的潮流。SIFT、SURF、HOG（定向梯度直方图）、<a href="https://en.wikipedia.org/wiki/Bag-of-words_model_in_computer_vision">bags of visual words</a>和类似的特征提取方法占据了主导地位。</p><p>深度学习从业人员想法则与众不同：他们认为特征本身应该被学习。此外，他们还认为，在合理地复杂性前提下，特征应该由多个共同学习的神经网络层组成，每个层都有可学习的参数。在机器视觉中，最底层可能检测边缘、颜色和纹理。事实上，Alex Krizhevsky、Ilya Sutskever和Geoff Hinton提出了一种新的卷积神经网络变体<em>AlexNet</em>。在2012年ImageNet挑战赛中取得了轰动一时的成绩。AlexNet以Alex Krizhevsky的名字命名，他是AlexNet论文的第一作者。</p><p>有趣的是，在网络的最底层，模型学习到了一些类似于传统滤波器的特征抽取器。 下图是从AlexNet论文复制的，描述了底层图像特征。<br><img src="/assets/post_img/article54/filters.png" alt="AlexNet第一层学习到的特征抽取器"></p><p>AlexNet的更高层建立在这些底层表示的基础上，以表示更大的特征，如眼睛、鼻子、草叶等等。而更高的层可以检测整个物体，如人、飞机、狗或飞盘。最终的隐藏神经元可以学习图像的综合表示，从而使属于不同类别的数据易于区分。尽管一直有一群执着的研究者不断钻研，试图学习视觉数据的逐级表征，然而很长一段时间里这些尝试都未有突破。深度卷积神经网络的突破出现在2012年。突破可归因于两个关键因素：<em>数据</em> 和 <em>硬件</em></p><h4 id="1-1-1-数据"><a href="#1-1-1-数据" class="headerlink" title="1.1.1. 数据"></a>1.1.1. 数据</h4><p>包含许多特征的深度模型需要大量的有标签数据，才能显著优于基于凸优化的传统方法（如线性方法和核方法）。<br>然而，限于早期计算机有限的存储和90年代有限的研究预算，大部分研究只基于小的公开数据集。例如，不少研究论文基于加州大学欧文分校（UCI）提供的若干个公开数据集，其中许多数据集只有几百至几千张在非自然环境下以低分辨率拍摄的图像。这一状况在2010年前后兴起的大数据浪潮中得到改善。<br>2009年，ImageNet数据集发布，并发起ImageNet挑战赛：要求研究人员从100万个样本中训练模型，以区分1000个不同类别的对象。ImageNet数据集由斯坦福教授李飞飞小组的研究人员开发，利用谷歌图像搜索（Google Image Search）对每一类图像进行预筛选，并利用亚马逊众包（Amazon Mechanical Turk）来标注每张图片的相关类别。这种规模是前所未有的。这项被称为ImageNet的挑战赛推动了计算机视觉和机器学习研究的发展，挑战研究人员确定哪些模型能够在更大的数据规模下表现最好。</p><h4 id="1-1-2-硬件"><a href="#1-1-2-硬件" class="headerlink" title="1.1.2. 硬件"></a>1.1.2. 硬件</h4><p>深度学习对计算资源要求很高，训练可能需要数百个迭代轮数，每次迭代都需要通过代价高昂的许多线性代数层传递数据。这也是为什么在20世纪90年代至21世纪初，优化凸目标的简单算法是研究人员的首选。然而，用GPU训练神经网络改变了这一格局。<em>图形处理器</em>（Graphics Processing Unit，GPU）早年用来加速图形处理，使电脑游戏玩家受益。GPU可优化高吞吐量的$4 \times 4$矩阵和向量乘法，从而服务于基本的图形任务。这些数学运算与卷积层的计算惊人地相似！由此，英伟达（NVIDIA）和ATI已经开始为通用计算操作优化gpu，甚至把它们作为<em>通用GPU</em>（general-purpose GPUs，GPGPU）来销售。</p><p>在解释GPU比CPU“强”之前，先来深度理解一下中央处理器（Central Processing Unit，CPU）的<em>核心</em>。<br>CPU的每个核心都拥有高时钟频率的运行能力，和高达数MB的三级缓存（L3Cache）。它们非常适合执行各种指令，具有分支预测器、深层流水线和其他使CPU能够运行各种程序的功能。<br>然而，这种明显的优势也是它的致命弱点：通用核心的制造成本非常高。<br>它们需要大量的芯片面积、复杂的支持结构（内存接口、内核之间的缓存逻辑、高速互连等等），而且它们在任何单个任务上的性能都相对较差。</p><p>相比于CPU，GPU由$100 \sim 1000$个小的处理单元组成（NVIDIA、ATI、ARM和其他芯片供应商之间的细节稍有不同），通常被分成更大的组（NVIDIA称之为warps）。<br>虽然每个GPU核心都相对较弱，有时甚至以低于1GHz的时钟频率运行，但庞大的核心数量使GPU比CPU快几个数量级。<br>例如，NVIDIA最近一代的Ampere GPU架构为每个芯片提供了高达312 TFlops的浮点性能，而CPU的浮点性能到目前为止还没有超过1 TFlops。<br>之所以有如此大的差距，原因其实很简单：首先，功耗往往会随时钟频率呈二次方增长。对于一个CPU核心，假设它的运行速度比GPU快4倍，你可以使用16个GPU内核取代，那么GPU的综合性能就是CPU的$16 \times 1/4 = 4$倍。其次，GPU内核要简单得多，这使得它们更节能。同时深度学习中的许多操作需要相对较高的内存带宽，而GPU拥有10倍于CPU的带宽。</p><p>回到2012年的重大突破，当Alex Krizhevsky和Ilya Sutskever实现了可以在GPU硬件上运行的深度卷积神经网络时，一个重大突破出现了。他们意识到卷积神经网络中的计算瓶颈：卷积和矩阵乘法，都是可以在硬件上并行化的操作。于是，他们使用两个显存为3GB的NVIDIA GTX580 GPU实现了快速卷积运算。他们的创新<a href="https://code.google.com/archive/p/cuda-convnet/">cuda-convnet</a>几年来它一直是行业标准，并推动了深度学习热潮。</p><h3 id="1-2-AlexNet"><a href="#1-2-AlexNet" class="headerlink" title="1.2. AlexNet"></a>1.2. AlexNet</h3><p>2012年，AlexNet横空出世。它首次证明了学习到的特征可以超越手工设计的特征，一举打破了计算机视觉研究的现状。<br>AlexNet使用了8层卷积神经网络，并以很大的优势赢得了2012年ImageNet图像识别挑战赛。</p><p>AlexNet和LeNet的架构非常相似，如下图所示。<br>注意，这里我们提供了一个稍微精简版本的AlexNet，去除了当年需要两个小型GPU同时运算的设计特点。</p><p><img src="/assets/post_img/article54/alexnet.svg" alt="从LeNet（左）到AlexNet（右）"></p><p>AlexNet和LeNet的设计理念非常相似，但也存在显著差异。<br>首先，AlexNet比相对较小的LeNet5要深得多。AlexNet由八层组成：五个卷积层、两个全连接隐藏层和一个全连接输出层。<br>其次，AlexNet使用ReLU而不是sigmoid作为其激活函数。</p><p>下面深入研究AlexNet的细节。</p><h4 id="1-2-1-模型设计"><a href="#1-2-1-模型设计" class="headerlink" title="1.2.1. 模型设计"></a>1.2.1. 模型设计</h4><p>在AlexNet的第一层，卷积窗口的形状是$11\times11$。由于ImageNet中大多数图像的宽和高比MNIST图像的多10倍以上，因此，需要一个更大的卷积窗口来捕获目标。<br>第二层中的卷积窗口形状被缩减为$5\times5$，然后是$3\times3$。<br>此外，在第一层、第二层和第五层卷积层之后，加入窗口形状为$3\times3$、步幅为2的最大池化层。而且，AlexNet的卷积通道数目是LeNet的10倍。</p><p>在最后一个卷积层后有两个全连接层，分别有4096个输出。这两个巨大的全连接层拥有将近1GB的模型参数。由于早期GPU显存有限，原版的AlexNet采用了双数据流设计，使得每个GPU只负责存储和计算模型的一半参数。<br>现在GPU显存相对充裕，所以现在很少需要跨GPU分解模型（因此这里的AlexNet模型在这方面与原始论文稍有不同）。</p><h4 id="1-2-2-激活函数"><a href="#1-2-2-激活函数" class="headerlink" title="1.2.2. 激活函数"></a>1.2.2. 激活函数</h4><p>此外，AlexNet将sigmoid激活函数改为更简单的ReLU激活函数。一方面，ReLU激活函数的计算更简单，它不需要如sigmoid激活函数那般复杂的求幂运算。另一方面，当使用不同的参数初始化方法时，ReLU激活函数使训练模型更加容易。<br>当sigmoid激活函数的输出非常接近于0或1时，这些区域的梯度几乎为0，因此反向传播无法继续更新一些模型参数。相反，ReLU激活函数在正区间的梯度总是1。<br>也就是说，如果模型参数没有正确初始化，sigmoid函数可能在正区间内得到几乎为0的梯度，从而使模型无法得到有效的训练。</p><h4 id="1-2-3-容量控制和预处理"><a href="#1-2-3-容量控制和预处理" class="headerlink" title="1.2.3. 容量控制和预处理"></a>1.2.3. 容量控制和预处理</h4><p>AlexNet通过暂退法控制全连接层的模型复杂度，而LeNet只使用了权重衰减。 为了进一步扩充数据，AlexNet在训练时增加了大量的图像增强数据，如翻转、裁切和变色。 这使得模型更健壮，更大的样本量有效地减少了过拟合。有关数据扩增的内容会在<em>计算机视觉</em>章节中讲到。</p><p>下面来构造一下AlexNet：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    <span class="comment"># 使用一个11*11的更大卷积窗口来捕捉对象。</span></span><br><span class="line">    <span class="comment"># 同时，步幅为4，以减少输出的高度和宽度。</span></span><br><span class="line">    <span class="comment"># 输出通道的数目（96）远大于LeNet</span></span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">96</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    <span class="comment"># 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数</span></span><br><span class="line">    nn.Conv2d(<span class="number">96</span>, <span class="number">256</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    <span class="comment"># 使用三个连续的卷积层和较小的卷积窗口。</span></span><br><span class="line">    <span class="comment"># 除了最后的卷积层，输出通道的数量进一步增加。</span></span><br><span class="line">    <span class="comment"># 在前两个卷积层之后，池化层不用于减少输入的高度和宽度</span></span><br><span class="line">    nn.Conv2d(<span class="number">256</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">384</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">384</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    <span class="comment"># 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合</span></span><br><span class="line">    nn.Linear(<span class="number">6400</span>, <span class="number">4096</span>), nn.ReLU(),</span><br><span class="line">    nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">    nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>), nn.ReLU(),</span><br><span class="line">    nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">    <span class="comment"># 最后是输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000</span></span><br><span class="line">    nn.Linear(<span class="number">4096</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure><br>构造一个高度和宽度都为224的单通道数据，来观察每一层输出的形状。 它与前面图的AlexNet架构相匹配。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">X = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X=layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__,<span class="string">&#x27;output shape:\t&#x27;</span>,X.shape)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">Conv2d output shape:         torch.Size([<span class="number">1</span>, <span class="number">96</span>, <span class="number">54</span>, <span class="number">54</span>])</span><br><span class="line">ReLU output shape:   torch.Size([<span class="number">1</span>, <span class="number">96</span>, <span class="number">54</span>, <span class="number">54</span>])</span><br><span class="line">MaxPool2d output shape:      torch.Size([<span class="number">1</span>, <span class="number">96</span>, <span class="number">26</span>, <span class="number">26</span>])</span><br><span class="line">Conv2d output shape:         torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">26</span>, <span class="number">26</span>])</span><br><span class="line">ReLU output shape:   torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">26</span>, <span class="number">26</span>])</span><br><span class="line">MaxPool2d output shape:      torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br><span class="line">Conv2d output shape:         torch.Size([<span class="number">1</span>, <span class="number">384</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br><span class="line">ReLU output shape:   torch.Size([<span class="number">1</span>, <span class="number">384</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br><span class="line">Conv2d output shape:         torch.Size([<span class="number">1</span>, <span class="number">384</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br><span class="line">ReLU output shape:   torch.Size([<span class="number">1</span>, <span class="number">384</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br><span class="line">Conv2d output shape:         torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br><span class="line">ReLU output shape:   torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br><span class="line">MaxPool2d output shape:      torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">5</span>, <span class="number">5</span>])</span><br><span class="line">Flatten output shape:        torch.Size([<span class="number">1</span>, <span class="number">6400</span>])</span><br><span class="line">Linear output shape:         torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">ReLU output shape:   torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">Dropout output shape:        torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">Linear output shape:         torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">ReLU output shape:   torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">Dropout output shape:        torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">Linear output shape:         torch.Size([<span class="number">1</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure></p><h3 id="1-3-读取数据集"><a href="#1-3-读取数据集" class="headerlink" title="1.3. 读取数据集"></a>1.3. 读取数据集</h3><p>在这里使用的是Fashion-MNIST数据集。因为即使在现代GPU上，训练ImageNet模型，同时使其收敛可能需要数小时或数天的时间。<br>将AlexNet直接应用于Fashion-MNIST的一个问题是，Fashion-MNIST图像的分辨率（$28 \times 28$像素）低于ImageNet图像。为了解决这个问题，这里将它们增加到$224 \times 224$（通常来讲这不是一个明智的做法，只是为了匹配AlexNet）。使用<code>d2l.load_data_fashion_mnist</code>函数中的<code>resize</code>参数执行此调整。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">128</span></span><br><span class="line"><span class="comment"># d2l包中的函数，省略其实现，后续训练过程同上</span></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=<span class="number">224</span>)</span><br></pre></td></tr></table></figure></p><h3 id="1-4-训练AlexNet"><a href="#1-4-训练AlexNet" class="headerlink" title="1.4. 训练AlexNet"></a>1.4. 训练AlexNet</h3><p>与LeNet相比，这里的主要变化是使用更小的学习速率训练，这是因为网络更深更广、图像分辨率更高，训练卷积神经网络就更昂贵。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lr, num_epochs = <span class="number">0.01</span>, <span class="number">10</span></span><br><span class="line"><span class="comment"># d2l包中的函数，省略其实现，后续训练过程同上</span></span><br><span class="line"><span class="comment"># 修改设备为mps，适配M1芯片设备</span></span><br><span class="line">train_ch6(net, train_iter, test_iter, num_epochs, lr, torch.device(<span class="string">&#x27;mps&#x27;</span>))</span><br></pre></td></tr></table></figure></p><h2 id="2-使用块的网络（VGG）"><a href="#2-使用块的网络（VGG）" class="headerlink" title="2. 使用块的网络（VGG）"></a>2. 使用块的网络（VGG）</h2><p>虽然AlexNet证明深层神经网络卓有成效，但它没有提供一个通用的模板来指导后续的研究人员设计新的网络。 下面将介绍一些常用于设计深层神经网络的启发式概念。</p><p>与芯片设计中工程师从放置晶体管到逻辑元件再到逻辑块的过程类似，神经网络架构的设计也逐渐变得更加抽象。研究人员开始从单个神经元的角度思考问题，发展到整个层，现在又转向块，重复层的模式。</p><p>使用块的想法首先出现在牛津大学的视觉几何组（visualgeometry group）的VGG网络中。通过使用循环和子程序，可以很容易地在任何现代深度学习框架的代码中实现这些重复的架构。</p><h3 id="2-1-VGG块"><a href="#2-1-VGG块" class="headerlink" title="2.1. VGG块"></a>2.1. VGG块</h3><p>经典卷积神经网络的基本组成部分是下面的这个序列：</p><ol><li>带填充以保持分辨率的卷积层；</li><li>非线性激活函数，如ReLU；</li><li>池化层，如最大池化层。</li></ol><p>而一个VGG块与之类似，由一系列卷积层组成，后面再加上用于空间下采样（缩小图像）的最大池化层。在最初的VGG论文中，作者使用了带有$3 \times 3$卷积核、填充为1（保持高度和宽度）的卷积层，和带有$2 \times 2$池化窗口、步幅为2（每个块后的分辨率减半）的最大池化层。在下面的代码中定义了一个名为<code>vgg_block</code>的函数来实现一个VGG块。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 该函数有三个参数，分别对应于卷积层的数量num_convs、</span></span><br><span class="line"><span class="comment"># 输入通道的数量in_channels 和输出通道的数量out_channels.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vgg_block</span>(<span class="params">num_convs, in_channels, out_channels</span>):</span></span><br><span class="line">    layers = []</span><br><span class="line">    <span class="comment"># for _ in range(n) 一般仅仅用于循环n次，不用设置变量，用 _ 指代临时变量，只在这个语句中使用一次。</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_convs):</span><br><span class="line">        layers.append(nn.Conv2d(in_channels, out_channels,</span><br><span class="line">                                kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">        layers.append(nn.ReLU())</span><br><span class="line">        in_channels = out_channels</span><br><span class="line">    layers.append(nn.MaxPool2d(kernel_size=<span class="number">2</span>,stride=<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*layers)</span><br></pre></td></tr></table></figure></p><h3 id="2-2-VGG网络"><a href="#2-2-VGG网络" class="headerlink" title="2.2. VGG网络"></a>2.2. VGG网络</h3><p>与AlexNet、LeNet一样，VGG网络可以分为两部分：第一部分主要由卷积层和池化层组成，第二部分由全连接层组成。</p><p><img src="/assets/post_img/article54/vgg.svg" alt="从AlexNet到VGG，本质上都是块设计"></p><p>VGG神经网络连接上图中的几个VGG块。其中有超参数变量<code>conv_arch</code>。该变量指定了每个VGG块里卷积层个数和输出通道数。全连接模块则与AlexNet中的相同。</p><p>原始VGG网络有5个卷积块，其中前两个块各有一个卷积层，后三个块各包含两个卷积层。第一个模块有64个输出通道，每个后续模块将输出通道数量翻倍，直到该数字达到512。由于该网络使用8个卷积层和3个全连接层，因此它通常被称为VGG-11。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conv_arch = ((<span class="number">1</span>, <span class="number">64</span>), (<span class="number">1</span>, <span class="number">128</span>), (<span class="number">2</span>, <span class="number">256</span>), (<span class="number">2</span>, <span class="number">512</span>), (<span class="number">2</span>, <span class="number">512</span>))</span><br></pre></td></tr></table></figure></p><p>下面的代码实现了VGG-11。可以通过在conv_arch上执行for循环来简单实现。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vgg</span>(<span class="params">conv_arch</span>):</span></span><br><span class="line">    conv_blks = []</span><br><span class="line">    in_channels = <span class="number">1</span></span><br><span class="line">    <span class="comment"># 卷积层部分</span></span><br><span class="line">    <span class="keyword">for</span> (num_convs, out_channels) <span class="keyword">in</span> conv_arch:</span><br><span class="line">        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))</span><br><span class="line">        in_channels = out_channels</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        <span class="comment"># 星号变量</span></span><br><span class="line">        *conv_blks, nn.Flatten(),</span><br><span class="line">        <span class="comment"># 全连接层部分</span></span><br><span class="line">        nn.Linear(out_channels * <span class="number">7</span> * <span class="number">7</span>, <span class="number">4096</span>), nn.ReLU(), nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">        nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>), nn.ReLU(), nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">        nn.Linear(<span class="number">4096</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">net = vgg(conv_arch)</span><br></pre></td></tr></table></figure></p><p>构建一个高度和宽度为224的单通道数据样本，以观察每个层输出的形状。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">X = torch.randn(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line"><span class="keyword">for</span> blk <span class="keyword">in</span> net:</span><br><span class="line">    X = blk(X)</span><br><span class="line">    <span class="built_in">print</span>(blk.__class__.__name__,<span class="string">&#x27;output shape:\t&#x27;</span>,X.shape)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">64</span>, <span class="number">112</span>, <span class="number">112</span>])</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">128</span>, <span class="number">56</span>, <span class="number">56</span>])</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">512</span>, <span class="number">14</span>, <span class="number">14</span>])</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">512</span>, <span class="number">7</span>, <span class="number">7</span>])</span><br><span class="line">Flatten output shape:        torch.Size([<span class="number">1</span>, <span class="number">25088</span>])</span><br><span class="line">Linear output shape:         torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">ReLU output shape:   torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">Dropout output shape:        torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">Linear output shape:         torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">ReLU output shape:   torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">Dropout output shape:        torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">Linear output shape:         torch.Size([<span class="number">1</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure></p><p>在每个块的高度和宽度减半，最终高度和宽度都为7。最后再展平表示，送入全连接层处理。</p><h3 id="2-3-训练模型"><a href="#2-3-训练模型" class="headerlink" title="2.3. 训练模型"></a>2.3. 训练模型</h3><p>VGG-11比AlexNet计算量更大，因此构建了一个通道数较少的网络，足够用于训练Fashion-MNIST数据集。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ratio = <span class="number">4</span></span><br><span class="line">small_conv_arch = [(pair[<span class="number">0</span>], pair[<span class="number">1</span>] // ratio) <span class="keyword">for</span> pair <span class="keyword">in</span> conv_arch]</span><br><span class="line">net = vgg(small_conv_arch)</span><br></pre></td></tr></table></figure></p><p>除了使用略高的学习率外，模型训练过程与之前的AlexNet类似。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lr, num_epochs, batch_size = <span class="number">0.05</span>, <span class="number">10</span>, <span class="number">128</span></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=<span class="number">224</span>)</span><br><span class="line">train_ch6(net, train_iter, test_iter, num_epochs, lr, torch.device(<span class="string">&#x27;mps&#x27;</span>))</span><br></pre></td></tr></table></figure></p><h2 id="3-网络中的网络（NiN）"><a href="#3-网络中的网络（NiN）" class="headerlink" title="3. 网络中的网络（NiN）"></a>3. 网络中的网络（NiN）</h2><p>LeNet、AlexNet和VGG都有一个共同的设计模式：通过一系列的卷积层与池化层来提取空间结构特征；然后通过全连接层对特征的表征进行处理。 AlexNet和VGG对LeNet的改进主要在于如何扩大和加深这两个模块。 然而如果在网络的早期使用全连接层，则可能会完全放弃表征的空间结构。 网络中的网络（NiN）提供了一个非常简单的解决方案：在每个像素的通道上分别使用多层感知机。</p><h3 id="3-1-NiN块"><a href="#3-1-NiN块" class="headerlink" title="3.1. NiN块"></a>3.1. NiN块</h3><p>卷积层的输入和输出由四维张量组成，张量的每个轴分别对应样本、通道、高度和宽度。另外，全连接层的输入和输出通常是分别对应于样本和特征的二维张量。</p><p>NiN的想法是在每个像素位置（针对每个高度和宽度）应用一个全连接层。如果将权重连接到每个空间位置，则可以将其视为$1 \times 1$卷积层，或作为在每个像素位置上独立作用的全连接层。从另一个角度看，即将空间维度中的每个像素视为单个样本，将通道维度视为不同特征。</p><p>NiN块以一个普通卷积层开始，后面是两个$1 \times 1$的卷积层。这两个$1 \times 1$卷积层充当带有ReLU激活函数的逐像素全连接层。第一层的卷积窗口形状通常由用户设置。随后的卷积窗口形状固定为$1 \times 1$。</p><p><img src="/assets/post_img/article54/nin.svg" alt="VGG和NiN及他们的块之间主要架构差异"></p><p>NiN块函数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nin_block</span>(<span class="params">in_channels, out_channels, kernel_size, strides, padding</span>):</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">        nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>), nn.ReLU())</span><br></pre></td></tr></table></figure></p><h3 id="3-2-NiN模型"><a href="#3-2-NiN模型" class="headerlink" title="3.2. NiN模型"></a>3.2. NiN模型</h3><p>最初的NiN网络从AlexNet中得到了一些启示。NiN使用窗口形状为$11\times11$、$5\times5$和$3\times3$的卷积层，输出通道数量与AlexNet中的相同。每个NiN块后有一个最大池化层，池化窗口形状为$3\times3$，步幅为2。</p><p>NiN和AlexNet之间的一个显著区别是NiN完全取消了全连接层。相反，NiN使用一个NiN块，其输出通道数等于标签类别的数量。最后放一个<em>全局平均池化层</em>（global average pooling layer），生成一个对数几率 （logits）。NiN设计的一个优点是，它显著减少了模型所需参数的数量。然而在实践中，这种设计有时会增加训练模型的时间。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    nin_block(<span class="number">1</span>, <span class="number">96</span>, kernel_size=<span class="number">11</span>, strides=<span class="number">4</span>, padding=<span class="number">0</span>),</span><br><span class="line">    <span class="comment"># 最大池化，第一个参数是池化窗口的大小</span></span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">96</span>, <span class="number">256</span>, kernel_size=<span class="number">5</span>, strides=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">256</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">    <span class="comment"># 标签类别数是10</span></span><br><span class="line">    nin_block(<span class="number">384</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    <span class="comment"># 自适应平均池化，参数为输出的形状</span></span><br><span class="line">    nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">    <span class="comment"># 将四维的输出转成二维的输出，其形状为(批量大小,10)</span></span><br><span class="line">    nn.Flatten())</span><br></pre></td></tr></table></figure></p><p>创建一个数据样本来查看每个块的输出形状：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__,<span class="string">&#x27;output shape:\t&#x27;</span>, X.shape)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">96</span>, <span class="number">54</span>, <span class="number">54</span>])</span><br><span class="line">MaxPool2d output shape:      torch.Size([<span class="number">1</span>, <span class="number">96</span>, <span class="number">26</span>, <span class="number">26</span>])</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">26</span>, <span class="number">26</span>])</span><br><span class="line">MaxPool2d output shape:      torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">384</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br><span class="line">MaxPool2d output shape:      torch.Size([<span class="number">1</span>, <span class="number">384</span>, <span class="number">5</span>, <span class="number">5</span>])</span><br><span class="line">Dropout output shape:        torch.Size([<span class="number">1</span>, <span class="number">384</span>, <span class="number">5</span>, <span class="number">5</span>])</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">10</span>, <span class="number">5</span>, <span class="number">5</span>])</span><br><span class="line">AdaptiveAvgPool2d output shape:      torch.Size([<span class="number">1</span>, <span class="number">10</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">Flatten output shape:        torch.Size([<span class="number">1</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure></p><h3 id="3-3-训练模型"><a href="#3-3-训练模型" class="headerlink" title="3.3. 训练模型"></a>3.3. 训练模型</h3><p>使用Fashion-MNIST来训练模型。训练NiN与训练AlexNet、VGG时相似。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lr, num_epochs, batch_size = <span class="number">0.1</span>, <span class="number">10</span>, <span class="number">128</span></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=<span class="number">224</span>)</span><br><span class="line">train_ch6(net, train_iter, test_iter, num_epochs, lr, torch.device(<span class="string">&#x27;mps&#x27;</span>))</span><br></pre></td></tr></table></figure></p><h2 id="4-含并行连结的网络（GoogLeNet）"><a href="#4-含并行连结的网络（GoogLeNet）" class="headerlink" title="4. 含并行连结的网络（GoogLeNet）"></a>4. 含并行连结的网络（GoogLeNet）</h2><p>在2014年的ImageNet图像识别挑战赛中，一个名叫<em>GoogLeNet</em>的网络架构大放异彩。GoogLeNet吸收了NiN中串联网络的思想，并在此基础上做了改进。这篇论文的一个重点是解决了什么样大小的卷积核最合适的问题。<br>毕竟，以前流行的网络使用小到$1\times1$，大到$11\times11$的卷积核。该文的一个观点是，有时使用不同大小的卷积核组合是有利的。本节将介绍一个稍微简化的GoogLeNet版本：省略了一些为稳定训练而添加的特殊特性，现在有了更好的训练方法，这些特性不是必要的。</p><h3 id="4-1-Inception块"><a href="#4-1-Inception块" class="headerlink" title="4.1. Inception块"></a>4.1. Inception块</h3><p>在GoogLeNet中，基本的卷积块被称为<em>Inception块</em>（Inception block）。这很可能得名于电影《盗梦空间》（Inception），因为电影中的一句话“我们需要走得更深”（“We need to go deeper”）。</p><p><img src="/assets/post_img/article54/inception.svg" alt="Inception块的架构"></p><p>如上图所示，Inception块由四条并行路径组成。前三条路径使用窗口大小为$1\times1$、$3\times3$和$5\times5$的卷积层，从不同空间大小中提取信息。中间的两条路径在输入上执行$1\times1$卷积，以减少通道数，从而降低模型的复杂性。第四条路径使用$3\times3$最大池化层，然后使用$1\times1$卷积层来改变通道数。</p><p>这四条路径都使用合适的填充来使输入与输出的高和宽一致，最后会将每条线路的输出在通道维度上连结，并构成Inception块的输出。在Inception块中，通常调整的超参数是<em>每层输出通道数</em>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Inception</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="comment"># c1--c4是每条路径的输出通道数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels, c1, c2, c3, c4, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Inception, self).__init__(**kwargs)</span><br><span class="line">        <span class="comment"># 线路1，单1x1卷积层</span></span><br><span class="line">        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 线路2，1x1卷积层后接3x3卷积层</span></span><br><span class="line">        self.p2_1 = nn.Conv2d(in_channels, c2[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p2_2 = nn.Conv2d(c2[<span class="number">0</span>], c2[<span class="number">1</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 线路3，1x1卷积层后接5x5卷积层</span></span><br><span class="line">        self.p3_1 = nn.Conv2d(in_channels, c3[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p3_2 = nn.Conv2d(c3[<span class="number">0</span>], c3[<span class="number">1</span>], kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 线路4，3x3最大池化层后接1x1卷积层</span></span><br><span class="line">        self.p4_1 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        p1 = F.relu(self.p1_1(x))</span><br><span class="line">        <span class="comment"># 为什么中间两条加激活函数呢？</span></span><br><span class="line">        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))</span><br><span class="line">        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))</span><br><span class="line">        p4 = F.relu(self.p4_2(self.p4_1(x)))</span><br><span class="line">        <span class="comment"># 在通道维度上连结输出</span></span><br><span class="line">        <span class="keyword">return</span> torch.cat((p1, p2, p3, p4), dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p><p>为什么GoogLeNet如此有效呢？首要原因是滤波器（也就是不同大小的卷积核，或者说卷积层）的组合，它们可以用各种滤波器尺寸探索图像，这意味着不同大小的滤波器可以有效地识别不同范围的图像细节。同时，我们可以为不同的滤波器分配不同数量的参数。</p><h3 id="4-2-GoogLeNet模型"><a href="#4-2-GoogLeNet模型" class="headerlink" title="4.2. GoogLeNet模型"></a>4.2. GoogLeNet模型</h3><p>如下图所示，GoogLeNet一共使用9个Inception块和全局平均池化层的堆叠来生成其估计值。Inception块之间的最大池化层可降低维度。第一个模块类似于AlexNet和LeNet，Inception块的组合从VGG继承，全局平均池化层避免了在最后使用全连接层。</p><p><img src="/assets/post_img/article54/inception-full.svg" alt="GoogleNet架构"></p><p>下面逐一实现GoogLeNet的每个模块。第一个模块使用64个通道、$7\times7$卷积层。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">b1 = nn.Sequential(nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure></p><p>第二个模块使用两个卷积层：第一个卷积层是64个通道、$1\times1$卷积层；第二个卷积层使用将通道数量增加三倍的$3\times3$卷积层。这对应于Inception块中的第二条路径。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">b2 = nn.Sequential(nn.Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">1</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.Conv2d(<span class="number">64</span>, <span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure></p><p>第三个模块串联两个完整的Inception块。<br>第一个Inception块的输出通道数为$64+128+32+32=256$，四个路径之间的输出通道数量比为$64:128:32:32=2:4:1:1$。第二个和第三个路径首先将输入通道的数量分别减少到$96/192=1/2$和$16/192=1/12$，然后连接第二个卷积层。第二个Inception块的输出通道数增加到$128+192+96+64=480$，四个路径之间的输出通道数量比为$128:192:96:64 = 4:6:3:2$。第二条和第三条路径首先将输入通道的数量分别减少到$128/256=1/2$和$32/256=1/8$。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">b3 = nn.Sequential(Inception(<span class="number">192</span>, <span class="number">64</span>, (<span class="number">96</span>, <span class="number">128</span>), (<span class="number">16</span>, <span class="number">32</span>), <span class="number">32</span>),</span><br><span class="line">                   Inception(<span class="number">256</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">192</span>), (<span class="number">32</span>, <span class="number">96</span>), <span class="number">64</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><br>第四模块更加复杂，它串联了5个Inception块，其输出通道数分别是$192+208+48+64=512$、$160+224+64+64=512$、$128+256+64+64=512$、$112+288+64+64=528$和$256+320+128+128=832$。这些路径的通道数分配和第三模块中的类似，首先是含$3×3$卷积层的第二条路径输出最多通道，其次是仅含$1×1$卷积层的第一条路径，之后是含$5×5$卷积层的第三条路径和含$3×3$最大汇聚层的第四条路径。其中第二、第三条路径都会先按比例减小通道数。这些比例在各个Inception块中都略有不同。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">b4 = nn.Sequential(Inception(<span class="number">480</span>, <span class="number">192</span>, (<span class="number">96</span>, <span class="number">208</span>), (<span class="number">16</span>, <span class="number">48</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">160</span>, (<span class="number">112</span>, <span class="number">224</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">256</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">112</span>, (<span class="number">144</span>, <span class="number">288</span>), (<span class="number">32</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">528</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><br>第五模块包含输出通道数为$256+320+128+128=832$和$384+384+128+128=1024$的两个Inception块。其中每条路径通道数的分配思路和第三、第四模块中的一致，只是在具体数值上有所不同。需要注意的是，第五模块的后面紧跟输出层，该模块同NiN一样使用全局平均池化层，将每个通道的高和宽变成1。最后将输出变成二维数组，再接上一个输出个数为标签类别数的全连接层。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">b5 = nn.Sequential(Inception(<span class="number">832</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   Inception(<span class="number">832</span>, <span class="number">384</span>, (<span class="number">192</span>, <span class="number">384</span>), (<span class="number">48</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>)),</span><br><span class="line">                   nn.Flatten())</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(<span class="number">1024</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure></p><p>GoogLeNet模型的计算复杂，而且不如VGG那样便于修改通道数。 为了使Fashion-MNIST上的训练短小精悍，这里将输入的高和宽从224降到96，简化了计算。下面演示各个模块输出的形状变化。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">96</span>, <span class="number">96</span>))</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__,<span class="string">&#x27;output shape:\t&#x27;</span>, X.shape)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">64</span>, <span class="number">24</span>, <span class="number">24</span>])</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">192</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">480</span>, <span class="number">6</span>, <span class="number">6</span>])</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">832</span>, <span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">1024</span>])</span><br><span class="line">Linear output shape:         torch.Size([<span class="number">1</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure></p><h3 id="4-3-训练模型"><a href="#4-3-训练模型" class="headerlink" title="4.3. 训练模型"></a>4.3. 训练模型</h3><p>使用Fashion-MNIST数据集来训练我们的模型。在训练之前将图片转换为$96×96$分辨率。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lr, num_epochs, batch_size = <span class="number">0.1</span>, <span class="number">10</span>, <span class="number">128</span></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=<span class="number">96</span>)</span><br><span class="line">train_ch6(net, train_iter, test_iter, num_epochs, lr, torch.device(<span class="string">&#x27;mps&#x27;</span>))</span><br></pre></td></tr></table></figure></p><h2 id="5-批量规范化（batch-normalization）"><a href="#5-批量规范化（batch-normalization）" class="headerlink" title="5. 批量规范化（batch normalization）"></a>5. 批量规范化（batch normalization）</h2><p>训练深层神经网络是十分困难的，特别是在较短的时间内使他们收敛更加棘手。本节中将介绍<em>批量规范化</em>（batch normalization），这是一种流行且有效的技术，可持续加速深层网络的收敛速度。再结合下节将介绍的残差块，批量规范化使得研究人员能够训练100层以上的网络。</p><h3 id="5-1-训练深层网络"><a href="#5-1-训练深层网络" class="headerlink" title="5.1. 训练深层网络"></a>5.1. 训练深层网络</h3><p>对于批量规范化层的需求源于人们在训练神经网络时遇到的一些实际挑战。<br>首先，数据预处理的方式通常会对最终结果产生巨大影响。以应用多层感知机来预测房价为例，使用真实数据时，第一步是标准化输入特征，使其平均值为0，方差为1。直观地说，这种标准化可以很好地与优化器配合使用，因为它可以将参数的量级进行统一。</p><p>第二，对于典型的多层感知机或卷积神经网络。当训练时，中间层中的变量（例如，多层感知机中的仿射变换输出）可能具有更广的变化范围：不论是沿着从输入到输出的层，跨同一层中的单元，或是随着时间的推移，模型参数的随着训练更新变幻莫测。<br>批量规范化的发明者非正式地假设，这些变量分布中的这种偏移可能会阻碍网络的收敛。也就是我们猜想，如果一个层的可变值是另一层的100倍，则可能需要对学习率进行补偿调整。</p><p>第三，更深层的网络很复杂，容易过拟合。这意味着正则化变得更加重要。批量规范化应用于单个可选层（也可以应用到所有层），其原理如下：在每次训练迭代中，我们首先规范化输入，即通过减去其均值并除以其标准差，这两步操作均基于当前小批量处理。然后应用比例系数和比例偏移。正是由于这个基于<em>批量</em>统计的标准化，才有了<em>批量规范化</em>的名称。</p><p>注意如果尝试使用大小为1的小批量应用批量规范化，将无法学到任何东西。这是因为在减去均值之后，每个隐藏单元将为0。所以，只有使用足够大的小批量，批量规范化这种方法才是有效且稳定的。并且在应用批量规范化时，批量大小的选择可能比没有批量规范化时更重要。</p><p>从形式上来说，用$\mathbf{x} \in \mathcal{B}$表示一个来自小批量$\mathcal{B}$的输入，批量规范化$\mathrm{BN}$根据以下表达式转换$\mathbf{x}$：</p><script type="math/tex; mode=display">\mathrm{BN}(\mathbf{x}) = \boldsymbol{\gamma} \odot \frac{\mathbf{x} - \hat{\boldsymbol{\mu}}_\mathcal{B}}{\hat{\boldsymbol{\sigma}}_\mathcal{B}} + \boldsymbol{\beta}</script><p>在上式中，$\hat{\boldsymbol{\mu}}_\mathcal{B}$是小批量$\mathcal{B}$的样本均值，$\hat{\boldsymbol{\sigma}}_\mathcal{B}$是小批量$\mathcal{B}$的样本标准差。应用标准化后，生成的小批量的平均值为0和单位方差为1。由于单位方差（与其他一些魔法数）是一个主观的选择，因此我们通常包含<em>拉伸参数</em>（scale）$\boldsymbol{\gamma}$和<em>偏移参数</em>（shift）$\boldsymbol{\beta}$，它们的形状与$\mathbf{x}$相同。$\boldsymbol{\gamma}$和$\boldsymbol{\beta}$是需要与其他模型参数一起学习的参数。</p><p>由于在训练过程中，中间层的变化幅度不能过于剧烈，而批量规范化将每一层主动居中，并将它们重新调整为给定的平均值和大小（通过$\hat{\boldsymbol{\mu}}_\mathcal{B}$和${\hat{\boldsymbol{\sigma}}_\mathcal{B}}$）。</p><p>从形式上来看，我们计算出上式中的$\hat{\boldsymbol{\mu}}_\mathcal{B}$和${\hat{\boldsymbol{\sigma}}_\mathcal{B}}$，如下所示：</p><script type="math/tex; mode=display">\begin{aligned} \hat{\boldsymbol{\mu}}_\mathcal{B} &= \frac{1}{|\mathcal{B}|} \sum_{\mathbf{x} \in \mathcal{B}} \mathbf{x},\\\hat{\boldsymbol{\sigma}}_\mathcal{B}^2 &= \frac{1}{|\mathcal{B}|} \sum_{\mathbf{x} \in \mathcal{B}} (\mathbf{x} - \hat{\boldsymbol{\mu}}_{\mathcal{B}})^2 + \epsilon\end{aligned}</script><p>我们在方差估计值中添加一个小的常量$\epsilon &gt; 0$，以确保我们永远不会尝试除以零，即使在经验方差估计值可能消失的情况下也是如此。估计值$\hat{\boldsymbol{\mu}}_\mathcal{B}$和${\hat{\boldsymbol{\sigma}}_\mathcal{B}}$通过使用平均值和方差的噪声（常量）估计来抵消缩放问题。这种噪声事实上是有益的。</p><p>由于某些无法用理论解释的原因，优化中的各种噪声源通常会促使更快的训练和较少的过拟合：这种变化似乎是正则化的一种形式。在一些初步研究中，将批量规范化的性质与贝叶斯先验相关联。这些理论揭示了为什么批量规范化最适应$50 \sim 100$范围中的中等批量大小的难题。</p><p>另外，批量规范化层在”训练模式“（通过小批量统计数据规范化）和“预测模式”（通过数据集统计规范化）中的功能不同。在训练过程中，我们无法得知使用整个数据集来估计平均值和方差，所以只能根据每个小批次的平均值和方差不断训练模型。而在预测模式下，可以根据整个数据集精确计算批量规范化所需的平均值和方差。</p><p>下面，了解一下批量规范化在实践中是如何工作的。</p><h3 id="5-2-批量规范化层"><a href="#5-2-批量规范化层" class="headerlink" title="5.2. 批量规范化层"></a>5.2. 批量规范化层</h3><p>批量规范化和其他层之间的一个关键区别是，由于批量规范化在完整的小批量上运行，因此不能像以前在引入其他层时那样忽略批量大小。 下面讨论两种情况：全连接层和卷积层，他们的批量规范化实现略有不同。</p><h4 id="5-2-1-全连接层"><a href="#5-2-1-全连接层" class="headerlink" title="5.2.1. 全连接层"></a>5.2.1. 全连接层</h4><p>通常会将批量规范化层置于全连接层中的仿射变换和激活函数之间。设全连接层的输入为x，权重参数和偏置参数分别为$\mathbf{W}$和$\mathbf{b}$，激活函数为$\phi$，批量规范化的运算符为$\mathrm{BN}$。那么，使用批量规范化的全连接层的输出的计算详情如下：</p><script type="math/tex; mode=display">\mathbf{h} = \phi(\mathrm{BN}(\mathbf{W}\mathbf{x} + \mathbf{b}) )</script><p>均值和方差是在应用变换的”相同”小批量上计算的。</p><h4 id="5-2-2-卷积层"><a href="#5-2-2-卷积层" class="headerlink" title="5.2.2. 卷积层"></a>5.2.2. 卷积层</h4><p>同样，对于卷积层，我们可以在卷积层之后和非线性激活函数之前应用批量规范化。当卷积有多个输出通道时，则需要对这些通道的“每个”输出执行批量规范化，每个通道都有自己的拉伸（scale）和偏移（shift）参数，这两个参数都是标量。假设当前小批量包含$m$个样本，并且对于每个通道，卷积的输出具有高度$p$和宽度$q$。那么对于卷积层，在每个输出通道的$m \cdot p \cdot q$个元素上同时执行每个批量规范化。因此，在计算平均值和方差时，会收集所有空间位置的值，然后在给定通道内应用相同的均值和方差，以便在每个空间位置对值进行规范化。</p><h4 id="5-2-3-预测过程中的批量规范化"><a href="#5-2-3-预测过程中的批量规范化" class="headerlink" title="5.2.3. 预测过程中的批量规范化"></a>5.2.3. 预测过程中的批量规范化</h4><p>批量规范化在训练模式和预测模式下的行为通常不同。首先，将训练好的模型用于预测时，我们不再需要样本均值中的噪声以及在微批次上估计每个小批次产生的样本方差了。其次，例如，我们可能需要使用模型对逐个样本进行预测。一种常用的方法是通过移动平均估算整个训练数据集的样本均值和方差，并在预测时使用它们得到确定的输出。可见，和暂退法一样，批量规范化层在训练模式和预测模式下的计算结果也是不一样的。</p><h3 id="5-3-从零实现"><a href="#5-3-从零实现" class="headerlink" title="5.3. 从零实现"></a>5.3. 从零实现</h3><p>从头开始实现一个具有张量的批量规范化层：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_norm</span>(<span class="params">X, gamma, beta, moving_mean, moving_var, eps, momentum</span>):</span></span><br><span class="line">    <span class="comment"># 通过is_grad_enabled来判断当前模式是训练模式还是预测模式</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> torch.is_grad_enabled():</span><br><span class="line">        <span class="comment"># 预测模式下，直接使用传入的移动平均所得的均值和方差</span></span><br><span class="line">        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 训练模式，判断是全连接还是卷积</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(X.shape) <span class="keyword">in</span> (<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(X.shape) == <span class="number">2</span>:</span><br><span class="line">            <span class="comment"># 使用全连接层的情况，计算特征维上的均值和方差</span></span><br><span class="line">            mean = X.mean(dim=<span class="number">0</span>)</span><br><span class="line">            var = ((X - mean) ** <span class="number">2</span>).mean(dim=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。</span></span><br><span class="line">            <span class="comment"># 这里需要保持X的形状以便后面可以做广播运算</span></span><br><span class="line">            mean = X.mean(dim=(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line">            var = ((X - mean) ** <span class="number">2</span>).mean(dim=(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 训练模式下，用当前的均值和方差做标准化</span></span><br><span class="line">        X_hat = (X - mean) / torch.sqrt(var + eps)</span><br><span class="line">        <span class="comment"># 更新移动平均的均值和方差</span></span><br><span class="line">        moving_mean = momentum * moving_mean + (<span class="number">1.0</span> - momentum) * mean</span><br><span class="line">        moving_var = momentum * moving_var + (<span class="number">1.0</span> - momentum) * var</span><br><span class="line">    <span class="comment"># Y为该批量规范化结果</span></span><br><span class="line">    Y = gamma * X_hat + beta  <span class="comment"># 缩放和移位</span></span><br><span class="line">    <span class="keyword">return</span> Y, moving_mean.data, moving_var.data</span><br></pre></td></tr></table></figure></p><p>现在创建一个正确的BatchNorm层。这个层将保持适当的参数：拉伸<code>gamma</code>和偏移<code>beta</code>,这两个参数将在训练过程中更新。此外该层将保存均值和方差的移动平均值，以便在模型预测期间随后使用。</p><p>撇开算法细节，注意这里实现层的基础设计模式。通常情况下会用一个单独的函数定义其数学原理，比如说<code>batch_norm</code>。然后将此功能集成到一个自定义层中，其代码主要处理数据移动到训练设备（如GPU）、分配和初始化任何必需的变量、跟踪移动平均线（此处为均值和方差）等问题。简单起见这里没有采用自动推断输入形状，因此我们需要指定整个特征的数量（num_dims）。但在调用深度学习框架内置的批量规范化API时该迎刃而解。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BatchNorm</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="comment"># num_features：完全连接层的输出数量或卷积层的输出通道数。</span></span><br><span class="line">    <span class="comment"># num_dims：2表示完全连接层，4表示卷积层</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_features, num_dims</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">if</span> num_dims == <span class="number">2</span>:</span><br><span class="line">            shape = (<span class="number">1</span>, num_features)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            shape = (<span class="number">1</span>, num_features, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 参与求梯度和迭代的拉伸和偏移参数，分别初始化成1和0</span></span><br><span class="line">        self.gamma = nn.Parameter(torch.ones(shape))</span><br><span class="line">        self.beta = nn.Parameter(torch.zeros(shape))</span><br><span class="line">        <span class="comment"># 非模型参数的变量初始化为0和1</span></span><br><span class="line">        self.moving_mean = torch.zeros(shape)</span><br><span class="line">        self.moving_var = torch.ones(shape)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        <span class="comment"># 如果X不在内存上，将moving_mean和moving_var</span></span><br><span class="line">        <span class="comment"># 复制到X所在显存上</span></span><br><span class="line">        <span class="keyword">if</span> self.moving_mean.device != X.device:</span><br><span class="line">            self.moving_mean = self.moving_mean.to(X.device)</span><br><span class="line">            self.moving_var = self.moving_var.to(X.device)</span><br><span class="line">        <span class="comment"># 保存更新过的moving_mean和moving_var</span></span><br><span class="line">        Y, self.moving_mean, self.moving_var = batch_norm(</span><br><span class="line">            X, self.gamma, self.beta, self.moving_mean,</span><br><span class="line">            self.moving_var, eps=<span class="number">1e-5</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">        <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure><h3 id="5-4-使用批量规范化层的-LeNet"><a href="#5-4-使用批量规范化层的-LeNet" class="headerlink" title="5.4. 使用批量规范化层的 LeNet"></a>5.4. 使用批量规范化层的 LeNet</h3><p>下面将<em>批量规范化</em>层应用于LeNet模型。批量规范化是在卷积层或全连接层之后、相应的激活函数之前应用的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>), BatchNorm(<span class="number">6</span>, num_dims=<span class="number">4</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>), BatchNorm(<span class="number">16</span>, num_dims=<span class="number">4</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>), nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">16</span>*<span class="number">4</span>*<span class="number">4</span>, <span class="number">120</span>), BatchNorm(<span class="number">120</span>, num_dims=<span class="number">2</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>), BatchNorm(<span class="number">84</span>, num_dims=<span class="number">2</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">84</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure><p>下面将在Fashion-MNIST数据集上训练网络。 这个代码与本书第一次训练LeNet时几乎相同，主要区别在于学习率大得多（大了0.1？）。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lr, num_epochs, batch_size = <span class="number">1.0</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size)</span><br><span class="line">train_ch6(net, train_iter, test_iter, num_epochs, lr, torch.device(<span class="string">&#x27;something&#x27;</span>))</span><br></pre></td></tr></table></figure></p><p>看看从第一个批量规范化层中学到的拉伸参数gamma和偏移参数beta:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">net[<span class="number">1</span>].gamma.reshape((-<span class="number">1</span>,)), net[<span class="number">1</span>].beta.reshape((-<span class="number">1</span>,))</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">(tensor([<span class="number">0.3362</span>, <span class="number">4.0349</span>, <span class="number">0.4496</span>, <span class="number">3.7056</span>, <span class="number">3.7774</span>, <span class="number">2.6762</span>], device=<span class="string">&#x27;cuda:0&#x27;</span>,</span><br><span class="line">        grad_fn=&lt;ReshapeAliasBackward0&gt;),</span><br><span class="line"> tensor([-<span class="number">0.5739</span>,  <span class="number">4.1376</span>,  <span class="number">0.5126</span>,  <span class="number">0.3060</span>, -<span class="number">2.5187</span>,  <span class="number">0.3683</span>], device=<span class="string">&#x27;cuda:0&#x27;</span>,</span><br><span class="line">        grad_fn=&lt;ReshapeAliasBackward0&gt;))</span><br></pre></td></tr></table></figure></p><h3 id="5-5-框架实现"><a href="#5-5-框架实现" class="headerlink" title="5.5. 框架实现"></a>5.5. 框架实现</h3><p>直接使用深度学习框架中定义的BatchNorm。 该代码看起来几乎与从零实现的代码相同。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    <span class="comment"># nn.BatchNorm2d自动推断输入形状</span></span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>), nn.BatchNorm2d(<span class="number">6</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>), nn.BatchNorm2d(<span class="number">16</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>), nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">256</span>, <span class="number">120</span>), nn.BatchNorm1d(<span class="number">120</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>), nn.BatchNorm1d(<span class="number">84</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">84</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure></p><p>使用相同超参数来训练模型。 注意通常高级API变体运行速度快得多，因为它的代码已编译为C++或CUDA，而我们的自定义代码由Python实现。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_ch6(net, train_iter, test_iter, num_epochs, lr, torch.device(<span class="string">&#x27;something&#x27;</span>))</span><br></pre></td></tr></table></figure></p><h3 id="5-6-争议，无法被解释的方法"><a href="#5-6-争议，无法被解释的方法" class="headerlink" title="5.6. 争议，无法被解释的方法"></a>5.6. 争议，无法被解释的方法</h3><p>直观地说，批量规范化被认为可以使优化更加平滑。然而我们必须区分直觉和对我们观察到的现象的真实解释。我们甚至不知道简单的神经网络（多层感知机和传统的卷积神经网络）为什么如此有效。即使在暂退法和权重衰减的情况下，它们仍然非常灵活，因此无法通过常规的学习理论泛化保证来解释它们是否能够泛化到看不见的数据。</p><p>在提出批量规范化的论文中，作者除了介绍了其应用，还解释了其原理：通过减少<em>内部协变量偏移</em>（internal covariate shift）。据推测，作者所说的“内部协变量转移”类似于上述的直觉（即对现象的猜测），即变量值的分布在训练过程中会发生变化。然而，这种解释有两个问题：<br>1、这种偏移与严格定义的<em>协变量偏移</em>（covariate shift）非常不同，所以这个名字用词不当。<br>2、这种解释只提供了一种不明确的直觉，但留下了一个有待后续挖掘的问题：为什么这项技术如此有效？</p><blockquote><p>本书旨在传达实践者用来发展深层神经网络的直觉。然而，重要的是将这些指导性直觉与既定的科学事实区分开来。最终，当你掌握了这些方法，并开始撰写自己的研究论文时，你会希望清楚地区分技术和直觉。</p></blockquote><p>随着批量规范化的普及，“内部协变量偏移”的解释反复出现在技术文献的辩论，特别是关于“如何展示机器学习研究”的更广泛的讨论中。Ali Rahimi在接受2017年NeurIPS大会的“接受时间考验奖”（Test of Time Award）时发表了一篇令人难忘的演讲。他将“内部协变量转移”作为焦点，将现代深度学习的实践比作炼金术。他对该示例进行了详细回顾，概述了机器学习中令人不安的趋势。此外，一些作者对批量规范化的成功提出了另一种解释：在某些方面，批量规范化的表现出与原始论文中声称的行为是相反的。</p><p>然而，与机器学习文献中成千上万类似模糊的说法相比，内部协变量偏移没有更值得批评。很可能，它作为这些辩论的焦点而产生共鸣，要归功于目标受众对它的广泛认可。<br>批量规范化已经被证明是一种不可或缺的方法。它适用于几乎所有图像分类器，并在学术界获得了数万引用。</p><h2 id="6-残差网络（ResNet）"><a href="#6-残差网络（ResNet）" class="headerlink" title="6. 残差网络（ResNet）"></a>6. 残差网络（ResNet）</h2><p>随着网络的设计越来越深，深刻理解“新添加的层如何提升神经网络的性能”变得至关重要。更重要的是设计网络的能力，在这种网络中，添加层会使网络更具表现力，为了取得质的突破，需要一些数学基础知识</p><h3 id="6-1-函数类"><a href="#6-1-函数类" class="headerlink" title="6.1. 函数类"></a>6.1. 函数类</h3><p>首先，假设有一类特定的神经网络架构$\mathcal{F}$，它包括学习速率和其他超参数设置。对于所有$f \in \mathcal{F}$，存在一些参数集（例如权重和偏置），这些参数可以通过在合适的数据集上进行训练而获得。现在假设 $f^{perfect}$ 是我们真正想要找到的函数，如果是$f^{perfect} \in \mathcal{F}$，那么可以轻而易举的训练得到它，但通常不会那么幸运，我们将尝试找到一个函数$f^{perfect}_\mathcal{F}$，这是在$\mathcal{F}$中的最佳选择。例如，给定一个具有$\mathbf{X}$特性和$\mathbf{y}$标签的数据集，我们可以尝试通过解决以下优化问题来找到它：</p><script type="math/tex; mode=display">f^{perfect}_\mathcal{F} := \mathop{\mathrm{argmin}}_f L(\mathbf{X}, \mathbf{y}, f) \text{ subject to } f \in \mathcal{F}.</script><p>为了更近似真正$f^{perfect}$的函数，我们只能尽量设计一个更强大的架构 $\mathcal{F}’$，并预计$f^{perfect}_{\mathcal{F}’}$比$f^{perfect}_{\mathcal{F}}$“更近似”。然而，如果$\mathcal{F} \not\subseteq \mathcal{F}’$，则无法保证新的体系“更近似”。事实上，$f^{perfect}_{\mathcal{F}’}$可能更糟：如下图所示，对于非嵌套函数（non-nested function）类，较复杂的函数类并不总是向“真”函数$f^{perfect}$靠拢（复杂度由$\mathcal{F}_1$向$\mathcal{F}_6$递增）。在图的左边，虽然$\mathcal{F}_3$比$\mathcal{F}_1$更接近$f^{perfect}$，但$\mathcal{F}_6$却离的更远了。而对于图右侧的嵌套函数（nested function）类$\mathcal{F}_1 \subseteq \ldots \subseteq \mathcal{F}_6$，则可以避免上述问题。</p><p><img src="/assets/post_img/article54/functionclasses.svg" alt="对于非嵌套函数类，较复杂（由较大区域表示）的函数类不能保证更接近“真”函数（$f^{perfect}$）。这种现象在嵌套函数类中不会发生"></p><p>因此，只有当较复杂的函数类包含较小的函数类时，才能确保提高它们的性能。对于深度神经网络，如果能将新添加的层训练成<em>恒等映射</em>（identity function）$f(\mathbf{x}) = \mathbf{x}$，新模型和原模型将同样有效。同时，由于新模型可能得出更优的解来拟合训练数据集，因此添加层似乎更容易降低训练误差。</p><p>针对这一问题，何恺明等人提出了<em>残差网络</em>（ResNet）。它在2015年的ImageNet图像识别挑战赛夺魁，并深刻影响了后来的深度神经网络的设计。残差网络的核心思想是：每个附加层都应该更容易地包含原始函数作为其元素之一。于是，残差块（residual blocks）便诞生了，这个设计对如何建立深层神经网络产生了深远的影响。凭借它，ResNet赢得了2015年ImageNet大规模视觉识别挑战赛。</p><h3 id="6-2-残差块"><a href="#6-2-残差块" class="headerlink" title="6.2. 残差块"></a>6.2. 残差块</h3><p>现在聚焦于神经网络局部：如下图所示，假设原始输入为x，而希望学出的理想映射为为$f(\mathbf{x})$（作为图中上方激活函数的输入）。左图虚线框中的部分需要直接拟合出该映射$f(\mathbf{x})$，而右图虚线框中的部分则需要拟合出残差映射$f(\mathbf{x}) - \mathbf{x}$。残差映射在现实中往往更容易优化。以恒等映射作为希望学出的理想映射$f(\mathbf{x})$，我们只需将右图虚线框内上方的加权运算（如仿射）的权重和偏置参数设成0<font color=#FF4500 size=4 face='黑体'>这样结果不就是零了？然后再加x？没看懂</font>，那么$f(\mathbf{x})$即为恒等映射。实际中，当理想映射$f(\mathbf{x})$极接近于恒等映射时，残差映射也易于捕捉恒等映射的细微波动。下图右图是ResNet的基础架构—<em>残差块</em>（residual block）。在残差块中，输入可通过跨层数据线路更快地向前传播。</p><p><img src="/assets/post_img/article54/residual-block.svg" alt="一个正常块（左）和一个残差块（右）"></p><p>ResNet沿用了VGG完整的$3×3$卷积层设计。残差块里首先有2个有相同输出通道数的$3×3$卷积层。每个卷积层后接一个批量规范化层和ReLU激活函数。然后通过跨层数据通路，跳过这2个卷积运算，将输入直接加在最后的ReLU激活函数前。这样的设计要求2个卷积层的输出与输入形状一样，从而使它们可以相加。如果想改变通道数，就需要引入一个额外的$1×1$卷积层来将输入变换成需要的形状后再做相加运算。</p><p>残差块的实现如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Residual</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_channels, num_channels,</span></span></span><br><span class="line"><span class="params"><span class="function">                 use_1x1conv=<span class="literal">False</span>, strides=<span class="number">1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(input_channels, num_channels,</span><br><span class="line">                               kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=strides)</span><br><span class="line">        self.conv2 = nn.Conv2d(num_channels, num_channels,</span><br><span class="line">                               kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> use_1x1conv:</span><br><span class="line">            self.conv3 = nn.Conv2d(input_channels, num_channels,</span><br><span class="line">                                   kernel_size=<span class="number">1</span>, stride=strides)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.conv3 = <span class="literal">None</span></span><br><span class="line">        self.bn1 = nn.BatchNorm2d(num_channels)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(num_channels)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        Y = F.relu(self.bn1(self.conv1(X)))</span><br><span class="line">        Y = self.bn2(self.conv2(Y))</span><br><span class="line">        <span class="keyword">if</span> self.conv3:</span><br><span class="line">            X = self.conv3(X)</span><br><span class="line">        Y += X</span><br><span class="line">        <span class="keyword">return</span> F.relu(Y)</span><br></pre></td></tr></table></figure></p><p>如下图所示，此代码生成两种类型的网络：一种是当 <code>use_1x1conv=False</code>时，应用ReLU非线性函数之前，将输入添加到输出。另一种是当<code>use_1x1conv=True</code>时，添加通过$1×1$卷积调整通道和分辨率。</p><p><img src="/assets/post_img/article54/resnet-block.svg" alt="包含及不包含$1 \times 1$卷积层的残差块"></p><p>下面查看输入和输出形状一致的情况：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">blk = Residual(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">X = torch.rand(<span class="number">4</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">6</span>)</span><br><span class="line">Y = blk(X)</span><br><span class="line">Y.shape</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">6</span>])</span><br></pre></td></tr></table></figure></p><p>也可以在增加输出通道数的同时，减半输出的高和宽。这里设置步长为2，原本的$6×6$ 在padding = 1后实际为$8×8$，在此基础上用$3×3$卷积核进行卷积，会得到$3×3$的大小，故减半了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">blk = Residual(<span class="number">3</span>,<span class="number">6</span>, use_1x1conv=<span class="literal">True</span>, strides=<span class="number">2</span>)</span><br><span class="line">blk(X).shape</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure><h3 id="6-3-ResNet模型"><a href="#6-3-ResNet模型" class="headerlink" title="6.3. ResNet模型"></a>6.3. ResNet模型</h3><p>ResNet的前两层跟之前介绍的GoogLeNet中的一样：在输出通道数为64、步幅为2的$7×7$卷积层后，接步幅为2的$3×3$的最大池化层。不同之处在于ResNet每个卷积层后增加了批量规范化层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">b1 = nn.Sequential(nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">                   nn.BatchNorm2d(<span class="number">64</span>), nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>GoogLeNet在后面接了4个由Inception块组成的模块。ResNet则使用4个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块。第一个模块的通道数同输入通道数一致。由于之前已经使用了步幅为2的最大池化层，所以无须减小高和宽。之后的每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半。</p><p>下面来实现这个模块。注意对第一个模块做了特别处理。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet_block</span>(<span class="params">input_channels, num_channels, num_residuals,</span></span></span><br><span class="line"><span class="params"><span class="function">                 first_block=<span class="literal">False</span></span>):</span></span><br><span class="line">    blk = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_residuals):</span><br><span class="line">        <span class="comment"># 不是第一个块的话需要变更输出通道数</span></span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> first_block:</span><br><span class="line">            blk.append(Residual(input_channels, num_channels,</span><br><span class="line">                                use_1x1conv=<span class="literal">True</span>, strides=<span class="number">2</span>))</span><br><span class="line">        <span class="comment"># 第一个块则不用                                </span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            blk.append(Residual(num_channels, num_channels))</span><br><span class="line">    <span class="keyword">return</span> blk</span><br></pre></td></tr></table></figure><p>接着在ResNet加入所有残差块，这里每个模块使用2个残差块：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">b2 = nn.Sequential(*resnet_block(<span class="number">64</span>, <span class="number">64</span>, <span class="number">2</span>, first_block=<span class="literal">True</span>))</span><br><span class="line">b3 = nn.Sequential(*resnet_block(<span class="number">64</span>, <span class="number">128</span>, <span class="number">2</span>))</span><br><span class="line">b4 = nn.Sequential(*resnet_block(<span class="number">128</span>, <span class="number">256</span>, <span class="number">2</span>))</span><br><span class="line">b5 = nn.Sequential(*resnet_block(<span class="number">256</span>, <span class="number">512</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure><p>最后，与GoogLeNet一样，在ResNet中加入全局平均池化层，以及全连接层输出。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(b1, b2, b3, b4, b5,</span><br><span class="line">                    nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>)),</span><br><span class="line">                    nn.Flatten(), nn.Linear(<span class="number">512</span>, <span class="number">10</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>每个模块有4个卷积层（不包括恒等映射的1×1卷积层）。加上第一个$7×7$卷积层和最后一个全连接层，共有18层。因此，这种模型通常被称为<strong>ResNet-18</strong>。通过配置不同的通道数和模块里的残差块数可以得到不同的ResNet模型，例如更深的含152层的ResNet-152。虽然ResNet的主体架构跟GoogLeNet类似，但ResNet架构更简单，修改也更方便。这些因素都导致了ResNet迅速被广泛使用。</p><p><img src="/assets/post_img/article54/resnet18.svg" alt="ResNet-18 架构"></p><p>观察一下ResNet中不同模块的输入形状是如何变化的：分辨率降低，通道数量增加，直到全局平均池化层聚集所有特征。 这些变化同样适用之前讲述的架构。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__,<span class="string">&#x27;output shape:\t&#x27;</span>, X.shape)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">64</span>, <span class="number">56</span>, <span class="number">56</span>])</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">64</span>, <span class="number">56</span>, <span class="number">56</span>])</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">128</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">14</span>, <span class="number">14</span>])</span><br><span class="line">Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">512</span>, <span class="number">7</span>, <span class="number">7</span>])</span><br><span class="line">AdaptiveAvgPool2d output shape:      torch.Size([<span class="number">1</span>, <span class="number">512</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">Flatten output shape:        torch.Size([<span class="number">1</span>, <span class="number">512</span>])</span><br><span class="line">Linear output shape:         torch.Size([<span class="number">1</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure><h3 id="6-4-训练模型"><a href="#6-4-训练模型" class="headerlink" title="6.4. 训练模型"></a>6.4. 训练模型</h3><p>在Fashion-MNIST数据集上训练ResNet：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lr, num_epochs, batch_size = <span class="number">0.05</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=<span class="number">96</span>)</span><br><span class="line">train_ch6(net, train_iter, test_iter, num_epochs, lr, torch.device(<span class="string">&#x27;mps&#x27;</span>))</span><br></pre></td></tr></table></figure></p><h2 id="7-稠密连接网络（DenseNet）"><a href="#7-稠密连接网络（DenseNet）" class="headerlink" title="7. 稠密连接网络（DenseNet）"></a>7. 稠密连接网络（DenseNet）</h2><p>ResNet极大地改变了如何参数化深层网络中函数的观点。 稠密连接网络（DenseNet）在某种程度上是ResNet的逻辑扩展。</p><h3 id="7-1-从ResNet到DenseNet"><a href="#7-1-从ResNet到DenseNet" class="headerlink" title="7.1. 从ResNet到DenseNet"></a>7.1. 从ResNet到DenseNet</h3><p>回想一下任意函数的泰勒展开式（Taylor expansion），它把这个函数分解成越来越高阶的项。在$x$接近0时，</p><script type="math/tex; mode=display">f(x) = f(0) + f'(0) x + \frac{f''(0)}{2!}  x^2 + \frac{f'''(0)}{3!}  x^3 + \ldots</script><p>同样，ResNet将函数展开为</p><script type="math/tex; mode=display">f(\mathbf{x}) = \mathbf{x} + g(\mathbf{x})</script><p>也就是说，ResNet将$f$分解为两部分：一个简单的线性项和一个复杂的非线性项。<br>那么再向前拓展一步，那么如果我们想将$f$拓展成超过两部分的信息呢？一种方案便是DenseNet。</p><p><img src="/assets/post_img/article54/densenet-block.svg" alt="ResNet（左）与 DenseNet（右）在跨层连接上的主要区别：使用相加和使用连结"></p><p>如上图所示，ResNet和DenseNet的关键区别在于，DenseNet输出是<em>连接</em>（用图中的$[,]$表示）而不是如ResNet的简单相加。因此在应用越来越复杂的函数序列后，我们执行从$\mathbf{x}$到其展开式的映射：</p><script type="math/tex; mode=display">\mathbf{x} \to \left[\mathbf{x},f_1(\mathbf{x}),f_2([\mathbf{x}, f_1(\mathbf{x})]), f_3([\mathbf{x}, f_1(\mathbf{x}), f_2([\mathbf{x}, f_1(\mathbf{x})])]), \ldots\right].</script><p>最后，将这些展开式结合到多层感知机中，再次减少特征的数量。实现起来非常简单：我们不需要添加术语，而是将它们连接起来。DenseNet这个名字由变量之间的“稠密连接”而得来，最后一层与之前的所有层紧密相连。稠密连接如下图所示。</p><p><img src="/assets/post_img/article54/densenet.svg" alt="稠密连接"></p><p>稠密网络主要由2部分构成：<em>稠密块</em>（dense block）和<em>过渡层</em>（transition layer）。前者定义如何连接输入和输出，而后者则控制通道数量，使其不会太复杂。</p><h3 id="7-2-稠密块体"><a href="#7-2-稠密块体" class="headerlink" title="7.2. 稠密块体"></a>7.2. 稠密块体</h3><p>DenseNet使用了ResNet改良版的“批量规范化、激活和卷积”架构。首先实现一下这个架构。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_block</span>(<span class="params">input_channels, num_channels</span>):</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.BatchNorm2d(input_channels), nn.ReLU(),</span><br><span class="line">        nn.Conv2d(input_channels, num_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>一个<em>稠密块</em>由多个卷积块组成，每个卷积块使用相同数量的输出通道。在前向传播中，将每个卷积块的输入和输出在通道维上连结。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DenseBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_convs, input_channels, num_channels</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(DenseBlock, self).__init__()</span><br><span class="line">        layer = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_convs):</span><br><span class="line">            layer.append(conv_block(</span><br><span class="line">                num_channels * i + input_channels, num_channels))</span><br><span class="line">        self.net = nn.Sequential(*layer)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        <span class="keyword">for</span> blk <span class="keyword">in</span> self.net:</span><br><span class="line">            Y = blk(X)</span><br><span class="line">            <span class="comment"># 连接通道维度上每个块的输入和输出</span></span><br><span class="line">            X = torch.cat((X, Y), dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><p>下例中，定义一个有2个卷积块、输出通道数为10的<code>DenseBlock</code>。使用通道数为3的输入时，我们会得到通道数为$3+2×10=23$的输出。卷积块的通道数控制了输出通道数相对于输入通道数的增长，因此也被称为<em>增长率</em>（growth rate）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">blk = DenseBlock(<span class="number">2</span>, <span class="number">3</span>, <span class="number">10</span>)</span><br><span class="line">X = torch.randn(<span class="number">4</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line">Y = blk(X)</span><br><span class="line">Y.shape</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">23</span>, <span class="number">8</span>, <span class="number">8</span>])</span><br></pre></td></tr></table></figure><h3 id="7-3-过渡层"><a href="#7-3-过渡层" class="headerlink" title="7.3. 过渡层"></a>7.3. 过渡层</h3><p>由于每个稠密块都会带来通道数的增加，使用过多则会过于复杂化模型。而过渡层可以用来控制模型复杂度。它通过$1×1$卷积层来减小通道数，并使用步幅为2的平均池化层减半高和宽，从而进一步降低模型复杂度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transition_block</span>(<span class="params">input_channels, num_channels</span>):</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.BatchNorm2d(input_channels), nn.ReLU(),</span><br><span class="line">        nn.Conv2d(input_channels, num_channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">        nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>))</span><br></pre></td></tr></table></figure><p>对上一个例子中稠密块的输出使用通道数为10的过渡层。 此时输出的通道数减为10，高和宽均减半。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">blk = transition_block(<span class="number">23</span>, <span class="number">10</span>)</span><br><span class="line">blk(Y).shape</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">10</span>, <span class="number">4</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure><h3 id="7-4-DenseNet模型"><a href="#7-4-DenseNet模型" class="headerlink" title="7.4. DenseNet模型"></a>7.4. DenseNet模型</h3><p>构造DenseNet模型。DenseNet首先使用同ResNet一样的单卷积层和最大池化层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">b1 = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">    nn.BatchNorm2d(<span class="number">64</span>), nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>接下来，类似于ResNet使用的4个残差块，DenseNet使用的是4个稠密块。与ResNet类似，可以设置每个稠密块使用多少个卷积层。这里设成4，从而与之前提到的ResNet-18保持一致。稠密块里的卷积层通道数（即增长率）设为32，所以每个稠密块将增加128个通道。<br>在每个模块之间，ResNet通过步幅为2的残差块减小高和宽，DenseNet则使用过渡层来减半高和宽，并减半通道数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># num_channels为当前的通道数</span></span><br><span class="line">num_channels, growth_rate = <span class="number">64</span>, <span class="number">32</span></span><br><span class="line">num_convs_in_dense_blocks = [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]</span><br><span class="line">blks = []</span><br><span class="line"><span class="keyword">for</span> i, num_convs <span class="keyword">in</span> <span class="built_in">enumerate</span>(num_convs_in_dense_blocks):</span><br><span class="line">    blks.append(DenseBlock(num_convs, num_channels, growth_rate))</span><br><span class="line">    <span class="comment"># 上一个稠密块的输出通道数</span></span><br><span class="line">    num_channels += num_convs * growth_rate</span><br><span class="line">    <span class="comment"># 在稠密块之间添加一个转换层，使通道数量减半</span></span><br><span class="line">    <span class="keyword">if</span> i != <span class="built_in">len</span>(num_convs_in_dense_blocks) - <span class="number">1</span>:</span><br><span class="line">        blks.append(transition_block(num_channels, num_channels // <span class="number">2</span>))</span><br><span class="line">        num_channels = num_channels // <span class="number">2</span></span><br></pre></td></tr></table></figure><p>与ResNet类似，最后接上全局池化层和全连接层来输出结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    b1, *blks,</span><br><span class="line">    nn.BatchNorm2d(num_channels), nn.ReLU(),</span><br><span class="line">    nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    nn.Linear(num_channels, <span class="number">10</span>))</span><br></pre></td></tr></table></figure><h3 id="7-5-训练模型"><a href="#7-5-训练模型" class="headerlink" title="7.5. 训练模型"></a>7.5. 训练模型</h3><p>由于这里使用了比较深的网络，此处将输入高和宽从224降到96来简化计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lr, num_epochs, batch_size = <span class="number">0.1</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=<span class="number">96</span>)</span><br><span class="line">train_ch6(net, train_iter, test_iter, num_epochs, lr, torch.device(<span class="string">&#x27;mps&#x27;</span>))</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;p&gt;介绍现代的卷积神经网络架构，本章中的每一个模型都曾一度占据主导地位，其中许多模型都是ImageNet竞赛的优胜者。ImageNet竞赛自2010年以来，一直是计算机视觉中监督学习进展的指向标。&lt;/p&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://silencezheng.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>卷积神经网络--《动手学深度学习》笔记0x07</title>
    <link href="http://silencezheng.top/2022/07/29/article53/"/>
    <id>http://silencezheng.top/2022/07/29/article53/</id>
    <published>2022-07-29T09:33:54.000Z</published>
    <updated>2022-07-29T09:46:01.517Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><p>本章介绍的卷积神经网络（convolutional neural network，CNN）是一类强大的、为处理图像数据而设计的神经网络。 基于卷积神经网络架构的模型在计算机视觉领域中已经占主导地位，当今几乎所有的图像识别、目标检测或语义分割相关的学术竞赛和商业应用都以这种方法为基础。</p><p>对应实践：<a href="https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x07.ipynb">https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x07.ipynb</a><br><span id="more"></span><br>在之前的学习中，我们将图像数据（二维像素网格，对于黑白图像来说每个像素只是1个数值，而彩色图像则有多个）展平为一维向量后送入MLP中，但这种方式忽略了每个图像的空间结构信息。最优的方式是利用先验知识，即利用相近像素之间的相互关联性，从图像数据中学习得到有效的模型。</p><p>现代卷积神经网络的设计得益于生物学、群论和一系列的补充实验。 卷积神经网络需要的参数少于全连接架构的网络，而且卷积也很容易用GPU并行计算。 卷积神经网络除了能够高效地采样从而获得精确的模型，还能够高效地计算。 即使在通常使用循环神经网络的一维序列结构任务上（例如音频、文本和时间序列分析），卷积神经网络也越来越受欢迎。 通过对卷积神经网络一些巧妙的调整，也使它们在图结构数据和推荐系统中发挥作用。</p><p>本章的主要内容：</p><ul><li>构成所有卷积网络主干的基本元素<ul><li>卷积层本身</li><li>填充（padding）和步幅（stride）的基本细节</li><li>用于在相邻区域池化信息的池化层（pooling）</li><li>在每一层中多通道（channel）的使用</li><li>有关现代卷积网络架构的仔细讨论</li></ul></li><li>一个完整的、可运行的LeNet模型：这是第一个成功应用的卷积神经网络，比现代深度学习兴起时间还要早<h3 id="0-1-结论"><a href="#0-1-结论" class="headerlink" title="0.1. 结论"></a>0.1. 结论</h3></li><li>图像的平移不变性使我们以相同的方式处理局部图像，而不在乎它的位置。</li><li>局部性意味着计算相应的隐藏表示只需一小部分局部图像像素。</li><li>在图像处理中，卷积层通常比全连接层需要更少的参数，但依旧获得高效用的模型。</li><li>卷积神经网络（CNN）是一类特殊的神经网络，它可以包含多个卷积层。</li><li>多个输入和输出通道使模型在每个空间位置可以获取图像的多方面特征。</li><li>二维卷积层的核心计算是二维互相关运算。最简单的形式是，对二维输入数据和卷积核执行互相关操作，然后添加一个偏置。</li><li>可以设计一个卷积核来检测图像的边缘。</li><li>可以从数据中学习卷积核的参数。</li><li>学习卷积核时，无论用严格卷积运算或互相关运算，卷积层的输出不会受太大影响。</li><li>当需要检测输入特征中更广区域时，可以构建一个更深的卷积网络。</li><li>填充可以增加输出的高度和宽度。这常用来使输出与输入具有相同的高和宽。</li><li>步幅可以减小输出的高和宽，例如输出的高和宽仅为输入的高和宽的$1/n$（$n$是一个大于的整数）。</li><li>填充和步幅可用于有效地调整数据的维度。</li><li>多输入多输出通道可以用来扩展卷积层的模型。</li><li>当以每像素为基础应用时，$1 \times 1$卷积层相当于全连接层。</li><li>$1 \times 1$卷积层通常用于调整网络层的通道数量和控制模型复杂性。</li><li>对于给定输入元素，最大池化层会输出该窗口内的最大值，平均池化层会输出该窗口内的平均值。</li><li>池化层的主要优点之一是减轻卷积层对位置的过度敏感。可以指定池化层的填充和步幅。</li><li>使用最大池化层以及大于1的步幅，可减少空间维度（如高度和宽度）。</li><li>池化层的输出通道数与输入通道数相同。</li><li>卷积神经网络（CNN）是一类使用卷积层的网络。</li><li>CNN中组合使用卷积层、非线性激活函数和汇聚层。</li><li>为了构造高性能的卷积神经网络，通常对卷积层进行排列，逐渐降低其表示的空间分辨率，同时增加通道数。</li><li>在传统的卷积神经网络中，卷积块编码得到的表征在输出之前需由一个或多个全连接层进行处理。</li><li>LeNet是最早发布的卷积神经网络之一。</li></ul><h2 id="1-从全连接层到卷积"><a href="#1-从全连接层到卷积" class="headerlink" title="1. 从全连接层到卷积"></a>1. 从全连接层到卷积</h2><p>多层感知机十分适合处理表格数据，其中行对应样本，列对应特征。 对于表格数据，我们寻找的模式可能涉及特征之间的交互，但是我们不能预先假设任何与特征交互相关的先验结构。 此时，多层感知机可能是最好的选择，然而对于高维感知数据，这种缺少结构的网络可能会变得不实用。</p><p>例如，在之前猫狗分类的例子中：假设我们有一个足够充分的照片数据集，数据集中是拥有标注的照片，每张照片具有百万级像素，这意味着网络的每次输入都有一百万个维度。 即使将隐藏层维度降低到1000，这个全连接层也将有$10^6 \times 10^3 = 10^9$个参数。这难以训练且需要大量样本进行拟合。</p><p>如今人类和机器都能很好地区分猫和狗，是因为图像中本就拥有丰富的结构，而这些结构可以被人类和机器学习模型使用。 卷积神经网络（convolutional neural networks，CNN）是机器学习利用自然图像中一些已知结构的创造性方法。</p><h3 id="1-1-不变性"><a href="#1-1-不变性" class="headerlink" title="1.1. 不变性"></a>1.1. 不变性</h3><p>假设想从一张图片中找到某个物体。 合理的假设是：无论哪种方法找到这个物体，都应该和物体的位置无关。 理想情况下的系统应该能够利用常识：猪通常不在天上飞，飞机通常不在水里游泳。 但是，如果一只猪出现在图片顶部，系统还是应该认出它。<br>在沃尔多游戏中包含了许多充斥着活动的混乱场景，而沃尔多通常潜伏在一些不太可能的位置，读者的目标就是找出他。 沃尔多的样子并不取决于他潜藏的地方，因此我们可以使用一个“沃尔多检测器”扫描图像。 该检测器将图像分割成多个区域，并为每个区域包含沃尔多的可能性打分。 卷积神经网络正是将空间不变性（spatial invariance）的这一概念系统化，从而基于这个模型使用较少的参数来学习有用的表示。<br><img src="/assets/post_img/article53/where-wally-walker-books.jpeg" alt="w"><br>总结一下适合于计算机视觉的神经网络架构原则：</p><ol><li>平移不变性（translation invariance）：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。</li><li>局部性（locality）：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。</li></ol><h3 id="1-2-多层感知机的限制"><a href="#1-2-多层感知机的限制" class="headerlink" title="1.2. 多层感知机的限制"></a>1.2. 多层感知机的限制</h3><p>首先, 多层感知机的输入是二维图像 $\mathbf{X}$, 其隐藏表示 $\mathbf{H}$ 在数学上是一个矩阵, 在代码中表示为二维张量。其中 $\mathbf{X}$ 和 $\mathbf{H}$ 具有相同的形状。即输入和隐藏表示都拥有空间结构。<br>用$[\mathbf{X}]_{i, j}$ 和 $[\mathbf{H}]_{i, j}$ 分别表示输入图像和隐藏表示中位置 $(i, j)$ 处的像素。为了使每个隐藏神经元都能接收到每个输入像素的信息, 我们将参数从权重矩阵替换为四阶权重张量$\mathbf{W}$。假设 $\mathbf{U}$ 包含偏置参数, 可以将全连接层形式化地表示为</p><script type="math/tex; mode=display">\begin{aligned}{[\mathbf{H}]_{i, j} } &=[\mathbf{U}]_{i, j}+\sum_{k} \sum_{l}[\mathbf{W}]_{i, j, k, l}[\mathbf{X}]_{k, l} \\&=[\mathbf{U}]_{i, j}+\sum_{a} \sum_{b}[\mathbf{V}]_{i, j, a, b}[\mathbf{X}]_{i+a, j+b} .\end{aligned}</script><p>其中, 从W到V的转换只是形式上的转换, 因为在这两个四阶张量的元素之间存在一一对应的关系。只需重新索引下标 $(k, l)$, 使 $k=i+a 、 l=j+b$, 由此可得 $[\mathrm{V}]_{i, j, a, b}=[\mathrm{W}]_{i, j, i+a, j+b}$ 。索引 $a$ 和 $b$ 通过在正偏移和负偏移之间移动覆盖了整个图像。对于隐藏表示中任意给定位置 $(i, j)$ 处的像素值 $[\mathbf{H}]_{i, j}$, 可以通过在 $x$ 中以 $(i, j)$ 为中心对像素进行加权求和得到, 加权使用的权重为 $[\mathrm{V}]_{i, j, a, b}$ 。</p><h4 id="1-2-1-平移不变性"><a href="#1-2-1-平移不变性" class="headerlink" title="1.2.1. 平移不变性"></a>1.2.1. 平移不变性</h4><p>引用上述的第一个原则：平移不变性。这意味着检测对象在输入 $\mathbf{X}$ 中的平移, 应该仅导致隐藏表示 $\mathbf{H}$ 中的平移。也就是$\mathrm{V}$ 和 $\mathbf{U}$ 实际上不依赖于 $(i, j)$ 的值, 即 $[\mathbf{V}]_{i, j, a, b}=[\mathbf{V}]_{a, b}$ 。并且 $\mathbf{U}$ 是一个常数, 比如 $u$ 。 故可以简化 $\mathbf{H}$ 定义为:</p><script type="math/tex; mode=display">[\mathbf{H}]_{i, j}=u+\sum_{a} \sum_{b}[\mathbf{V}]_{a, b}[\mathbf{X}]_{i+a, j+b}</script><p>这就是<strong>卷积 (convolution)</strong>。我们是在使用系数 $[\mathbf{V}]_{a, b}$ 对位置 $(i, j)$ 附近的像素 $(i+a, j+b)$ 进行加权得到 $[\mathbf{H}]_{i, j}$ 。 $[\mathbf{V}]_{a, b}$ 的系数比 $[\mathbf{V}]_{i, j, a, b}$ 少很多, 因为前者不再依赖于图像中的位置，这是显著的进步!</p><h4 id="1-2-2-局部性"><a href="#1-2-2-局部性" class="headerlink" title="1.2.2. 局部性"></a>1.2.2. 局部性</h4><p>引用上述的第二个原则：局部性。为了收集用来训练参数 $[\mathbf{H}]_{i, j}$ 的相关信息, 我们不应偏离到距 $(i, j)$ 很远的地方。这意味着在 $|a|&gt;\Delta$ 或 $|b|&gt;\Delta$ 的范围之外, 我们可以设置 $[\mathbf{V}]_{a, b}=0$ 。因此, 我们可以将 $[\mathbf{H}]_{i, j}$ 重写为</p><script type="math/tex; mode=display">[\mathbf{H}]_{i, j}=u+\sum_{a=-\Delta}^{\Delta} \sum_{b=-\Delta}^{\Delta}[\mathbf{V}]_{a, b}[\mathbf{X}]_{i+a, j+b} .</script><p>上式是一个卷积层 (convolutional layer) ，而卷积神经网络是包含卷积层的一类特殊的神经网络。在深度学习研究社区中, $\mathbf{V}$ 被称为<em>卷积核 (convolution kernel)</em>或者<em>滤波器（filter）</em>，亦或简单地称之为该卷积层的<em>权重</em>, 通常该权重是可学习的参数。</p><p>当图像处理的局部区域很小时, 卷积神经网络与多层感知机的训练差异可能是巨大的: 以前, 多层感知机可能需要数十亿个参数来表示网络中的一层, 而现在卷积神经网络通常只需要几百个参数, 而且不需要改变输入或隐藏表示的维数。<br>参数大幅减少的代价是, 现在的特征是平移不变的, 并且当确定每个隐藏活性值时, 每一层只包含局部的信息。以上所有的权重学习都将依赖于归纳偏置。当这种偏置与现实相符时, 我们就能得到样本有效的模型, 并且这些模型能很好地泛化到末知数据中。不符时, 如图像不满足平移不变时, 模型可能难以拟合训练数据。</p><h3 id="1-3-卷积"><a href="#1-3-卷积" class="headerlink" title="1.3. 卷积"></a>1.3. 卷积</h3><p>为什么上面的操作被称为卷积？<br>在数学中, 两个函数（比如 $\left.f, g: \mathbb{R}^{d} \rightarrow \mathbb{R}\right)$ 之间的 “卷积”被定义为</p><script type="math/tex; mode=display">(f * g)(\mathbf{x})=\int f(\mathbf{z}) g(\mathbf{x}-\mathbf{z}) d \mathbf{z}</script><p>卷积是当把一个函数”翻转”并移位 $\mathbf{x}$ 时, 测量 $f$ 和 $g$ 之间的重叠。当为离散对象时, 积分就变成求和。例如：对于由 索引为 $\mathbb{Z}$ 的、平方可和的、无限维向量集合中抽取的向量，我们得到以下定义：</p><script type="math/tex; mode=display">(f * g)(i)=\sum_{a} f(a) g(i-a)</script><p>对于二维张量, 则为 $f$ 的索引 $(a, b)$ 和 $g$ 的索引 $(i-a, j-b)$ 上的对应加和：</p><script type="math/tex; mode=display">(f * g)(i, j)=\sum_{a} \sum_{b} f(a, b) g(i-a, j-b)</script><p>这看起来类似于1.2.2中的卷积层公式, 只有一个主要区别：这里不是使用 $(i+a, j+b)$, 而是使用差值。但这种区别是表面的, 因为我们总是可以对应两式之间的符号。我们在中的原始定义更正确地描述了<em>互相关 (cross-correlation)</em>。</p><h3 id="1-4-沃尔多游戏回顾"><a href="#1-4-沃尔多游戏回顾" class="headerlink" title="1.4. 沃尔多游戏回顾"></a>1.4. 沃尔多游戏回顾</h3><p>回到“沃尔多在哪里”游戏，卷积层根据滤波器$V$选取给定大小的窗口，并加权处理图片，如下图所示。我们的目标是学习一个模型，以便探测出在“沃尔多”最可能出现的地方。<br><img src="/assets/post_img/article53/waldo-mask.jpeg" alt="waldo"></p><h4 id="1-4-1-通道"><a href="#1-4-1-通道" class="headerlink" title="1.4.1. 通道"></a>1.4.1. 通道</h4><p>这种方法有一个问题: 忽略了图像一般包含三个通道/三种原色（红色、绿色和蓝色）。实际上图像不是二维张量, 而是一个由高度、宽度和颜色组成的三维张量, 比如包含 $1024 \times 1024 \times 3$ 个像素。前两个轴与像素的空间位置有关, 而第三个轴可以看作是每个像素的多维表示。因此, 我们将X索引为 $[\mathrm{X}]_{i, j, k}$ 。由此卷积相应地调整为 $[\mathbf{V}]_{a, b, c}$，而不是 $[\mathbf{V}]_{a, b}$ 。<br>由于输入图像是三维的, 隐藏表示$\mathbf{H}$也最好采用三维张量。也就是对于每一个空间位置, 采用一组隐藏表示而不是单个。这样一组隐藏表示可以想象成一些互相堆叠的二维网格。<br>把隐藏表示想象为一系列具有二维张量的<em>通道（channel）</em>,也被称为特征映射（feature maps），因为每个通道都向后续层提供一组空间化的学习特征。直观上你可以想象在靠近输入的底层, 一些通道专门识别边缘, 而一些通道专门识别纹理。<br>为了支持输入$\mathbf{X}$和隐藏表示 $\mathrm{H}$ 中的多个通道, 可以在V中添加第四个坐标, 即 $[\mathrm{V}]_{a, b, c, d}$ 综上所述:</p><script type="math/tex; mode=display">[\mathrm{H}]_{i, j, d}=\sum_{a=-\Delta}^{\Delta} \sum_{b=-\Delta}^{\Delta} \sum_{c}[\mathbf{V}]_{a, b, c, d}[\mathbf{X}]_{i+a, j+b, c}</script><p>其中隐藏表示$\mathbf{H}$中的索引 $d$ 表示输出通道, 而随后的输出将继续以三维张量 $\mathrm{H}$ 作为输入进入下一个卷积层。所以上式可以定义具有多个通道的卷积层,其中V是该卷积层的权重。</p><p>然而仍有许多问题需要解决。例如，图像中是否到处都有存在沃尔多的可能? 如何有效地计算输出层? 如何选择适当的激活函数? 为了训练有效的网络，如何做出合理的网络设计选择? 后续节会讨论这些。</p><h2 id="2-图像卷积"><a href="#2-图像卷积" class="headerlink" title="2. 图像卷积"></a>2. 图像卷积</h2><p>以图像为例，探索卷积神经网络的实际应用。</p><h3 id="2-1-互相关运算"><a href="#2-1-互相关运算" class="headerlink" title="2.1. 互相关运算"></a>2.1. 互相关运算</h3><p>严格来说，卷积层是个错误的叫法，因为它所表达的运算其实是<strong>互相关运算（cross-correlation）</strong>，而不是卷积运算。 在卷积层中，输入张量和核张量通过互相关运算产生输出张量。</p><p>首先暂时忽略通道（第三维）这一情况, 看看如何处理二维图像数据和隐藏表示。下图中, 输入是高度为3 、宽度为3的二维张量（形状为 $3 \times 3$ ）。卷积核的高度和宽度都是 2 , 而卷积核窗口（或卷积窗口，即输入处的窗口）的形状由内核的高度和宽度决定 $($ 即 $2 \times 2 ）$。<br><img src="/assets/post_img/article53/correlation.svg" alt="co"><br>图中的阴影部分是第一个输出元素，以及用于计算输出的输入张量元素和核张量元素。</p><p>在二维互相关运算中, 卷积窗口从输入张量的左上角开始, 从左到右、从上到下滑动。当卷积窗口滑动到新一个位置时, 包含在该窗口中的部分张量与卷积核张量进行按元素相乘, 得到的张量再求和得到一个单一的标量值, 由此我们得出了这一位置的 输出张量值。在如上例子中, 输出张量的四个元素由二维互相关运算得到, 这个输出高度为 2 、宽度为 2 , 如下所示：</p><script type="math/tex; mode=display">\begin{aligned}&0 \times 0+1 \times 1+3 \times 2+4 \times 3=19 \\&1 \times 0+2 \times 1+4 \times 2+5 \times 3=25 \\&3 \times 0+4 \times 1+6 \times 2+7 \times 3=37 \\&4 \times 0+5 \times 1+7 \times 2+8 \times 3=43\end{aligned}</script><p>输出大小略小于输入大小是因为卷积核的宽度和高度大于1, 并且<font color=#FF4500 size=4 face='黑体'>卷积核只与图像中每个大小完全适合的位置进行互相关运算</font>。所以，输出大小等于输入大小 $n_{h} \times n_{w}$ 减去卷积核大小 $k_{h} \times k_{w}$, 即：</p><script type="math/tex; mode=display">\left(n_{h}-k_{h}+1\right) \times\left(n_{w}-k_{w}+1\right)</script><p>这是因为需要足够的空间在图像上“移动”卷积核。稍后将看到如何通过在图像边界周围填充零来保证有足够的空间移动卷积核, 从而保持输出大小不变。接下来, 我们在 $\operatorname{corr} 2 \mathrm{~d}$ 函数中实现如上过程, 该函数接受输入张量X和卷积核张量K, 并返回输出张量Y。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">corr2d</span>(<span class="params">X, K</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算二维互相关运算&quot;&quot;&quot;</span></span><br><span class="line">    h, w = K.shape</span><br><span class="line">    <span class="comment"># 确定输出的形状</span></span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>] - h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - w + <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="comment"># 依次计算互相关运算</span></span><br><span class="line">            Y[i, j] = (X[i:i + h, j:j + w] * K).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure><br>验证上述二维互相关运算的输出：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">X = torch.tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]])</span><br><span class="line">K = torch.tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>], [<span class="number">2.0</span>, <span class="number">3.0</span>]])</span><br><span class="line">corr2d(X, K)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[<span class="number">19.</span>, <span class="number">25.</span>],</span><br><span class="line">        [<span class="number">37.</span>, <span class="number">43.</span>]])</span><br></pre></td></tr></table></figure></p><h3 id="2-2-卷积层"><a href="#2-2-卷积层" class="headerlink" title="2.2. 卷积层"></a>2.2. 卷积层</h3><p>卷积层对输入和卷积核权重进行互相关运算，并在添加标量偏置之后产生输出。 所以，卷积层中的两个被训练的参数是卷积核权重和标量偏置。 基于卷积层训练模型时，同样随机初始化卷积核权重。</p><p>基于上面定义的corr2d函数实现二维卷积层。在<strong>init</strong>构造函数中，将weight和bias声明为两个模型参数。前向传播函数调用corr2d函数并添加偏置。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Conv2D</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, kernel_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.rand(kernel_size))</span><br><span class="line">        self.bias = nn.Parameter(torch.zeros(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> corr2d(x, self.weight) + self.bias</span><br></pre></td></tr></table></figure><br>高度和宽度分别为 $h$ 和 $w$ 的卷积核可以被称为 $h \times w$ 卷积或 $h \times w$ 卷积核。 我们也将带有 $h \times w$ 卷积核的卷积层称为 $h \times w$ 卷积层。</p><h3 id="2-3-图像中目标的边缘检测"><a href="#2-3-图像中目标的边缘检测" class="headerlink" title="2.3. 图像中目标的边缘检测"></a>2.3. 图像中目标的边缘检测</h3><p>卷积层的一个简单应用：通过找到像素变化的位置，来检测图像中不同颜色的边缘。<br>首先构造一个$6 \times 8$像素的黑白图像。中间四列为黑色，其余像素为白色。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">X = torch.ones((<span class="number">6</span>, <span class="number">8</span>))</span><br><span class="line">X[:, <span class="number">2</span>:<span class="number">6</span>] = <span class="number">0</span></span><br><span class="line">X</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br></pre></td></tr></table></figure><br>接下来，构造一个高度为1、宽度为2的卷积核K。当进行互相关运算时，如果水平相邻的两元素相同，则输出为零，否则输出为非零。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">K = torch.tensor([[<span class="number">1.0</span>, -<span class="number">1.0</span>]])</span><br></pre></td></tr></table></figure><br>对参数X（输入）和K（卷积核）执行互相关运算。 如下所示，输出Y中的1代表从白色到黑色的边缘，-1代表从黑色到白色的边缘，其他情况的输出为0。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Y = corr2d(X, K)</span><br><span class="line">Y</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, -<span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, -<span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, -<span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, -<span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, -<span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, -<span class="number">1.</span>,  <span class="number">0.</span>]])</span><br></pre></td></tr></table></figure><br>现在将输入的二维图像转置，再进行如上的互相关运算。 其输出如下，之前检测到的垂直边缘消失了。 这是合理的，卷积核K只可以检测垂直边缘，无法检测水平边缘。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">corr2d(X.t(), K)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]])</span><br></pre></td></tr></table></figure></p><h3 id="2-4-学习卷积核"><a href="#2-4-学习卷积核" class="headerlink" title="2.4. 学习卷积核"></a>2.4. 学习卷积核</h3><p>如果只需寻找黑白边缘，那么以上[1, -1]的边缘检测器足以。然而当有了更复杂数值的卷积核，或者连续的卷积层时，我们不可能手动设计滤波器。<br>我们需要学习由X生成Y的卷积核，即根据输入和输出，让程序学习应该使用什么样的卷积核。</p><p>现在看看是否可以通过仅查看“输入-输出”对来学习由X生成Y的卷积核。<br>先构造一个卷积层，并将其卷积核初始化为随机张量。接下来，在每次迭代中比较Y与卷积层输出的平方误差，然后计算梯度来更新卷积核。简单起见在此使用内置的二维卷积层，并忽略偏置。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构造一个二维卷积层，它具有1个输出通道和形状为（1，2）的卷积核</span></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>,<span class="number">1</span>, kernel_size=(<span class="number">1</span>, <span class="number">2</span>), bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这个二维卷积层使用四维输入和输出格式（批量大小、通道、高度、宽度），</span></span><br><span class="line"><span class="comment"># 其中批量大小和通道数都为1</span></span><br><span class="line">X = X.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">8</span>))</span><br><span class="line">Y = Y.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">7</span>))</span><br><span class="line">lr = <span class="number">3e-2</span>  <span class="comment"># 学习率</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    Y_hat = conv2d(X)</span><br><span class="line">    <span class="comment"># 平方损失</span></span><br><span class="line">    l = (Y_hat - Y) ** <span class="number">2</span></span><br><span class="line">    conv2d.zero_grad()</span><br><span class="line">    <span class="comment"># 求和、反向传播</span></span><br><span class="line">    l.<span class="built_in">sum</span>().backward()</span><br><span class="line">    <span class="comment"># 迭代卷积核</span></span><br><span class="line">    conv2d.weight.data[:] -= lr * conv2d.weight.grad</span><br><span class="line">    <span class="keyword">if</span> (i + <span class="number">1</span>) % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;i+<span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;l.<span class="built_in">sum</span>():<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">epoch <span class="number">2</span>, loss <span class="number">1.618</span></span><br><span class="line">epoch <span class="number">4</span>, loss <span class="number">0.298</span></span><br><span class="line">epoch <span class="number">6</span>, loss <span class="number">0.061</span></span><br><span class="line">epoch <span class="number">8</span>, loss <span class="number">0.015</span></span><br><span class="line">epoch <span class="number">10</span>, loss <span class="number">0.004</span></span><br></pre></td></tr></table></figure><br>10次迭代之后，误差已经降到足够低。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">conv2d.weight.data.reshape((<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[ <span class="number">0.9879</span>, -<span class="number">0.9993</span>]])</span><br></pre></td></tr></table></figure><br>结果非常接近之前的卷积核K。</p><h3 id="2-5-互相关和卷积"><a href="#2-5-互相关和卷积" class="headerlink" title="2.5. 互相关和卷积"></a>2.5. 互相关和卷积</h3><p>基于互相关运算和卷积运算之间的对应关系。 为了得到正式的卷积运算输出，我们需要执行中定义的严格卷积运算，而不是互相关运算。 但好在它们差别不大，只需水平和垂直翻转二维卷积核张量，然后对输入张量执行互相关运算。</p><p>由于卷积核是从数据中学习到的，因此无论这些层执行严格的卷积运算还是互相关运算，卷积层的输出都不会受到影响。假设卷积层执行互相关运算并学习之前例子中的卷积核，该卷积核在这里由矩阵$\mathbf{K}$表示。 假设其他条件不变，当这个层执行严格的卷积时，学习的卷积核$\mathbf{K}’$在水平和垂直翻转之后将与$\mathbf{K}$相同。 也就是说，当卷积层对例子中的输入和$\mathbf{K}’$执行严格卷积运算时，将得到与互相关运算相同的输出。</p><p>在深度学习文献中，将继续把“互相关运算”称为卷积运算，尽管它们略有不同。 对于<em>卷积核张量上的权重</em>，我们称其为<strong>元素</strong>。</p><h3 id="2-6-特征映射和感受域"><a href="#2-6-特征映射和感受域" class="headerlink" title="2.6. 特征映射和感受域"></a>2.6. 特征映射和感受域</h3><p>输出的卷积层有时被称为<em>特征映射 (feature map)</em> ，因为它可以被视为一个输入映射到下一层的空间维度的转换器。在卷积神经网络中, 对于某一层的任意元素 $x$, 其感受域 (receptive field) 是指在前向传播期间可能影响 $x$ 计算的所有元素（来自所有先前层）。</p><p>感受野可能大于输入的实际大小。用2.1的图为例来解释：给定 $2 \times 2$ 卷积核, 阴影输出元素值19的感受域是输入阴影部分的四个元素。假设之前输出为 $\mathbf{Y}$, 其大小为 $2 \times 2$, 现在我们在其后附加一个卷积层, 该卷积层以 $\mathbf{Y}$ 为输入, 输出单个元素 $z$ 。此时$z$ 的感受域包括 $\mathbf{Y}$ 的所有四个元素, 以及最初所有九个输入元素。<br>重点来啦，根据这一特质，当一个特征图中的任意元素需要检测更广区域的输入特征时，可以构建一个更深的网络。</p><p>PS：实在受不了感受野这翻译，浅动一个字改成感受域吧。</p><h2 id="3-填充和步幅"><a href="#3-填充和步幅" class="headerlink" title="3. 填充和步幅"></a>3. 填充和步幅</h2><p>现在已经知道假设输入形状为 $n_{h} \times n_{w}$, 卷积核形状为 $k_{h} \times k_{w}$, 那么输出形状将是 $\left(n_{h}-k_{h}+1\right) \times\left(n_{w}-k_{w}+1\right)$ 。 即卷积的输出形状取决于输入形状和卷积核的形状。</p><p>本节将介绍填充（padding）和步幅（stride）。假设以下情景：有时在应用了连续的卷积之后, 最终得到的输出远小于输入大小。这是由于卷积核的宽度和高度通常大于 1 所导致的。比如, 一个 $240 \times 240$ 像素的图像, 经过 10 层 $5 \times 5$ 的卷积后, 将减少到 $200 \times 200$ 像素。这会导致原始图像边界上的有用信息被丢弃，<strong>填充</strong>是解决此问题最有效的方法。有时可能希望大幅降低图像的宽度和高度。例如原始的输入分辨率十分冗余，<strong>步幅</strong>可以在这类情况下提供帮助。</p><h3 id="3-1-填充（padding）"><a href="#3-1-填充（padding）" class="headerlink" title="3.1. 填充（padding）"></a>3.1. 填充（padding）</h3><p>在应用多层卷积时，常常丢失边缘像素。 由于我们通常使用小卷积核，因此对于任何单个卷积，可能只会丢失几个像素。 但随着应用许多连续卷积层，累积丢失的像素数就多了。 解决这个问题的简单方法为填充（padding）：在输入图像的边界填充元素（通常填充元素是）。<br>以上节中的例子来说，将$3 \times 3$输入填充到$5 \times 5$，那么它的输出就增加为$4 \times 4$。如下图，阴影部分是第一个输出元素以及用于输出计算的输入和核张量元素：<br><img src="/assets/post_img/article53/conv-pad.svg" alt="padding"><br>通常, 如果添加 $p_{h}$ 行填充（大约一半在顶部, 一半在底部）和 $p_{w}$ 列填充（左侧大约一半, 右侧一半）, 则输出形状将为</p><script type="math/tex; mode=display">\left(n_{h}-k_{h}+p_{h}+1\right) \times\left(n_{w}-k_{w}+p_{w}+1\right)</script><p>即输出的高度和宽度将分别增加 $p_{h}$ 和 $p_{w}$ 。</p><p>在许多情况下, 我们需要设置 $p_{h}=k_{h}-1$ 和 $p_{w}=k_{w}-1$, 使输入和输出具有相同的高度和宽度。这样可以在构建网络时更容易地预测每个图层的输出形状。假设 $k_{h}$ 是奇数, 将在高度的两侧填充 $p_{h} / 2$ 行。如果 $k_{h}$ 是偶数, 则可以在输入顶部填充 $\left\lceil p_{h} / 2\right\rceil$ 行, 在底部填充 $\left\lfloor p_{h} / 2\right\rfloor$ 行。然后对宽度两侧按同样的道理填充。</p><p>卷积神经网络中卷积核的高度和宽度通常为奇数, 例如 $1、3、5、7$。选择奇数的好处是在保持空间维度的同时，可以在顶部和底部填充相同数量的行, 在左侧和右侧填充相同数量的列。</p><p>使用奇数的核大小和填充大小也提供了书写上的便利。对于任何二维张量X, 当满足：1. 卷积核的大小是奇数；2. 所有边的填充行数和列数相同; 3. 输出与输入具有相同高度和宽度 则可以得出：输出 $Y[i, j]$ 是通过以输入 $[i, j]$ 为中心, 与卷积核进行互相关计算得到的。</p><p>下例中, 创建一个高度和宽度为3的二维卷积层, 并在所有侧边填充1个像素。给定高度和宽度为8的输入, 则输出的高度和宽度也是8。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了方便起见，定义了一个计算卷积层的函数。</span></span><br><span class="line"><span class="comment"># 此函数初始化卷积层权重，并对输入和输出提高和缩减相应的维数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">comp_conv2d</span>(<span class="params">conv2d, X</span>):</span></span><br><span class="line">    <span class="comment"># 这里的（1，1）表示批量大小和通道数都是1</span></span><br><span class="line">    X = X.reshape((<span class="number">1</span>, <span class="number">1</span>) + X.shape)</span><br><span class="line">    Y = conv2d(X)</span><br><span class="line">    <span class="comment"># 省略前两个维度：批量大小和通道</span></span><br><span class="line">    <span class="keyword">return</span> Y.reshape(Y.shape[<span class="number">2</span>:])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 请注意，这里每边都填充了1行或1列，因此总共添加了2行或2列</span></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">X = torch.rand(size=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">torch.Size([<span class="number">8</span>, <span class="number">8</span>])</span><br></pre></td></tr></table></figure><br>当卷积核的高度和宽度不同时，则可以填充不同的高度和宽度，使输出和输入具有相同的高度和宽度。在下例中，使用高度为5，宽度为3的卷积核，高度和宽度两边的填充分别为2和1。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=(<span class="number">5</span>, <span class="number">3</span>), padding=(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">torch.Size([<span class="number">8</span>, <span class="number">8</span>])</span><br></pre></td></tr></table></figure></p><h3 id="3-2-步幅（stride）"><a href="#3-2-步幅（stride）" class="headerlink" title="3.2. 步幅（stride）"></a>3.2. 步幅（stride）</h3><p>在计算互相关时，卷积窗口从输入张量的左上角开始，向下、向右滑动。 在前面的例子中默认每次滑动一个元素。 但有时候为了高效计算或是缩减采样次数，卷积窗口可以跳过中间位置，每次滑动多个元素。</p><p>通常将每次滑动元素的数量称为步幅（stride）。 下图是垂直步幅为3，水平步幅为2的二维互相关运算。 着色部分是输出元素以及用于输出计算的输入和内核张量元素：$0\times0+0\times1+1\times2+2\times3=8$、$0\times0+6\times1+0\times2+0\times3=6$。<br><img src="/assets/post_img/article53/conv-stride.svg" alt="stride"><br>可以看到，为了计算输出中第一列的第二个元素和第一行的第二个元素，卷积窗口分别向下滑动三行和向右滑动两列。但是，当卷积窗口继续向右滑动两列时，没有输出，因为输入元素无法填充窗口。</p><p>通常, 当垂直步幅为 $s_{h}$ 、水平步幅为 $s_{w}$ 时, 输出形状为</p><script type="math/tex; mode=display">\left\lfloor\left(n_{h}-k_{h}+p_{h}+s_{h}\right) / s_{h}\right\rfloor \times\left\lfloor\left(n_{w}-k_{w}+p_{w}+s_{w}\right) / s_{w}\right\rfloor .</script><p>如果设置了 $p_{h}=k_{h}-1$ 和 $p_{w}=k_{w}-1$, 则输出形状将简化为 $\left\lfloor\left(n_{h}+s_{h}-1\right) / s_{h}\right\rfloor \times\left\lfloor\left(n_{w}+s_{w}-1\right) / s_{w}\right\rfloor$ 。如果在此基础上，输入的高度和宽度可以被垂直和水平步幅整除, 则输出形状将为 $\left(n_{h} / s_{h}\right) \times\left(n_{w} / s_{w}\right)$ 。</p><p>下面将高度和宽度的步幅设置为 2 , 从而将输入的高度和宽度减半。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure><br>另一个复杂点的例子，只填充列：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=(<span class="number">3</span>, <span class="number">5</span>), padding=(<span class="number">0</span>, <span class="number">1</span>), stride=(<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure><br>为了简洁起见, 当输入高度和宽度两侧的填充数量分别为 $p_{h}$ 和 $p_{w}$ 时, 我们称之为填充 $\left(p_{h}, p_{w}\right)$ 。当 $p_{h}=p_{w}=p$ 时, 填充是 $p_{\text {。 }}$<br>默认情况下, 填充为 0 , 步幅为 1 。<br><em>在实践中很少使用不一致的步幅或填充, 通常有 $p_{h}=p_{w}$ 和 $s_{h}=s_{w}$ 。</em></p><h2 id="4-多输入多输出通道"><a href="#4-多输入多输出通道" class="headerlink" title="4. 多输入多输出通道"></a>4. 多输入多输出通道</h2><p>目前为止展示的单个输入和单个输出通道的简单例子，使得我们可以将输入、卷积核和输出看作二维张量。</p><p>当添加通道时，输入和隐藏的表示都变成了三维张量。例如，每个RGB输入图像具有$3\times h\times w$的形状。我们将这个大小为$3$的轴称为通道（channel）维度。</p><h3 id="4-1-多输入通道"><a href="#4-1-多输入通道" class="headerlink" title="4.1. 多输入通道"></a>4.1. 多输入通道</h3><p>当输入包含多个通道时，需要构造一个与输入数据具有相同输入通道数的卷积核，以便与输入数据进行互相关运算。假设输入的通道数为$c_i$，那么卷积核的输入通道数也需要为$c_i$。</p><p>如果卷积核的窗口形状是$k_h\times k_w$，当$c_i=1$时，可以把卷积核看作形状为$k_h\times k_w$的二维张量。<br>当$c_i&gt;1$时，卷积核的每个输入通道将包含形状为$k_h\times k_w$的张量。将这些张量$c_i$连结在一起可以得到形状为$c_i\times k_h\times k_w$的卷积核。</p><p>输入和卷积核都有$c_i$个通道，所以可以对每个通道输入的二维张量和卷积核的二维张量进行互相关运算，再对通道求和（将$c_i$的结果相加）得到二维张量。这是多通道输入和多输入通道卷积核之间进行二维互相关运算的结果。</p><p>下图演示了一个具有两个输入通道的二维互相关运算的示例。阴影部分是第一个输出元素以及用于计算这个输出的输入和核张量元素：</p><script type="math/tex; mode=display">(1\times1+2\times2+4\times3+5\times4)+(0\times0+1\times1+3\times2+4\times3)=56</script><p><img src="/assets/post_img/article53/conv-multi-in.svg" alt="multi-channel"><br>实现一下多输入通道互相关运算，所做的就是对每个通道执行互相关操作，然后将结果相加。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">corr2d_multi_in</span>(<span class="params">X, K</span>):</span></span><br><span class="line">    <span class="comment"># 先遍历“X”和“K”的第0个维度（通道维度），再把它们加在一起</span></span><br><span class="line">    <span class="comment"># 前面定义的corr2d函数</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(corr2d(x, k) <span class="keyword">for</span> x, k <span class="keyword">in</span> <span class="built_in">zip</span>(X, K))</span><br></pre></td></tr></table></figure><br>构造上图中的例子，验证互相关运算的输出：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">X = torch.tensor([[[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]],</span><br><span class="line">               [[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], [<span class="number">7.0</span>, <span class="number">8.0</span>, <span class="number">9.0</span>]]])</span><br><span class="line">K = torch.tensor([[[<span class="number">0.0</span>, <span class="number">1.0</span>], [<span class="number">2.0</span>, <span class="number">3.0</span>]], [[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>]]])</span><br><span class="line"></span><br><span class="line">corr2d_multi_in(X, K)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[ <span class="number">56.</span>,  <span class="number">72.</span>],</span><br><span class="line">        [<span class="number">104.</span>, <span class="number">120.</span>]])</span><br></pre></td></tr></table></figure></p><h3 id="4-2-多输出通道"><a href="#4-2-多输出通道" class="headerlink" title="4.2. 多输出通道"></a>4.2. 多输出通道</h3><p>目前为止还只有一个输出通道，但每一层有多个输出通道是至关重要的。在最流行的神经网络架构中，随着神经网络层数的加深，经常会增加输出通道的维数，通过减少空间分辨率以获得更大的通道深度。直观地说，可以将每个通道看作是对不同特征的响应，但他们不是互相独立，而是一个不可分割的整体。因为每个通道不是独立学习的，而是为了共同使用而优化的，多输出通道并不仅是学习多个单通道的检测器。</p><p>用$c_i$和$c_o$分别表示输入和输出通道的数目，并让$k_h$和$k_w$为卷积核的高度和宽度。为了获得多个通道的输出，可以为每个输出通道创建一个形状为$c_i\times k_h\times k_w$的卷积核张量，这样卷积核的形状是$c_o\times c_i\times k_h\times k_w$。在互相关运算中，每个输出通道先获取所有输入通道，再以对应该输出通道的卷积核计算出结果。</p><p>下面实现一个计算多个通道的输出的互相关函数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">corr2d_multi_in_out</span>(<span class="params">X, K</span>):</span></span><br><span class="line">    <span class="comment"># 迭代“K”的第0个维度，每次都对输入“X”执行互相关运算。</span></span><br><span class="line">    <span class="comment"># 最后将所有结果都叠加在一起，stack</span></span><br><span class="line">    <span class="keyword">return</span> torch.stack([corr2d_multi_in(X, k) <span class="keyword">for</span> k <span class="keyword">in</span> K], <span class="number">0</span>)</span><br></pre></td></tr></table></figure><br>通过将核张量K与K+1（K中每个元素加1）和K+2连接起来，构造了一个具有3个输出通道的卷积核：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">K = torch.stack((K, K + <span class="number">1</span>, K + <span class="number">2</span>), <span class="number">0</span>)</span><br><span class="line">K.shape</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure><br>此时，卷积核K的形状为$3\times2\times2\times2$，可以想作是3片2通道的2维张量。然后对输入张量X与卷积核张量K执行互相关运算。现在的输出包含3个通道，第一个通道的结果与先前输入张量X和多输入单输出通道的结果一致：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">corr2d_multi_in_out(X, K)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[[ <span class="number">56.</span>,  <span class="number">72.</span>],</span><br><span class="line">         [<span class="number">104.</span>, <span class="number">120.</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">76.</span>, <span class="number">100.</span>],</span><br><span class="line">         [<span class="number">148.</span>, <span class="number">172.</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">96.</span>, <span class="number">128.</span>],</span><br><span class="line">         [<span class="number">192.</span>, <span class="number">224.</span>]]])</span><br></pre></td></tr></table></figure></p><h3 id="4-3-1-times1-卷积层"><a href="#4-3-1-times1-卷积层" class="headerlink" title="4.3. $1\times1$卷积层"></a>4.3. $1\times1$卷积层</h3><p>$1 \times 1$卷积，即$k_h = k_w = 1$，看起来似乎没有多大意义。<br>毕竟，卷积的本质是有效提取相邻像素间的相关特征，而$1 \times 1$卷积并没有这种作用。<br>尽管如此，$1 \times 1$卷积核仍然十分流行，经常包含在复杂深层网络的设计中。下面详细地解读一下它的实际作用。</p><p>因为使用了最小窗口，$1\times 1$卷积失去了卷积层的特有能力——在高度和宽度维度上，识别相邻元素间相互作用的能力。<br>其实，$1\times 1$卷积的唯一计算发生在通道上。</p><p>下图展示了使用$1\times 1$卷积核与$3$个输入通道和$2$个输出通道的互相关计算。<br>这里输入和输出具有相同的高度和宽度，输出中的每个元素都是从输入图像中同一位置的元素的线性组合。对于图像来说，实际上是对每个像素点，在不同的通道上进行线性组合（信息整合），且保留了图片的原有平面结构，完成升维或降维的功能。</p><p>可以将$1 \times 1$卷积层看作是在每个像素位置应用的全连接层，以$c_i$个输入值转换为$c_o$个输出值。也就是说$1 \times 1$卷积核的一个作用是调整通道数，类比多层感知机中的全连接层，调整输入和输出的大小。<br>这仍然是一个卷积层，所以跨像素的权重是一致的。同时，$1\times 1$卷积层需要的权重维度为$c_o\times c_i$，再额外加上一个偏置。<br><img src="/assets/post_img/article53/conv-1x1.svg" alt="1*1"><br>使用全连接层实现$1 \times 1$卷积，注意需要对输入和输出的数据形状进行调整：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># X是输入，K是卷积核</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">corr2d_multi_in_out_1x1</span>(<span class="params">X, K</span>):</span></span><br><span class="line">    c_i, h, w = X.shape</span><br><span class="line">    c_o = K.shape[<span class="number">0</span>]</span><br><span class="line">    X = X.reshape((c_i, h * w))</span><br><span class="line">    K = K.reshape((c_o, c_i))</span><br><span class="line">    <span class="comment"># 全连接层中的矩阵乘法</span></span><br><span class="line">    Y = torch.matmul(K, X)</span><br><span class="line">    <span class="keyword">return</span> Y.reshape((c_o, h, w))</span><br></pre></td></tr></table></figure><br>当执行$1 \times 1$卷积运算时，上述函数相当于先前实现的互相关函数corr2d_multi_in_out：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 注意这里的 X 和 K 的形状，在reshape后的矩阵乘法应用了广播机制</span></span><br><span class="line"><span class="comment"># 3行9列的X 和 2行3列的K 进行矩阵乘法，将K扩展为9行3列进行运算</span></span><br><span class="line">X = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">K = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">Y1 = corr2d_multi_in_out_1x1(X, K)</span><br><span class="line">Y2 = corr2d_multi_in_out(X, K)</span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">float</span>(torch.<span class="built_in">abs</span>(Y1 - Y2).<span class="built_in">sum</span>()) &lt; <span class="number">1e-6</span></span><br></pre></td></tr></table></figure><br>综上，$1 \times 1$卷积层通常用于调整网络层的通道数量和控制模型复杂性。</p><h2 id="5-池化层"><a href="#5-池化层" class="headerlink" title="5. 池化层"></a>5. 池化层</h2><p>当处理图像时，通常希望逐渐降低隐藏表示的空间分辨率、聚集信息，这样随着我们在神经网络中层叠的上升，每个神经元对其敏感的感受域（输入）就越大。</p><p>而机器学习任务通常会跟全局图像的问题有关（例如，“图像是否包含一只猫呢？”），所以最后一层的神经元应该对整个输入的全局敏感。通过逐渐聚合信息，生成越来越粗糙的映射，最终实现学习全局表示的目标，同时将卷积图层的所有优势保留在中间层。</p><p>此外，当检测较底层的特征时（例如边缘检测），通常希望这些特征保持某种程度上的平移不变性。例如，如果拍摄黑白之间轮廓清晰的图像X，并将整个图像向右移动一个像素，即Z[i, j] = X[i, j + 1]，则新图像Z的输出可能大不相同。而在现实中，随着拍摄角度的移动，任何物体不可能出现在同一位置上。即使用三脚架拍摄一个静止的物体，由于快门的移动而引起的相机振动，可能会使所有物体左右移动一个像素。</p><p>本节介绍池化（pooling）层，它具有双重目的：降低卷积层对位置的敏感性，同时降低对空间降采样表示的敏感性。</p><h3 id="5-1-最大池化层和平均池化层"><a href="#5-1-最大池化层和平均池化层" class="headerlink" title="5.1. 最大池化层和平均池化层"></a>5.1. 最大池化层和平均池化层</h3><p>与卷积层类似，池化层运算符由一个固定形状的窗口组成，该窗口根据其步幅大小在输入的所有区域上滑动，为固定形状窗口（有时称为池化窗口）遍历的每个位置计算一个输出。 但不同于卷积层中的输入与卷积核之间的互相关计算，池化层不包含参数。 相反，池运算是确定性的，我们通常计算池化窗口中所有元素的最大值或平均值。这些操作分别称为最大池化层（maximum pooling）和平均池化层（average pooling）。</p><p>在这两种情况下，与互相关运算符一样，池化窗口从输入张量的左上角开始，从左往右、从上往下的在输入张量内滑动。在池化窗口到达的每个位置，它计算该窗口中输入子张量的最大值或平均值。计算最大值或平均值是取决于使用了最大池化层还是平均池化层。<br><img src="/assets/post_img/article53/pooling.svg" alt="pooling"><br>着色部分是第一个输出元素，以及用于计算这个输出的输入元素:$\max(0, 1, 3, 4)=4$</p><p>池化窗口形状为$p \times q$的池化层称为$p \times q$池化层，池化操作称为$p \times q$池化。</p><p>回到开头提到的对象边缘检测示例，现在我们将使用卷积层的输出作为$2\times 2$最大池化的输入。<br>设置卷积层输入为<code>X</code>，池化层输出为<code>Y</code>。<br>无论是移动<code>X[i, j]</code>到<code>X[i, j + 1]</code>，或移动<code>X[i, j]</code>到<code>X[i + 1, j]</code>，池化层始终输出<code>Y[i, j] = 1</code>。<br>也就是说，使用$2\times 2$最大池化层，即使在高度或宽度上移动一个元素，卷积层仍然可以识别到模式。</p><p>在下面的代码中的<code>pool2d</code>函数，实现了池化层的正向传播。这类似于之前实现的<code>corr2d</code>函数。<br>但这里我们没有卷积核，输出为输入中每个区域的最大值或平均值。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pool2d</span>(<span class="params">X, pool_size, mode=<span class="string">&#x27;max&#x27;</span></span>):</span></span><br><span class="line">    p_h, p_w = pool_size</span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>] - p_h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - p_w + <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">if</span> mode == <span class="string">&#x27;max&#x27;</span>:</span><br><span class="line">                Y[i, j] = X[i: i + p_h, j: j + p_w].<span class="built_in">max</span>()</span><br><span class="line">            <span class="keyword">elif</span> mode == <span class="string">&#x27;avg&#x27;</span>:</span><br><span class="line">                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure><br>验证二维最大池化层的输出：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X = torch.tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]])</span><br><span class="line">pool2d(X, (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[<span class="number">4.</span>, <span class="number">5.</span>],</span><br><span class="line">        [<span class="number">7.</span>, <span class="number">8.</span>]])</span><br></pre></td></tr></table></figure><br>验证平均池化层：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pool2d(X, (<span class="number">2</span>, <span class="number">2</span>), <span class="string">&#x27;avg&#x27;</span>)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[<span class="number">2.</span>, <span class="number">3.</span>],</span><br><span class="line">        [<span class="number">5.</span>, <span class="number">6.</span>]])</span><br></pre></td></tr></table></figure></p><h3 id="5-2-填充和步幅"><a href="#5-2-填充和步幅" class="headerlink" title="5.2. 填充和步幅"></a>5.2. 填充和步幅</h3><p>与卷积层一样，池化层也可以通过填充和步幅以获得所需的输出形状。 下面用深度学习框架中内置的二维最大池化层，来演示池化层中填充和步幅的使用。 首先构造了一个输入张量X，它有四个维度，其中样本数和通道数都是1。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">X = torch.arange(<span class="number">16</span>, dtype=torch.float32).reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">X</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[[[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>],</span><br><span class="line">          [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>],</span><br><span class="line">          [ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>],</span><br><span class="line">          [<span class="number">12.</span>, <span class="number">13.</span>, <span class="number">14.</span>, <span class="number">15.</span>]]]])</span><br></pre></td></tr></table></figure><br>默认情况下，深度学习框架中的步幅与池化窗口的大小相同。 因此，如果我们使用形状为(3, 3)的池化窗口，那么默认情况下，我们得到的步幅形状为(3, 3)。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pool2d = nn.MaxPool2d(<span class="number">3</span>)</span><br><span class="line">pool2d(X)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[[[<span class="number">10.</span>]]]])</span><br></pre></td></tr></table></figure><br>可以设定一个任意大小的矩形池化窗口，并分别设定填充和步幅的高度和宽度。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pool2d = nn.MaxPool2d((<span class="number">2</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">3</span>), padding=(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">pool2d(X)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[[[ <span class="number">5.</span>,  <span class="number">7.</span>],</span><br><span class="line">          [<span class="number">13.</span>, <span class="number">15.</span>]]]])</span><br></pre></td></tr></table></figure></p><h3 id="5-3-多个通道"><a href="#5-3-多个通道" class="headerlink" title="5.3. 多个通道"></a>5.3. 多个通道</h3><p>在处理多通道输入数据时，<em>池化层在每个输入通道上单独运算，而不是像卷积层一样在通道上对输入进行汇总</em>。 这意味着池化层的输出通道数与输入通道数相同。 下面在通道维度上连结张量X和X + 1，以构建具有2个通道的输入。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">X = torch.cat((X, X + <span class="number">1</span>), <span class="number">1</span>)</span><br><span class="line">X</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[[[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>],</span><br><span class="line">          [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>],</span><br><span class="line">          [ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>],</span><br><span class="line">          [<span class="number">12.</span>, <span class="number">13.</span>, <span class="number">14.</span>, <span class="number">15.</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">4.</span>],</span><br><span class="line">          [ <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>,  <span class="number">8.</span>],</span><br><span class="line">          [ <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>, <span class="number">12.</span>],</span><br><span class="line">          [<span class="number">13.</span>, <span class="number">14.</span>, <span class="number">15.</span>, <span class="number">16.</span>]]]])</span><br></pre></td></tr></table></figure><br>池化后输出通道的数量仍然是2：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">pool2d = nn.MaxPool2d(<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">pool2d(X)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[[[ <span class="number">5.</span>,  <span class="number">7.</span>],</span><br><span class="line">          [<span class="number">13.</span>, <span class="number">15.</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">6.</span>,  <span class="number">8.</span>],</span><br><span class="line">          [<span class="number">14.</span>, <span class="number">16.</span>]]]])</span><br></pre></td></tr></table></figure></p><h2 id="6-卷积神经网络（LeNet）"><a href="#6-卷积神经网络（LeNet）" class="headerlink" title="6. 卷积神经网络（LeNet）"></a>6. 卷积神经网络（LeNet）</h2><p>现在，对于Fashion-MNIST数据集中的服装图片，我们已经掌握了卷积层的处理方法，可以在图像中保留空间结构（不需要像MLP中一样展平）。 同时，用卷积层代替全连接层的另一个好处是：模型更简洁、所需的参数更少。</p><p>本节将介绍LeNet，它是最早发布的卷积神经网络之一，因其在计算机视觉任务中的高效性能而受到广泛关注。 这个模型是由AT&amp;T贝尔实验室的研究员Yann LeCun在1989年提出的（并以其命名），目的是识别图像中的手写数字。 当时，Yann LeCun发表了第一篇通过反向传播成功训练卷积神经网络的研究，这项工作代表了十多年来神经网络研究开发的成果。</p><p>LeNet在当时取得了与支持向量机（support vector machines, SVM）性能相媲美的成果，成为监督学习的主流方法。 LeNet被广泛用于自动取款机（ATM）机中，帮助识别处理支票的数字。 时至今日，一些自动取款机仍在运行Yann LeCun和他的同事Leon Bottou在上世纪90年代写的代码！</p><h3 id="6-1-LeNet"><a href="#6-1-LeNet" class="headerlink" title="6.1. LeNet"></a>6.1. LeNet</h3><p>总体来看，LeNet（LeNet-5）由两个部分组成：</p><ul><li>卷积编码器：由两个卷积层组成;</li><li>全连接层密集块：由三个全连接层组成。</li></ul><p>该架构如下所示（LeNet中的数据流。输入是手写数字，输出为10种可能结果的概率）：<br><img src="/assets/post_img/article53/lenet.svg" alt="lenet"><br>每个卷积块中的基本单元是一个卷积层、一个sigmoid激活函数和平均池化层。虽然ReLU和最大池化层更有效，但它们在20世纪90年代还没有出现。</p><p>每个卷积层使用$5\times 5$卷积核和一个sigmoid激活函数。这些层将输入映射到多个二维特征输出，通常同时增加通道的数量。第一卷积层有6个输出通道，而第二个卷积层有16个输出通道。每个$2\times2$池操作（步幅2）通过空间下采样将维数减少4倍。卷积的输出形状由批量大小、通道数、高度、宽度决定。</p><p>为了将卷积块的输出传递给稠密块，必须在小批量中展平每个样本。也就是将这个四维输入转换成全连接层所期望的二维输入。这里的二维表示的第一个维度索引小批量中的样本，第二个维度给出每个样本的平面向量表示。LeNet的稠密块有三个全连接层，分别有120、84和10个输出。因为我们在执行分类任务，所以输出层的10维对应于最后输出结果的数量。</p><p>用深度学习框架实现此类模型非常简单，只需要实例化一个<code>Sequential</code>块并将需要的层连接在一起。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    <span class="comment"># 通道数 1 -&gt; 6</span></span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    <span class="comment"># 通道数 6 -&gt; 16</span></span><br><span class="line">    nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">84</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure><br>这对原始模型做了一点小改动，去掉了最后一层的高斯激活。除此之外，这个网络与最初的LeNet-5一致。</p><p>下面将一个大小为$28 \times 28$的单通道（黑白）图像通过LeNet。通过在每一层打印输出的形状可以检查模型，以确保其操作与我们期望的一致。<br><img src="/assets/post_img/article53/lenet-vert.svg" alt="vert"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>), dtype=torch.float32)</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__,<span class="string">&#x27;output shape: \t&#x27;</span>,X.shape)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">Conv2d output shape:         torch.Size([<span class="number">1</span>, <span class="number">6</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line">Sigmoid output shape:        torch.Size([<span class="number">1</span>, <span class="number">6</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line">AvgPool2d output shape:      torch.Size([<span class="number">1</span>, <span class="number">6</span>, <span class="number">14</span>, <span class="number">14</span>])</span><br><span class="line">Conv2d output shape:         torch.Size([<span class="number">1</span>, <span class="number">16</span>, <span class="number">10</span>, <span class="number">10</span>])</span><br><span class="line">Sigmoid output shape:        torch.Size([<span class="number">1</span>, <span class="number">16</span>, <span class="number">10</span>, <span class="number">10</span>])</span><br><span class="line">AvgPool2d output shape:      torch.Size([<span class="number">1</span>, <span class="number">16</span>, <span class="number">5</span>, <span class="number">5</span>])</span><br><span class="line">Flatten output shape:        torch.Size([<span class="number">1</span>, <span class="number">400</span>])</span><br><span class="line">Linear output shape:         torch.Size([<span class="number">1</span>, <span class="number">120</span>])</span><br><span class="line">Sigmoid output shape:        torch.Size([<span class="number">1</span>, <span class="number">120</span>])</span><br><span class="line">Linear output shape:         torch.Size([<span class="number">1</span>, <span class="number">84</span>])</span><br><span class="line">Sigmoid output shape:        torch.Size([<span class="number">1</span>, <span class="number">84</span>])</span><br><span class="line">Linear output shape:         torch.Size([<span class="number">1</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure><p>注意，在整个卷积块中，与上一层相比，每一层特征的高度和宽度都减小了。 第一个卷积层使用2个像素的填充，来补偿$5 \times 5$卷积核导致的特征减少。 相反，第二个卷积层没有填充，因此高度和宽度都减少了4个像素。 随着层叠的上升，通道的数量从输入时的1个，增加到第一个卷积层之后的6个，再到第二个卷积层之后的16个。 同时，每个池化层的高度和宽度都减半。最后，每个全连接层减少维数，最终输出一个维数与结果分类数相匹配的输出。</p><h3 id="6-2-模型训练"><a href="#6-2-模型训练" class="headerlink" title="6.2. 模型训练"></a>6.2. 模型训练</h3><p>现在已经实现了LeNet，看看它在Fashion-MNIST数据集上的表现：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line"><span class="comment"># 省略了load_data_fashion_mnist的实现</span></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size=batch_size)</span><br></pre></td></tr></table></figure><br>虽然卷积神经网络的参数较少，但与深度的多层感知机相比，它们的计算成本仍然很高，因为每个参数都参与更多的乘法。 可以使用GPU加快训练。<br>由于完整的数据集位于内存中，因此在模型使用GPU计算数据集之前需要将其复制到显存中。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_accuracy_gpu</span>(<span class="params">net, data_iter, device=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用GPU计算模型在数据集上的精度&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module):</span><br><span class="line">        net.<span class="built_in">eval</span>()  <span class="comment"># 设置为评估模式</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> device:</span><br><span class="line">            device = <span class="built_in">next</span>(<span class="built_in">iter</span>(net.parameters())).device</span><br><span class="line">    <span class="comment"># 正确预测的数量，总预测的数量</span></span><br><span class="line">    <span class="comment"># 省略了Accumulator的实现</span></span><br><span class="line">    metric = Accumulator(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="comment"># 复制入显存</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(X, <span class="built_in">list</span>):</span><br><span class="line">                <span class="comment"># BERT微调所需的（之后将介绍）</span></span><br><span class="line">                X = [x.to(device) <span class="keyword">for</span> x <span class="keyword">in</span> X]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                X = X.to(device)</span><br><span class="line">            y = y.to(device)</span><br><span class="line">            <span class="comment"># 省略了accuracy的实现</span></span><br><span class="line">            metric.add(accuracy(net(X), y), y.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br></pre></td></tr></table></figure><br>训练方面，在进行正向和反向传播之前，我们需要将每一小批量数据移动到指定的设备（例如GPU）上。<br>以下训练函数假定从高级API创建的模型作为输入，并进行相应的优化，使用Xavier随机初始化模型参数。 与全连接层一样，使用交叉熵损失函数和小批量随机梯度下降。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_ch6</span>(<span class="params">net, train_iter, test_iter, num_epochs, lr, device</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;用GPU训练模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_weights</span>(<span class="params">m</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear <span class="keyword">or</span> <span class="built_in">type</span>(m) == nn.Conv2d:</span><br><span class="line">            nn.init.xavier_uniform_(m.weight)</span><br><span class="line">    net.apply(init_weights)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;training on&#x27;</span>, device)</span><br><span class="line">    net.to(device)</span><br><span class="line">    optimizer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">    <span class="comment"># 交叉熵</span></span><br><span class="line">    loss = nn.CrossEntropyLoss()</span><br><span class="line">    <span class="comment"># 省略了Animator实现</span></span><br><span class="line">    animator = Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs],</span><br><span class="line">                            legend=[<span class="string">&#x27;train loss&#x27;</span>, <span class="string">&#x27;train acc&#x27;</span>, <span class="string">&#x27;test acc&#x27;</span>])</span><br><span class="line">    <span class="comment"># 省略了Timer实现</span></span><br><span class="line">    timer, num_batches = Timer(), <span class="built_in">len</span>(train_iter)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="comment"># 训练损失之和，训练准确率之和，样本数</span></span><br><span class="line">        metric = Accumulator(<span class="number">3</span>)</span><br><span class="line">        net.train()</span><br><span class="line">        <span class="keyword">for</span> i, (X, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_iter):</span><br><span class="line">            timer.start()</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            X, y = X.to(device), y.to(device)</span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            l = loss(y_hat, y)</span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                metric.add(l * X.shape[<span class="number">0</span>], accuracy(y_hat, y), X.shape[<span class="number">0</span>])</span><br><span class="line">            timer.stop()</span><br><span class="line">            train_l = metric[<span class="number">0</span>] / metric[<span class="number">2</span>]</span><br><span class="line">            train_acc = metric[<span class="number">1</span>] / metric[<span class="number">2</span>]</span><br><span class="line">            <span class="keyword">if</span> (i + <span class="number">1</span>) % (num_batches // <span class="number">5</span>) == <span class="number">0</span> <span class="keyword">or</span> i == num_batches - <span class="number">1</span>:</span><br><span class="line">                animator.add(epoch + (i + <span class="number">1</span>) / num_batches,</span><br><span class="line">                             (train_l, train_acc, <span class="literal">None</span>))</span><br><span class="line">        test_acc = evaluate_accuracy_gpu(net, test_iter)</span><br><span class="line">        animator.add(epoch + <span class="number">1</span>, (<span class="literal">None</span>, <span class="literal">None</span>, test_acc))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;loss <span class="subst">&#123;train_l:<span class="number">.3</span>f&#125;</span>, train acc <span class="subst">&#123;train_acc:<span class="number">.3</span>f&#125;</span>, &#x27;</span></span><br><span class="line">          <span class="string">f&#x27;test acc <span class="subst">&#123;test_acc:<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;metric[<span class="number">2</span>] * num_epochs / timer.<span class="built_in">sum</span>():<span class="number">.1</span>f&#125;</span> examples/sec &#x27;</span></span><br><span class="line">          <span class="string">f&#x27;on <span class="subst">&#123;<span class="built_in">str</span>(device)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><br>训练和评估LeNet-5模型（M1芯片GPU）：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lr, num_epochs = <span class="number">0.9</span>, <span class="number">10</span></span><br><span class="line">train_ch6(net, train_iter, test_iter, num_epochs, lr, torch.device(<span class="string">&#x27;mps&#x27;</span>))</span><br></pre></td></tr></table></figure><br>关于这一部分的完整代码在<em>实践</em>中可以找到。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;p&gt;本章介绍的卷积神经网络（convolutional neural network，CNN）是一类强大的、为处理图像数据而设计的神经网络。 基于卷积神经网络架构的模型在计算机视觉领域中已经占主导地位，当今几乎所有的图像识别、目标检测或语义分割相关的学术竞赛和商业应用都以这种方法为基础。&lt;/p&gt;
&lt;p&gt;对应实践：&lt;a href=&quot;https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x07.ipynb&quot;&gt;https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x07.ipynb&lt;/a&gt;&lt;br&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://silencezheng.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Python机器学习库简介</title>
    <link href="http://silencezheng.top/2022/07/28/article52/"/>
    <id>http://silencezheng.top/2022/07/28/article52/</id>
    <published>2022-07-28T14:09:25.000Z</published>
    <updated>2022-07-28T14:15:47.372Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>学深度学习也有一段时间了，想着同时也需要看一看机器学习的算法，对机器学习的基础有一个全面些的了解，过程中发现对个别机器学习库的了解不多，写个博客简单总结一下。</p><p>主要介绍的库有：</p><ul><li>pandas</li><li>numpy</li><li>scipy</li><li>sklearn<span id="more"></span><h2 id="NumPy"><a href="#NumPy" class="headerlink" title="NumPy"></a>NumPy</h2>NumPy是使用Python进行科学计算的基础软件包。以纯数学的矩阵计算为基础。<br>核心功能包括：</li><li>功能强大的N维数组对象。</li><li>精密广播功能函数。</li><li>集成 C/C+和Fortran 代码的工具。</li><li>强大的线性代数、傅立叶变换和随机数功能。</li></ul><p>NumPy 最重要的一个特点是其 N 维数组对象 ndarray，它是一系列同类型数据的集合，以 0 下标为开始进行集合中元素的索引。ndarray 对象是用于存放同类型元素的多维数组。ndarray 中的每个元素在内存中都有相同存储大小的区域。</p><p>ndarray对象的内容可以通过索引或切片来访问和修改，与 Python 中 list 的切片操作一样。ndarray 数组可以基于 0 - n 的下标进行索引，切片对象可以通过内置的 slice 函数，并设置 start, stop 及 step 参数进行，从原数组中切割出一个新数组。</p><h2 id="Pandas"><a href="#Pandas" class="headerlink" title="Pandas"></a>Pandas</h2><p>Pandas是一个强大的分析结构化数据的工具集；它的使用基础是Numpy（提供高性能的矩阵运算）；用于数据挖掘和数据分析，同时也提供数据清洗功能。</p><p>Series是一种类似于一维数组的对象，由一组数据(各种NumPy数据类型)以及一组与之相关的数据标签(即索引)组成。仅由一组数据也可产生简单的Series对象。<br><img src="/assets/post_img/article52/Series.jpeg" alt="Series"><br>DataFrame是Pandas中核心的一个表格型的数据结构，包含有一组有序的列，每列可以是不同的值类型(数值、字符串、布尔型等)，DataFrame即有行索引也有列索引，可以被看做是由Series组成的字典。</p><p>Pandas 适用于处理以下类型的数据：</p><ul><li>与 SQL 或 Excel 表类似的，含异构列的表格数据;</li><li>有序和无序（非固定频率）的时间序列数据;</li><li>带行列标签的矩阵数据，包括同构或异构型数据;</li><li>任意其它形式的观测、统计数据集, 数据转入 Pandas 数据结构时不必事先标记。</li></ul><p>利用Pandas做数据清洗也是一个非常常见的应用，例如空值、重复数据和错误数据的清洗。</p><h2 id="SciPy"><a href="#SciPy" class="headerlink" title="SciPy"></a>SciPy</h2><p>SciPy 是基于 Numpy 的科学计算库，用于数学、科学、工程学等领域，很多有一些高阶抽象和物理模型需要使用 SciPy。</p><p>NumPy 能够做一些基础的分析或变换，比如转置/逆矩阵/均值方差的计算等; SciPy则可以提供高阶的分析，比如拟合/回归/参数估计等。</p><p>SciPy被组织成覆盖不同科学计算领域的子包，如下：</p><div class="table-container"><table><thead><tr><th style="text-align:center">子包</th><th style="text-align:center">应用</th></tr></thead><tbody><tr><td style="text-align:center">scipy.cluster</td><td style="text-align:center">矢量量化/Kmeans</td></tr><tr><td style="text-align:center">scipy.constants</td><td style="text-align:center">物理和数学常数</td></tr><tr><td style="text-align:center">scipy.fftpack</td><td style="text-align:center">傅里叶变换</td></tr><tr><td style="text-align:center">scipy.integrate</td><td style="text-align:center">集成例程</td></tr><tr><td style="text-align:center">scipy.interpolate</td><td style="text-align:center">插值</td></tr><tr><td style="text-align:center">scipy.io</td><td style="text-align:center">数据输入和输出</td></tr><tr><td style="text-align:center">scipy.linalg</td><td style="text-align:center">线性代数例程</td></tr><tr><td style="text-align:center">scipy.ndimage</td><td style="text-align:center">n维图像包</td></tr><tr><td style="text-align:center">scipy.odr</td><td style="text-align:center">正交距离回归</td></tr><tr><td style="text-align:center">scipy.optimize</td><td style="text-align:center">优化</td></tr><tr><td style="text-align:center">scipy.signal</td><td style="text-align:center">信号处理</td></tr><tr><td style="text-align:center">scipy.sparse</td><td style="text-align:center">稀疏矩阵</td></tr><tr><td style="text-align:center">scipy.spatial</td><td style="text-align:center">空间数据结构和算法</td></tr><tr><td style="text-align:center">scipy.special</td><td style="text-align:center">任何特殊的数学函数</td></tr><tr><td style="text-align:center">scipy.stats</td><td style="text-align:center">统计</td></tr></tbody></table></div><h2 id="Sklearn"><a href="#Sklearn" class="headerlink" title="Sklearn"></a>Sklearn</h2><p>全称Scikit-learn，Scikit-learn是一个开源的机器学习库，它支持有监督和无监督的学习。它还提供了用于模型拟合，数据预处理，模型选择和评估以及许多其他实用程序的各种工具。</p><p>支持机器学习的六大任务模块：分类（Classification）、回归(Regression)、聚类（Clustering）、降维、模型选择和预处理。</p><p>分类：识别某个对象属于哪个类别，常用的算法有：SVM（支持向量机）、nearest neighbors（最近邻）、random forest（随机森林），常见的应用有：垃圾邮件识别、图像识别。</p><p>回归：预测与对象相关联的连续值属性，常见的算法有：SVR（支持向量机）、 ridge regression（岭回归）、Lasso，常见的应用有：药物反应，预测股价。</p><p>聚类：将相似对象自动分组，常用的算法有：k-Means、 spectral clustering、mean-shift，常见的应用有：客户细分，分组实验结果。</p><p>降维：减少要考虑的随机变量的数量，常见的算法有：PCA（主成分分析）、feature selection（特征选择）、non-negative matrix factorization（非负矩阵分解），常见的应用有：可视化，提高效率。</p><p>模型选择：比较，验证，选择参数和模型，常用的模块有：grid search（网格搜索）、cross validation（交叉验证）、 metrics（度量）。它的目标是通过参数调整提高精度。</p><p>预处理：特征提取和归一化，常用的模块有：preprocessing，feature extraction，常见的应用有：把输入数据（如文本）转换为机器学习算法可用的数据。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;学深度学习也有一段时间了，想着同时也需要看一看机器学习的算法，对机器学习的基础有一个全面些的了解，过程中发现对个别机器学习库的了解不多，写个博客简单总结一下。&lt;/p&gt;
&lt;p&gt;主要介绍的库有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;pandas&lt;/li&gt;
&lt;li&gt;numpy&lt;/li&gt;
&lt;li&gt;scipy&lt;/li&gt;
&lt;li&gt;sklearn</summary>
    
    
    
    
    <category term="Python" scheme="http://silencezheng.top/tags/Python/"/>
    
    <category term="机器学习" scheme="http://silencezheng.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习计算--《动手学深度学习》笔记0x06</title>
    <link href="http://silencezheng.top/2022/07/24/article51/"/>
    <id>http://silencezheng.top/2022/07/24/article51/</id>
    <published>2022-07-24T04:27:41.000Z</published>
    <updated>2022-07-24T04:31:03.103Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><p>本章将深入探索深度学习计算的关键组件， 即模型构建、参数访问与初始化、设计自定义层和块、将模型读写到磁盘， 以及利用GPU实现显著的加速。<br>本节知识极其重要，开始真正窥探深度学习框架的使用。我的所有笔记，都是以PyTorch为基础的（PyTorch不支持的部分使用TensorFlow）。</p><span id="more"></span><h3 id="0-1-结论"><a href="#0-1-结论" class="headerlink" title="0.1. 结论"></a>0.1. 结论</h3><ul><li>一个块可以由许多层组成；一个块可以由许多块组成。</li><li>块可以包含代码。</li><li>块负责大量的内部处理，包括参数初始化和反向传播。</li><li>层和块的顺序连接由Sequential块处理。</li><li>有多种方法可以访问、初始化和绑定模型参数，以及使用自定义初始化方法。</li><li>延后初始化使框架能够自动推断参数形状，使修改模型架构变得容易，避免了一些常见的错误。</li><li>可以通过模型传递数据，使框架最终初始化参数。</li><li>可以通过基本层类设计自定义层，允许我们定义灵活的新层。</li><li>层可以有局部参数，这些参数可以通过内置函数创建。</li><li>save和load函数可用于张量对象的文件读写。</li><li>可以通过参数字典保存和加载网络的全部参数。</li><li>保存架构必须在代码中完成，而不是在参数中完成。</li><li>可以指定用于存储和计算的设备，例如CPU或GPU。默认情况下，数据在主内存中创建，然后使用CPU进行计算。</li><li>深度学习框架要求计算的所有输入数据都在同一设备上，无论是CPU还是GPU。</li><li>不经意地移动数据可能会显著降低性能。一个典型的错误如下：计算GPU上每个小批量的损失，并在命令行中将其报告给用户（或将其记录在NumPy ndarray中）时，将触发全局解释器锁，从而使所有GPU阻塞。最好是为GPU内部的日志分配内存，并且只移动较大的日志。</li></ul><h2 id="1-层和块"><a href="#1-层和块" class="headerlink" title="1. 层和块"></a>1. 层和块</h2><p>具有单一输出的线性模型中整个模型只有一个输出：单个神经网络 （1）接受一些输入； （2）生成相应的标量输出； （3）具有一组相关 参数（parameters），更新这些参数可以优化某目标函数。</p><p>当考虑具有多个输出的网络时， 我们利用矢量化算法来描述整层神经元。 像单个神经元一样，层（1）接受一组输入， （2）生成相应的输出， （3）由一组可调整参数描述。 当我们使用softmax回归时，一个单层本身就是模型。</p><p>对于多层感知机而言，整个模型及其组成层都是上述架构。 整个模型接受原始输入（特征），生成输出（预测）， 并包含一些参数（所有组成层的参数集合）。 同样，每个单独的层接收输入（由前一层提供）， 生成输出（到下一层的输入），并且具有一组可调参数， 这些参数根据从下一层反向传播的信号进行更新。</p><p>事实证明，研究讨论“比单个层大”但“比整个模型小”的组件更有价值，如在计算机视觉中广泛流行的ResNet-152架构就有数百层， 这些层是由层组（groups of layers）的重复模式组成。</p><p>为了实现这些复杂的网络，我们引入了<em>神经网络块</em>的概念。 块（block）可以描述单个层、由多个层组成的组件或整个模型本身。 使用块进行抽象的一个好处是可以将一些块组合成更大的组件， 这一过程通常是递归的，如下图所示。 通过定义代码来按需生成任意复杂度的块，我们可以通过简洁的代码实现复杂的神经网络。<br><img src="/assets/post_img/article51/blocks.svg" alt="blocks"></p><p>从编程的角度来看，块由类（class）表示。 它的任何子类都必须定义一个将其输入转换为输出的正向传播函数， 并且必须存储任何必需的参数（有些块不需要任何参数）。 最后，为了计算梯度，块必须具有反向传播函数。 在定义我们自己的块时，由于自动微分提供了一些后端实现，我们只需要考虑正向传播函数和必需的参数。</p><p>回顾一下多层感知机，下面的代码生成一个网络，其中包含一个具有256个单元和ReLU激活函数的全连接隐藏层， 然后是一个具有10个隐藏单元且不带激活函数的全连接输出层。<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from torch<span class="selector-class">.nn</span> import functional as F</span><br><span class="line"></span><br><span class="line">net = nn<span class="selector-class">.Sequential</span>(nn<span class="selector-class">.Linear</span>(<span class="number">20</span>, <span class="number">256</span>), nn<span class="selector-class">.ReLU</span>(), nn<span class="selector-class">.Linear</span>(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">X = torch<span class="selector-class">.rand</span>(<span class="number">2</span>, <span class="number">20</span>)</span><br><span class="line"><span class="function"><span class="title">net</span><span class="params">(X)</span></span></span><br></pre></td></tr></table></figure><br>这里通过实例化nn.Sequential来构建模型，层的执行顺序是作为参数传递的。 简而言之，n<em>n.Sequential定义了一种特殊的Module，即在PyTorch中表示一个块的类，它维护了一个由Module组成的有序列表。</em><br>值得注意：<br>1、两个全连接层都是Linear类的实例， Linear类是Module的子类。<br>2、通过net(X)调用模型来获得模型的输出，实际上是net.<strong>call</strong>(X)的简写。 这个正向传播函数非常简单：它将列表中的每个块连接在一起，将每个块的输出作为下一个块的输入。</p><h3 id="1-1-自定义块"><a href="#1-1-自定义块" class="headerlink" title="1.1. 自定义块"></a>1.1. 自定义块</h3><p>通过实现一个块（在torch中为nn.Module)来了解块是如何工作的，首先总结一下块的基本功能：</p><ol><li>将输入数据作为其正向传播函数的参数。</li><li>通过正向传播函数来生成输出。输出的形状可能与输入的形状不同。</li><li>计算其输出关于输入的梯度，可通过其反向传播函数进行访问。通常这是自动发生的。</li><li>存储和访问正向传播计算所需的参数。</li><li>根据需要初始化模型参数。</li></ol><p>下面从零开始编写一个块，它含有一个多层感知机，其具有256个隐藏单元的隐藏层和一个10维输出层。MLP类继承了表示块的类。 我们的实现只需要提供我们自己的构造函数（Python中的<strong>init</strong>函数）和正向传播函数。<br><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span>(<span class="title">nn</span>.<span class="title">Module</span>):</span></span><br><span class="line">    <span class="comment"># 用模型参数声明层。这里声明两个全连接的层</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span></span>(<span class="keyword">self</span>):</span><br><span class="line">        <span class="comment"># 调用MLP的父类Module的构造函数来执行必要的初始化。</span></span><br><span class="line">        <span class="comment"># 这样，在类实例化时也可以指定其他函数参数，例如模型参数params</span></span><br><span class="line">        <span class="keyword">super</span>().__init__()</span><br><span class="line">        <span class="keyword">self</span>.hidden = nn.Linear(<span class="number">20</span>, <span class="number">256</span>)  <span class="comment"># 隐藏层</span></span><br><span class="line">        <span class="keyword">self</span>.<span class="keyword">out</span> = nn.Linear(<span class="number">256</span>, <span class="number">10</span>)  <span class="comment"># 输出层</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义模型的正向传播，即如何根据输入X返回所需的模型输出</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span></span>(<span class="keyword">self</span>, X):</span><br><span class="line">        <span class="comment"># 这里使用ReLU的函数版本，其在nn.functional模块中定义。</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">self</span>.<span class="keyword">out</span>(F.relu(<span class="keyword">self</span>.hidden(X)))</span><br></pre></td></tr></table></figure></p><p>此处的前向传播函数以X作为输入， 计算带有激活函数的隐藏表示，并输出其未规范化的输出值。<br>在这个MLP实现中，两个层都是实例变量，在每次调用前向传播函数时调用这些层。 注意一些关键细节： 首先我们的<strong>init</strong>函数通过<code>super().__init__()</code>调用父类的<strong>init</strong>函数， 省去了重复编写模版代码的痛苦。 然后我们实例化两个全连接层，分别为self.hidden和self.out。<br>注意：除非我们需要实现一个新的运算符，否则我们不必担心反向传播函数或参数初始化，系统将自动生成这些。</p><figure class="highlight dos"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 使用自定义块！</span><br><span class="line"><span class="built_in">net</span> = MLP()</span><br><span class="line"><span class="built_in">net</span>(X)</span><br></pre></td></tr></table></figure><p>块的一个主要优点是它的多功能性。 可以利用子类化块以创建层（如全连接层的类）、 整个模型或具有中等复杂度的各种组件。</p><h3 id="1-2-顺序块"><a href="#1-2-顺序块" class="headerlink" title="1.2. 顺序块"></a>1.2. 顺序块</h3><p>Sequential的设计是为了把其他模块串起来。为了构建自己的简化顺序块MySequential， 我们只需要定义两个关键函数：</p><ol><li>一种将块逐个追加到列表中的函数。</li><li>一种正向传播函数，用于将输入按追加块的顺序传递给块组成的“链条”。</li></ol><p>下面的MySequential类提供了与默认Sequential类相同的功能：<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySequential</span>(<span class="title">nn</span>.<span class="title">Module</span>):</span></span><br><span class="line">    <span class="comment"># __init__函数将每个模块逐个添加到有序字典_modules中</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>, *args)</span></span>:</span><br><span class="line">        <span class="keyword">super</span>().__init__()</span><br><span class="line">        <span class="keyword">for</span> idx, <span class="class"><span class="keyword">module</span> <span class="title">in</span> <span class="title">enumerate</span>(<span class="title">args</span>):</span></span><br><span class="line">            <span class="comment"># 这里module是Module子类的一个实例，</span></span><br><span class="line">            <span class="comment"># 把它保存在&#x27;Module&#x27;类的成员变量_modules中。module的类型是OrderedDict。</span></span><br><span class="line">            <span class="keyword">self</span>._modules[str(idx)] = <span class="class"><span class="keyword">module</span></span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(<span class="keyword">self</span>, X)</span></span>:</span><br><span class="line">        <span class="comment"># OrderedDict保证了按照成员添加的顺序遍历它们</span></span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> <span class="keyword">self</span>._modules.values():</span><br><span class="line">            X = block(X)</span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><br>使用_modules属性的主要优点是：在模块的参数初始化过程中，系统知道在_modules字典中查找需要初始化参数的子块。（默认规定）</p><p>当MySequential的正向传播函数被调用时，每个添加的块都按照它们被添加的顺序执行。</p><p>使用我们自定义的MySequential类重新实现多层感知机：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net = MySequential(nn<span class="selector-class">.Linear</span>(<span class="number">20</span>, <span class="number">256</span>), nn<span class="selector-class">.ReLU</span>(), nn<span class="selector-class">.Linear</span>(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line"><span class="function"><span class="title">net</span><span class="params">(X)</span></span></span><br></pre></td></tr></table></figure></p><h3 id="1-3-在正向传播函数中执行代码"><a href="#1-3-在正向传播函数中执行代码" class="headerlink" title="1.3. 在正向传播函数中执行代码"></a>1.3. 在正向传播函数中执行代码</h3><p>Sequential类使模型构造变得简单，允许我们组合新的架构，而不必定义自己的类。 然而并不是所有的架构都是简单的顺序架构，当需要更强的灵活性时，我们需要定义自己的块。 例如，我们可能希望在前向传播函数中执行Python的控制流。 此外，我们可能希望执行任意的数学运算，而不是简单地依赖预定义的神经网络层。</p><p>到目前为止, 我们网络中的所有操作都对网络的激活值及网络的参数起作用。然而有时可能希望合并一些常数参数(constant parameter)，即那些既不是上一层的结果也不是可更新参数的项。例如,我们需要一个计算函数 $f(\mathbf{x}, \mathbf{w})=c \cdot \mathbf{w}^{\top} \mathbf{x}$ 的层, 其中 $\mathbf{x}$ 是输入, $\mathbf{w}$ 是参数, $c$ 是某个在优化过程中没有更新的指定常量。<br>对此情况，书中实现了一个FixedHiddenMLP类, 如下所示：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FixedHiddenMLP</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 不计算梯度的随机权重参数。因此其在训练期间保持不变</span></span><br><span class="line">        <span class="comment"># 常量参数c</span></span><br><span class="line">        self.rand_weight = torch.rand((<span class="number">20</span>, <span class="number">20</span>), requires_grad=<span class="literal">False</span>)</span><br><span class="line">        self.linear = nn.Linear(<span class="number">20</span>, <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        X = self.linear(X)</span><br><span class="line">        <span class="comment"># 使用创建的 常量参数 以及relu和mm函数</span></span><br><span class="line">        X = F.relu(torch.mm(X, self.rand_weight) + <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 复用全连接层。这相当于两个全连接层共享参数</span></span><br><span class="line">        X = self.linear(X)</span><br><span class="line">        <span class="comment"># 控制流</span></span><br><span class="line">        <span class="keyword">while</span> X.<span class="built_in">abs</span>().<span class="built_in">sum</span>() &gt; <span class="number">1</span>:</span><br><span class="line">            X /= <span class="number">2</span></span><br><span class="line">        <span class="keyword">return</span> X.<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure></p><p>在这个FixedHiddenMLP模型中实现了一个隐藏层，其权重（self.rand_weight）在实例化时被随机初始化，之后为常量。 这个权重不是一个模型参数，因此它永远不会被反向传播更新。 然后，神经网络将这个固定层的输出通过一个全连接层。</p><p>在返回输出之前, 模型做了一些不寻常的事情: 它运行了一个while循环, 在 $L_{1}$ 范数大于 1 的条件下, 将 输出向量除以 2 , 直到它满足条件为止。最后, 模型返回了X中所有项的和。这个操作可能不会常用于在任何实际任务中, 只是展示了如何将任意代码集成到神经网络计算的流程中。</p><p>使用该类实现多层感知机，same thing：<br><figure class="highlight isbl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">net</span> = <span class="function"><span class="title">FixedHiddenMLP</span>()</span></span><br><span class="line"><span class="function"><span class="title">net</span>(<span class="variable">X</span>)</span></span><br></pre></td></tr></table></figure></p><p>混合搭配各种组合块的实现举例：<br><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">NestMLP</span>(<span class="title">nn</span>.<span class="type">Module</span>):</span></span><br><span class="line"><span class="class">    def __init__(<span class="title">self</span>):</span></span><br><span class="line"><span class="class">        super().__init__()</span></span><br><span class="line"><span class="class">        self.net = nn.<span class="type">Sequential</span>(<span class="title">nn</span>.<span class="type">Linear</span>(20, 64), nn.<span class="type">ReLU</span>(),</span></span><br><span class="line"><span class="class">                                 nn.<span class="type">Linear</span>(64, 32), nn.<span class="type">ReLU</span>())</span></span><br><span class="line"><span class="class">        self.linear = nn.<span class="type">Linear</span>(32, 16)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    def forward(<span class="title">self</span>, <span class="type">X</span>):</span></span><br><span class="line"><span class="class">        return self.linear(<span class="title">self</span>.<span class="title">net</span>(<span class="type">X</span>))</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">chimera = nn.<span class="type">Sequential</span>(<span class="type">NestMLP</span>(), nn.<span class="type">Linear</span>(16, 20), <span class="type">FixedHiddenMLP</span>())</span></span><br><span class="line"><span class="class">chimera(<span class="type">X</span>)</span></span><br></pre></td></tr></table></figure></p><h3 id="1-4-效率问题"><a href="#1-4-效率问题" class="headerlink" title="1.4. 效率问题"></a>1.4. 效率问题</h3><p>我们在一个高性能的深度学习库中进行了大量的字典查找、 代码执行和许多其他的Python代码。<br>Python的全局解释器锁问题很可能导致运行速度变慢。 CPython 中，全局解释器锁或GIL是一个互斥锁，用于保护对 Python 对象的访问，防止多个线程同时执行 Python 字节码。GIL 防止竞争条件并确保线程安全。这个互斥锁是必要的，主要是因为 CPython 的内存管理不是线程安全的。<br>在深度学习环境中，速度极快的GPU可能要等到CPU运行Python代码后才能运行另一个作业。</p><h2 id="2-参数管理"><a href="#2-参数管理" class="headerlink" title="2. 参数管理"></a>2. 参数管理</h2><p>在选择了架构并设置了超参数后，就进入了训练阶段。 此时，我们的目标是找到使损失函数最小化的模型参数值。 经过训练后，将需要使用这些参数来做出未来的预测。<br>此外，有时我们希望提取参数，以便在其他环境中复用它们， 将模型保存下来，以便它可以在其他软件中执行， 或者为了获得科学的理解而进行检查。</p><p>此节将介绍以下内容：</p><ul><li>访问参数，用于调试、诊断和可视化。</li><li>参数初始化。</li><li>在不同模型组件间共享参数。</li></ul><p>首先，对于一个单隐含层的多层感知机进行研究：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line"></span><br><span class="line">net = nn<span class="selector-class">.Sequential</span>(nn<span class="selector-class">.Linear</span>(<span class="number">4</span>, <span class="number">8</span>), nn<span class="selector-class">.ReLU</span>(), nn<span class="selector-class">.Linear</span>(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line">X = torch<span class="selector-class">.rand</span>(size=(<span class="number">2</span>, <span class="number">4</span>))</span><br><span class="line"><span class="function"><span class="title">net</span><span class="params">(X)</span></span></span><br></pre></td></tr></table></figure></p><h3 id="2-1-参数访问"><a href="#2-1-参数访问" class="headerlink" title="2.1. 参数访问"></a>2.1. 参数访问</h3><p>当通过Sequential类定义模型时， 我们可以通过索引来访问模型的任意层。 这就像模型是一个列表一样，每层的参数都在其属性中。例如检查第二个全连接层的参数：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="title">print</span><span class="params">(net[<span class="number">2</span>].state_dict()</span></span>)</span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">OrderedDict</span>([(&#x27;weight&#x27;, tensor([[ <span class="number">0</span>.<span class="number">3231</span>, -<span class="number">0</span>.<span class="number">3373</span>,  <span class="number">0</span>.<span class="number">1639</span>, -<span class="number">0</span>.<span class="number">3125</span>,  <span class="number">0</span>.<span class="number">0527</span>, -<span class="number">0</span>.<span class="number">2957</span>,  <span class="number">0</span>.<span class="number">0192</span>,  <span class="number">0</span>.<span class="number">0039</span>]])), (&#x27;bias&#x27;, tensor([-<span class="number">0</span>.<span class="number">2930</span>]))])</span><br></pre></td></tr></table></figure><br>结果说明了这个全连接层包含两个参数，分别是该层的权重和偏置。 两者都存储为单精度浮点数（float32）。 注意参数名称允许唯一标识每个参数，即使在包含数百个层的网络中也是如此。</p><h4 id="2-1-1-目标参数"><a href="#2-1-1-目标参数" class="headerlink" title="2.1.1. 目标参数"></a>2.1.1. 目标参数</h4><p>层中的每个参数都表示为参数类的一个实例。 要对参数执行任何操作，首先需要访问底层的数值。 有几种方法可以做到这一点。有些比较简单，而另一些则比较通用。<br>下面的代码从第二个全连接层（即第三个神经网络层）提取偏置， 提取后返回的是一个参数类实例，并进一步访问该参数的值。<br><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span>(<span class="keyword">type</span>(<span class="keyword">net</span>[2].<span class="keyword">bias</span>))</span><br><span class="line"><span class="keyword">print</span>(<span class="keyword">net</span>[2].<span class="keyword">bias</span>)</span><br><span class="line"><span class="keyword">print</span>(<span class="keyword">net</span>[2].<span class="keyword">bias</span>.data)</span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;class <span class="string">&#x27;torch.nn.parameter.Parameter&#x27;</span>&gt;</span><br><span class="line">Parameter containing:</span><br><span class="line"><span class="function"><span class="title">tensor</span><span class="params">([-<span class="number">0.2930</span>], requires_grad=True)</span></span></span><br><span class="line"><span class="function"><span class="title">tensor</span><span class="params">([-<span class="number">0.2930</span>])</span></span></span><br></pre></td></tr></table></figure><br>参数是复合的对象，包含值、梯度和额外信息。 这就是我们需要显式参数值的原因。 除了值之外，还可以访问每个参数的梯度。 在上面这个网络中，由于我们还没有调用反向传播，所以参数的梯度处于初始状态。<br><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net[2]<span class="string">.weight.grad</span> == None <span class="comment"># 为真</span></span><br></pre></td></tr></table></figure></p><h4 id="2-1-2-一次性访问所有参数"><a href="#2-1-2-一次性访问所有参数" class="headerlink" title="2.1.2. 一次性访问所有参数"></a>2.1.2. 一次性访问所有参数</h4><p>有些情况需要访问全部参数，甚至是递归的访问嵌套块的参数。<br>对比访问第一个全连接层的参数和访问所有层的参数：<br><figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># python星号变量的作用是将变量拆分为单个元素</span></span><br><span class="line">print(*[(<span class="built_in">name</span>, <span class="built_in">param</span>.shape) <span class="keyword">for</span> <span class="built_in">name</span>, <span class="built_in">param</span> <span class="built_in">in</span> net[<span class="number">0</span>].named_parameters()])</span><br><span class="line">print(*[(<span class="built_in">name</span>, <span class="built_in">param</span>.shape) <span class="keyword">for</span> <span class="built_in">name</span>, <span class="built_in">param</span> <span class="built_in">in</span> net.named_parameters()])</span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(<span class="symbol">&#x27;weight</span>&#x27;, torch.Size([<span class="name">8</span>, <span class="number">4</span>])) (<span class="symbol">&#x27;bias</span>&#x27;, torch.Size([<span class="name">8</span>]))</span><br><span class="line">(<span class="symbol">&#x27;0.weight</span>&#x27;, torch.Size([<span class="name">8</span>, <span class="number">4</span>])) (<span class="symbol">&#x27;0.bias</span>&#x27;, torch.Size([<span class="name">8</span>])) (<span class="symbol">&#x27;2.weight</span>&#x27;, torch.Size([<span class="name">1</span>, <span class="number">8</span>])) (<span class="symbol">&#x27;2.bias</span>&#x27;, torch.Size([<span class="name">1</span>]))</span><br></pre></td></tr></table></figure><br>可以看到访问所有层参数时，以<code>数字.参数名</code>的形式表示参数，这为我们提供了另一种访问网络参数的方式：<br><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># 访问网络第三层的bias参数值</span></span><br><span class="line"><span class="title">net</span>.state_dict()[&#x27;<span class="number">2.</span>bias&#x27;].<span class="class"><span class="keyword">data</span></span></span><br></pre></td></tr></table></figure></p><h4 id="2-1-3-从嵌套块收集参数"><a href="#2-1-3-从嵌套块收集参数" class="headerlink" title="2.1.3. 从嵌套块收集参数"></a>2.1.3. 从嵌套块收集参数</h4><p>探索如果将多个块相互嵌套，参数命名约定是如何工作的。<br>定义一个生成块的函数（“块工厂”），然后将这些块组合到更大的块中，打印最终的网络：<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">def block1():</span><br><span class="line">    return nn.<span class="built_in">Sequential</span>(nn.<span class="built_in">Linear</span>(<span class="number">4</span>, <span class="number">8</span>), nn.<span class="built_in">ReLU</span>(),</span><br><span class="line">                         nn.<span class="built_in">Linear</span>(<span class="number">8</span>, <span class="number">4</span>), nn.<span class="built_in">ReLU</span>())</span><br><span class="line"></span><br><span class="line">def <span class="built_in">block2</span>():</span><br><span class="line">    net = nn.<span class="built_in">Sequential</span>()</span><br><span class="line">    # <span class="number">4</span>个块<span class="number">1</span>堆叠</span><br><span class="line">    for i in <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">        # 利用add_module嵌套</span><br><span class="line">        net.<span class="built_in">add_module</span>(f<span class="string">&#x27;block &#123;i&#125;&#x27;</span>, <span class="built_in">block1</span>())</span><br><span class="line">    return net</span><br><span class="line"></span><br><span class="line"># 组合块<span class="number">2</span>和一个全连接层</span><br><span class="line">rgnet = nn.<span class="built_in">Sequential</span>(<span class="built_in">block2</span>(), nn.<span class="built_in">Linear</span>(<span class="number">4</span>, <span class="number">1</span>))</span><br><span class="line"><span class="built_in">rgnet</span>(X)</span><br><span class="line"></span><br><span class="line"># 打印设计好的网络</span><br><span class="line"><span class="built_in">print</span>(rgnet)</span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">Sequential(</span><br><span class="line">  (0): Sequential(</span><br><span class="line">    (block 0): Sequential(</span><br><span class="line">      (0): Linear(<span class="attribute">in_features</span>=4, <span class="attribute">out_features</span>=8, <span class="attribute">bias</span>=<span class="literal">True</span>)</span><br><span class="line">      (1): ReLU()</span><br><span class="line">      (2): Linear(<span class="attribute">in_features</span>=8, <span class="attribute">out_features</span>=4, <span class="attribute">bias</span>=<span class="literal">True</span>)</span><br><span class="line">      (3): ReLU()</span><br><span class="line">    )</span><br><span class="line">    (block 1): Sequential(</span><br><span class="line">      (0): Linear(<span class="attribute">in_features</span>=4, <span class="attribute">out_features</span>=8, <span class="attribute">bias</span>=<span class="literal">True</span>)</span><br><span class="line">      (1): ReLU()</span><br><span class="line">      (2): Linear(<span class="attribute">in_features</span>=8, <span class="attribute">out_features</span>=4, <span class="attribute">bias</span>=<span class="literal">True</span>)</span><br><span class="line">      (3): ReLU()</span><br><span class="line">    )</span><br><span class="line">    (block 2): Sequential(</span><br><span class="line">      (0): Linear(<span class="attribute">in_features</span>=4, <span class="attribute">out_features</span>=8, <span class="attribute">bias</span>=<span class="literal">True</span>)</span><br><span class="line">      (1): ReLU()</span><br><span class="line">      (2): Linear(<span class="attribute">in_features</span>=8, <span class="attribute">out_features</span>=4, <span class="attribute">bias</span>=<span class="literal">True</span>)</span><br><span class="line">      (3): ReLU()</span><br><span class="line">    )</span><br><span class="line">    (block 3): Sequential(</span><br><span class="line">      (0): Linear(<span class="attribute">in_features</span>=4, <span class="attribute">out_features</span>=8, <span class="attribute">bias</span>=<span class="literal">True</span>)</span><br><span class="line">      (1): ReLU()</span><br><span class="line">      (2): Linear(<span class="attribute">in_features</span>=8, <span class="attribute">out_features</span>=4, <span class="attribute">bias</span>=<span class="literal">True</span>)</span><br><span class="line">      (3): ReLU()</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (1): Linear(<span class="attribute">in_features</span>=4, <span class="attribute">out_features</span>=1, <span class="attribute">bias</span>=<span class="literal">True</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><br>因为层是分层嵌套的，所以可以像通过嵌套列表索引一样访问它们。<br>下面访问第一个主要的块中第二个子块的第一层的偏置项：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rgnet<span class="selector-attr">[0]</span><span class="selector-attr">[1]</span><span class="selector-attr">[0]</span><span class="selector-class">.bias</span>.data</span><br></pre></td></tr></table></figure></p><h3 id="2-2-参数初始化"><a href="#2-2-参数初始化" class="headerlink" title="2.2. 参数初始化"></a>2.2. 参数初始化</h3><p>深度学习框架提供默认随机初始化， 也允许用户创建自定义初始化方法， 满足通过其他规则实现初始化权重的需要。</p><p>对于PyTorch框架来说，默认情况下，PyTorch会根据一个范围均匀地初始化权重和偏置矩阵， 这个范围是根据输入和输出维度计算出的。 PyTorch的nn.init模块提供了多种预置初始化方法。</p><h4 id="2-2-1-内置初始化"><a href="#2-2-1-内置初始化" class="headerlink" title="2.2.1. 内置初始化"></a>2.2.1. 内置初始化</h4><p>首先调用内置的初始化器,下面的代码将所有权重参数初始化为标准差为0.01的高斯(正态）随机变量，且将偏置参数设置为0。<br><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def init<span class="constructor">_normal(<span class="params">m</span>)</span>:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">type</span>(m)<span class="operator"> == </span>nn.Linear:</span><br><span class="line">        nn.init.normal<span class="constructor">_(<span class="params">m</span>.<span class="params">weight</span>, <span class="params">mean</span>=0, <span class="params">std</span>=0.01)</span></span><br><span class="line">        nn.init.zeros<span class="constructor">_(<span class="params">m</span>.<span class="params">bias</span>)</span></span><br><span class="line"># 注意用法，apply</span><br><span class="line">net.apply(init_normal)</span><br><span class="line">net<span class="literal">[<span class="number">0</span>]</span>.weight.data<span class="literal">[<span class="number">0</span>]</span>, net<span class="literal">[<span class="number">0</span>]</span>.bias.data<span class="literal">[<span class="number">0</span>]</span></span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="name">tensor</span>([<span class="number">-0.0017</span>,  <span class="number">0.0232</span>, <span class="number">-0.0026</span>,  <span class="number">0.0026</span>]), tensor(<span class="number">0</span>.))</span><br></pre></td></tr></table></figure><br>还可以将所有参数初始化为给定的常数，比如初始化为1：<br><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def init<span class="constructor">_constant(<span class="params">m</span>)</span>:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">type</span>(m)<span class="operator"> == </span>nn.Linear:</span><br><span class="line">        nn.init.constant<span class="constructor">_(<span class="params">m</span>.<span class="params">weight</span>, 1)</span></span><br><span class="line">        nn.init.zeros<span class="constructor">_(<span class="params">m</span>.<span class="params">bias</span>)</span></span><br><span class="line">net.apply(init_constant)</span><br></pre></td></tr></table></figure><br>还可以对某些块应用不同的初始化方法。 例如，使用Xavier初始化方法初始化第一个神经网络层，然后将第三个神经网络层初始化为常量值42：<br><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">def</span> xavier(m):</span><br><span class="line">    <span class="keyword">if</span> <span class="class"><span class="keyword">type</span>(<span class="title">m</span>) == nn.<span class="type">Linear</span>:</span></span><br><span class="line">        nn.init.xavier_uniform_(m.weight)</span><br><span class="line"><span class="title">def</span> init_42(m):</span><br><span class="line">    <span class="keyword">if</span> <span class="class"><span class="keyword">type</span>(<span class="title">m</span>) == nn.<span class="type">Linear</span>:</span></span><br><span class="line">        nn.init.constant_(m.weight, <span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="title">net</span>[<span class="number">0</span>].apply(xavier)</span><br><span class="line"><span class="title">net</span>[<span class="number">2</span>].apply(init_42)</span><br><span class="line"><span class="title">print</span>(net[<span class="number">0</span>].weight.<span class="class"><span class="keyword">data</span>[0])</span></span><br><span class="line"><span class="title">print</span>(net[<span class="number">2</span>].weight.<span class="class"><span class="keyword">data</span>)</span></span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">tensor</span>([-<span class="number">0</span>.<span class="number">4645</span>,  <span class="number">0</span>.<span class="number">0062</span>, -<span class="number">0</span>.<span class="number">5186</span>,  <span class="number">0</span>.<span class="number">3513</span>])</span><br><span class="line"><span class="attribute">tensor</span>([[<span class="number">42</span>., <span class="number">42</span>., <span class="number">42</span>., <span class="number">42</span>., <span class="number">42</span>., <span class="number">42</span>., <span class="number">42</span>., <span class="number">42</span>.]])</span><br></pre></td></tr></table></figure></p><h4 id="2-2-2-自定义初始化"><a href="#2-2-2-自定义初始化" class="headerlink" title="2.2.2. 自定义初始化"></a>2.2.2. 自定义初始化</h4><p>有时, 深度学习框架没有提供我们需要的初始化方法。下面的例子中, 我们使用以下的分布为任意权重参数 $w$ 定义初始化方法：</p><script type="math/tex; mode=display">w \sim \begin{cases}U(5,10) & \text { 可能性 } \frac{1}{4} \\ 0 & \text { 可能性 } \frac{1}{2} \\ U(-10,-5) & \text { 可能性 } \frac{1}{4}\end{cases}</script><p>对应<code>my_init</code>：<br><figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def my_init(m):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">type</span>(m) == nn.Linear:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Init&quot;</span>, *[(<span class="keyword">name</span>, param.<span class="built_in">shape</span>)</span><br><span class="line">                        for <span class="keyword">name</span>, param <span class="keyword">in</span> m.named_parameters()][<span class="number">0</span>])</span><br><span class="line">        # -<span class="number">10</span> 到 <span class="number">10</span> 的范围均匀分布</span><br><span class="line">        nn.init.uniform_(m.weight, -<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">        # 这里m.weight.<span class="keyword">data</span>.<span class="built_in">abs</span>() &gt;= <span class="number">5</span>为一个布尔矩阵</span><br><span class="line">        # 将原矩阵与该布尔矩阵按元素乘，小于<span class="number">5</span>的值会变为<span class="number">0</span>，其余元素不变</span><br><span class="line">        m.weight.<span class="keyword">data</span> *= m.weight.<span class="keyword">data</span>.<span class="built_in">abs</span>() &gt;= <span class="number">5</span></span><br><span class="line"></span><br><span class="line">net.apply(my_init)</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">0</span>].weight[:<span class="number">2</span>])</span><br></pre></td></tr></table></figure><br>By the way，参数当然也可以被直接的设置和修改：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net<span class="selector-attr">[0]</span><span class="selector-class">.weight</span><span class="selector-class">.data</span><span class="selector-attr">[:]</span> += <span class="number">1</span></span><br><span class="line">net<span class="selector-attr">[0]</span><span class="selector-class">.weight</span><span class="selector-class">.data</span><span class="selector-attr">[0, 0]</span> = <span class="number">42</span></span><br><span class="line">net<span class="selector-attr">[0]</span><span class="selector-class">.weight</span><span class="selector-class">.data</span><span class="selector-attr">[0]</span></span><br></pre></td></tr></table></figure></p><h3 id="2-3-参数绑定"><a href="#2-3-参数绑定" class="headerlink" title="2.3. 参数绑定"></a>2.3. 参数绑定</h3><p>有时我们希望在多个层间共享参数：可以定义一个稠密层，然后使用它的参数来设置另一个层的参数。<br><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 我们需要给共享层一个名称，以便可以引用它的参数</span></span><br><span class="line"><span class="attribute">shared</span> = nn.Linear(<span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line"><span class="attribute">net</span> = nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(),</span><br><span class="line">                    <span class="attribute">shared</span>, nn.ReLU(),</span><br><span class="line">                    <span class="attribute">shared</span>, nn.ReLU(),</span><br><span class="line">                    <span class="attribute">nn</span>.Linear(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line"><span class="attribute">net</span>(X)</span><br><span class="line"><span class="comment"># 检查参数是否相同</span></span><br><span class="line"><span class="attribute">print</span>(net[<span class="number">2</span>].weight.data[<span class="number">0</span>] == net[<span class="number">4</span>].weight.data[<span class="number">0</span>])</span><br><span class="line"><span class="attribute">net</span>[<span class="number">2</span>].weight.data[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">100</span></span><br><span class="line"><span class="comment"># 确保它们实际上是同一个对象，而不只是有相同的值</span></span><br><span class="line"><span class="attribute">print</span>(net[<span class="number">2</span>].weight.data[<span class="number">0</span>] == net[<span class="number">4</span>].weight.data[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>])</span><br><span class="line">tensor([<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>])</span><br></pre></td></tr></table></figure></p><h2 id="3-延后初始化"><a href="#3-延后初始化" class="headerlink" title="3. 延后初始化"></a>3. 延后初始化</h2><p>非PyTorch的实践中忽略了建立网络时需要做的以下这些事情：</p><ul><li>定义了网络架构，但没有指定输入维度。</li><li>添加层时没有指定前一层的输出维度。</li><li>在初始化参数时，甚至没有足够的信息来确定模型应该包含多少参数。</li></ul><p>但代码却依然能够运行，即便深度学习框架无法判断网络的输入维度是什么。这里的诀窍是框架的延后初始化（defers initialization）， 即直到数据第一次通过模型传递时，框架才会动态地推断出每个层的大小。</p><p>当使用卷积神经网络时，由于输入维度（即图像的分辨率）将影响每个后续层的维数，有了该技术将更加方便。<br>现在我们在编写代码时无须知道维度是什么就可以设置参数， 这种能力可以大大简化定义和修改模型的任务。<br>下面开始研究初始化机制，以TensorFlow为例。似乎PyTorch也准备推出这个功能，为<code>torch.nn.LazyLinear</code>。</p><h3 id="3-1-实例化网络"><a href="#3-1-实例化网络" class="headerlink" title="3.1. 实例化网络"></a>3.1. 实例化网络</h3><p>实例化一个多层感知机：<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow <span class="keyword">as</span> <span class="keyword">tf</span></span><br><span class="line"></span><br><span class="line"># keras是TensorFlow中的高层神经网络API</span><br><span class="line"># Dense层为全连接层，等同Linear</span><br><span class="line">net = <span class="keyword">tf</span>.keras.models.Sequential([</span><br><span class="line">    # <span class="number">256</span>处的参数为units，代表输出维度</span><br><span class="line">    <span class="keyword">tf</span>.keras.layers.Dense(<span class="number">256</span>, activation=<span class="keyword">tf</span>.<span class="keyword">nn</span>.relu),</span><br><span class="line">    <span class="keyword">tf</span>.keras.layers.Dense(<span class="number">10</span>),</span><br><span class="line">])</span><br></pre></td></tr></table></figure><br>此时输入维数是未知的，所以网络不可能知道输入层权重的维数。 因此框架尚未初始化任何参数，通过尝试访问以下参数进行确认：<br><figure class="highlight hy"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[net.layers[i].get_weights() for i in range(<span class="name"><span class="builtin-name">len</span></span>(<span class="name">net.layers</span>))]</span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[], []]</span><br></pre></td></tr></table></figure><br>每个层对象都存在，但权重为空。 使用net.get_weights()将抛出一个错误，因为权重尚未初始化。<br>接下来将数据通过网络，最终使框架初始化参数。<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X = tf<span class="selector-class">.random</span><span class="selector-class">.uniform</span>((<span class="number">2</span>, <span class="number">20</span>))</span><br><span class="line"><span class="function"><span class="title">net</span><span class="params">(X)</span></span></span><br><span class="line"><span class="selector-attr">[w.shape for w in net.get_weights()]</span></span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(<span class="number">20</span>, <span class="number">256</span>), (<span class="number">256</span>,), (<span class="number">256</span>, <span class="number">10</span>), (<span class="number">10</span>,)]</span><br></pre></td></tr></table></figure><br>一旦知道输入维数是20，框架可以通过代入值20来识别第一层权重矩阵的形状。 识别出第一层的形状后，框架处理第二层，依此类推，直到所有形状都已知为止。 注意在这种情况下，只有第一层需要<strong>延迟初始化</strong>，但是框架仍是按顺序初始化的。 等到知道了所有的参数形状，框架就可以初始化参数。</p><h2 id="4-自定义层"><a href="#4-自定义层" class="headerlink" title="4. 自定义层"></a>4. 自定义层</h2><p>深度学习成功背后的一个因素是神经网络的灵活性： 我们可以用创造性的方式组合不同的层，从而设计出适用于各种任务的架构。<br>例如，研究人员发明了专门用于处理图像、文本、序列数据和执行动态规划的层。 当深度学习框架并未提供你需要的层时，则必须构建自定义层。</p><h3 id="4-1-不带参数的层"><a href="#4-1-不带参数的层" class="headerlink" title="4.1. 不带参数的层"></a>4.1. 不带参数的层</h3><p>构造一个没有任何参数的自定义层，CenteredLayer类要从其输入中减去均值，构建它只需继承基础层类并实现前向传播功能：<br><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="title">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">CenteredLayer</span>(<span class="title">nn</span>.<span class="type">Module</span>):</span></span><br><span class="line"><span class="class">    def __init__(<span class="title">self</span>):</span></span><br><span class="line"><span class="class">        super().__init__()</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    def forward(<span class="title">self</span>, <span class="type">X</span>):</span></span><br><span class="line"><span class="class">        return <span class="type">X</span> - <span class="type">X</span>.mean()</span></span><br></pre></td></tr></table></figure><br>向该层提供一些数据，验证它是否能按预期工作：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">layer = CenteredLayer()</span><br><span class="line"><span class="function"><span class="title">layer</span><span class="params">(torch.FloatTensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span></span>)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line"><span class="function"><span class="title">tensor</span><span class="params">([-<span class="number">2</span>., -<span class="number">1</span>.,  <span class="number">0</span>.,  <span class="number">1</span>.,  <span class="number">2</span>.])</span></span></span><br></pre></td></tr></table></figure></p><p>将层作为组件合并到更复杂的模型中：<br><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net = nn.<span class="constructor">Sequential(<span class="params">nn</span>.Linear(8, 128)</span>, <span class="constructor">CenteredLayer()</span>)</span><br></pre></td></tr></table></figure><br>在向该网络发送随机数据后，检查一下均值是否为0：<br><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">Y</span> = net(torch.rand(<span class="number">4</span>, <span class="number">8</span>))</span><br><span class="line"><span class="attribute">Y</span>.mean()</span><br></pre></td></tr></table></figure><br>由于处理的是浮点数，因为存储精度的原因，仍然可能会看到一个非常小的非零数（正常来说Y的均值应该是0）。</p><h3 id="4-2-带参数的层"><a href="#4-2-带参数的层" class="headerlink" title="4.2. 带参数的层"></a>4.2. 带参数的层</h3><p>下面定义具有参数的层， 这些参数可以通过训练进行调整。 我们可以使用内置函数来创建参数，这些函数提供一些基本的管理功能。 比如管理访问、初始化、共享、保存和加载模型参数。 这样做的好处之一是：我们不需要为每个自定义层编写自定义的序列化程序。</p><p>全连接层层需要两个参数，一个用于表示权重，另一个用于表示偏置项。 在这个版本的自定义实现中，使用修正线性单元作为激活函数。该层需要输入参数in_units和units，分别表示输入数和输出数：<br><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># 自定义实现全连接层</span><br><span class="line"><span class="keyword">class</span> <span class="constructor">MyLinear(<span class="params">nn</span>.Module)</span>:</span><br><span class="line">    def <span class="constructor">__init__(<span class="params">self</span>, <span class="params">in_units</span>, <span class="params">units</span>)</span>:</span><br><span class="line">        super<span class="literal">()</span>.<span class="constructor">__init__()</span></span><br><span class="line">        # nn.Parameter处理参数会有很多好处</span><br><span class="line">        # 例如可以使用named<span class="constructor">_parameters()</span>获取参数等</span><br><span class="line">        # 即可以像内置的Module一样使用自定义层</span><br><span class="line">        self.weight = nn.<span class="constructor">Parameter(<span class="params">torch</span>.<span class="params">randn</span>(<span class="params">in_units</span>, <span class="params">units</span>)</span>)</span><br><span class="line">        self.bias = nn.<span class="constructor">Parameter(<span class="params">torch</span>.<span class="params">randn</span>(<span class="params">units</span>,)</span>)</span><br><span class="line">    def forward(self, X):</span><br><span class="line">        # 全连接层计算</span><br><span class="line">        linear = torch.matmul(X, self.weight.data) + self.bias.data</span><br><span class="line">        # 激活函数</span><br><span class="line">        return <span class="module-access"><span class="module"><span class="identifier">F</span>.</span></span>relu(linear)</span><br></pre></td></tr></table></figure><br>实例化MyLinear类并访问其模型参数：<br><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">linear = MyLinear(5, 3)</span><br><span class="line">linear.weight</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor(<span class="comment">[<span class="comment">[ 1.9054, -3.4102, -0.9792]</span>,</span></span><br><span class="line"><span class="comment">        <span class="comment">[ 1.5522,  0.8707,  0.6481]</span>,</span></span><br><span class="line"><span class="comment">        <span class="comment">[ 1.0974,  0.2568,  0.4034]</span>,</span></span><br><span class="line"><span class="comment">        <span class="comment">[ 0.1416, -1.1389,  0.5875]</span>,</span></span><br><span class="line"><span class="comment">        <span class="comment">[-0.7209,  0.4432,  0.1222]</span>]</span>, requires_grad=True)</span><br></pre></td></tr></table></figure><br>使用自定义层直接执行前向传播计算：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="title">linear</span><span class="params">(torch.rand(<span class="number">2</span>, <span class="number">5</span>)</span></span>)</span><br></pre></td></tr></table></figure><br>使用自定义层构建模型：<br><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">net</span> = nn.Sequential(MyLinear(<span class="number">64</span>, <span class="number">8</span>), MyLinear(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line"><span class="attribute">net</span>(torch.rand(<span class="number">2</span>, <span class="number">64</span>))</span><br></pre></td></tr></table></figure></p><h2 id="5-读写文件"><a href="#5-读写文件" class="headerlink" title="5. 读写文件"></a>5. 读写文件</h2><p>有时我们希望保存训练的模型， 以备将来在各种环境中使用（比如在部署中进行预测）。 此外，当运行一个耗时较长的训练过程时， 最佳的做法是定期保存中间结果， 以确保在服务器电源被不小心断掉时不会损失几天的计算结果。<br>本节学习如何加载和存储权重向量和整个模型。</p><h3 id="5-1-加载和保存张量"><a href="#5-1-加载和保存张量" class="headerlink" title="5.1. 加载和保存张量"></a>5.1. 加载和保存张量</h3><p>对于单个张量，可以直接调用load和save函数分别读写它们。 这两个函数都要求我们提供一个名称，save要求将要保存的变量作为输入。<br><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">x = torch.arange(<span class="number">4</span>)</span><br><span class="line">torch.save(x, <span class="string">&#x27;x-file&#x27;</span>)</span><br></pre></td></tr></table></figure><br>将存储在文件中的数据读回内存：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x2 = torch<span class="selector-class">.load</span>(<span class="string">&#x27;x-file&#x27;</span>)</span><br><span class="line">x2</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line"><span class="function"><span class="title">tensor</span><span class="params">([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span></span></span><br></pre></td></tr></table></figure><br>存储一个张量列表，然后读回内存：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">y = torch<span class="selector-class">.zeros</span>(<span class="number">4</span>)</span><br><span class="line">torch<span class="selector-class">.save</span>(<span class="selector-attr">[x, y]</span>,<span class="string">&#x27;x-files&#x27;</span>)</span><br><span class="line">x2, y2 = torch<span class="selector-class">.load</span>(<span class="string">&#x27;x-files&#x27;</span>)</span><br><span class="line">(x2, y2)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">(tensor(<span class="selector-attr">[0, 1, 2, 3]</span>), tensor(<span class="selector-attr">[0., 0., 0., 0.]</span>))</span><br></pre></td></tr></table></figure><br>可以写入或读取从字符串映射到张量的字典，当需要读取或写入模型中的所有权重时很有用：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mydict = &#123;<span class="string">&#x27;x&#x27;</span>: x, <span class="string">&#x27;y&#x27;</span>: y&#125;</span><br><span class="line">torch<span class="selector-class">.save</span>(mydict, <span class="string">&#x27;mydict&#x27;</span>)</span><br><span class="line">mydict2 = torch<span class="selector-class">.load</span>(<span class="string">&#x27;mydict&#x27;</span>)</span><br><span class="line">mydict2</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">&#123;<span class="string">&#x27;x&#x27;</span>: tensor(<span class="selector-attr">[0, 1, 2, 3]</span>), <span class="string">&#x27;y&#x27;</span>: tensor(<span class="selector-attr">[0., 0., 0., 0.]</span>)&#125;</span><br></pre></td></tr></table></figure></p><h3 id="5-2-加载和保存模型参数"><a href="#5-2-加载和保存模型参数" class="headerlink" title="5.2. 加载和保存模型参数"></a>5.2. 加载和保存模型参数</h3><p>深度学习框架提供了内置函数来保存和加载整个网络。注意这将<em>保存模型的参数而不是保存整个模型</em>。<br>例如，如果我们有一个3层多层感知机，我们需要单独指定架构。 因为模型本身可以包含任意代码，难以序列化。 为了恢复模型，我们需要用代码生成架构， 然后从磁盘加载参数。<br>以下面的多层感知机为例：<br><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">MLP</span>(<span class="title">nn</span>.<span class="type">Module</span>):</span></span><br><span class="line"><span class="class">    def __init__(<span class="title">self</span>):</span></span><br><span class="line"><span class="class">        super().__init__()</span></span><br><span class="line"><span class="class">        self.hidden = nn.<span class="type">Linear</span>(20, 256)</span></span><br><span class="line"><span class="class">        self.output = nn.<span class="type">Linear</span>(256, 10)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    def forward(<span class="title">self</span>, <span class="title">x</span>):</span></span><br><span class="line"><span class="class">        return self.output(<span class="type">F</span>.<span class="title">relu</span>(<span class="title">self</span>.<span class="title">hidden</span>(<span class="title">x</span>)))</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">net = <span class="type">MLP</span>()</span></span><br><span class="line"><span class="class"><span class="type">X</span> = torch.randn(<span class="title">size</span>=(2, 20))</span></span><br><span class="line"><span class="class"><span class="type">Y</span> = net(<span class="type">X</span>)</span></span><br></pre></td></tr></table></figure><br>存储模型参数到文件：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch<span class="selector-class">.save</span>(net<span class="selector-class">.state_dict</span>(), <span class="string">&#x27;mlp.params&#x27;</span>)</span><br></pre></td></tr></table></figure><br>恢复模型，实例化了原始多层感知机模型的一个备份。这里不需要随机初始化模型参数，而是直接读取文件中存储的参数：<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">clone</span> = MLP()</span><br><span class="line"><span class="keyword">clone</span>.load_state_dict(torch.load(<span class="string">&#x27;mlp.params&#x27;</span>))</span><br><span class="line"><span class="comment"># 进入评估模式,对这个例子没啥用</span></span><br><span class="line"><span class="keyword">clone</span>.<span class="keyword">eval</span>()</span><br></pre></td></tr></table></figure><br>由于两个实例具有相同的模型参数，在输入相同的X时，两个实例的计算结果应该相同，即：<code>clone(X) == Y</code></p><h2 id="6-GPU"><a href="#6-GPU" class="headerlink" title="6. GPU"></a>6. GPU</h2><p>自2000年以来，GPU性能每十年增长1000倍。本节将讨论如何使用单个GPU，然后是如何使用多个GPU和多个服务器（具有多个GPU）。</p><p>先看看如何使用单个NVIDIA GPU进行计算。<br>首先，确保实验机器至少安装了一个NVIDIA GPU。 然后，下载<a href="https://developer.nvidia.com/cuda-downloads">NVIDIA驱动和CUDA</a> 并按照提示设置适当的路径。 当这些准备工作完成，就可以使用nvidia-smi命令来查看显卡信息。<br>例如：<br><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">Thu Mar 31 23:45:57 2022</span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">|<span class="string"> NVIDIA-SMI 460.27.04    Driver Version: 460.27.04    CUDA Version: 11.2     </span>|</span><br><span class="line">|<span class="string">-------------------------------+----------------------+----------------------+</span></span><br><span class="line"><span class="string"></span>|<span class="string"> GPU  Name        Persistence-M</span>|<span class="string"> Bus-Id        Disp.A </span>|<span class="string"> Volatile Uncorr. ECC </span>|</span><br><span class="line">|<span class="string"> Fan  Temp  Perf  Pwr:Usage/Cap</span>|<span class="string">         Memory-Usage </span>|<span class="string"> GPU-Util  Compute M. </span>|</span><br><span class="line">|<span class="string">                               </span>|<span class="string">                      </span>|<span class="string">               MIG M. </span>|</span><br><span class="line">|<span class="string">===============================+======================+======================</span>|</span><br><span class="line">|<span class="string">   0  Tesla V100-SXM2...  Off  </span>|<span class="string"> 00000000:00:1B.0 Off </span>|<span class="string">                    0 </span>|</span><br><span class="line">|<span class="string"> N/A   44C    P0    77W / 300W </span>|<span class="string">   1612MiB / 16160MiB </span>|<span class="string">     63%      Default </span>|</span><br><span class="line">|<span class="string">                               </span>|<span class="string">                      </span>|<span class="string">                  N/A </span>|</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line">|<span class="string">   1  Tesla V100-SXM2...  Off  </span>|<span class="string"> 00000000:00:1C.0 Off </span>|<span class="string">                    0 </span>|</span><br><span class="line">|<span class="string"> N/A   47C    P0    38W / 300W </span>|<span class="string">      0MiB / 16160MiB </span>|<span class="string">      0%      Default </span>|</span><br><span class="line">|<span class="string">                               </span>|<span class="string">                      </span>|<span class="string">                  N/A </span>|</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line">|<span class="string">   2  Tesla V100-SXM2...  Off  </span>|<span class="string"> 00000000:00:1D.0 Off </span>|<span class="string">                    0 </span>|</span><br><span class="line">|<span class="string"> N/A   44C    P0    77W / 300W </span>|<span class="string">   1624MiB / 16160MiB </span>|<span class="string">     61%      Default </span>|</span><br><span class="line">|<span class="string">                               </span>|<span class="string">                      </span>|<span class="string">                  N/A </span>|</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line">|<span class="string">   3  Tesla V100-SXM2...  Off  </span>|<span class="string"> 00000000:00:1E.0 Off </span>|<span class="string">                    0 </span>|</span><br><span class="line">|<span class="string"> N/A   52C    P0    42W / 300W </span>|<span class="string">      0MiB / 16160MiB </span>|<span class="string">      0%      Default </span>|</span><br><span class="line">|<span class="string">                               </span>|<span class="string">                      </span>|<span class="string">                  N/A </span>|</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line"></span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">|<span class="string"> Processes:                                                                  </span>|</span><br><span class="line">|<span class="string">  GPU   GI   CI        PID   Type   Process name                  GPU Memory </span>|</span><br><span class="line">|<span class="string">        ID   ID                                                   Usage      </span>|</span><br><span class="line">|<span class="string">=============================================================================</span>|</span><br><span class="line">|<span class="string">    0   N/A  N/A     94484      C   ...l-zh-release-0/bin/python     1609MiB </span>|</span><br><span class="line">|<span class="string">    2   N/A  N/A     94484      C   ...l-zh-release-0/bin/python     1621MiB </span>|</span><br><span class="line">+-----------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure><br>在PyTorch中，每个数组都有一个设备（device）， 我们通常将其称为上下文（context）。 默认情况下，所有变量和相关的计算都分配给CPU。 有时上下文可能是GPU。 当我们跨多个服务器部署作业时，事情会变得更加棘手。 通过智能地将数组分配给上下文（设备）， 我们可以最大限度地减少在设备之间传输数据的时间。 例如当在带有GPU的服务器上训练神经网络时，通常希望模型的参数在GPU上。</p><p>要运行此部分中的程序，至少需要两个GPU。（目前一个都没有😭）</p><h3 id="6-1-计算设备"><a href="#6-1-计算设备" class="headerlink" title="6.1. 计算设备"></a>6.1. 计算设备</h3><p>我们可以指定用于存储和计算的设备，如CPU和GPU。 默认情况下，张量是在内存中创建的，然后使用CPU计算它。</p><p>在PyTorch中，CPU和GPU可以用torch.device(‘cpu’) 和torch.device(‘cuda’)表示。 应该注意的是，cpu设备意味着所有物理CPU和内存，这意味着PyTorch的计算将尝试使用所有CPU核心。 然而，gpu设备只代表一个卡和相应的显存。 如果有多个GPU，我们使用torch.device(f’cuda:{i}’) 来表示第i块GPU（i从0开始）。 另外，cuda:0和cuda是等价的。</p><p>对于M1芯片的设备，<code>torch.has_mps</code>可以检查是否有gpu😆。<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"><span class="keyword">from</span> torch import nn</span><br><span class="line"></span><br><span class="line">torch.device(<span class="string">&#x27;cpu&#x27;</span>), torch.device(<span class="string">&#x27;cuda&#x27;</span>), torch.device(<span class="string">&#x27;cuda:1&#x27;</span>)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">(device(<span class="attribute">type</span>=<span class="string">&#x27;cpu&#x27;</span>), device(<span class="attribute">type</span>=<span class="string">&#x27;cuda&#x27;</span>), device(<span class="attribute">type</span>=<span class="string">&#x27;cuda&#x27;</span>, <span class="attribute">index</span>=1))</span><br></pre></td></tr></table></figure><br>查询可用gpu数量：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch<span class="selector-class">.cuda</span><span class="selector-class">.device_count</span>()</span><br></pre></td></tr></table></figure><br>定义两个函数,允许我们在不存在所需所有GPU的情况下运行代码。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">try_gpu</span>(<span class="params">i=<span class="number">0</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;如果存在，则返回gpu(i)，否则返回cpu()&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> torch.cuda.device_count() &gt;= i + <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.device(<span class="string">f&#x27;cuda:<span class="subst">&#123;i&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.device(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">try_all_gpus</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回所有可用的GPU，如果没有GPU，则返回[cpu(),]&quot;&quot;&quot;</span></span><br><span class="line">    devices = [torch.device(<span class="string">f&#x27;cuda:<span class="subst">&#123;i&#125;</span>&#x27;</span>)</span><br><span class="line">             <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(torch.cuda.device_count())]</span><br><span class="line">    <span class="keyword">return</span> devices <span class="keyword">if</span> devices <span class="keyword">else</span> [torch.device(<span class="string">&#x27;cpu&#x27;</span>)]</span><br><span class="line"></span><br><span class="line">try_gpu(), try_gpu(<span class="number">10</span>), try_all_gpus()</span><br></pre></td></tr></table></figure></p><h3 id="6-2-张量与GPU"><a href="#6-2-张量与GPU" class="headerlink" title="6.2. 张量与GPU"></a>6.2. 张量与GPU</h3><p>查询张量所在的设备，默认情况下，张量是在CPU上创建的：<br><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">x</span> = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="attribute">x</span>.device</span><br></pre></td></tr></table></figure><br>需要注意的是，无论何时我们要对多个项进行操作， 它们都必须在同一个设备上。 例如，如果我们对两个张量求和， 我们需要确保两个张量都位于同一个设备上，否则框架将不知道在哪里存储结果，甚至不知道在哪里执行计算。</p><h4 id="6-2-1-存储在GPU上"><a href="#6-2-1-存储在GPU上" class="headerlink" title="6.2.1. 存储在GPU上"></a>6.2.1. 存储在GPU上</h4><p>有几种方法可以在GPU上存储张量。 例如可以在创建张量时指定存储设备。<br>接下来在第一个gpu上创建张量变量X。在GPU上创建的张量只消耗这个GPU的显存。我们可以使用nvidia-smi命令查看显存使用情况。 一般需要确保不创建超过GPU显存限制的数据。<br><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">X</span> = torch.ones(<span class="number">2</span>, <span class="number">3</span>, device=try_gpu())</span><br><span class="line"><span class="attribute">X</span></span><br></pre></td></tr></table></figure><br>假设至少有两个GPU，下面的代码将在第二个GPU上创建一个随机张量:<br><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">Y</span> = torch.rand(<span class="number">2</span>, <span class="number">3</span>, device=try_gpu(<span class="number">1</span>))</span><br><span class="line"><span class="attribute">Y</span></span><br></pre></td></tr></table></figure></p><p>对于M1芯片的设备，可以这样进行操作：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X = torch<span class="selector-class">.ones</span>(<span class="number">2</span>, <span class="number">3</span>, device=torch<span class="selector-class">.device</span>(<span class="string">&#x27;mps&#x27;</span>))</span><br><span class="line"><span class="function"><span class="title">print</span><span class="params">(X.device)</span></span></span><br><span class="line"></span><br><span class="line">Y = torch<span class="selector-class">.ones</span>(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="function"><span class="title">print</span><span class="params">(Y.device)</span></span></span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">mps:</span><span class="number">0</span> <span class="meta"># 并且只有这一个GPU，合理。</span></span><br><span class="line">cpu</span><br></pre></td></tr></table></figure><br>可以看出默认是在cpu上，但是可以指定到mps上。</p><h4 id="6-2-2-复制"><a href="#6-2-2-复制" class="headerlink" title="6.2.2. 复制"></a>6.2.2. 复制</h4><p>如果要计算X + Y则需要决定在哪里执行这个操作。如下图，我们可以将X传输到第二个GPU并在那里执行操作。<br><img src="/assets/post_img/article51/copyto.svg" alt="copyto"><br>不要简单地X加上Y，这会导致异常，运行时引擎不知道该怎么做：它在同一设备上找不到数据会导致失败。 由于Y位于第二个GPU上，所以需要将X移到那里， 然后才能执行相加运算。<br><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Z = X.cuda(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(X)</span><br><span class="line"><span class="built_in">print</span>(Z)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor(<span class="string">[[1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1.]]</span>, device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">tensor(<span class="string">[[1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1.]]</span>, device=<span class="string">&#x27;cuda:1&#x27;</span>)</span><br></pre></td></tr></table></figure><br>现在数据在同一个GPU上（Z和Y都在），可以将它们相加。<br><figure class="highlight tp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Y</span> + <span class="keyword">Z</span></span><br></pre></td></tr></table></figure><br>假设变量Z已经存在于第二个GPU上。 此时调用Z.cuda(1)将返回Z，而不会复制并分配新内存。<br><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Z.cuda(<span class="number">1</span>) <span class="keyword">is</span> Z</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure></p><h4 id="6-2-3-旁注"><a href="#6-2-3-旁注" class="headerlink" title="6.2.3. 旁注"></a>6.2.3. 旁注</h4><p>人们使用GPU来进行机器学习，因为单个GPU相对运行速度快。 但是在设备（CPU、GPU和其他机器）之间传输数据比计算慢得多。 这也使得并行化变得更加困难，因为我们必须等待数据被发送（或者接收）， 然后才能继续进行更多的操作。 这就是为什么拷贝操作要格外小心。 根据经验，多个小操作比一个大操作糟糕得多。 此外，一次执行几个操作比代码中散布的许多单个操作要好得多（除非你确信自己在做什么）。 如果一个设备必须等待另一个设备才能执行其他操作， 那么这样的操作可能会阻塞。 </p><p>最后，当我们打印张量或将张量转换为NumPy格式时， 如果数据不在内存中，框架会首先将其复制到内存中， 这会导致额外的传输开销。 更糟糕的是，它现在受制于全局解释器锁，使得一切都得等待Python完成。</p><h3 id="6-3-神经网络与GPU"><a href="#6-3-神经网络与GPU" class="headerlink" title="6.3. 神经网络与GPU"></a>6.3. 神经网络与GPU</h3><p>神经网络模型可以指定设备,将模型参数放在GPU上:<br><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net = nn.<span class="constructor">Sequential(<span class="params">nn</span>.Linear(3, 1)</span>)</span><br><span class="line">net = net.<span class="keyword">to</span>(device=<span class="keyword">try</span><span class="constructor">_gpu()</span>)</span><br></pre></td></tr></table></figure><br>当输入为GPU上的张量时，模型将在同一GPU上计算结果:<br><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">net(X)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor(<span class="string">[[-0.7803],</span></span><br><span class="line"><span class="string">        [-0.7803]]</span>, device=<span class="string">&#x27;cuda:0&#x27;</span>, grad_fn=&lt;AddmmBackward0&gt;)</span><br></pre></td></tr></table></figure><br>确认模型参数存储在同一个GPU上：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net<span class="selector-attr">[0]</span><span class="selector-class">.weight</span><span class="selector-class">.data</span>.device</span><br></pre></td></tr></table></figure><br>只要所有的数据和参数都在同一个设备上，就可以有效地学习模型。</p><h3 id="6-4-比较M1芯片设备CPU与GPU速度差异"><a href="#6-4-比较M1芯片设备CPU与GPU速度差异" class="headerlink" title="6.4. 比较M1芯片设备CPU与GPU速度差异"></a>6.4. 比较M1芯片设备CPU与GPU速度差异</h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">t1 = <span class="selector-tag">time</span><span class="selector-class">.time</span>()</span><br><span class="line">X = torch<span class="selector-class">.ones</span>(<span class="number">100000</span>, <span class="number">10000</span>, device=torch<span class="selector-class">.device</span>(<span class="string">&#x27;mps&#x27;</span>))</span><br><span class="line"><span class="function"><span class="title">print</span><span class="params">(X.device)</span></span></span><br><span class="line">net = nn<span class="selector-class">.Sequential</span>(nn<span class="selector-class">.Linear</span>(<span class="number">10000</span>, <span class="number">1000</span>))</span><br><span class="line">net = net<span class="selector-class">.to</span>(device=torch<span class="selector-class">.device</span>(<span class="string">&#x27;mps&#x27;</span>))</span><br><span class="line"><span class="function"><span class="title">net</span><span class="params">(X)</span></span></span><br><span class="line"><span class="function"><span class="title">print</span><span class="params">(time.time()</span></span> - t1)</span><br><span class="line"></span><br><span class="line">t2 = <span class="selector-tag">time</span><span class="selector-class">.time</span>()</span><br><span class="line">Y = torch<span class="selector-class">.ones</span>(<span class="number">100000</span>, <span class="number">10000</span>)</span><br><span class="line"><span class="function"><span class="title">print</span><span class="params">(Y.device)</span></span></span><br><span class="line">net2 = nn<span class="selector-class">.Sequential</span>(nn<span class="selector-class">.Linear</span>(<span class="number">10000</span>, <span class="number">1000</span>))</span><br><span class="line"><span class="function"><span class="title">net2</span><span class="params">(Y)</span></span></span><br><span class="line"><span class="function"><span class="title">print</span><span class="params">(time.time()</span></span> - t2)</span><br></pre></td></tr></table></figure><p>输出：<br><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">mps</span>:<span class="number">0</span></span><br><span class="line"><span class="attribute">0</span>.<span class="number">2889399528503418</span></span><br><span class="line"><span class="attribute">cpu</span></span><br><span class="line"><span class="attribute">4</span>.<span class="number">196138620376587</span></span><br></pre></td></tr></table></figure><br>可以看到差异极大。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;p&gt;本章将深入探索深度学习计算的关键组件， 即模型构建、参数访问与初始化、设计自定义层和块、将模型读写到磁盘， 以及利用GPU实现显著的加速。&lt;br&gt;本节知识极其重要，开始真正窥探深度学习框架的使用。我的所有笔记，都是以PyTorch为基础的（PyTorch不支持的部分使用TensorFlow）。&lt;/p&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://silencezheng.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>房价预测--《动手学深度学习》笔记0x05</title>
    <link href="http://silencezheng.top/2022/07/20/article50/"/>
    <id>http://silencezheng.top/2022/07/20/article50/</id>
    <published>2022-07-20T14:13:40.000Z</published>
    <updated>2022-07-20T14:16:47.641Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><p>Kaggle的<a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques">房价预测比赛</a>，数据集由Bart de Cock于2011年收集，涵盖了2006-2010年期间亚利桑那州埃姆斯市的房价。这个数据集是相当通用的，不会需要使用复杂模型架构。 它比哈里森和鲁宾菲尔德的波士顿房价数据集要大得多，也有更多的特征。</p><p>对应实践：<a href="https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x05.ipynb">https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x05.ipynb</a><br><span id="more"></span></p><h2 id="1-下载、缓存数据集"><a href="#1-下载、缓存数据集" class="headerlink" title="1. 下载、缓存数据集"></a>1. 下载、缓存数据集</h2><p>实现几个函数来方便下载数据，这些函数是用来下载、管理多个数据集的。</p><p>字典DATA_HUB将 数据集名称的字符串 映射到 数据集相关的二元组上， 这个二元组包含数据集的url和验证文件完整性的sha-1密钥。 所有这样的数据集都托管在地址为DATA_URL的站点上，这个链接是d2l的数据托管地址。<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">DATA_HUB</span> = dict()</span><br><span class="line"><span class="attr">DATA_URL</span> = <span class="string">&#x27;http://d2l-data.s3-accelerate.amazonaws.com/&#x27;</span></span><br></pre></td></tr></table></figure></p><p>实现一个download函数，当本地不存在数据时，下载数据集，当本地存在时，对本地数据集的sha-1密钥与数据托管地址中的比对，验证符合后使用本地数据集。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download</span>(<span class="params">name, cache_dir=os.path.join(<span class="params"><span class="string">&#x27;..&#x27;</span>, <span class="string">&#x27;data&#x27;</span></span>)</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载一个DATA_HUB中的文件，返回本地文件名&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> name <span class="keyword">in</span> DATA_HUB, <span class="string">f&quot;<span class="subst">&#123;name&#125;</span> 不存在于 <span class="subst">&#123;DATA_HUB&#125;</span>&quot;</span></span><br><span class="line">    url, sha1_hash = DATA_HUB[name]</span><br><span class="line">    os.makedirs(cache_dir, exist_ok=<span class="literal">True</span>)</span><br><span class="line">    fname = os.path.join(cache_dir, url.split(<span class="string">&#x27;/&#x27;</span>)[-<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">if</span> os.path.exists(fname):</span><br><span class="line">        sha1 = hashlib.sha1()</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(fname, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">                data = f.read(<span class="number">1048576</span>)</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> data:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                sha1.update(data)</span><br><span class="line">        <span class="keyword">if</span> sha1.hexdigest() == sha1_hash:</span><br><span class="line">            <span class="keyword">return</span> fname  <span class="comment"># 命中缓存</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;正在从<span class="subst">&#123;url&#125;</span>下载<span class="subst">&#123;fname&#125;</span>...&#x27;</span>)</span><br><span class="line">    r = requests.get(url, stream=<span class="literal">True</span>, verify=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(fname, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(r.content)</span><br><span class="line">    <span class="keyword">return</span> fname</span><br></pre></td></tr></table></figure><br>实现两个实用函数： 一个将下载并解压缩文件， 另一个是将d2l书中使用的所有数据集从DATA_HUB下载到缓存目录中。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download_extract</span>(<span class="params">name, folder=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载并解压zip/tar文件&quot;&quot;&quot;</span></span><br><span class="line">    fname = download(name)</span><br><span class="line">    base_dir = os.path.dirname(fname)</span><br><span class="line">    data_dir, ext = os.path.splitext(fname)</span><br><span class="line">    <span class="keyword">if</span> ext == <span class="string">&#x27;.zip&#x27;</span>:</span><br><span class="line">        fp = zipfile.ZipFile(fname, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    <span class="keyword">elif</span> ext <span class="keyword">in</span> (<span class="string">&#x27;.tar&#x27;</span>, <span class="string">&#x27;.gz&#x27;</span>):</span><br><span class="line">        fp = tarfile.<span class="built_in">open</span>(fname, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">assert</span> <span class="literal">False</span>, <span class="string">&#x27;只有zip/tar文件可以被解压缩&#x27;</span></span><br><span class="line">    fp.extractall(base_dir)</span><br><span class="line">    <span class="keyword">return</span> os.path.join(base_dir, folder) <span class="keyword">if</span> folder <span class="keyword">else</span> data_dir</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download_all</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载DATA_HUB中的所有文件&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> DATA_HUB:</span><br><span class="line">        download(name)</span><br></pre></td></tr></table></figure></p><h2 id="2-访问和读取数据集"><a href="#2-访问和读取数据集" class="headerlink" title="2. 访问和读取数据集"></a>2. 访问和读取数据集</h2><p>竞赛数据分为训练集和测试集。 每条记录都包括房屋的属性值和属性。这些特征由各种数据类型组成。 例如，建筑年份由整数表示，屋顶类型由离散类别表示，其他特征由浮点数表示。<br>这就是现实让事情变得复杂的地方：例如，一些数据完全丢失了，缺失值被简单地标记为“NA”。<br>比赛最终要求我们预测测试集的价格。我们将划分训练集以创建验证集，但是在将预测结果上传到Kaggle之后，只能在官方测试集中评估模型。</p><p>数据集可以直接在Kaggle上下载，也可以用d2l的HUB下载。总之，最后得到<code>all_features = pd.concat((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:]))</code> （去掉第一列是因为该数据集中第一列是ID属性，对模型和预测没有帮助；去掉train_data的最后一列，因为是标签）。</p><p><code>pd.concat()</code> 可以沿着指定的轴将多个dataframe或者series拼接到一起，在这里就沿第一维（纵向）拼接到一起。</p><h2 id="3-数据预处理"><a href="#3-数据预处理" class="headerlink" title="3. 数据预处理"></a>3. 数据预处理</h2><p>建模之前先对数据进行预处理。<br>首先将所有缺失的值替换为相应特征的平均值。然后，为了将所有特征放在一个共同尺度上，将特征重新缩放到零均值和统一方差来标准化数据，通过以下公式：</p><script type="math/tex; mode=display">x \leftarrow \frac{x - \mu}{\sigma}，\mu和\sigma分别表示通过已有数据计算出的对应特征的均值和标准差。</script><p>替换后的特征具有零均值和统一的方差，也就是说缩放后的每个特征均值都为零且方差为实际方差, 即 $E\left[\frac{x-\mu}{\sigma}\right]=\frac{\mu-\mu}{\sigma}=0$ 和 $E\left[(x-\mu)^{2}\right]=\left(\sigma^{2}+\mu^{2}\right)-2 \mu^{2}+\mu^{2}=\sigma^{2}$（回忆$D(X)=E\left((X-E(X))^{2}\right)=E\left(X^{2}\right)-E^{2}(X)$）。</p><p>标准化数据有两个原因: 一是方便优化。二是在不明确特征相关性的情况下不让惩罚分配给某一特征的系数比分配给其他特征的系数更大。</p><figure class="highlight sas"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 若无法获得测试数据，则可根据训练数据计算均值和标准差</span><br><span class="line">numeric_features = all_features.dtypes[all_features.dtypes != <span class="string">&#x27;object&#x27;</span>].<span class="meta">index</span></span><br><span class="line">all_features[numeric_features] = all_features[numeric_features].apply(</span><br><span class="line">    lambda <span class="meta">x</span>: (<span class="meta">x</span> - <span class="meta">x</span><span class="meta">.mean(</span>)) / (<span class="meta">x</span><span class="meta">.std(</span>)))</span><br><span class="line"># 在标准化数据之后，所有原值为均值的元素消失，可以将缺失值设置为0</span><br><span class="line">all_features[numeric_features] = all_features[numeric_features].fillna(0)</span><br></pre></td></tr></table></figure><p>这里有个值得注意的点就是筛选<code>dtype != &#39;object&#39;</code>的元素，NumPy中数组存储为连续的内存块，通常具有单一数据类型(例如整数、浮点数或固定长度的字符串)，然后内存中的位被解释为具有该数据类型的值。<br><img src="/assets/post_img/article50/pandasObject.jpg" alt="po"><br>但用object类型创建数组是不同的，数组占用的内存此时由指向存储在内存中其他位置的 Python 对象的指针构成。</p><p>pandas的Dataframe使用了NumPy的object类型，Pandas存储字符串时正是使用这种类型，每一个object类型元素事实上是一个指针。</p><p>再处理离散值，使用独热编码替换它们，这样做将离散的字符串特征值也用数字表示出来。<br>例如，“MSZoning”包含值“RL”和“Rm”。 我们将创建两个新的指示器特征“MSZoning_RL”和“MSZoning_RM”，其值为0或1。 根据独热编码，如果“MSZoning”的原始值为“RL”， 则：“MSZoning_RL”为1，“MSZoning_RM”为0。 pandas软件包可以自动实现这一点。<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># “Dummy_na=True”将“na”（缺失值）视为有效的特征值，并为其创建一个特征。</span></span><br><span class="line">all_features = pd.get_dummies(all_features, <span class="attribute">dummy_na</span>=<span class="literal">True</span>)</span><br><span class="line">all_features.shape</span><br></pre></td></tr></table></figure><br>由于我们对所有字符串类型的特征都应用了独热编码（该编码会使每一个值都形成一个特征），最终此转换会将特征的总数量从79个增加到331个。<br>最后通过values属性从pandas格式中提取NumPy格式，并将其转换为pytorch张量表示用于训练。<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">n_train = train_data<span class="selector-class">.shape</span><span class="selector-attr">[0]</span></span><br><span class="line">train_features = torch<span class="selector-class">.tensor</span>(all_features<span class="selector-attr">[:n_train]</span><span class="selector-class">.values</span>, dtype=torch.float32)</span><br><span class="line">test_features = torch<span class="selector-class">.tensor</span>(all_features<span class="selector-attr">[n_train:]</span><span class="selector-class">.values</span>, dtype=torch.float32)</span><br><span class="line">train_labels = torch<span class="selector-class">.tensor</span>(</span><br><span class="line">    train_data<span class="selector-class">.SalePrice</span><span class="selector-class">.values</span><span class="selector-class">.reshape</span>(-<span class="number">1</span>, <span class="number">1</span>), dtype=torch.float32)</span><br></pre></td></tr></table></figure></p><h2 id="4-训练"><a href="#4-训练" class="headerlink" title="4. 训练"></a>4. 训练</h2><p>首先训练一个带有损失平方的线性模型作为基线（baseline）模型，让我们直观地知道最好的模型有超出简单的模型多少。</p><p>线性模型是最简单的深度学习模型（对比赛并无帮助），仅提供了一种查看数据中是否存在有意义的信息的功能。 如果它不能做得比随机猜测更好，那很可能存在数据处理错误。</p><p>对于房价的预测我们更多关心相对值, 而不是绝对值，即更关心相对误差 $\frac{y-\hat{y}}{y}$, 而不是绝对误差 $y-\hat{y}_{\text {。 }}$ 对一个房子价格的预测误差要与其本身的数量级进行比较，对千万级房子的价格预测出现10万左右的偏差时，比对百万级房子或价格更低的房子产生同样的误差更容易令人接受。</p><p>解决这个问题的一种方法是用价格预测的对数来衡量差异。事实上, 这也是比赛中官方用来评价提交质量的误差指标。即将 $\delta$ for $|\log y-\log \hat{y}| \leq \delta$ 转换为 $e^{-\delta} \leq \frac{\hat{y}}{y} \leq e^{\delta}$ 。这使得预测价格的对数与真实标签价格的对数之间出现以下均方根误差（Root Mean Square Error）：</p><script type="math/tex; mode=display">\sqrt{\frac{1}{n} \sum_{i=1}^{n}\left(\log y_{i}-\log \hat{y}_{i}\right)^{2}}</script><p>训练函数将借助Adam优化器，Adam优化器的主要吸引力在于它对初始学习率不那么敏感。</p><p>到这里，排除数据处理部分，基本上完成的工作就是：<br>1、提取了输入的特征张量<br>2、写一个获取单层网络的函数，输入特征输出标签<br>3、选定了对数RMSE的误差计算方法，并实现他<br>4、选定Adam优化器，实现了训练过程</p><p>以上代码详见实践对应仓库。</p><h2 id="5-K折交叉验证"><a href="#5-K折交叉验证" class="headerlink" title="5. K折交叉验证"></a>5. K折交叉验证</h2><p>K折交叉验证有助于模型选择和超参数调整。 首先定义一个函数get_k_fold_data，在K折交叉验证过程中返回第 $i$ 折的数据。 具体地说，它选择第 $i$ 个切片作为验证数据，其余部分作为训练数据。 注意，这并不是处理数据的最有效方法，如果我们的数据集大得多，会有其他解决办法。 k_fold函数则负责在K折交叉验证中训练K次后，返回训练和验证误差的平均值。</p><p>总之，实现了K折交叉验证的部分，重新强调一下K折交叉验证解决的问题：在训练数据稀缺、无法构成足够的验证集的情况下，估计模型的误差。</p><h2 id="6-模型选择"><a href="#6-模型选择" class="headerlink" title="6. 模型选择"></a>6. 模型选择</h2><p>到了这里，剩下的事情就是调参啦！<br><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">k, num_epochs, lr, weight_decay, batch_size = <span class="number">5</span>, <span class="number">100</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">64</span></span><br><span class="line">train_l, valid_l = k<span class="constructor">_fold(<span class="params">k</span>, <span class="params">train_features</span>, <span class="params">train_labels</span>, <span class="params">num_epochs</span>, <span class="params">lr</span>, <span class="params">weight_decay</span>, <span class="params">batch_size</span>)</span></span><br></pre></td></tr></table></figure><br>可调整的超参数有：验证折数、训练周期、学习率、权重和批次大小。</p><p>目前为止，我们还没有开始真正的预测，只是用比赛中的训练数据，通过K折交叉验证来选择模型（调整超参数），使模型能够达到较好的误差，之后，我们再将模型应用于全部数据集，即包含测试数据集。</p><p>有时一组超参数的训练误差可能非常低，但K折交叉验证的误差要高得多，这表明模型过拟合了。<br>在训练过程中监控训练误差和验证误差，较少的过拟合可能表明现有数据可以支撑一个更强大的模型，较大的过拟合可能意味着可以通过正则化技术来获益。</p><h2 id="7-预测、提交"><a href="#7-预测、提交" class="headerlink" title="7. 预测、提交"></a>7. 预测、提交</h2><p>选定了模型之后，就可以将模型训练并应用于测试集进行预测，如果测试集上的预测与K折交叉验证过程中的预测相似，则可以将结果提交给Kaggle，判断预测标签与实际标签的差异。</p><p>提交后得到的score应该就是误差了，误差为0表示完全正确！</p><h2 id="8-小结"><a href="#8-小结" class="headerlink" title="8. 小结"></a>8. 小结</h2><ul><li><p>真实数据通常混合了不同的数据类型，需要进行预处理。</p></li><li><p>常用的预处理方法：将实值数据重新缩放为零均值和单位方法；用均值替换缺失值。</p></li><li><p>将类别特征转化为指标特征，可以使我们把这个特征当作一个独热向量来对待。</p></li><li><p>我们可以使用K折交叉验证来选择模型并调整超参数。</p></li><li><p>对数对于相对误差很有用。</p></li></ul><h2 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h2><p>改进模型来提高分数，应用多层、暂退法等技术。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;p&gt;Kaggle的&lt;a href=&quot;https://www.kaggle.com/c/house-prices-advanced-regression-techniques&quot;&gt;房价预测比赛&lt;/a&gt;，数据集由Bart de Cock于2011年收集，涵盖了2006-2010年期间亚利桑那州埃姆斯市的房价。这个数据集是相当通用的，不会需要使用复杂模型架构。 它比哈里森和鲁宾菲尔德的波士顿房价数据集要大得多，也有更多的特征。&lt;/p&gt;
&lt;p&gt;对应实践：&lt;a href=&quot;https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x05.ipynb&quot;&gt;https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x05.ipynb&lt;/a&gt;&lt;br&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://silencezheng.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Swing布局及快速开发</title>
    <link href="http://silencezheng.top/2022/06/29/article49/"/>
    <id>http://silencezheng.top/2022/06/29/article49/</id>
    <published>2022-06-29T06:48:59.000Z</published>
    <updated>2022-08-31T05:58:14.645Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Swing的各种布局方式介绍及利用IDEA GUI Designer进行快速开发（可生成源代码）。</p><span id="more"></span><h2 id="BorderLayout（边框布局）"><a href="#BorderLayout（边框布局）" class="headerlink" title="BorderLayout（边框布局）"></a>BorderLayout（边框布局）</h2><p>Window、JFrame 和 JDialog 的默认布局管理器。边框布局管理器将窗口分为 5 个区域：North、South、East、West 和 Center。</p><p><img src="/assets/post_img/article49/border.jpg" alt="BL"></p><p>边框布局管理器并不要求所有区域都必须有组件，如果四周的区域（North、South、East 和 West 区域）没有组件，则由 Center 区域去补充。如果单个区域中添加的不只一个组件，那么后来添加的组件将覆盖原来的组件，所以，区域中只显示最后添加的一个组件。</p><p>BorderLayout的构造方法有：<br><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="constructor">BorderLayout()</span>：创建一个 Border 布局，组件之间没有间隙。</span><br><span class="line"></span><br><span class="line"><span class="constructor">BorderLayout(<span class="params">int</span> <span class="params">hgap</span>,<span class="params">int</span> <span class="params">vgap</span>)</span>：创建一个 Border 布局，其中 hgap 表示组件之间的横向间隔；vgap 表示组件之间的纵向间隔，单位是像素。</span><br></pre></td></tr></table></figure></p><h2 id="FlowLayout（流式布局）"><a href="#FlowLayout（流式布局）" class="headerlink" title="FlowLayout（流式布局）"></a>FlowLayout（流式布局）</h2><p>JPanel 和 JApplet 的默认布局管理器。</p><p>FlowLayout 会将组件按照从上到下、从左到右的放置规律逐行进行定位。与其他布局管理器不同的是，FlowLayout 布局管理器不限制它所管理组件的大小，而是允许它们有自己的最佳大小（Preferred Size）。</p><p>FlowLayout的构造方法：<br><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="constructor">FlowLayout()</span>：创建一个布局管理器，使用默认的</span><br><span class="line">居中对齐方式和默认 <span class="number">5</span> 像素的水平和垂直间隔。</span><br><span class="line"></span><br><span class="line"><span class="constructor">FlowLayout(<span class="params">int</span> <span class="params">align</span>)</span>：创建一个布局管理器，使用默认 <span class="number">5</span> 像素的水平和垂直间隔。</span><br><span class="line">其中，align 表示组件的对齐方式，对齐的值必须是 FlowLayoutLEFT、FlowLayout.RIGHT 或 FlowLayout.CENTER，</span><br><span class="line">指定组件在这一行的位置是居左对齐、居右对齐或居中对齐。</span><br><span class="line"></span><br><span class="line"><span class="constructor">FlowLayout(<span class="params">int</span> <span class="params">align</span>, <span class="params">int</span> <span class="params">hgap</span>,<span class="params">int</span> <span class="params">vgap</span>)</span>：创建一个布局管理器，其中 align 表示组件的对齐方式；</span><br><span class="line">hgap 表示组件之间的横向间隔；vgap 表示组件之间的纵向间隔，单位是像素。</span><br></pre></td></tr></table></figure></p><h2 id="CardLayout（卡片布局）"><a href="#CardLayout（卡片布局）" class="headerlink" title="CardLayout（卡片布局）"></a>CardLayout（卡片布局）</h2><p>CardLayout（卡片布局管理器）能够帮助用户实现多个成员共享同一个显示空间，并且一次只显示一个容器组件的内容，该布局将容器分成许多层，每层的显示空间占据整个容器的大小，但每层只允许放置一个组件。</p><p>CardLayout的构造方法：<br><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="constructor">CardLayout()</span>：构造一个新布局，默认间隔为<span class="number">0</span>。</span><br><span class="line"></span><br><span class="line"><span class="constructor">CardLayout(<span class="params">int</span> <span class="params">hgap</span>, <span class="params">int</span> <span class="params">vgap</span>)</span>：创建布局，并指定组件间的水平间隔（hgap）和垂直间隔（vgap）。</span><br></pre></td></tr></table></figure></p><p>下面是一个示例，介绍如何使用卡片布局实现分层显示：<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="regexp">//</span> 假设有两个JPanel p1和p2</span><br><span class="line">JPanel cards=new JPanel(new CardLayout());</span><br><span class="line">cards.add(p1,<span class="string">&quot;card1&quot;</span>);    <span class="regexp">//</span>向卡片式布局面板中添加面板<span class="number">1</span></span><br><span class="line">cards.add(p2,<span class="string">&quot;card2&quot;</span>);    <span class="regexp">//</span>向卡片式布局面板中添加面板<span class="number">2</span></span><br><span class="line"><span class="regexp">//</span> 显示面板</span><br><span class="line">CardLayout cl=(CardLayout)(cards.getLayout());</span><br><span class="line">cl.show(cards,<span class="string">&quot;card1&quot;</span>);    <span class="regexp">//</span>调用show()方法显示面板<span class="number">1</span></span><br></pre></td></tr></table></figure></p><h2 id="GridLayout（网格布局）"><a href="#GridLayout（网格布局）" class="headerlink" title="GridLayout（网格布局）"></a>GridLayout（网格布局）</h2><p>GridLayout（网格布局管理器）为组件的放置位置提供了更大的灵活性。它将区域分割成行数（rows）和列数（columns）的网格状布局，组件按照由左至右、由上而下的次序排列填充到各个单元格中。</p><p>GridLayout的构造方法：<br><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="constructor">GridLayout(<span class="params">int</span> <span class="params">rows</span>,<span class="params">int</span> <span class="params">cols</span>)</span>：创建一个指定行（rows）和列（cols）的网格布局。</span><br><span class="line">布局中所有组件的大小一样，组件之间没有间隔。</span><br><span class="line"></span><br><span class="line"><span class="constructor">GridLayout(<span class="params">int</span> <span class="params">rows</span>,<span class="params">int</span> <span class="params">cols</span>,<span class="params">int</span> <span class="params">hgap</span>,<span class="params">int</span> <span class="params">vgap</span>)</span>：创建一个指定行（rows）和列（cols）的网格布局，</span><br><span class="line">并且可以指定组件之间横向（hgap）和纵向（vgap）的间隔，单位是像素。</span><br></pre></td></tr></table></figure></p><p>GridLayout 布局管理器总是忽略组件的最佳大小，而是根据提供的行和列进行平分。该布局管理的所有单元格的宽度和高度都是一样的。这也是一个缺点，在实际使用中，通常通过与其他布局进行协作，才能达到一个很好的展示效果。</p><h2 id="GridBagLayout（网格包布局）"><a href="#GridBagLayout（网格包布局）" class="headerlink" title="GridBagLayout（网格包布局）"></a>GridBagLayout（网格包布局）</h2><p>GridBagLayout（网格包布局管理器）是在网格基础上提供复杂的布局，是最灵活、 最复杂的布局管理器。GridBagLayout 不需要组件的尺寸一致，允许组件扩展到多行多列。每个 GridBagLayout 对象都维护了一组动态的矩形网格单元，每个组件占一个或多个单元，所占有的网格单元称为组件的显示区域。</p><p>GridBagLayout 所管理的每个组件都与一个 GridBagConstraints 约束类的对象相关。这个约束类对象指定了组件的显示区域在网格中的位置，以及在其显示区域中应该如何摆放组件。除了组件的约束对象，GridBagLayout 还要考虑每个组件的最小和首选尺寸，以确定组件的大小。</p><p>为了有效地利用网格包布局，在向容器中添加组件时，必须定制某些组件的相关约束对象。GridBagConstraints 对象的定制是通过下列变量实现的：</p><ol><li>gridx 和 gridy<br>用来指定组件左上角在网格中的行和列。容器中最左边列的 gridx 为 0，最上边行的 gridy 为 0。这两个变量的默认值是 GridBagConstraints.RELATIVE，表示对应的组件将放在前一个组件的右边或下面。</li><li>gridwidth 和 gridheight<br>用来指定组件显示区域所占的列数和行数，以网格单元为单位，默认值为 1。</li><li>fill<br>指定组件填充网格的方式，可以是如下值：<ul><li>GridBagConstraints.NONE（默认值）</li><li>GridBagConstraints.HORIZONTAL（组件横向充满显示区域，但是不改变组件高度）</li><li>GridBagConstraints.VERTICAL（组件纵向充满显示区域，但是不改变组件宽度）</li><li>GridBagConstraints.BOTH（组件横向、纵向充满其显示区域）。</li></ul></li><li>ipadx 和 ipady<br>指定组件显示区域的内部填充，即在组件最小尺寸之外需要附加的像素数，默认值为 0。</li><li>insets<br>指定组件显示区域的外部填充，即组件与其显示区域边缘之间的空间，默认组件没有外部填充。</li><li>anchor<br>指定组件在显示区域中的摆放位置。可选值有：<ul><li>GridBagConstraints.CENTER（默认值）</li><li>GridBagConstraints.NORTH</li><li>GridBagConstraints.NORTHEAST</li><li>GridBagConstraints.EAST</li><li>GridBagConstraints.SOUTH</li><li>GridBagConstraints.SOUTHEAST</li><li>GridBagConstraints.WEST</li><li>GridBagConstraints.SOUTHWEST</li><li>GridBagConstraints.NORTHWEST</li></ul></li><li>weightx 和 weighty<br>用来指定在容器大小改变时，增加或减少的空间如何在组件间分配，默认值为 0，即所有的组件将聚拢在容器的中心，多余的空间将放在容器边缘与网格单元之间。weightx 和 weighty 的取值一般在 0.0 与 1.0 之间，数值大表明组件所在的行或者列将获得更多的空间。</li></ol><h2 id="BoxLayout（盒布局）"><a href="#BoxLayout（盒布局）" class="headerlink" title="BoxLayout（盒布局）"></a>BoxLayout（盒布局）</h2><p>BoxLayout（盒布局管理器）通常和 Box 容器联合使用。</p><p>Box 类有以下两个静态方法：<br><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">createHorizontalBox()：返回一个 <span class="keyword">Box </span>对象，它采用水平 <span class="keyword">BoxLayout，</span></span><br><span class="line"><span class="keyword"></span>即 <span class="keyword">BoxLayout </span>沿着水平方向放置组件，让组件在容器内从左到右排列。</span><br><span class="line"></span><br><span class="line">createVerticalBox()：返回一个 <span class="keyword">Box </span>对象，它采用垂直 <span class="keyword">BoxLayout，</span></span><br><span class="line"><span class="keyword"></span>即 <span class="keyword">BoxLayout </span>沿着垂直方向放置组件，让组件在容器内从上到下进行排列。</span><br></pre></td></tr></table></figure></p><p>Box 还提供了用于决定组件之间间隔的静态方法：</p><div class="table-container"><table><thead><tr><th style="text-align:center">方法</th><th style="text-align:center">说明</th></tr></thead><tbody><tr><td style="text-align:center">static Component createHorizontalGlue()</td><td style="text-align:center">创建一个不可见的、可以被水平拉伸和收缩的组件</td></tr><tr><td style="text-align:center">static Component createVerticalGlue()</td><td style="text-align:center">创建一个不可见的、可以被垂直拉伸和收缩的组件</td></tr><tr><td style="text-align:center">static Component createHorizontalStrut(int width)</td><td style="text-align:center">创建一个不可见的、固定宽度的组件</td></tr><tr><td style="text-align:center">static Component createVerticalStrut(int height)</td><td style="text-align:center">创建一个不可见的、固定高度的组件</td></tr><tr><td style="text-align:center">static Component createRigidArea(Dimension d)</td><td style="text-align:center">创建一个不可见的、总是具有指定大小的组件</td></tr></tbody></table></div><p>BoxLayout 类只有一个构造方法:<br><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="constructor">BoxLayout(Container <span class="params">c</span>,<span class="params">int</span> <span class="params">axis</span>)</span></span><br><span class="line">Container 是一个容器对象，即该布局管理器在哪个容器中使用。</span><br><span class="line">axis 用来决定容器上的组件水平（X_AXIS）或垂直（Y_AXIS）放置,可以使用 BoxLayout 类访问这两个属性。</span><br></pre></td></tr></table></figure></p><h2 id="使用GUI-Designer快速开发"><a href="#使用GUI-Designer快速开发" class="headerlink" title="使用GUI Designer快速开发"></a>使用GUI Designer快速开发</h2><p>通常在编写Swing程序时，各种布局需要交错在一起使用，从而达到更好的界面效果。</p><p>InteliJ IDEA自带的可视化创建工具可以为图形界面的开发提供一定程度上的便利，下图是GUI Designer的设置：<br><img src="/assets/post_img/article49/GD-setting.jpg" alt="settings"></p><p>该工具提供了两种生成GUI界面的方式，一种是直接产生编译后的二进制class文件，一种是在运行时生成Java源代码，但是该源代码并非原生的Swing代码，而是调用了JetBrains自家开发的库产生的源代码。</p><p>在可视化编辑的时候也可以选择一些布局管理工具（不包括Box布局），设置UI组件的隐私性等等。</p><p>如果想要保留源代码的话，可以在确定界面符合要求后，运行一次程序产生源代码，然后删除掉跟随类的form文件即可，否则每次运行程序都会重置界面源代码。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;Swing的各种布局方式介绍及利用IDEA GUI Designer进行快速开发（可生成源代码）。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Java" scheme="http://silencezheng.top/tags/Java/"/>
    
    <category term="Idea" scheme="http://silencezheng.top/tags/Idea/"/>
    
    <category term="Swing" scheme="http://silencezheng.top/tags/Swing/"/>
    
  </entry>
  
  <entry>
    <title>Swing事件响应三种方式</title>
    <link href="http://silencezheng.top/2022/06/26/article48/"/>
    <id>http://silencezheng.top/2022/06/26/article48/</id>
    <published>2022-06-26T08:30:14.000Z</published>
    <updated>2022-06-26T08:31:37.446Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>简单总结Swing开发中对组件添加事件响应的三种方法。<br>参考：<a href="https://blog.csdn.net/moridehuixiang/article/details/45394577">https://blog.csdn.net/moridehuixiang/article/details/45394577</a><br><span id="more"></span></p><h2 id="事件响应"><a href="#事件响应" class="headerlink" title="事件响应"></a>事件响应</h2><p>在Swing中,事件响应是通过监听器对象来处理事件的方式实行的,这种方式被称为事件委托模型。</p><p>以JButton举例,它内部有一个名为listenerList的链表,在点击按钮时,会产生一个ActionEvent事件,此后内部会依次调用位于listenerList中的每一个实现ActionListener接口的类的实例的actionPerformed方法,这就是事件响应的过程。</p><p>当调用JButton的addActionListener方法时, 外部实现了ActionListene接口的类的实例的指针就被放入了listenerList中,当按钮点击事件产生时,这个实例的actionPerformed方法就会被调用,从而按钮的点击事件处理就被委托到了该实例中进行处理。</p><h2 id="实现方式"><a href="#实现方式" class="headerlink" title="实现方式"></a>实现方式</h2><p>实现ActionListener接口的三种方式如下：<br>1.实现一个ActionListener接口的子类,再把按钮的事件响应委托给这个子类的实例处理.这种方式并不常用。</p><p>2.让界面类（通常是继承了JFrame）实现ActionListener接口,再把事件响应委托给界面类。这种方式适合于处理一些短小简单或要求内聚的事件响应。</p><p>3.用匿名类实现ActionListener接口,再把事件委托给这个匿名类的实例，这种方式是Swing事件处理的主流。</p><h2 id="方法一：创建一个实现了ActionListener接口的子类"><a href="#方法一：创建一个实现了ActionListener接口的子类" class="headerlink" title="方法一：创建一个实现了ActionListener接口的子类"></a>方法一：创建一个实现了ActionListener接口的子类</h2><figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ButtonActionListener</span> <span class="keyword"><span class="keyword">implements</span> <span class="type">ActionListener</span></span></span>&#123;</span><br><span class="line">  <span class="keyword">public</span> void actionPerformed(ActionEvent e) &#123;</span><br><span class="line">    <span class="keyword">String</span> buttonText=((JButton)e.getSource()).getText();</span><br><span class="line">    System.out.println(<span class="string">&quot;你按下了&quot;</span> + buttonText);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">button.addActionListener(<span class="keyword">new</span> <span class="type">ButtonActionListener</span>());</span><br></pre></td></tr></table></figure><h2 id="方法二：整个窗体类实现ActionListener接口"><a href="#方法二：整个窗体类实现ActionListener接口" class="headerlink" title="方法二：整个窗体类实现ActionListener接口"></a>方法二：整个窗体类实现ActionListener接口</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">MyWindow</span> <span class="keyword">extends</span> <span class="title">JFrame</span> <span class="title">implements</span> <span class="title">ActionListener</span></span>&#123;</span><br><span class="line">  public <span class="type">MyFrame</span>() &#123;</span><br><span class="line">    button.addActionListener(<span class="keyword">this</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  public void actionPerformed(<span class="type">ActionEvent</span> e) &#123;</span><br><span class="line">    <span class="keyword">if</span>(e.getSource()==button)&#123;</span><br><span class="line">      <span class="type">System</span>.out.println(<span class="string">&quot;你按下了&quot;</span> + button.getText());</span><br><span class="line">    &#125;   </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="方法三：匿名类方式添加"><a href="#方法三：匿名类方式添加" class="headerlink" title="方法三：匿名类方式添加"></a>方法三：匿名类方式添加</h2><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">button.add<span class="constructor">ActionListener(<span class="params">new</span> ActionListener()</span> &#123;</span><br><span class="line">      public void action<span class="constructor">Performed(ActionEvent <span class="params">e</span>)</span> &#123;</span><br><span class="line">        <span class="module-access"><span class="module"><span class="identifier">System</span>.</span></span>out.println(<span class="string">&quot;你按下了&quot;</span> + button.get<span class="constructor">Text()</span>);</span><br><span class="line">      &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;简单总结Swing开发中对组件添加事件响应的三种方法。&lt;br&gt;参考：&lt;a href=&quot;https://blog.csdn.net/moridehuixiang/article/details/45394577&quot;&gt;https://blog.csdn.net/moridehuixiang/article/details/45394577&lt;/a&gt;&lt;br&gt;</summary>
    
    
    
    
    <category term="Java" scheme="http://silencezheng.top/tags/Java/"/>
    
    <category term="Swing" scheme="http://silencezheng.top/tags/Swing/"/>
    
  </entry>
  
  <entry>
    <title>线程同步、异步</title>
    <link href="http://silencezheng.top/2022/06/23/article47/"/>
    <id>http://silencezheng.top/2022/06/23/article47/</id>
    <published>2022-06-23T09:43:55.000Z</published>
    <updated>2022-06-23T09:48:56.891Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>之前也做过不少的分布式、多线程系统，对线程同步异步的理解还是不到位，现归纳总结一下。<br><span id="more"></span></p><h2 id="多线程"><a href="#多线程" class="headerlink" title="多线程"></a>多线程</h2><p>多线程并发程序通过同时执行多个任务来提高 CPU 利用率，线程通过共享对象引用和成员变量相互通信。</p><p>当多个线程共享同一内存（资源）时，多个对同一数据执行不同操作的线程可能会相互交错，并在内存中创建不一致的数据，产生线程干扰错误。</p><p>使线程安全的办法有许多，如：</p><ol><li>同步</li><li>施加多个线程对同一对象的访问限制</li><li>将变量声明为final</li><li>将变量声明为volatile</li><li>创建不可变对象</li><li>等等</li></ol><p>也就是说，在多线程的场景中，同步是维护线程安全的一种方式。<a href="https://blog.csdn.net/u011033906/article/details/53840525">这篇文章</a>可以学习借鉴一下。</p><h2 id="同步、异步"><a href="#同步、异步" class="headerlink" title="同步、异步"></a>同步、异步</h2><p><strong>同步（synchronized）</strong>：A线程要请求某资源，但是此资源正在被B线程使用中，因为同步机制存在，A线程阻塞，等待资源可用再进行。</p><p><strong>异步（asynchronized）</strong>：A线程要请求某资源，但是此资源正在被B线程使用中，因为没有同步机制存在，A线程可以请求得到此资源，无需等待。</p><p>同步方法调用一旦开始，调用者必须等到方法调用返回后，才能继续后续的行为。<br>异步方法调用更像一个消息传递，一旦开始，方法调用就会立即返回，调用者就可以继续后续的操作。而异步方法通常会在另外一个线程中“真实”地执行着。整个过程，不会阻碍调用者的工作（即不阻碍当前进程）。最终通过状态来通知调用者，或通过回调函数来处理执行结果。</p><p>同步显然是安全的，但使系统性能降低；异步速度快，但可能导致死锁，引发更大安全问题。</p><h2 id="阻塞与同步"><a href="#阻塞与同步" class="headerlink" title="阻塞与同步"></a>阻塞与同步</h2><p>同步与异步和<strong>阻塞机制</strong>不能混为一谈，虽然通常有所关联。</p><p>同步和异步强调的是消息通信机制，而 阻塞和非阻塞 强调的是程序在等待调用结果（消息，返回值）时的状态。</p><p>阻塞时，在调用结果返回前，当前线程会被挂起，并在得到结果之后返回。<br>非阻塞调用指在不能立刻得到结果之前，该调用不会阻塞当前线程。</p><p>对于同步调用来说，很多时候当前线程还是激活的状态，只是从逻辑上当前函数没有返回而已，即同步等待时什么都不干，白白占用着资源。</p><h2 id="举例说说"><a href="#举例说说" class="headerlink" title="举例说说"></a>举例说说</h2><h3 id="同步"><a href="#同步" class="headerlink" title="同步"></a>同步</h3><p>Tomcat，通过自建的线程池实现同步阻塞式IO模型。详细见<a href="https://juejin.cn/post/6977719729947738142">这篇文章</a></p><h3 id="异步"><a href="#异步" class="headerlink" title="异步"></a>异步</h3><p>Node.js 是一个基于 Chrome V8 引擎的 JavaScript 运行环境。 Node.js 使用了一个事件驱动、非阻塞式 I/O 的模型。Node.js是单线程的，利用异步IO和事件驱动（回调函数）来解决高并发的问题。详细可以看看<a href="https://blog.csdn.net/fengqiaojiangshui/article/details/55819930">这篇文章</a>。</p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>随想随写写，后续再完善吧，有很多值得深挖的点。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;之前也做过不少的分布式、多线程系统，对线程同步异步的理解还是不到位，现归纳总结一下。&lt;br&gt;</summary>
    
    
    
    
    <category term="编程思想" scheme="http://silencezheng.top/tags/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/"/>
    
  </entry>
  
  <entry>
    <title>VSCode代码段使用</title>
    <link href="http://silencezheng.top/2022/06/19/article46/"/>
    <id>http://silencezheng.top/2022/06/19/article46/</id>
    <published>2022-06-19T11:18:38.000Z</published>
    <updated>2022-06-19T11:29:04.238Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>越用VSCode越有一种轻便感，麻雀虽小，五脏俱全，又有微软在做背书，同步设置什么的也很方便，要是国产也能有这样的好工具就好了。<br>之前在编写博客时，总觉得代码块、标题的输入有些麻烦，偶然发现可以使用代码段功能来解决这一问题，遂记录一下。<br><span id="more"></span></p><h2 id="代码段功能"><a href="#代码段功能" class="headerlink" title="代码段功能"></a>代码段功能</h2><p>VSCode提供的代码段功能，主要是通过输入预先设置好的“前缀”字符，触发输出对应的代码段。</p><p>该功能具有三种作用域：</p><ol><li>全局（global），该类代码段被存放在软件内部的文件中，不会随着项目的关闭而失效。在创建后除非手动删除该代码段文件，否则它会一直存在。</li><li>文件夹（folder），该类代码段被存放在选定目录下<code>.vscode</code>隐藏文件夹中，这个代码段只适用于当前文件夹。</li><li>文件类型（file type），该类代码段与全局作用域代码段类似，被存放在软件内部的文件中。但这类代码段只适用于用户指定的文件类型，严格的匹配文件后缀。</li></ol><p>三类代码段文件的书写格式是一致的，给个简单的例子如下：<br><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;Add a Markdown code block&quot;</span>:&#123;</span><br><span class="line"><span class="attr">&quot;scope&quot;</span>: <span class="string">&quot;&quot;</span>,</span><br><span class="line"><span class="attr">&quot;prefix&quot;</span>: <span class="string">&quot;amcb&quot;</span>,</span><br><span class="line"><span class="attr">&quot;body&quot;</span>: [</span><br><span class="line"><span class="string">&quot;```&quot;</span>,</span><br><span class="line"><span class="string">&quot;$1&quot;</span>,</span><br><span class="line"><span class="string">&quot;```&quot;</span></span><br><span class="line">],</span><br><span class="line"><span class="attr">&quot;description&quot;</span>: <span class="string">&quot;Add a Markdown code block&quot;</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>所有的代码段都书写在<code>xx.code-snippets</code>文件中的花括号内，每一个代码段包含四个属性：<br><strong>scope</strong>：作用文件类型，多种类型之间用逗号隔开。如果值为空，或不写该属性，默认所有类型文件都支持该代码段。同时该属性仅支持全局和文件夹作用域，因为文件类型作用域代码段本身已经指定了该属性。<br><strong>prefix</strong>： 触发代码段的“前缀”字符。<br><strong>body</strong>：代码段主体内容，格式为<em>字符串</em>数组，也就是说每行代码都需要用引号括起。<br><strong>description</strong>：代码块的描述，输入“前缀”时会有提示。</p><p>除此之外，在代码段主体的书写上还有一些规则：<br><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1、每个字符串元素就代表一行，在单行内可以使用\r或者使用\<span class="keyword">n</span>换行。</span><br><span class="line">2、行内不能使用<span class="keyword">tab</span>键缩进，只能使用空格或者\t缩进。</span><br><span class="line">3、用<span class="variable">$1</span>表示敲击回车或者<span class="keyword">tab</span>键一次后光标定位的位置。<span class="variable">$2</span>，<span class="variable">$3</span>，<span class="variable">$4</span>，...同理。</span><br></pre></td></tr></table></figure></p><h2 id="利用代码段功能方便Markdown博客编写"><a href="#利用代码段功能方便Markdown博客编写" class="headerlink" title="利用代码段功能方便Markdown博客编写"></a>利用代码段功能方便Markdown博客编写</h2><p>了解这个功能后，就可以应用它来帮助在VSCode中编写Markdown格式的博客。</p><p>我通常习惯在一个固定的工作区编写我的博客，所以新建一个“文件夹域”的代码段文件，如下：<br><img src="/assets/post_img/article46/md文章增强.png" alt="wzzq"></p><p>书写代码段：<br><figure class="highlight ceylon"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">   <span class="string">&quot;Add a Markdown code block&quot;</span>:&#123;</span><br><span class="line"><span class="string">&quot;scope&quot;</span>: <span class="string">&quot;markdown&quot;</span>,</span><br><span class="line"><span class="string">&quot;prefix&quot;</span>: <span class="string">&quot;amcb&quot;</span>,</span><br><span class="line"><span class="string">&quot;body&quot;</span>: [</span><br><span class="line"><span class="string">&quot;``<span class="subst">`<span class="string">&quot;,</span></span></span></span><br><span class="line"><span class="string"><span class="subst"><span class="string">&quot;</span><span class="number">$1</span><span class="string">&quot;,</span></span></span></span><br><span class="line"><span class="string"><span class="subst"><span class="string">&quot;</span></span>```&quot;</span></span><br><span class="line">],</span><br><span class="line"><span class="string">&quot;description&quot;</span>: <span class="string">&quot;Add a Markdown code block&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>这时在.md文件中还不能直接使用，因为VSCode默认没有开启quickSuggestions。</p><p>打开当前工作区的<code>settings.json</code>，添加如下配置：<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&quot;<span class="selector-attr">[markdown]</span>&quot;:&#123;</span><br><span class="line">    <span class="string">&quot;editor.quickSuggestions&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;other&quot;</span>: <span class="string">&quot;on&quot;</span>,</span><br><span class="line">        <span class="string">&quot;comments&quot;</span>: <span class="string">&quot;off&quot;</span>,</span><br><span class="line">        <span class="string">&quot;strings&quot;</span>: <span class="string">&quot;off&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>这样以后，虽然代码段是有提示了:<br><img src="/assets/post_img/article46/amcb.png" alt="amcb"><br>可是还有很多不必要的提示，比如前文中出现的语句也会被加入到提示中来，有些麻烦：<br><img src="/assets/post_img/article46/麻烦.png" alt="trouble"></p><p>于是我到VSCode<a href="https://code.visualstudio.com/docs/getstarted/settings#_languagespecific-editor-settings">官方文档</a>中寻找，看看有没有什么办法，发现这其实是因为”editor.quickSuggestions”这个设置的属性粒度不够细致造成的，它将文件的内容区分为字符串、注释和其他，也就是说只要我启用了其他，那么文件中非注释和非字符串的内容都会被提示，我的个人建议是增加一个snippets属性，将该属性从other中划分出来，就可以完美的解决我的需求😄。</p><p>顺便一提VSCode有两种设置方式，用户设置和工作区（Workplace）设置，前者应用于全局，后者存在于项目根目录的<code>.vscode</code>默认文件夹内，可以对当前项目覆盖用户设置（优先级更高）。<br><img src="/assets/post_img/article46/vsc设置.png" alt="setting"><br>关于更多设置优先级相关，<a href="https://code.visualstudio.com/docs/getstarted/settings#_settings-precedence">官网</a>有更详细的解释。所以之前做的配置实际上是覆盖了用户配置。</p><p>最终还是选择了这种配置：<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="regexp">//</span> on、off、onlySnippets</span><br><span class="line"><span class="string">&quot;editor.tabCompletion&quot;</span>: <span class="string">&quot;onlySnippets&quot;</span></span><br></pre></td></tr></table></figure><br>即：使用tab自动补全，但仅对于代码段使用。<br>这样虽然不会有输入建议提示，但胜在简洁，只需要把代码段的前缀设置的相对短，并记住即可，基本上我常用的也就是Markdown代码块和标题，用于填补Markdown All in One插件的缺点。</p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>后续可能给VSCode提个issue，建议一下关于粒度划分问题。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;越用VSCode越有一种轻便感，麻雀虽小，五脏俱全，又有微软在做背书，同步设置什么的也很方便，要是国产也能有这样的好工具就好了。&lt;br&gt;之前在编写博客时，总觉得代码块、标题的输入有些麻烦，偶然发现可以使用代码段功能来解决这一问题，遂记录一下。&lt;br&gt;</summary>
    
    
    
    
    <category term="VSCode" scheme="http://silencezheng.top/tags/VSCode/"/>
    
  </entry>
  
  <entry>
    <title>.gitignore文件使用</title>
    <link href="http://silencezheng.top/2022/06/18/article45/"/>
    <id>http://silencezheng.top/2022/06/18/article45/</id>
    <published>2022-06-18T15:21:59.000Z</published>
    <updated>2022-06-18T15:35:33.561Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>记录一下如何编写git忽略文件，文件的格式，以及使用pycharm插件帮助生成。<br><span id="more"></span></p><h2 id="Git-ignore文件"><a href="#Git-ignore文件" class="headerlink" title="Git ignore文件"></a>Git ignore文件</h2><p>Git的忽略文件名为 <code>.gitignore</code>，在这个文件中列出那些不希望添加到git中的文件名后，当使用<code>git add .</code>时这些文件就会被自动忽略掉。</p><p>忽视文件的格式很简单，同时也支持格式匹配（包括文件和目录），用 <code>*</code> 表示省略，用<code>#</code>表示注释，如：<br><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># Java class <span class="keyword">Files</span></span><br><span class="line"><span class="comment">*.class</span></span><br><span class="line"></span><br><span class="line"># Package <span class="keyword">Files</span></span><br><span class="line"><span class="comment">*.jar</span></span><br><span class="line"><span class="comment">*.war</span></span><br><span class="line"><span class="comment">*.ear</span></span><br><span class="line"></span><br><span class="line"># 忽略名称中末尾为bin的目录</span><br><span class="line"><span class="comment">*bin/</span></span><br><span class="line"></span><br><span class="line"># 忽略名称中间包含bin的目录</span><br><span class="line"><span class="comment">*bin*/</span></span><br></pre></td></tr></table></figure></p><p>GitHub也给出了各种各样项目的忽视文件模版，见<a href="https://github.com/github/gitignore">这里</a>。</p><h2 id="忽视文件的一般原则"><a href="#忽视文件的一般原则" class="headerlink" title="忽视文件的一般原则"></a>忽视文件的一般原则</h2><p>忽略操作系统自动生成的文件，比如缩略图等。</p><p>忽略编译生成的中间文件、可执行文件等，也就是如果一个文件是通过另一个文件自动生成的，那自动生成的文件就没必要放进版本库，比如Java编译产生的.class文件。</p><p>忽略自己带有敏感信息的配置文件，比如存放口令的配置文件。</p><h2 id="Pycharm添加git忽视文件"><a href="#Pycharm添加git忽视文件" class="headerlink" title="Pycharm添加git忽视文件"></a>Pycharm添加git忽视文件</h2><p>下载.ignore插件：<br><img src="/assets/post_img/article45/plugin.png" alt=".ignore"></p><p>然后在项目根目录上右键移到New，就可以看到.ignore的选项。从中选择gitignore文件，还可以选择一些模版创建，也可以直接建立空的忽视文件。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;记录一下如何编写git忽略文件，文件的格式，以及使用pycharm插件帮助生成。&lt;br&gt;</summary>
    
    
    
    
    <category term="Git" scheme="http://silencezheng.top/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>多层感知机--《动手学深度学习》笔记0x04</title>
    <link href="http://silencezheng.top/2022/06/16/article44/"/>
    <id>http://silencezheng.top/2022/06/16/article44/</id>
    <published>2022-06-16T07:03:39.000Z</published>
    <updated>2022-06-18T07:00:39.691Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><p>这一章开始学习真正的深度网络。<br>最简单的深度网络称为多层感知机。多层感知机由多层神经元组成， 每一层与它的上一层相连，从中接收输入； 同时每一层也与它的下一层相连，影响当前层的神经元。<br>这一章从基本的概念介绍开始讲起，包括过拟合、欠拟合和模型选择。 为了解决这些问题，本章将介绍权重衰减和暂退法等正则化技术，以及将讨论数值稳定性和参数初始化相关的问题。最后应用一个真实的案例：房价预测。</p><p>对应实践：<a href="https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x04.ipynb">https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x04.ipynb</a><br><span id="more"></span></p><h3 id="0-1-结论"><a href="#0-1-结论" class="headerlink" title="0.1. 结论"></a>0.1. 结论</h3><ul><li>多层感知机通过激活函数+隐藏层摆脱线性模型的限制。</li><li>欠拟合是指模型无法继续减少训练误差。过拟合是指训练误差远小于验证误差。</li><li>由于不能基于训练误差来估计泛化误差，因此简单地最小化训练误差并不一定意味着泛化误差的减小。机器学习模型需要注意防止过拟合，即防止泛化误差过大。</li><li>验证集可以用于模型选择，但不能过于随意地使用它。</li><li>应该选择一个复杂度适当的模型，避免使用数量不足的训练样本。简单模型导致欠拟合，复杂模型导致过拟合。</li><li>正则化是处理过拟合的常用方法：在训练集的损失函数中加入惩罚项，以降低学习到的模型的复杂度。</li><li>保持模型简单的一个特别的选择是使用$L_2$惩罚的权重衰减。这会导致学习算法在更新步骤中递减权重。权重衰减功能在深度学习框架的优化器中提供。</li><li>在同一训练代码实现中，不同的参数集可以有不同的更新行为。</li><li>暂退法在前向传播过程中，计算每一内部层的同时丢弃一些神经元。</li><li>暂退法可以避免过拟合，它通常与控制权重向量的维数和大小结合使用的。</li><li>暂退法将活性值$h$替换为具有期望值$h$的随机变量。</li><li>暂退法仅在训练期间使用。</li><li>前向传播在神经网络定义的计算图中按顺序计算和存储中间变量，它的顺序是从输入层到输出层。</li><li>反向传播按相反的顺序（从输出层到输入层）计算和存储神经网络的中间变量和参数的梯度。</li><li>在训练深度学习模型时，前向传播和反向传播是相互依赖的。</li><li>训练比预测需要更多的内存。</li><li>梯度消失和梯度爆炸是深度网络中常见的问题。在参数初始化时需要非常小心，以确保梯度和参数可以得到很好的控制。</li><li>需要用启发式的初始化方法来确保初始梯度既不太大也不太小</li><li>ReLU激活函数缓解了梯度消失问题，这样可以加速收敛。</li><li>随机初始化是保证在进行优化前打破对称性的关键。</li><li>Xavier初始化表明，对于每一层，输出的方差不受输入数量的影响，任何梯度的方差不受输出数量的影响。</li><li>在许多情况下，训练集和测试集并不来自同一个分布。这就是所谓的分布偏移。</li><li>真实风险是从真实分布中抽取的所有数据的总体损失的预期。然而，这个数据总体通常是无法获得的。经验风险是训练数据的平均损失，用于近似真实风险。在实践中，我们进行经验风险最小化。</li><li>在相应的假设条件下，可以在测试时检测并纠正协变量偏移和标签偏移。在测试时，不考虑这种偏移可能会成为问题。</li></ul><h2 id="1-多层感知机"><a href="#1-多层感知机" class="headerlink" title="1. 多层感知机"></a>1. 多层感知机</h2><p>开始对深度神经网络的探索！</p><h3 id="1-1-隐藏层"><a href="#1-1-隐藏层" class="headerlink" title="1.1. 隐藏层"></a>1.1. 隐藏层</h3><p>在线性神经网络章节描述了仿射变换， 它是一种带有偏置项的线性变换。<br>softmax回归模型通过单个仿射变换将输入直接映射到输出，然后进行softmax操作。<br>如果我们的标签通过仿射变换后确实与我们的输入数据相关，那么这种方法确实足够了。 但是，仿射变换中的线性是一个很强的假设。</p><h4 id="1-1-1-线性模型可能会出错"><a href="#1-1-1-线性模型可能会出错" class="headerlink" title="1.1.1. 线性模型可能会出错"></a>1.1.1. 线性模型可能会出错</h4><p>线性意味着单调假设： 任何特征的增大都会导致模型输出的增大（如果对应的权重为正）， 或者导致模型输出的减小（如果对应的权重为负）。<br>有时这是有道理的。 例如，如果我们试图预测一个人是否会偿还贷款。 我们可以认为，在其他条件不变的情况下， 收入较高的申请人比收入较低的申请人更有可能偿还贷款。<br>但是，虽然收入与还款概率存在单调性，但它们不是线性相关的。 收入从0增加到5万，可能比从100万增加到105万带来更大的还款可能性。 处理这一问题的一种方法是对我们的数据进行预处理， 使线性变得更合理，如使用收入的对数作为我们的特征。</p><p>然而我们可以很容易找出违反单调性的例子。 例如，我们想要根据体温预测死亡率。 对于体温高于37摄氏度的人来说，温度越高风险越大。 然而，对于体温低于37摄氏度的人来说，温度越高风险就越低。 在这种情况下，我们也可以通过一些巧妙的预处理来解决问题。 例如，我们可以使用与37摄氏度的距离作为特征。</p><p>但如果是对于图像分类问题，单个像素的强度大小对预测整张图片为某个类别的作用并非简单的线性关系。任何像素的重要性都以复杂的方式取决于该像素的上下文（周围像素的值）。 </p><p>然而数据可能会有一种考虑到特征之间的相关交互作用的表示，在这个表示的基础上可以使用线性模型。对于深度神经网络，我们使用观测数据来联合学习<em>隐藏层</em>表示和应用于该表示的线性预测器。</p><h4 id="1-1-2-在网络中加入隐藏层"><a href="#1-1-2-在网络中加入隐藏层" class="headerlink" title="1.1.2. 在网络中加入隐藏层"></a>1.1.2. 在网络中加入隐藏层</h4><p>隐藏层也称隐含层。<br>可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制， 使其能处理更普遍的函数关系类型。 要做到这一点，最简单的方法是将许多<em>全连接层</em>堆叠在一起。 每一层都输出到上面的层，直到生成最后的输出。<br>我们可以把前L-1层看作表示，把最后一层(输出）看作线性预测器。 这种架构通常称为多层感知机（multilayer perceptron），通常缩写为MLP。 下面，我们以图的方式描述了多层感知机（单隐藏层的多层感知机）。<br><img src="/assets/post_img/article44/隐藏层.svg" alt="mlp"></p><p>这个多层感知机有4个输入，3个输出，其隐藏层包含5个隐藏单元。 输入层不涉及任何计算，因此使用此网络产生输出只需要实现隐藏层和输出层的计算。 因此，这个多层感知机中的层数为2。 注意，这两个层都是全连接的。 每个输入都会影响隐藏层中的每个神经元， 而隐藏层中的每个神经元又会影响输出层中的每个神经元。</p><p>但全连接层的多层感知机的参数开销可能很高，在不改变输入或输出大小的情况下，需要在参数节约和模型有效性之间进行权衡。</p><h4 id="1-1-3-从线性到非线性，激活函数！"><a href="#1-1-3-从线性到非线性，激活函数！" class="headerlink" title="1.1.3. 从线性到非线性，激活函数！"></a>1.1.3. 从线性到非线性，激活函数！</h4><p>我们通过矩阵 $\mathbf{X} \in \mathbb{R}^{n \times d}$ 来表示 $n$ 个样本的小批量，其中每个样本具有$d$个输入特征。对于具有$h$个隐藏单元的单隐藏层多层感知机， 用 $\mathbf{H} \in \mathbb{R}^{n \times h}$ 表示隐藏层的输出，称为隐藏表示（hidden representations）。 在数学或代码中，$\mathbf{H}$ 也被称为隐藏层变量（hidden-layer variable）或隐藏变量（hidden variable）。</p><p>因为隐藏层和输出层都是全连接的， 所以我们有隐藏层权重 $\mathbf{W}^{(1)} \in \mathbb{R}^{d \times h}$ 和隐藏层偏置 $\mathbf{b}^{(1)} \in \mathbb{R}^{1 \times h}$ 以及输出层权重 $\mathbf{W}^{(2)} \in \mathbb{R}^{h \times q}$ 和输出层偏置 $\mathbf{b}^{(2)} \in \mathbb{R}^{1 \times q}$。 形式上，我们按如下方式计算单隐藏层多层感知机的输出 $\mathbf{O} \in \mathbb{R}^{n \times q}$：</p><script type="math/tex; mode=display">\begin{split}\begin{aligned} \mathbf{H} & = \mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}, \\ \mathbf{O} & = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}. \end{aligned}\end{split}</script><p>首先回顾一下什么是仿射函数：<br>仿射函数即由由1阶多项式构成的函数，一般形式为 f (x) = A x + b，这里，A 是一个 m×k 矩阵，x 是一个 k 向量,b是一个m向量，实际上反映了一种从 k 维到 m 维的空间映射关系。</p><p>添加隐藏层之后，模型现在需要跟踪和更新额外的参数（权重和偏移），但却仍然是线性模型！因为隐藏单元由输入的仿射函数给出，而输出（在softmax操作前）只是隐藏单元的仿射函数。仿射的仿射依然是仿射函数（线性模型可以表示任意一个仿射函数）。<br>下面证明这一等价性，即对于任意权重值，只需合并隐藏层，便可产生具有参数 $\mathbf{W} = \mathbf{W}^{(1)}\mathbf{W}^{(2)}$ 和  $\mathbf{b} = \mathbf{b}^{(1)} \mathbf{W}^{(2)} + \mathbf{b}^{(2)}$ 的等价单层模型（即同样是线性模型）：</p><script type="math/tex; mode=display">\mathbf{O} = (\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)})\mathbf{W}^{(2)} + \mathbf{b}^{(2)} = \mathbf{X} \mathbf{W}^{(1)}\mathbf{W}^{(2)} + \mathbf{b}^{(1)} \mathbf{W}^{(2)} + \mathbf{b}^{(2)} = \mathbf{X} \mathbf{W} + \mathbf{b}</script><p>因此为了克服线性模型的限制，还需要一个额外的关键要素：在仿射变换之后对每个隐藏单元应用非线性的<strong>激活函数</strong>（activation function）   $\sigma$。 <strong>激活函数</strong>的输出（例如，$\sigma(\cdot)$）被称为活性值（activations）。<br>有了激活函数，就不可能再将我们的多层感知机退化成线性模型，如下：</p><script type="math/tex; mode=display">\begin{split}\begin{aligned} \mathbf{H} & = \sigma(\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}), \\ \mathbf{O} & = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}.\\ \end{aligned}\end{split}</script><p>由于 $\mathbf{X}$ 中的每一行对应于小批量中的一个样本， 出于记号习惯的考量， 我们定义非线性函数 $\sigma$ 也以按行的方式作用于其输入， 即一次计算一个样本（与之前的softmax函数相同）。<br>这里我们应用于隐藏层的激活函数通常不仅按行操作，也按元素操作。 这意味着在计算每一层的线性部分之后，我们可以计算每个活性值，而不需要查看其他隐藏单元所取的值。</p><p>为了构建更通用的多层感知机， 我们可以继续堆叠这样的隐藏层， 例如$\mathbf{H}^{(1)} = \sigma_1(\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}$ 和 $\mathbf{H}^{(2)} = \sigma_2(\mathbf{H}^{(1)} \mathbf{W}^{(2)} + \mathbf{b}^{(2)})$， 一层叠一层，从而产生更有表达能力的模型。</p><h4 id="1-1-4-通用近似定理（Universal-Approximation-Theorem）"><a href="#1-1-4-通用近似定理（Universal-Approximation-Theorem）" class="headerlink" title="1.1.4. 通用近似定理（Universal Approximation Theorem）"></a>1.1.4. 通用近似定理（Universal Approximation Theorem）</h4><p>人工神经网络最有价值的地方可能就在于，它可以在理论上证明：“一个包含足够多隐含层神经元的多层前馈网络，能以任意精度逼近任意预定的连续函数”。这个定理即为通用近似定理，这里的“Universal”，也有人将其翻译成“万能的”，也有译为“万能逼近定理”。</p><p>也就是说，通用近似定理告诉我们，不管连续函数$f(x)$在形式上有多复杂，我们总能确保找到一个神经网络，对任何可能的输入$x$，以任意高的精度（通过增加隐含层神元的个数来提升近似的精度）近似输出$f(x)$（即使函数有多个输入和输出）。</p><p>多层感知机可以通过隐藏神经元，捕捉到输入之间复杂的相互作用， 这些神经元依赖于每个输入的值。 我们可以很容易地设计隐藏节点来执行任意计算。即使是网络只有一个隐藏层，给定足够的神经元和正确的权重， 我们可以对任意函数建模，尽管实际中学习该函数是很困难的。</p><p>虽然一个单隐层网络能学习任何函数， 但我们不应该尝试使用单隐藏层网络来解决所有问题，因为这种网络结构可能会格外庞大，进而无法正确地学习和泛化。通过使用更深（而不是更广）的网络，可以更容易地逼近许多函数。 </p><p>参考：<br><a href="https://blog.csdn.net/qq_41554005/article/details/110821533">通用近似定理（学习笔记）</a></p><h3 id="1-2-激活函数（activation-function）"><a href="#1-2-激活函数（activation-function）" class="headerlink" title="1.2. 激活函数（activation function）"></a>1.2. 激活函数（activation function）</h3><p>激活函数通过计算加权和并加上偏置来确定神经元是否应该被激活，它们将输入信号转换为输出的可微运算。 大多数激活函数都是<em>非线性</em>的。激活函数是深度学习的基础，下面简要介绍一些常见的激活函数。</p><h4 id="1-2-1-ReLU函数"><a href="#1-2-1-ReLU函数" class="headerlink" title="1.2.1. ReLU函数"></a>1.2.1. ReLU函数</h4><p>最受欢迎的激活函数是修正线性单元（Rectified linear unit，ReLU），因为它实现简单，同时在各种预测任务中表现良好。<br>ReLU提供了一种非常简单的非线性变换。 给定元素$x$，ReLU函数被定义为该元素与0的最大值：</p><script type="math/tex; mode=display">\operatorname{ReLU}(x) = \max(x, 0)</script><p>图像：<br><img src="/assets/post_img/article44/relu.png" alt="relu"><br>通俗地说，ReLU函数通过将相应的活性值设为0，仅保留正元素并丢弃所有负元素。ReLU激活函数是分段线性的。</p><p>ReLU导数图像：<br><img src="/assets/post_img/article44/grelu.png" alt="relu"><br>当输入为负时，ReLU函数的导数为0，而当输入为正时，ReLU函数的导数为1。 注意，当输入值精确等于0时，ReLU函数不可导。此时默认使用左侧的导数，即当输入为0时导数为0。可以忽略这种情况，因为输入可能永远都不会是0。<br>“如果微妙的边界条件很重要，我们很可能是在研究数学而非工程”。</p><p>使用ReLU的原因是，它求导表现得特别好：要么让参数消失，要么让参数通过。 这使得优化表现得更好，并且ReLU减轻了困扰以往神经网络的梯度消失问题（稍后详细介绍）。</p><p>ReLU函数有许多变体，包括参数化ReLU函数（pReLU）。该变体为ReLU添加了一个线性项，因此即使参数是负的，某些信息仍然可以通过：</p><script type="math/tex; mode=display">\operatorname{pReLU}(x) = \max(0, x) + \alpha \min(0, x)</script><h4 id="1-2-2-sigmoid函数"><a href="#1-2-2-sigmoid函数" class="headerlink" title="1.2.2. sigmoid函数"></a>1.2.2. sigmoid函数</h4><p>对于一个定义域在$\mathbb{R}$中的输入， sigmoid函数将输入变换为区间(0, 1)上的输出。 因此，sigmoid通常称为挤压函数（squashing function）： 它将范围（-inf, inf）中的任意输入压缩到区间（0, 1）中的某个值：</p><script type="math/tex; mode=display">\operatorname{sigmoid}(x) = \frac{1}{1 + \exp(-x)}</script><p>图像：<br><img src="/assets/post_img/article44/sigmoid.png" alt="relu"><br>当输入接近0时，sigmoid函数接近线性变换。</p><p>在最早的神经网络中，科学家们专注于阈值单元。阈值单元在其输入低于某个阈值时取值0，当输入超过阈值时取值1。<br>当人们逐渐关注到到基于梯度的学习时， sigmoid函数是一个自然的选择，因为它是一个平滑的、可微的阈值单元近似。 当我们想要将输出视作二元分类问题的概率时， sigmoid仍然被广泛用作输出单元上的激活函数 （可以将sigmoid视为softmax的特例）。</p><p>sigmoid函数的导数为下面的公式：</p><script type="math/tex; mode=display">\frac{d}{dx} \operatorname{sigmoid}(x) = \frac{\exp(-x)}{(1 + \exp(-x))^2} = \operatorname{sigmoid}(x)\left(1-\operatorname{sigmoid}(x)\right)</script><p>sigmoid导数图像：<br><img src="/assets/post_img/article44/gsigmoid.png" alt="relu"><br>当输入为0时，sigmoid函数的导数达到最大值0.25； 而输入在任一方向上越远离0点时，导数越接近0。</p><p>然而，sigmoid在隐藏层中已经较少使用，它在大部分时候被更简单、更容易训练的ReLU所取代。 在后面关于循环神经网络的章节中将描述利用sigmoid单元来控制时序信息流的架构。</p><h4 id="1-2-3-tanh函数"><a href="#1-2-3-tanh函数" class="headerlink" title="1.2.3. tanh函数"></a>1.2.3. tanh函数</h4><p>tanh(双曲正切)函数也能将其输入压缩转换到区间(-1, 1)上。 tanh函数的公式如下：</p><script type="math/tex; mode=display">\operatorname{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}</script><p>图像：<br><img src="/assets/post_img/article44/tanh.png" alt="relu"><br>当输入在0附近时，tanh函数接近线性变换。 函数的形状类似于sigmoid函数，不同的是tanh函数关于坐标系原点中心对称。</p><p>tanh函数的导数是：</p><script type="math/tex; mode=display">\frac{d}{dx} \operatorname{tanh}(x) = 1 - \operatorname{tanh}^2(x)</script><p>tanh导数图像：<br><img src="/assets/post_img/article44/gtanh.png" alt="relu"><br>当输入接近0时，tanh函数的导数接近最大值1。与sigmoid函数导数图像中看到的类似，输入在任一方向上越远离0点，导数越接近0。</p><h4 id="1-2-4-关于激活函数的小结"><a href="#1-2-4-关于激活函数的小结" class="headerlink" title="1.2.4. 关于激活函数的小结"></a>1.2.4. 关于激活函数的小结</h4><p>这些知识只是掌握了一个类似于1990年左右深度学习从业者的工具。<br>使用深度学习框架只需几行代码就可以快速构建模型，而以前训练这些网络需要研究人员编写数千行的C或Fortran代码。<br>这些激活函数在框架中可以直接调用，如<code>y = torch.relu(x)</code>。</p><h3 id="1-3-实现多层感知机"><a href="#1-3-实现多层感知机" class="headerlink" title="1.3. 实现多层感知机"></a>1.3. 实现多层感知机</h3><p><strong>从零实现</strong>：<br>继续使用Fashion-MNIST图像分类数据集，并与之前softmax回归的结果进行比较。将每个图像视为具有784个输入特征（使用reshape将每个二维图像转换为一个长度为784的向量）和10个类的简单分类数据集，实现一个具有单隐藏层的多层感知机，包含256个隐藏单元。选择2的若干次幂作为层的宽度（隐含层单元的个数）是因为考虑到内存在硬件中的分配和寻址方式，这么做往往可以在计算上更高效。<br>对于每一层都要记录一个权重矩阵和一个偏置向量，并为损失关于这些参数的梯度分配内存。<br>手动实现一个简单的多层感知机是很容易的。然而如果有大量的层，从零开始实现多层感知机会变得很麻烦（例如，要命名和记录大量参数）。<br><strong>框架实现</strong>：<br>使用高级API更简洁地实现多层感知机。<br>对于相同的分类问题，多层感知机的实现与softmax回归的实现相同，只是多层感知机的实现里增加了带有激活函数的隐藏层。</p><h2 id="2-模型选择、欠拟合和过拟合"><a href="#2-模型选择、欠拟合和过拟合" class="headerlink" title="2. 模型选择、欠拟合和过拟合"></a>2. 模型选择、欠拟合和过拟合</h2><p>机器学习问题中，我们的目标是发现模式（pattern），确定模型真正发现了一种可泛化的模式而不是简单的记住了所有数据。<br>我们的目标是发现某些模式， 这些模式捕捉到了我们训练集潜在总体的规律。 如果成功做到了这点，即使是对以前从未遇到过的个体， 模型也可以成功地评估风险。 如何发现可以泛化的模式是机器学习的根本问题。</p><p>困难在于，在训练模型时，我们只能访问数据中的小部分样本。 最大的公开图像数据集包含大约一百万张图像。 而在大部分时候，我们只能从数千或数万个数据样本中学习。 在大型医院系统中，我们可能会访问数十万份医疗记录。 当我们使用有限的样本时，可能会遇到这样的问题： 当收集到更多的数据时，会发现之前找到的明显关系并不成立。</p><p>模型在训练数据上拟合的比在潜在分布中更接近的现象称为过拟合（overfitting）， 用于对抗过拟合的技术称为正则化（regularization）。<br>比如在使用Fashion-MNIST数据集时如果有足够多的神经元、层数和训练迭代周期， 模型最终可以在训练集上达到完美的精度，但此时在测试集上的准确性却下降了。</p><h3 id="2-1-训练误差和泛化误差"><a href="#2-1-训练误差和泛化误差" class="headerlink" title="2.1. 训练误差和泛化误差"></a>2.1. 训练误差和泛化误差</h3><p>误差越小，模型越有效。<br><strong>训练误差</strong>（training error）是指， 模型在训练数据集上计算得到的误差。<br><strong>泛化误差</strong>（generalization error）是指， 模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。</p><p>泛化误差永远不能被准确地计算出。 这是因为无限多的数据样本是一个虚构的对象。 实际中只能通过将模型应用于一个独立的测试集来估计泛化误差， 该测试集由随机选取的、未曾在训练集中出现的数据样本构成。</p><p>对于之前提到的记住所有数据的模式举一个例子，应用查表法来进行预测，对于$28\times28$的灰度图像，如果每个像素可以取$256$个灰度值中的一个， 则有$256^{784}$个可能的图像。 这意味着指甲大小的低分辨率灰度图像的数量比宇宙中的原子要多得多。 即使我们可能遇到这样的数据，我们也不可能存储整个查找表。</p><h4 id="2-1-1-独立同分布假设"><a href="#2-1-1-独立同分布假设" class="headerlink" title="2.1.1. 独立同分布假设"></a>2.1.1. 独立同分布假设</h4><p>监督学习情景中， 通常会假设训练数据和测试数据都是从相同的分布中独立提取的（独立同分布）。 这意味着对数据进行采样的过程没有进行“记忆”。 换句话说，抽取的第2个样本和第3个样本的相关性， 并不比抽取的第2个样本和第200万个样本的相关性更强。</p><p>但这个假设很多时候是有漏洞的，例如一个模型应用在多个数据集时，不同数据集的分布可能都是不同的。其次，抽样过程可能与时间有关，从而违反独立性。有时轻微违法独立同分布假设时模型依然可以运行良好，但另一些时候在违背独立同分布的数据上应用模型会导致错误。</p><p>当训练模型时，我们试图找到一个能够尽可能拟合训练数据的函数。 但是如果它执行地“太好了”，而不能对看不见的数据做到很好泛化，就会导致过拟合。 这种情况正是我们想要避免或控制的。 深度学习中有许多启发式的技术旨在防止过拟合。</p><h4 id="2-1-2-模型复杂性"><a href="#2-1-2-模型复杂性" class="headerlink" title="2.1.2. 模型复杂性"></a>2.1.2. 模型复杂性</h4><p>模型复杂性由什么构成是一个复杂的问题。 一个模型是否能很好地泛化取决于很多因素。 例如，具有更多参数的模型可能被认为更复杂， 参数有更大取值范围的模型可能更为复杂。 通常对于神经网络，我们认为需要更多训练迭代的模型比较复杂， 而需要“早停”（early stopping）的模型（即较少训练迭代周期）就不那么复杂。</p><p>很难比较本质上不同大类的模型之间（例如，决策树与神经网络）的复杂性。统计学家认为，能够轻松解释任意事实的模型是复杂的， 而表达能力有限但仍能很好地解释数据的模型可能更有现实用途。<br>几个影响模型泛化的因素：</p><ol><li>可调整参数的数量。当可调整参数的数量（有时称为自由度）很大时，模型往往更容易过拟合。</li><li>参数采用的值。当权重的取值范围较大时，模型可能更容易过拟合。</li><li>训练样本的数量。即使你的模型很简单，也很容易过拟合只包含一两个样本的训练集。而过拟合一个有数百万个样本的数据集则需要一个极其灵活的模型。<h3 id="2-2-模型选择"><a href="#2-2-模型选择" class="headerlink" title="2.2. 模型选择"></a>2.2. 模型选择</h3>在机器学习中，我们通常在评估几个候选模型后选择最终的模型。 这个过程叫做模型选择。<br>有时进行比较的模型在本质上是完全不同的（比如，决策树与线性模型）。 又有时我们需要比较不同的超参数设置下的同一类模型。</li></ol><p>例如，训练多层感知机模型时，我们可能希望比较具有不同数量的隐藏层、不同数量的隐藏单元以及不同的的激活函数组合的模型。 为了确定候选模型中的最佳模型，我们通常会使用验证集。</p><h4 id="2-2-1-验证集"><a href="#2-2-1-验证集" class="headerlink" title="2.2.1. 验证集"></a>2.2.1. 验证集</h4><p>原则上，在确定所有的超参数之前，我们不希望用到测试集。 如果在模型选择过程中使用测试数据，可能会有过拟合测试数据的风险。过拟合了训练数据，还可以在测试数据上的评估来判断过拟合。 但我们无法知晓模型是否过拟合了测试数据。<br>因此决不能依靠测试数据进行模型选择，但也不能仅仅依靠训练数据来选择模型，因为我们无法估计训练数据的泛化误差。</p><p>同时虽然理想情况下我们只会使用测试数据一次，以评估最好的模型或比较一些模型效果，但现实是测试数据很少在使用一次后被丢弃。我们很少能有充足的数据来对每一轮实验采用全新测试集。</p><p>解决此问题的常见做法是将数据分成三份， 除了训练和测试数据集之外，还增加一个验证数据集（validation dataset）， 也叫验证集（validation set）。<br>但现实是验证数据和测试数据之间的边界也十分模糊。d2l书中每次实验都在使用训练集和验证集，而没有真正的测试集，实验报告的准确度都是验证集准确度。</p><h4 id="2-2-2-K-折交叉验证"><a href="#2-2-2-K-折交叉验证" class="headerlink" title="2.2.2.  $K$折交叉验证"></a>2.2.2.  $K$折交叉验证</h4><p>当训练数据稀缺时，可能无法提供足够的数据来构成一个合适的验证集。 一个解决方案是采用$K$折交叉验证。<br>原始训练数据被分成$K$个不重叠的子集。 然后执行$K$次模型训练和验证，每次在$K-1$个子集上进行训练， 并在剩余的一个子集（在该轮中没有用于训练的子集）上进行验证。 最后，通过对$K$次实验的结果取平均来估计训练和验证误差。</p><h3 id="2-3-欠拟合，过拟合"><a href="#2-3-欠拟合，过拟合" class="headerlink" title="2.3. 欠拟合，过拟合"></a>2.3. 欠拟合，过拟合</h3><p>比较训练和验证误差时要注意两种常见的情况： 欠拟合，过拟合。<br><strong>欠拟合</strong>：训练误差和验证误差都很严重，但它们之间仅有一点差距。如果模型不能降低训练误差，这可能意味着模型过于简单（即表达能力不足），无法捕获试图学习的模式。 又由于训练和验证误差之间的泛化误差很小， 我们有理由相信可以用一个更复杂的模型降低训练误差。 这种现象被称为欠拟合（underfitting）。</p><p><strong>过拟合</strong>：当我们的训练误差明显低于验证误差时要小心， 这表明严重的过拟合（overfitting）。过拟合并不总是一件坏事，特别是在深度学习领域，最好的预测模型在训练数据上的表现往往比在保留（验证）数据上好得多。最终通常更关心验证误差，而不是训练误差和验证误差之间的差距。</p><p>是否过拟合或欠拟合可能取决于模型复杂性和可用训练数据集的大小。</p><h4 id="2-3-1-模型复杂性"><a href="#2-3-1-模型复杂性" class="headerlink" title="2.3.1. 模型复杂性"></a>2.3.1. 模型复杂性</h4><p>关于过拟合和模型复杂性的经典直觉：给定由单个特征$x$和对应实数标签$y$组成的训练数据， 我们试图找到下面的$d$阶多项式来估计标签$y$。</p><script type="math/tex; mode=display">\hat{y}= \sum_{i=0}^d x^i w_i</script><p>这只是一个线性回归问题，特征是$x$的幂给出的， 模型的权重是$w_i$给出的，偏置是$w_0$给出的 （因为对于所有的$x$都有$x^0$ = 1）。线性回归问题可以使用平方误差作为损失函数。</p><p>高阶多项式函数比低阶多项式函数复杂得多。高阶多项式的参数较多，模型函数的选择范围较广。 因此在固定训练数据集的情况下， 高阶多项式函数相对于低阶多项式的训练误差应该始终更低（最坏也是相等）。 其实当数据样本包含了$x$的不同值时， 函数阶数等于数据样本数量的多项式函数可以完美拟合训练集（因为y就是一个多项式，通过优化参数总可以表示出来y）。下图直观地描述了多项式的阶数和欠拟合与过拟合之间的关系。<br><img src="/assets/post_img/article44/模型复杂度对拟合的影响.svg" alt="fit"></p><h4 id="2-3-2-数据集大小"><a href="#2-3-2-数据集大小" class="headerlink" title="2.3.2. 数据集大小"></a>2.3.2. 数据集大小</h4><p>训练数据集中的样本越少，我们就越有可能（且更严重地）过拟合。 随着训练数据量的增加，泛化误差通常会减小。一般来说，更多的数据不会有什么坏处。<br>对于固定的任务和数据分布，模型复杂性和数据集大小之间通常存在关系。 给出更多的数据，我们可能会尝试拟合一个更复杂的模型。 能够拟合更复杂的模型可能是有益的。 如果没有足够的数据，简单的模型可能更有用。 对于许多任务，深度学习只有在有数千个训练样本时才优于线性模型。 从一定程度上来说，深度学习目前的生机要归功于廉价存储、互联设备以及数字化经济带来的海量数据集。</p><h3 id="2-4-多项式回归"><a href="#2-4-多项式回归" class="headerlink" title="2.4. 多项式回归"></a>2.4. 多项式回归</h3><p>通过多项式拟合来探索上述概念。 一样放到notebook中，话说如果都放到notebook中我为什么不把博客格式都做成notebook…<br>给定$x$，使用以下三阶多项式来生成训练和测试数据的标签：</p><script type="math/tex; mode=display">y = 5 + 1.2x - 3.4\frac{x^2}{2!} + 5.6 \frac{x^3}{3!} + \epsilon \text{ where } \epsilon \sim \mathcal{N}(0, 0.1^2)</script><p>噪声项$\epsilon$服从均值为0且标准差为0.1的正态分布。 在优化的过程中，我们通常希望避免非常大的梯度值或损失值，所以将特征从$x^i$调整为$\frac{x^i}{i!}$，这样可以避免很大的$i$带来的特别大的指数值。训练集和测试集各生成100个样本。</p><p>分别对3阶、2阶（线性函数）和高阶多项式训练并观察结果，可以看出3阶是正常的，线性函数会产生欠拟合问题，而高阶（20）则会产生过拟合问题。<br>结果说明2阶多项式模型过于简单。而在20阶多项式模型的实验中，实际上只有前5阶有非零的权值，其余阶均为0，这种情况下，高阶多项式模型的参数过多，但训练样本只有100个，数量较少，模型无法从中学习到高阶权值应为0的特点，即模型被数据中的噪声轻易的影响了。虽然高阶模型能够得到更小的训练误差，但在测试集上的表现会比在训练集上要差，故过拟合。<br>但对于这个实验，使用高阶多项式模型的损失仍然小于线性模型（在同样的迭代周期下）。</p><h2 id="3-权重衰减"><a href="#3-权重衰减" class="headerlink" title="3. 权重衰减"></a>3. 权重衰减</h2><p>正则化就是把额外的约束或者惩罚项加到已有模型（损失函数）上，以防止过拟合并提高泛化能力。假设我们已经拥有尽可能多的高质量数据，我们便可以将重点放在正则化技术上。</p><p>在多项式回归的例子中， 我们可以通过调整拟合多项式的阶数来限制模型的容量。事实上这种限制特征的数量是缓解过拟合的一种常用技术，但这种直接丢弃特征的做法可能过于生硬。 多项式对多变量数据的自然扩展称为单项式（monomials），也就是多变量幂的乘积。单项式的阶数是幂的和。 例如，$x_1^2 x_2$和$x_3 x_5^2$都是3次单项式。<br>随着阶数$d$的增长，带有阶数$d$的项数迅速增加。 给定$k$个变量，阶数为$d$的项的个数为：（这个式子的含义就是从k-1+d个中取k-1个，即$C^{k-1}_{k-1+d}$）</p><script type="math/tex; mode=display">{k - 1 + d} \choose {k - 1}</script><p>例如对于3个变量（x1，x2，x3）的多项式，阶数为1的项有3个（x1，x2，x3），对应从3个（3-1+1）中选2个（3-1），也就是$C^2_3 = C^1_3 = 3$。具体计算方式如下（就是排列组合）：<br><img src="/assets/post_img/article44/排列组合.jpeg" alt="排列组合"><br>那么对于2阶项就对应从4个（3-1+2）中选2个，即$C^2_4 = 6种$，分别是：x1x2，x1x3，x2x3和三个变量单独的2次项。<br>因此即使是阶数上的微小变化，也会显著增加模型的复杂性。仅仅通过简单的限制特征数量（在多项式回归中体现为限制阶数）可能仍然使模型在过简单和过复杂中徘徊，我们需要一个更细粒度的工具来调整函数的复杂性，使其达到一个合适的平衡位置。</p><h3 id="3-1-范数与权重衰减"><a href="#3-1-范数与权重衰减" class="headerlink" title="3.1. 范数与权重衰减"></a>3.1. 范数与权重衰减</h3><p>回顾范数的三个性质，以及$L_1$范数：向量元素的绝对值之和 和 $L_2$范数：向量元素平方和的平方根。</p><p>在训练参数化机器学习模型时， 权重衰减（weight decay）是最广泛使用的正则化的技术之一， 它通常也被称为$L_2$正则化。 这项技术通过函数与零的距离来衡量函数的复杂度， 因为在所有函数$f$中，函数$f = 0$（所有输入都得到值$0$） 在某种意义上是最简单的。 但是应该如何精确地测量一个函数和零之间的距离并没有标准答案。</p><p>一种简单的方法是通过线性函数 $f(\mathbf{x}) = \mathbf{w}^\top \mathbf{x}$ 中的权重向量的某个范数来度量其复杂性（这是一种正则化线性模型）， 例如$| \mathbf{w} |^2$。 要保证权重向量比较小， 最常用方法是将其范数作为惩罚项加到最小化损失的问题中。 （为什么权重大会导致模型复杂度提高？）</p><p>这样一来训练目标就由最小化训练标签上的预测损失变为最小化预测损失和惩罚项之和。 此时如果权重向量增长的太大， 我们的学习算法可能会更集中于最小化权重范数$| \mathbf{w} |^2$（简而言之我们想起到的效果就是在最小化损失函数的同时保证权重不太大，至于为什么这样做，不太懂）。现在回顾一下之前的线性回归例子。其中损失由下式给出：</p><script type="math/tex; mode=display">L(\mathbf{w}, b) = \frac{1}{n}\sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2</script><p>$\mathbf{x}^{(i)}$是样本$i$的特征， $y^{(i)}$是样本$i$的标签， $(\mathbf{w}, b)$是权重和偏置参数。 为了惩罚权重向量的大小， 我们必须以某种方式在损失函数中添加$| \mathbf{w} |^2$， 但是模型应该如何平衡这个新的额外惩罚的损失？通常通过正则化常数 $\lambda$ 来描述这种权衡， 这是一个非负超参数，我们使用验证数据拟合：</p><script type="math/tex; mode=display">L(\mathbf{w}, b) + \frac{\lambda}{2} \|\mathbf{w}\|^2</script><p>这样操作后对于$\lambda = 0$，保持了原来的损失函数。 对于$\lambda &gt; 0$，则可以通过这一惩罚项来增大损失函数，从而使参数向$| \mathbf{w} |$更小的方向进行优化（我的理解是这样的）。 这里仍然除以$2$是因为当求导时， $2$和$1/2$会抵消，以确保更新表达式看起来既漂亮又简单。 关于为什么这里使用范数的平方而不是标准范数（即欧几里得距离）则是为了便于计算，通过使用$L_2$范数的平方（之前也提到过机器学习经常使用该范数的平方而不是它本身），我们去掉平方根，留下权重向量每个分量的平方和。 这使得惩罚的导数很容易计算，即惩罚项导数的和等于和的导数。</p><p>此外，关于为什么我们首先使用$L_2$范数，而不是$L_1$范数。这是因为这个选择在整个统计领域中都是有效的和受欢迎的。 $L_2$正则化线性模型构成经典的岭回归（ridge regression）算法， $L_1$正则化线性回归是统计学中类似的基本模型， 通常被称为套索回归（lasso regression）。 </p><p>使用$L_2$范数的一个原因是它对权重向量的大分量施加了巨大的惩罚。 这使得我们的学习算法偏向于在大量特征上均匀分布权重的模型。 在实践中，这可能使它们对单个变量中的观测误差更为稳定。 相比之下，$L_1$惩罚会导致模型将权重集中在一小部分特征上， 而将其他权重清除为零。 这称为<strong>特征选择</strong>（feature selection），这可能是其他场景下需要的。<br>$L_2$正则化回归的小批量随机梯度下降更新如下式，其中$\mathcal{B}$为小批量，$\eta$为预先确定的正数，$\lambda$是正则化常数 ：</p><script type="math/tex; mode=display">\begin{aligned} \mathbf{w} & \leftarrow \left(1- \eta\lambda \right) \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right). \end{aligned}</script><p>我们依然是根据估计值与观测值之间的差异来更新$\mathbf{w}$，但同时也在试图将$\mathbf{w}$的大小缩小到零（有点没看懂）。 这就是为什么这种方法有时被称为权重衰减。 我们仅考虑惩罚项，优化算法在训练的每一步衰减权重。 与特征选择相比，权重衰减为我们提供了一种连续的机制来调整函数的复杂度。 较小的$\lambda$值对应较少约束的$\mathbf{w}$， 而较大的$\lambda$值对$\mathbf{w}$的约束更大（这里约束就是使值更小的意思吧）。</p><p>是否对相应的偏置$b^2$（为什么是b方？可能也是范数平方吧）进行惩罚在不同的实践中会有所不同， 在神经网络的不同层中也会有所不同。 通常，网络输出层的偏置项不会被正则化。</p><h3 id="3-2-权重衰减演示的从零实现"><a href="#3-2-权重衰减演示的从零实现" class="headerlink" title="3.2. 权重衰减演示的从零实现"></a>3.2. 权重衰减演示的从零实现</h3><p>通过高维线性回归演示如何实现权重衰减。（见实践）<br>简单说就是先生成了一些高维（200个特征）的数据，用如下公式：</p><script type="math/tex; mode=display">y = 0.05 + \sum_{i = 1}^d 0.01 x_i + \epsilon \text{ where } \epsilon \sim \mathcal{N}(0, 0.01^2)</script><p>标签是关于输入的线性函数。 标签同时被均值为0，标准差为0.01高斯噪声破坏。</p><p>然后将$L_2$的平方惩罚添加到原始目标函数中。最终通过对比使用正则化和不使用正则化的过拟合优化情况可以看到正则化后测试误差减小。</p><h3 id="3-3-权重衰减演示的框架实现"><a href="#3-3-权重衰减演示的框架实现" class="headerlink" title="3.3. 权重衰减演示的框架实现"></a>3.3. 权重衰减演示的框架实现</h3><p>由于权重衰减在神经网络优化中很常用， 深度学习框架为了便于我们使用权重衰减， 将权重衰减集成到优化算法中，以便与任何损失函数结合使用。 此外，这种集成还有计算上的好处， 允许在不增加任何额外的计算开销的情况下向算法中添加权重衰减。 由于更新的权重衰减部分仅依赖于每个参数的当前值， 因此优化器必须至少接触每个参数一次。</p><h2 id="4-暂退法（Dropout）"><a href="#4-暂退法（Dropout）" class="headerlink" title="4. 暂退法（Dropout）"></a>4. 暂退法（Dropout）</h2><p>在概率角度看，可以通过以下论证来证明这一技术的合理性： 我们已经假设了一个先验，即权重的值取自均值为0的正态分布。我们希望模型深度挖掘特征，即将其权重分散到许多特征中， 而不是过于依赖少数潜在的虚假关联。</p><h3 id="4-1-重新审视过拟合"><a href="#4-1-重新审视过拟合" class="headerlink" title="4.1. 重新审视过拟合"></a>4.1. 重新审视过拟合</h3><p>当面对更多的特征而样本不足时，线性模型往往会过拟合。 相反，当给出更多样本而不是特征，通常线性模型不会过拟合。<br>但线性模型泛化的可靠性是有代价的，线性模型不会考虑到特征之间的交互作用。对于每个特征，线性模型必须指定正的或负的权重，而忽略其他特征。</p><p>泛化性和灵活性之间的这种基本权衡被描述为偏差-方差权衡（bias-variance tradeoff）。 线性模型有很高的偏差：它们只能表示一小类函数。 然而，这些模型的方差很低（泛化性好）：它们在不同的随机数据样本上可以得出相似的结果。</p><p>深度神经网络位于偏差-方差谱的另一端。 与线性模型不同，神经网络并不局限于单独查看每个特征，而是学习特征之间的交互。 例如，神经网络可能推断“尼日利亚”和“西联汇款”一起出现在电子邮件中表示垃圾邮件， 但单独出现则不表示垃圾邮件。</p><p>即使我们有比特征多得多的样本，深度神经网络也有可能过拟合。深度网络的泛化性质令人费解，而这种泛化性质的数学基础仍然是悬而未决的研究问题。</p><h3 id="4-2-扰动的稳健性"><a href="#4-2-扰动的稳健性" class="headerlink" title="4.2. 扰动的稳健性"></a>4.2. 扰动的稳健性</h3><p>在探究泛化性之前，我们先来定义一下什么是一个“好”的预测模型？ 我们期待“好”的预测模型能在未知的数据上有很好的表现： 经典泛化理论认为，为了缩小训练和测试性能之间的差距，应该以简单的模型为目标。 简单性以较小维度的形式展现，此外，参数的范数也代表了一种有用的简单性度量。</p><p>简单性的另一个角度是平滑性，即函数不应该对其输入的微小变化敏感。 例如，当我们对图像进行分类时，我们预计向像素添加一些随机噪声应该是基本无影响的。1995年，克里斯托弗·毕晓普用数学证实了“要求函数光滑”和“要求函数对输入的随机噪声具有适应性”之间的联系。</p><p>在2014年，斯里瓦斯塔瓦等人就如何将毕晓普的想法应用于网络的内部层提出了一个想法： 在训练过程中，他们建议在计算后续层之前向网络的每一层注入噪声。 因为当训练一个有多层的深层网络时，注入噪声只会在输入-输出映射上增强平滑性。</p><p>这个想法被称为<strong>暂退法（dropout）</strong>。 暂退法在<strong>前向传播</strong>过程中，计算每一内部层的同时注入噪声，这已经成为训练神经网络的常用技术。 这种方法之所以被称为暂退法，因为我们从表面上看是在训练过程中丢弃（drop out）一些神经元。 在整个训练过程的每一次迭代中，标准暂退法包括在计算下一层之前将当前层中的一些节点置零。<br>暂退法的原始论文提到了一个关于有性繁殖的类比： 神经网络过拟合与每一层都依赖于前一层激活值相关，称这种情况为“共适应性”。 作者认为，暂退法会破坏共适应性，就像有性生殖会破坏共适应的基因一样。</p><p>那么关键的挑战就是如何注入这种噪声。 一种想法是以一种无偏向（unbiased）的方式注入噪声。 这样在固定住其他层时，每一层的期望值等于没有噪音时的值。</p><p>在毕晓普的工作中，他将正态分布的噪声添加到线性模型的输入中。 在每次训练迭代中，他将从均值为零的分布$\epsilon \sim \mathcal{N}(0,\sigma^2)$ 采样噪声添加到输入$\mathbf{x}$， 从而产生扰动点$\mathbf{x}’ = \mathbf{x} + \epsilon$， 预期是$E[\mathbf{x}’] = \mathbf{x}$。</p><p>在标准暂退法正则化中，通过按保留（未丢弃）的节点的分数进行规范化来消除每一层的偏差。 换言之，每个中间活性值$h$以暂退概率$p$由随机变量$h’$替换，如下所示：</p><script type="math/tex; mode=display">\begin{split}\begin{aligned} h' = \begin{cases} 0 & \text{ 概率为 } p \\ \frac{h}{1-p} & \text{ 其他情况} \end{cases} \end{aligned}\end{split}</script><p>根据此模型的设计，其期望值保持不变，即$E[h’] = h$。</p><h3 id="4-3-暂退法实践"><a href="#4-3-暂退法实践" class="headerlink" title="4.3. 暂退法实践"></a>4.3. 暂退法实践</h3><p>下图为dropout前后的多层感知机，在左图中，删除了$h_2$和$h_5$， 因此输出的计算不再依赖于$h_2$或$h_5$，并且它们各自的梯度在执行反向传播时也会消失。 这样，输出层的计算不能过度依赖于$h_1, \ldots, h_5$的任何一个元素。<br><img src="/assets/post_img/article44/dropout.svg" alt="暂退法"><br>通常在测试时不使用暂退法（训练时用）。 给定一个训练好的模型和一个新的样本，我们不会丢弃任何节点，因此不需要标准化。 然而也有一些例外：一些研究人员在测试时使用暂退法， 用于估计神经网络预测的“不确定性”： 如果通过许多不同的暂退法遮盖后得到的预测结果都是一致的，那么我们可以说网络发挥更稳定。</p><h3 id="4-4-暂退法的实现"><a href="#4-4-暂退法的实现" class="headerlink" title="4.4. 暂退法的实现"></a>4.4. 暂退法的实现</h3><p>从零实现和框架实现具体见<a href="https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x04.ipynb">对应实践</a>,几个可以注意的点如下：</p><ul><li>在靠近输入层的地方设置较低的暂退概率。</li><li>框架中，在训练时，Dropout层将根据指定的暂退概率随机丢弃上一层的输出（相当于下一层的输入）。 在测试时，Dropout层仅传递数据。</li></ul><h2 id="5-前向传播与反向传播"><a href="#5-前向传播与反向传播" class="headerlink" title="5. 前向传播与反向传播"></a>5. 前向传播与反向传播</h2><p>前向传播（forward propagation或forward pass） 指的是：按顺序（从输入层到输出层）计算和存储神经网络中每层的结果。</p><p>反向传播（backward propagation或backpropagation）指的是计算神经网络参数梯度的方法。 简言之，该方法根据微积分中的链式规则，按相反的顺序从输出层到输入层遍历网络。 该算法存储了计算某些参数梯度时所需的任何中间变量（偏导数）。假设有函数 $Y=f(\mathrm{X})$ 和 $\mathrm{Z}=g(\mathrm{Y})$, 其中输入 和输出 $X, Y, Z$ 是任意形状的张量。利用链式法则, 我们可以计算Z关于 $X$ 的导数</p><script type="math/tex; mode=display">\frac{\partial \mathrm{Z}}{\partial \mathrm{X}}=\operatorname{prod}\left(\frac{\partial \mathrm{Z}}{\partial \mathrm{Y}}, \frac{\partial \mathrm{Y}}{\partial \mathrm{X}}\right)</script><p>这里使用prod运算符在执行必要的操作 (如换位和交换输入位置) 后将其参数相乘。对于向量只是矩阵矩阵乘法。对于高维张量则使用适当的对应项。运算符prod指代了所有的这些符号。</p><p>在训练神经网络时，在初始化模型参数后， 我们交替使用前向传播和反向传播，利用反向传播给出的梯度来更新模型参数。 注意，反向传播重复利用前向传播中存储的中间值，以避免重复计算。 带来的影响之一是我们需要保留中间值，直到反向传播完成。 这也是训练比单纯的预测需要更多的内存（显存）的原因之一。 此外，这些中间值的大小与网络层的数量和批量的大小大致成正比。 因此，使用更大的批量来训练更深层次的网络更容易导致内存不足（out of memory）错误。</p><h2 id="6-数值稳定性和模型初始化"><a href="#6-数值稳定性和模型初始化" class="headerlink" title="6. 数值稳定性和模型初始化"></a>6. 数值稳定性和模型初始化</h2><p>到目前为止学习的每个模型都是根据某个预先指定的分布来初始化模型的参数。但事实上这种初始化方案并不是理所当然的，初始化方案的选择在神经网络学习中起着举足轻重的作用，它对保持数值稳定性至关重要。 此外，这些初始化方案的选择可以与非线性激活函数的选择有趣的结合在一起。 我们选择哪个函数以及如何初始化参数可以决定优化算法收敛的速度有多快。 糟糕选择可能会导致我们在训练时遇到<strong>梯度爆炸</strong>或<strong>梯度消失</strong>。<br>一些有用的启发式方法在整个深度学习生涯中都很有用。</p><h3 id="6-1-梯度消失和梯度爆炸"><a href="#6-1-梯度消失和梯度爆炸" class="headerlink" title="6.1. 梯度消失和梯度爆炸"></a>6.1. 梯度消失和梯度爆炸</h3><p>在深度神经网络中输出关于任何一组参数（权重）的梯度都可以表示n个矩阵与梯度向量的乘积。其中n个矩阵和梯度向量取决于关于哪组参数求梯度，设当前深度神经网络为L层，当关于 $l$ 层参数时，n为 $L-l$ ，梯度向量为$\partial_{\mathbf{W}}(l) \boldsymbol{h}^{(l)}$。<br>注：每一层 $l$ 由变换 $f_{l}$ 定义, 该变换的参数为权重 $\mathbf{W}^{(l)}$, 其隐藏变量是 $\mathbf{h}^{(l)}$ (令 $\mathbf{h}^{(0)}=输入\mathbf{x}$ )。</p><p>这样不稳定的梯度（太多概率相乘）造成两个问题：<br>一是容易受到数值下溢影响（概率乘积过小），即便对概率取对数，将数值表示的压力从尾数转移到指数，也有可能出现数值溢出问题（当概率趋近零，取对数后对数值趋近负无穷）。</p><p>二是威胁到优化算法的稳定性。 此时面临的问题要么是<strong>梯度爆炸</strong>（gradient exploding）问题： 参数更新过大，破坏了模型的稳定收敛； 要么是<strong>梯度消失</strong>（gradient vanishing）问题： 参数更新过小，在每次更新时几乎不会移动，导致模型无法学习。</p><h4 id="6-1-1-梯度消失（gradient-vanishing）"><a href="#6-1-1-梯度消失（gradient-vanishing）" class="headerlink" title="6.1.1. 梯度消失（gradient vanishing）"></a>6.1.1. 梯度消失（gradient vanishing）</h4><p>sigmoid函数曾经是导致梯度消失问题的一个常见的原因。<br><img src="/assets/post_img/article44/sigmoid_gradient.svg" alt="sg"><br>当sigmoid函数的输入很大或是很小时，它的梯度都会消失。 此外，当反向传播通过许多层时，除非我们在刚刚好的地方， 这些地方sigmoid函数的输入接近于零，否则整个乘积的梯度可能会消失。 当我们的网络有很多层时，除非我们很小心，否则在某一层可能会切断梯度。 这个问题曾经困扰着深度网络的训练，因此，更稳定的ReLU系列函数已经成为从业者的默认选择（虽然在神经科学的角度看起来不太合理）。</p><h4 id="6-1-2-梯度爆炸（gradient-exploding）"><a href="#6-1-2-梯度爆炸（gradient-exploding）" class="headerlink" title="6.1.2. 梯度爆炸（gradient exploding）"></a>6.1.2. 梯度爆炸（gradient exploding）</h4><p>当梯度爆炸是由于深度网络的初始化所导致时，我们没有机会让梯度下降优化器收敛。</p><h4 id="6-1-3-打破对称性"><a href="#6-1-3-打破对称性" class="headerlink" title="6.1.3. 打破对称性"></a>6.1.3. 打破对称性</h4><p>神经网络设计中的另一个问题是其参数化所固有的对称性。例如对于简单的多层感知机，每一层的隐藏单元之间具有排列对称性。</p><p>例如对于一个含两个隐藏单元隐藏层的多层感知机，输出层将两个隐藏单元的多层感知机转换为仅一个输出单元。如果将隐藏层的所有参数初始化为 $\mathbf{W}^{(1)}=c ， c$ 为常量。在这种情况下, 在前向传播期间, 两个隐藏单元采用相同的输入和参数, 产生相同的激活, 该激活被送到输出单元。在反向传播期间, 根据参数 $\mathbf{W}^{(1)}$ 对输出单元进行微分, 得到一个梯度, 其元素都取相同的值。因此，在基于梯度的迭代 (例如, 小批量随机梯度下降）之后, $\mathbf{W}^{(1)}$ 的所有元素仍然采用相同的值。这样的迭代永远不会打破对称性, 则可能永远也无法实现网络的表达能力。隐藏层的行为就好像只有一个单元。 小批量随机梯度下降不会打破这种对称性，但暂退法正则化可以。</p><h3 id="6-2-参数初始化"><a href="#6-2-参数初始化" class="headerlink" title="6.2. 参数初始化"></a>6.2. 参数初始化</h3><p>减缓上述三个问题的一种办法是在参数初始化方面进行改良，在优化期间做一些工作和适当的正则化也可以进一步提高稳定性。</p><h4 id="6-2-1-默认初始化"><a href="#6-2-1-默认初始化" class="headerlink" title="6.2.1. 默认初始化"></a>6.2.1. 默认初始化</h4><p>如果不人为指定初始化方法，框架将使用默认的随机初始化方法，对于中等难度的问题，这种方法通常很有效。</p><h4 id="6-2-2-Xavier初始化"><a href="#6-2-2-Xavier初始化" class="headerlink" title="6.2.2. Xavier初始化"></a>6.2.2. Xavier初始化</h4><p>对于只有线性计算的全连接层的输出$o_{i}$，设有$n_{\mathrm{in}}$个输入 $x_{j}$ 及对应的相关权重 $w_{i j}$，则输出可由下式表示：</p><script type="math/tex; mode=display">o_{i}=\sum_{j=1}^{n_{\text {in }}} w_{i j} x_{j}</script><p>权重 $w_{i j}$ 都是从同一分布中独立抽取的。假设该分布（并不一定是正态分布，只是需要均值和方差存在）具有零均值和方差 $\sigma^{2}$，同时假设层 $x_{j}$ 的输入也具有零均值和方差 $\gamma^{2}$, 它们独立于 $w_{i j}$ 并且彼此独立。在这种假设下就可以按如下方式计算 $o_{i}$ 的期望和方差：</p><script type="math/tex; mode=display">\begin{aligned}E\left[o_{i}\right] &=\sum_{j=1}^{n_{\text {in }}} E\left[w_{i j} x_{j}\right] \\&=\sum_{j=1}^{n_{\text {in }}} E\left[w_{i j}\right] E\left[x_{j}\right] \\&=0, \\\operatorname{Var}\left[o_{i}\right] &=E\left[o_{i}^{2}\right]-\left(E\left[o_{i}\right]\right)^{2} \\&=\sum_{j=1}^{n_{\text {in }}} E\left[w_{i j}^{2} x_{j}^{2}\right]-0 \\&=\sum_{j=1}^{n_{\text {in }}} E\left[w_{i j}^{2}\right] E\left[x_{j}^{2}\right] \\&=n_{\text {in }} \sigma^{2} \gamma^{2}\end{aligned}</script><p>所以保持输出与输入的方差不变的一种方法是设置 $n_{\text {in }} \sigma^{2}=1$ 。 在反向传播过程中也是类似的问题, 梯度从更靠近输出的层传播的。使用与正向传播相同的推断, 可以得出除非 $n_{\text {out }} \sigma^{2}=1$, 否则梯度的方差可能会增大（$n_{\text {out }}$ 是该层的输出的数量）。但很明显不可能同时满足这两个条件（$n_{\text {in}}$和$n_{\text {out}}$相等且与$\sigma^{2}$乘积为1）。则只需满足：</p><script type="math/tex; mode=display">\frac{1}{2}\left(n_{\text {in }}+n_{\text {out }}\right) \sigma^{2}=1 \text { 或等价于 } \sigma=\sqrt{\frac{2}{n_{\text {in }}+n_{\mathrm{out}}}}</script><p>这就是现在标准且实用的Xavier初始化的基础，它以其提出者第一作者的名字命名。</p><p>通常, Xavier初始化从均值为零, 方差 $\sigma^{2}=\frac{2}{n_{\text {in }}+n_{\text {aut }}}$ 的正态分布中采样权重。我们也可以利用 Xavier的直觉来选择从均匀分布中抽取权重时的方差。注意均匀分布 $U(-a, a)$ 的方差为 $\frac{a^{2}}{3}$ 。将 $\frac{a^{2}}{3}$ 代 入到 $\sigma^{2}$ 的条件中，将得到初始化值域：</p><script type="math/tex; mode=display">U\left(-\sqrt{\frac{6}{n_{\text {in }}+n_{\text {out }}}}, \sqrt{\frac{6}{n_{\text {in }}+n_{\text {out }}}}\right) .</script><p>虽然上述的“不存在非线性计算”的假设在神经网络中很容易被违反, 但Xavier初始化方法在实践中被证明是有效的。</p><h4 id="6-2-3-更多初始化方法"><a href="#6-2-3-更多初始化方法" class="headerlink" title="6.2.3. 更多初始化方法"></a>6.2.3. 更多初始化方法</h4><p>上面的推理仅仅触及了现代参数初始化方法的皮毛。 深度学习框架通常实现十几种不同的启发式方法。 参数初始化一直是深度学习基础研究的热点领域。 其中包括专门用于参数绑定（共享）、超分辨率、序列模型和其他情况的启发式算法。</p><h2 id="7-环境和分布偏移"><a href="#7-环境和分布偏移" class="headerlink" title="7. 环境和分布偏移"></a>7. 环境和分布偏移</h2><p>许多机器学习应用中存在的问题之一就是在将基于模型的决策引入环境时可能会破坏模型。比如用户根据模型的某些特点，做出对应更改从而获取非法利益。</p><h3 id="7-1-分布偏移的类型"><a href="#7-1-分布偏移的类型" class="headerlink" title="7.1. 分布偏移的类型"></a>7.1. 分布偏移的类型</h3><p>分布偏移，在我理解就是两个（或多个）数据集间分布不同的情况，比如在训练集和测试集之间，如果没有一个关于两者间相互关系的预估，那将不能学习到有用的模型。<br>基于对未来数据可能发生变化的一些限制性假设，有些算法可以检测这种偏移，甚至可以动态调整，提高原始分类器的精度。</p><h4 id="7-1-1-协变量偏移（covariate-shift）"><a href="#7-1-1-协变量偏移（covariate-shift）" class="headerlink" title="7.1.1. 协变量偏移（covariate shift）"></a>7.1.1. 协变量偏移（covariate shift）</h4><p>在不同分布偏移中, 协变量偏移可能是最为广泛研究的。 协变量偏移是指：输入的分布可能随时间而改变, 但标签函数（即条件分布 $P(y \mid \mathbf{x})$ ）没有改变。<br>统计学家称之为协变量偏移是因为这个问题是由于协变量（特征）分布的变化而产生的。具体的例子比如在训练分类器时，训练集为真实图片，而测试集为卡通图片。<br>当认为 $\mathbf{x}$ 导致 $y$ 时, 标签偏移是一个合理的假设。</p><h4 id="7-1-2-标签偏移（label-shift）"><a href="#7-1-2-标签偏移（label-shift）" class="headerlink" title="7.1.2. 标签偏移（label shift）"></a>7.1.2. 标签偏移（label shift）</h4><p>标签偏移（label shift）描述了与协变量偏移相反的问题。它假设标签边缘概率 $P(y)$ 可以改变, 但是类别条件分布 $P(\mathbf{x} \mid y)$ 在不同的领域之间保持不变。<br>当认为 $y$ 导致 $\mathbf{x}$ 时, 标签偏移是一个合理的假设。例如预测患者的疾病, 我们可能根据症状来判断, 即使疾病的相对流行率随着时间的推移而变化。标签偏移在这里是恰当的假设, 因为疾病会引起症状，输入为症状（不变），输出为可能的疾病（随时间而分布变化）。</p><p>有时标签偏移和协变量偏移假设可以同时成立。在这些情况下，使用基于标签偏移假设的方法通常是有利的。这是因为这些方法倾向于包含看起来像标签 (低维) 的对象, 而不是像输入（高维）的对象。</p><h4 id="7-1-3-概念偏移（concept-shift）"><a href="#7-1-3-概念偏移（concept-shift）" class="headerlink" title="7.1.3. 概念偏移（concept shift）"></a>7.1.3. 概念偏移（concept shift）</h4><p>当标签的定义发生变化时，就会出现这种问题。精神疾病的诊断标准、所谓的时髦、以及工作头衔等等，都是概念偏移的日常映射。</p><h3 id="7-2-分布偏移纠正"><a href="#7-2-分布偏移纠正" class="headerlink" title="7.2. 分布偏移纠正"></a>7.2. 分布偏移纠正</h3><p>许多情况下训练分布与测试分布是不同的，有时这对模型没什么影响，有时则需要运用一些手段去应对这种偏移。</p><h4 id="7-2-1-经验风险与实际风险"><a href="#7-2-1-经验风险与实际风险" class="headerlink" title="7.2.1. 经验风险与实际风险"></a>7.2.1. 经验风险与实际风险</h4><p>在模型的训练期间，训练数据 ${(\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_n, y_n)}$  的特征和相关的标签经过迭代，在每一个小批量之后更新模型 $f$ 的参数。为了简单起见，我们先不考虑正则化，此时极大地降低了训练损失（即经验风险最小化）：</p><script type="math/tex; mode=display">\underset{f}{\operatorname{minimize}} \frac{1}{n} \sum_{i=1}^{n} l\left(f\left(\mathbf{x}_{i}\right), y_{i}\right),</script><p>其中 $l$ 是损失函数, 用来度量给定标签 $y_{i}$, 预测 $f\left(\mathbf{x}_{i}\right)$ 的 “糟糕程度”。 统计学家称上式中的这一项 $l\left(f\left(\mathbf{x}_{i}\right), y_{i}\right)$ 为经验风险。 经验风险 (empirical risk) 是为了近似真实风险（true risk）。</p><p>真实风险是整个训练数据上的平均损失, 即从其真实分布 $p(\mathbf{x}, y)$ 中抽取的所有数据的总体损失的期望值：</p><script type="math/tex; mode=display">E_{p(\mathbf{x}, y)}[l(f(\mathbf{x}), y)]=\iint l(f(\mathbf{x}), y) p(\mathbf{x}, y) d \mathbf{x} d y</script><p>然而在实践中通常无法获得总体数据。因此, 经验风险最小化是一种实用的机器学习策略，希望能近似最小化真实风险。</p><h4 id="7-2-2-协变量偏移纠正"><a href="#7-2-2-协变量偏移纠正" class="headerlink" title="7.2.2. 协变量偏移纠正"></a>7.2.2. 协变量偏移纠正</h4><p><a href="https://zh-v2.d2l.ai/chapter_multilayer-perceptrons/environment.html#subsec-covariate-shift-correction">下面三种纠正原理</a>暂时放一下，详细看还是得多读几遍书。<br>给出完整的协变量偏移纠正算法，假设我们有一个训练集 ${(\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_n, y_n)}$ 和一个未标记的测试集 ${\mathbf{u}_1, \ldots, \mathbf{u}_m}$ 对于协变量偏移，我们假设 $1 \leq i \leq n$ 的 $\mathbf{x}_{i}$ 来自某个源分布 $q(\mathbf{x})$, $\mathbf{u}_{i}$ 来自目标分布 $p(\mathbf{x})$。以下是纠正协变量偏移的典型算法：</p><ol><li>生成一个二元分类训练集： ${(\mathbf{x}_1, -1), \ldots, (\mathbf{x}_n, -1), (\mathbf{u}_1, 1), \ldots, (\mathbf{u}_m, 1)}$ 。</li><li>用对数几率回归训练二元分类器得到函数 $h_{\circ}$</li><li>使用 $\beta_{i}=\exp \left(h\left(\mathbf{x}_{i}\right)\right)$ 或更好的 $\beta_{i}=\min \left(\exp \left(h\left(\mathbf{x}_{i}\right)\right), c\right)$ (c为常量）对训练数据进行加权。</li><li>使用权重 $\beta_{i}$ 进行”加权经验风险最小化“中 ${(\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_n, y_n)}$ 的训练。</li></ol><p>上述算法依赖于一个重要的假设：需要目标分布(例如, 测试分布)中的每个数据样本在训练时出现的概率非零。如果找到 $p(\mathbf{x})&gt;0$ 但 $q(\mathbf{x})=0$ 的点，那么相应的重要性权重会是无穷大（因为 $\beta_{i} \stackrel{\operatorname{def}}{=} \frac{p\left(\mathbf{x}_{i}\right)}{q\left(\mathbf{x}_{i}\right)}$）。</p><p>加权经验风险最小化:</p><script type="math/tex; mode=display">\underset{f}{\operatorname{minimize}} \frac{1}{n} \sum_{i=1}^{n} \beta_{i} l\left(f\left(\mathbf{x}_{i}\right), y_{i}\right) .</script><h4 id="7-2-3-标签偏移纠正"><a href="#7-2-3-标签偏移纠正" class="headerlink" title="7.2.3. 标签偏移纠正"></a>7.2.3. 标签偏移纠正</h4><p>标签偏移的一个好处是，如果我们在源分布上有一个相当好的模型，那么我们可以得到对这些权重的一致估计，而不需要处理周边的其他维度。 在深度学习中，输入往往是高维对象（如图像），而标签通常是低维（如类别）。</p><h4 id="7-2-4-概念偏移纠正"><a href="#7-2-4-概念偏移纠正" class="headerlink" title="7.2.4. 概念偏移纠正"></a>7.2.4. 概念偏移纠正</h4><p>概念偏移很难用原则性的方式解决。 例如，在一个问题突然从“区分猫和狗”偏移为“区分白色和黑色动物”的情况下， 除了从零开始收集新标签和训练没有其他的办法。 但在实践中这种极端的偏移是罕见的，通常情况下，概念的变化总是缓慢的。 在这种情况下，我们可以使用与训练网络相同的方法，使其适应数据的变化。 换言之，我们使用新数据更新现有的网络权重，而不是从头开始训练。</p><h3 id="7-3-学习问题的分类方法"><a href="#7-3-学习问题的分类方法" class="headerlink" title="7.3. 学习问题的分类方法"></a>7.3. 学习问题的分类方法</h3><p>机器学习研究的是计算机怎样模拟人类的学习行为，以获取新的知识或技能，并重新组织已有的知识结构使之不断改善自身。<br>机器学习能解决的问题包括以下几种（<a href="https://blog.csdn.net/lovenankai/article/details/99965501">参考</a>）：</p><ol><li>分类问题：根据数据样本上抽取出的特征，判定其属于有限个类别中的哪一个。</li><li>回归问题：根据数据样本上抽取出的特征，预测一个连续值的结果。</li><li>聚类问题：根据数据样本上抽取出的特征，让样本抱抱团(相近/相关的样本在一团内)。</li></ol><p>在机器学习模拟学习的方法上，有许多分类，如批量学习、强化学习等等，目前就是在讨论这方面（这些学习问题）的类别有哪些。</p><h4 id="7-3-1-批量学习（batch-learning）"><a href="#7-3-1-批量学习（batch-learning）" class="headerlink" title="7.3.1. 批量学习（batch learning）"></a>7.3.1. 批量学习（batch learning）</h4><p>在批量学习中, 我们可以访问一组训练特征和标签 ${(\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_n, y_n)}$, 使用这些特性和标签训练 $f(\mathbf{x})$ 。 然后部署此模型来对来自同一分布的新数据 $(\mathbf{x}, y)$ 进行评分。 例如, 我们可以根据猫和狗的大量图片训练一个猫检测器。 一旦我们训练了它, 就把它作为智能猫门计算视觉系统的一部分, 来控制只允许猫进入该门。 然后这个系统会被安装在客户家中, 基本再也不会更新。</p><h4 id="7-3-2-在线学习（online-learning）"><a href="#7-3-2-在线学习（online-learning）" class="headerlink" title="7.3.2. 在线学习（online learning）"></a>7.3.2. 在线学习（online learning）</h4><p>“在线”逐个学习数据$\left(\mathbf{x}_{i}, y_{i}\right)$。我们首先观测到$\mathbf{x}_{i}$，然后得出一个估计值 $f\left(\mathbf{x}_{i}\right)$, 当完成估计后，我们才观测到 $y_{i}$。然后根据模型的决定（估计）, 给予模型奖励或惩罚。<br>例如, 我们预测明天的股票价格来根据这个预测进行交易。在一天结束时, 我们会评估模型的预测是否盈利。<br>在线学习的循环如下，这样看起来就清晰易懂了：</p><script type="math/tex; mode=display">\text { model } f_{t} \longrightarrow \text { data } \mathbf{x}_{t} \longrightarrow \text { estimate } f_{t}\left(\mathbf{x}_{t}\right) \longrightarrow \text { observation } y_{t} \longrightarrow \operatorname{loss} l\left(y_{t}, f_{t}\left(\mathbf{x}_{t}\right)\right) \longrightarrow \operatorname{model} f_{t+1}</script><h4 id="7-3-3-老虎机（bandits）"><a href="#7-3-3-老虎机（bandits）" class="headerlink" title="7.3.3. 老虎机（bandits）"></a>7.3.3. 老虎机（bandits）</h4><p>在一个老虎机问题中，只有有限数量的手臂可以拉动。 也就是说我们可以采取的行动是有限的。 老虎机问题是一个相较于上述问题更简单的情景，可以获得更强的最优性理论保证。 这个问题经常被视为一个单独的学习问题的情景。</p><h4 id="7-3-4-控制"><a href="#7-3-4-控制" class="headerlink" title="7.3.4. 控制"></a>7.3.4. 控制</h4><p>在很多情况下，环境（许多算法形成的环境模型）会记住我们所做的事，虽然不一定是以对抗的方式，并且环境会根据记忆做出相应的反应。 例如，咖啡锅炉控制器将根据之前是否加热锅炉来观测到不同的温度。 在这种情况下，PID（比例—积分—微分）控制器算法是一个流行的选择，温度PID控制器的原理是将温度偏差的比例、积分和微分通过线性组合构成控制量，对控制对象进行控制。</p><blockquote><p>这里还是有点没看明白，英文翻译的不好。 22.06.16</p></blockquote><p>近年来，控制理论（如PID的变体）也被用于自动调整超参数， 以获得更好的解构和重建质量，提高生成文本的多样性和生成图像的重建质量。</p><h4 id="7-3-5-强化学习（reinforcement-learning）"><a href="#7-3-5-强化学习（reinforcement-learning）" class="headerlink" title="7.3.5. 强化学习（reinforcement learning）"></a>7.3.5. 强化学习（reinforcement learning）</h4><p>强化学习强调如何基于环境而行动，以取得最大化的预期利益。 国际象棋、围棋、西洋双陆棋或星际争霸都是强化学习的应用实例，如果在游戏中采取某种策略可以取得较高的得分，那么就进一步「强化」这种策略，以期继续取得较好的结果。</p><h4 id="7-3-6-基于应用环境选择学习方法"><a href="#7-3-6-基于应用环境选择学习方法" class="headerlink" title="7.3.6. 基于应用环境选择学习方法"></a>7.3.6. 基于应用环境选择学习方法</h4><p>上述不同情况之间的一个关键区别是： 在静止环境中可能一直有效的相同策略，在环境能够改变的情况下可能不会始终有效。<br>环境变化的速度和方式在很大程度上决定了我们可以采用的算法类型。 例如，如果我们知道事情只会缓慢地变化，就可以迫使任何估计也只能缓慢地发生改变。 如果我们知道环境可能会瞬间发生变化，但这种变化非常罕见，我们就可以在使用算法时考虑到这一点。<br>当一个数据科学家试图解决的问题会随着时间的推移而发生变化时这些类型的知识至关重要。</p><h3 id="7-4-机器学习中的公平、责任和透明度"><a href="#7-4-机器学习中的公平、责任和透明度" class="headerlink" title="7.4. 机器学习中的公平、责任和透明度"></a>7.4. 机器学习中的公平、责任和透明度</h3><p>当部署机器学习系统时，你不仅仅是在优化一个预测模型，通常是在提供一个会被用来（部分或完全）进行自动化决策的工具。 这些技术系统可能会通过其进行的决定而影响到每个人的生活。<br>从 预测 到 决策 的飞跃不仅提出了新的技术问题， 而且还提出了一系列必须仔细考虑的伦理问题。</p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>Kaggle房价预测比赛的实践，放到下一篇笔记中吧。<br>还遇到了kramed渲染不了公式中大括号的问题，先将就，等我学了JS回来再修复。<br>还发现了链接图片的原理，前面加斜杠是表示在域名下一级。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;p&gt;这一章开始学习真正的深度网络。&lt;br&gt;最简单的深度网络称为多层感知机。多层感知机由多层神经元组成， 每一层与它的上一层相连，从中接收输入； 同时每一层也与它的下一层相连，影响当前层的神经元。&lt;br&gt;这一章从基本的概念介绍开始讲起，包括过拟合、欠拟合和模型选择。 为了解决这些问题，本章将介绍权重衰减和暂退法等正则化技术，以及将讨论数值稳定性和参数初始化相关的问题。最后应用一个真实的案例：房价预测。&lt;/p&gt;
&lt;p&gt;对应实践：&lt;a href=&quot;https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x04.ipynb&quot;&gt;https://github.com/silenceZheng66/deep_learning/blob/master/d2l/0x04.ipynb&lt;/a&gt;&lt;br&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://silencezheng.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>回调函数</title>
    <link href="http://silencezheng.top/2022/06/07/article43/"/>
    <id>http://silencezheng.top/2022/06/07/article43/</id>
    <published>2022-06-07T08:20:56.000Z</published>
    <updated>2022-08-31T05:57:09.700Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>回调函数（callback）是一个常用的概念，最近在写代码的过程中遇到了相关问题，写个文梳理一下。以下都是个人的浅显理解，如有错误请指正。<br><span id="more"></span></p><h2 id="什么是回调函数？"><a href="#什么是回调函数？" class="headerlink" title="什么是回调函数？"></a>什么是回调函数？</h2><p>回调函数，本身实际上就是普通的功能函数，当功能函数以参数的方式被传入到其他函数中时，它便成为了回调函数。</p><p>下面我们将回调过程中的函数分为两部分，调用者称为调用函数，被调用者称为回调函数。</p><h2 id="为什么要使用回调函数？"><a href="#为什么要使用回调函数？" class="headerlink" title="为什么要使用回调函数？"></a>为什么要使用回调函数？</h2><p>回调函数首先的作用就是解耦，调用函数不用关心回调函数的具体实现，只是按照其格式拿来即用就可以了。<br>回调函数的一个直观作用就是可以将事件与函数绑定，当事件发生时触发回调函数。（也是目前我遇到的场景）<br>关于其他的作用，我认为<a href="https://blog.csdn.net/qiuhuanghe/article/details/109245579">这篇文章</a>讲的还不错，可以看一下学习学习。<br>关于同步回调和异步回调，<a href="https://cloud.tencent.com/developer/article/1373683">这篇文章</a>可以看看。</p><h2 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h2><p>应用场景是这样的：在编写一个游戏时，需要通过鼠标点击组件来使用人物背包中的物品。</p><p>其中背包组件类的实现大致如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">InventoryView</span>(<span class="params">tk.Frame</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, master: <span class="type">Union</span>[tk.Tk, tk.Frame], **kwargs</span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(master)</span><br><span class="line">        self.callback = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_click_callback</span>(<span class="params">self, callback: <span class="type">Callable</span>[[<span class="built_in">str</span>], <span class="literal">None</span>]</span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Sets the function to be called when an item is clicked.</span></span><br><span class="line"><span class="string">        The provided callback function should take one argument: the string name of the item.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.callback = callback</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_draw_item</span>(<span class="params">self, name: <span class="built_in">str</span>, num: <span class="built_in">int</span>, colour: <span class="built_in">str</span></span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Creates and binds (if a callback exists) a single tk.Label in the InventoryView frame.</span></span><br><span class="line"><span class="string">        name is the name of the item, num is the quantity currently in the users inventory,</span></span><br><span class="line"><span class="string">        and colour is the background colour for this item label (determined by the type of item).</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        label = tk.Label(self, text=<span class="string">f&quot;<span class="subst">&#123;name&#125;</span>: <span class="subst">&#123;<span class="built_in">str</span>(num)&#125;</span>&quot;</span>, background=colour)</span><br><span class="line">        label.bind(<span class="string">&quot;&lt;Button-1&gt;&quot;</span>, self.callback)</span><br><span class="line">        label.pack(side=<span class="string">&#x27;top&#x27;</span>, fill=tk.X)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">draw_inventory</span>(<span class="params">self, inventory: Inventory</span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        ...</span><br><span class="line"></span><br></pre></td></tr></table></figure><br>类中的callback为回调函数（这里也就是”物品使用函数“），通过与label组件上的鼠标左键点击事件绑定，实现点击调用“物品使用函数”。 可以看出，这里背包并不需要关心物品是如何被使用的，他只是接收一个回调函数，并将这个函数与背包中的每一个物品（label）绑定。</p><p>但这里其实是简化过的版本，真正在使用时，由于“物品使用函数”需要通过接收一个str类型的参数来分辨被使用的物品是何类型，所以不能直接绑定（tkinter的bind方法会将事件作为参数传入到绑定的回调函数中，这个参数表与我们规定的“物品使用函数”参数表不同）。</p><p>所以对于想要使用Event的同时绑定带参函数的情况，需要中间函数来匹配两边的规则（“物品使用函数”和bind函数）。如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_draw_item</span>(<span class="params">self, name: <span class="built_in">str</span>, num: <span class="built_in">int</span>, colour: <span class="built_in">str</span></span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Creates and binds (if a callback exists) a single tk.Label in the InventoryView frame.</span></span><br><span class="line"><span class="string">    name is the name of the item, num is the quantity currently in the users inventory,</span></span><br><span class="line"><span class="string">    and colour is the background colour for this item label (determined by the type of item).</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">handlerAdaptor</span>(<span class="params">function, **kwds</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">lambda</span> event, fun = function, kwds = kwds: fun(event, **kwds)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">handler</span>(<span class="params">event, item_name</span>):</span></span><br><span class="line">        self.callback(item_name)</span><br><span class="line"></span><br><span class="line">    label = tk.Label(self, text=<span class="string">f&quot;<span class="subst">&#123;name&#125;</span>: <span class="subst">&#123;<span class="built_in">str</span>(num)&#125;</span>&quot;</span>, background=colour)</span><br><span class="line">    label.bind(<span class="string">&quot;&lt;Button-1&gt;&quot;</span>, handlerAdaptor(handler, item_name=name))</span><br><span class="line">    label.pack(side=<span class="string">&#x27;top&#x27;</span>, fill=tk.X)</span><br></pre></td></tr></table></figure><br>这里我们在函数内部又定义了两个中间函数用来实现目的，handler增加了event参数来接收bind函数交给回调函数的Event，但此时不能直接绑定，因为绑定时无处获取Event作为实参传入。</p><p>所以又定义了handlerAdaptor来实现一个不含event在参数表中的函数，使用lambda表达式来解决这一问题，虽然参数表中不包含event，但在实际调用的过程中，bind函数依然会传入event参数（我猜想bind传入的参数的名字恰好就是event，所以这个匿名函数才能成立），使得lambda表达式的参数表匹配，返回调用handler的结果。</p><p>写两个中间函数的好处是（其实也比较牵强）：handler函数可以规范一下回调函数的参数表命名，我们可以不去管self.callback中的参数实际名字是如何，直接用item_name这一参数名去传入即可。</p><p>但实际上，如果self.callback的参数表是已知的，我们只需要一个中间函数即可实现“使用Event的同时绑定带参函数”，如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_draw_item</span>(<span class="params">self, name: <span class="built_in">str</span>, num: <span class="built_in">int</span>, colour: <span class="built_in">str</span></span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">middle</span>(<span class="params">function, **kwds</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">lambda</span> event, fun = function, kwds = kwds: fun(**kwds)</span><br><span class="line"></span><br><span class="line">    label = tk.Label(self, text=<span class="string">f&quot;<span class="subst">&#123;name&#125;</span>: <span class="subst">&#123;<span class="built_in">str</span>(num)&#125;</span>&quot;</span>, background=colour)</span><br><span class="line">    <span class="comment"># 这里已知回调函数中的参数名为item_name</span></span><br><span class="line">    label.bind(<span class="string">&quot;&lt;Button-1&gt;&quot;</span>, middle(self.callback, item_name=name))</span><br><span class="line">    label.pack(side=<span class="string">&#x27;top&#x27;</span>, fill=tk.X)</span><br></pre></td></tr></table></figure><br>总的来说，使用lambda表达式就可以解决这个问题～<br>参考：<a href="https://blog.csdn.net/tinym87/article/details/6957438">https://blog.csdn.net/tinym87/article/details/6957438</a></p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>理解的还是很基础，只是知道个大概，以后用到了再总结补充吧。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;回调函数（callback）是一个常用的概念，最近在写代码的过程中遇到了相关问题，写个文梳理一下。以下都是个人的浅显理解，如有错误请指正。&lt;br&gt;</summary>
    
    
    
    
    <category term="Python" scheme="http://silencezheng.top/tags/Python/"/>
    
    <category term="编程思想" scheme="http://silencezheng.top/tags/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch适配M1芯片测试</title>
    <link href="http://silencezheng.top/2022/06/01/article42/"/>
    <id>http://silencezheng.top/2022/06/01/article42/</id>
    <published>2022-05-31T17:03:49.000Z</published>
    <updated>2022-09-08T12:30:15.207Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近用mac学习深度学习时越来越感觉硬件的重要…随便跑个小demo都温度90+，听闻最近Pytorch适配了M1芯片，正好试试能否有所提升！</p><h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><p>miniforge是要安的，<code>bash Miniforge3-MacOSX-arm64.sh</code>。</p><p>然后我们上pytorch官网看一下pytorch的情况，可以看到有稳定版和预览版两种，其中预览版是支持GPU加速的。<br>稳定版：<br><img src="/assets/post_img/article42/stable.jpg" alt="stable"><br>预览版：<br><img src="/assets/post_img/article42/preview.png" alt="preview"><br>这里提到的MPS加速是指用苹果的Metal Performance Shaders (MPS)作为Pytorch后端启用GPU加速训练。</p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>我的conda环境之前安装过稳定版：<br><code>conda install pytorch torchvision -c pytorch</code></p><p>这里的 -c 参数是指定channel，即软件下载的渠道。所以conda本身是没有这个包的，要从pytorch的channel下载。</p><p>这时在终端中进入安装稳定版的环境，启用python，输入<code>torch.__version__</code>可以查看到当前的版本是1.8.0（这里挺奇怪的，官网说MacOS不支持1.8的…）:<br><img src="/assets/post_img/article42/原版本.jpg" alt="1.8.0"></p><p>下面我们安装预览版试一下：<br><code>conda install pytorch torchvision -c pytorch-nightly</code></p><p>可以看到下载的已经是1.13版本了：<br><img src="/assets/post_img/article42/1.13版本.jpg" alt="下载提示"></p><p><img src="/assets/post_img/article42/环境中加载成功.jpg" alt="完成"></p><p>安完之后有个无语的问题，<code>conda help</code>不能用了，可能是安装了4.13版本的问题..用-h就好了。</p><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>有一个现成的<a href="https://github.com/rasbt/machine-learning-notes/tree/main/benchmark/pytorch-m1-gpu">测试</a></p><p>但是发现mps和cpu没有区别呀，如图：<br><img src="/assets/post_img/article42/test1.jpg" alt="benchmark1"></p><p>倒是发现了一个BUG，标准化地方要改成如下，否则报错：<br><code>transforms.Normalize((0.5,),(0.5,)),</code></p><p>然后又用我自己的代码小测一下，发现好像…还是没区别：<br><img src="/assets/post_img/article42/test2.jpg" alt="benchmark2"></p><p><del>结论：Mac目前的GPU加速就是个笑话😅。</del></p><h2 id="22-09-08更新"><a href="#22-09-08更新" class="headerlink" title="22.09.08更新"></a>22.09.08更新</h2><p>不知为何之前的测试代码中mps和cpu表现相似，但确实有一些训练使用mps是更加迅速的，如下图：</p><p><img src="/assets/post_img/article42/test3.png" alt="比较"></p><p>明显可以看到速度是快很多的，<a href="https://silencezheng.top/2022/07/24/article51">详细信息在链接文章的末尾。</a></p><p>但使用gpu（mps）训练有些时候也很麻烦，可能是因为显存不足的原因，有些时候会训练不出结果，收敛性很差。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;最近用mac学习深度学习时越来越感觉硬件的重要…随便跑个小demo都温度90+，听闻最近Pytorch适配了M1芯片，正好试试能否有所提升！</summary>
      
    
    
    
    
    <category term="MacOS" scheme="http://silencezheng.top/tags/MacOS/"/>
    
  </entry>
  
</feed>
