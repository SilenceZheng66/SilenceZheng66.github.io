<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>SilenceZheng66-BLOG</title>
  
  <subtitle>log</subtitle>
  <link href="http://silencezheng.top/atom.xml" rel="self"/>
  
  <link href="http://silencezheng.top/"/>
  <updated>2023-01-03T14:07:08.226Z</updated>
  <id>http://silencezheng.top/</id>
  
  <author>
    <name>SilenceZheng66</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>滑动窗口</title>
    <link href="http://silencezheng.top/2023/01/03/article88/"/>
    <id>http://silencezheng.top/2023/01/03/article88/</id>
    <published>2023-01-03T14:05:40.000Z</published>
    <updated>2023-01-03T14:07:08.226Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>滑动窗口算法的基本思想和一些实践。</p><span id="more"></span><h2 id="滑动窗口算法（Sliding-Window-Algorithm）"><a href="#滑动窗口算法（Sliding-Window-Algorithm）" class="headerlink" title="滑动窗口算法（Sliding Window Algorithm）"></a>滑动窗口算法（Sliding Window Algorithm）</h2><blockquote><p>Sliding window algorithm is used to perform required operation on specific window size of given large buffer or array.<br>滑动窗口算法是在给定特定窗口大小的数组或字符串上执行要求的操作。</p><p>This technique shows how a nested for loop in few problems can be converted to single for loop and hence reducing the time complexity.<br>该技术可以将一部分问题中的嵌套循环转变为一个单循环，因此它可以减少时间复杂度。</p></blockquote><p><strong>滑动</strong>：表示窗口是移动的。</p><p><strong>窗口</strong>：窗口的大小并不一定是固定的，可以不断扩容直到满足一定的条件；也可以不断缩小，直到找到一个满足条件的最小窗口；当然也可以是固定大小。</p><p><img src="/assets/post_img/article88/slide%20window.gif" alt="sw"></p><p>滑动窗口算法主要应用在<strong>数组类结构</strong>上（包括字符串），其思想可以用来解决一些 <em>查找满足一定条件的连续区间的性质（如长度等）的问题</em>。<strong>由于区间连续，因此当区间发生变化时，可以通过已有的计算结果对搜索空间进行剪枝</strong>，这样便减少了重复计算，降低了时间复杂度。</p><p><strong>总之，滑动窗口法的核心思想就是利用已有计算结果减少重复计算，关键就是如何利用，所谓的滑动窗口只是其外在表现。</strong></p><p>学习过计算机网络的读者应该对滑动窗口的模式并不陌生，而其他初次听说该思想的同学可能会一头雾水，下面我们通过一个简单的例子解释它。</p><h2 id="例一：长度最小的子数组"><a href="#例一：长度最小的子数组" class="headerlink" title="例一：长度最小的子数组"></a>例一：<a href="https://leetcode.cn/problems/minimum-size-subarray-sum/">长度最小的子数组</a></h2><blockquote><p>给定一个含有 n 个正整数的数组和一个正整数 target 。</p><p>找出该数组中满足其和 ≥ target 的长度最小的 连续子数组 [numsl, numsl+1, …, numsr-1, numsr] ，并返回其长度。如果不存在符合条件的子数组，返回 0 。</p></blockquote><h3 id="暴力法"><a href="#暴力法" class="headerlink" title="暴力法"></a>暴力法</h3><p>暴力法：双循环，每次依据一个开始位置寻找长度最小的符合条件子数组。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: SilenceZheng66</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">minSubArrayLen</span><span class="params">(<span class="keyword">int</span> target, <span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> subLen = nums.length+<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;nums.length;i++)&#123;</span><br><span class="line">            <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">int</span> tmpSubLen = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">int</span> j = i;</span><br><span class="line">            <span class="keyword">while</span>(j&lt;nums.length&amp;&amp;sum&lt;target)&#123;</span><br><span class="line">                sum += nums[j++];</span><br><span class="line">                tmpSubLen++;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(sum&gt;=target&amp;&amp;tmpSubLen&lt;subLen) subLen = tmpSubLen;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(subLen&lt;=nums.length) <span class="keyword">return</span> subLen;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">return</span> <span class="number">0</span>; </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最终超时，原因在于重复计算过多。时间复杂度$O(n^2)$。</p><h3 id="滑动窗口"><a href="#滑动窗口" class="headerlink" title="滑动窗口"></a>滑动窗口</h3><p>滑动窗口：通过左右指针划定窗口位置，指针由左向右滑动模拟窗口滑动，初始时左右指针重合指向索引0，而后右指针移动至第一个符合条件的位置（窗口内子数组符合条件），记录长度，此后左指针逐位移动，每次移动若窗口符合条件则再记录，直至窗口不符合条件，执行此左右移动过程直至右指针推出数组。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: SilenceZheng66</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">minSubArrayLen</span><span class="params">(<span class="keyword">int</span> target, <span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 滑动窗口</span></span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>, left = <span class="number">0</span>, right = <span class="number">0</span>, minSubLen = nums.length + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(right&lt;nums.length)&#123;</span><br><span class="line">            <span class="comment">// 寻找符合条件的最小窗口</span></span><br><span class="line">            <span class="keyword">while</span>(sum&lt;target&amp;&amp;right&lt;nums.length) sum+=nums[right++];</span><br><span class="line">            <span class="comment">// 缩小窗口直至不符合条件</span></span><br><span class="line">            <span class="keyword">while</span>(sum&gt;=target)&#123;</span><br><span class="line">                <span class="comment">// 记录最小窗口大小</span></span><br><span class="line">                <span class="keyword">if</span>((right-left)&lt;minSubLen&amp;&amp;sum&gt;=target) minSubLen = right-left;</span><br><span class="line">                sum-=nums[left++];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(minSubLen&lt;=nums.length) <span class="keyword">return</span> minSubLen;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">return</span> <span class="number">0</span>; </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>该算法的最终时间复杂度为$O(n)$，关于这个时间复杂度，可以想像数组中的每个元素至多被操作两次（右指针划过、左指针划过），因此操作数为 $2n$，即$O(n)$。被减少的运算就是各符合条件的窗口的交集。</p><p>该例中，窗口的大小是不固定的，事实上窗口的固定与否并不重要（固定窗口用单指针+长度即可），<strong>关键在于如何构造窗口并利用已有计算结果</strong>。现在我们已经了解了如何使用滑动窗口算法，下面进行更多实践来加深理解。</p><h2 id="例二：尽可能使字符串相等"><a href="#例二：尽可能使字符串相等" class="headerlink" title="例二：尽可能使字符串相等"></a>例二：<a href="https://leetcode.cn/problems/get-equal-substrings-within-budget/">尽可能使字符串相等</a></h2><blockquote><p>给你两个长度相同的字符串，s 和 t。</p><p>将 s 中的第 i 个字符变到 t 中的第 i 个字符需要 |s[i] - t[i]| 的开销（开销可能为 0），也就是两个字符的 ASCII 码值的差的绝对值。</p><p>用于变更字符串的最大预算是 maxCost。在转化字符串时，总开销应当小于等于该预算，这也意味着字符串的转化可能是不完全的。</p><p>如果你可以将 s 的子字符串转化为它在 t 中对应的子字符串，则返回可以转化的最大长度。</p><p>如果 s 中没有子字符串可以转化成 t 中对应的子字符串，则返回 0。</p></blockquote><p>采用滑动窗口解决，主要思路还是通过左右指针划定窗口位置，但要注意例二和例一的显著区别：<strong>例二求的是符合条件的最大长度，而例一求的是最小长度</strong>。在求解时，我们的滑动窗口通常都是由小至大进行扩张，求符合条件的最小子数组时，我们可以认为最初符合条件的窗口为符合条件的最小长度；而在求最大子数组时，只有当窗口内子数组初次不符合条件时，我们才能知道前一窗口为当前符合条件的最大窗口。</p><p>下面给出两种解法：</p><h3 id="滑动窗口一"><a href="#滑动窗口一" class="headerlink" title="滑动窗口一"></a>滑动窗口一</h3><p>我写的滑动窗口法的思路是每次右指针都移动到初次不符合条件的位置，然后以右指针位置减去一表示真实符合条件的右指针位置。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: SilenceZheng66</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">equalSubstring</span><span class="params">(String s, String t, <span class="keyword">int</span> maxCost)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> left = <span class="number">0</span>, right = <span class="number">0</span>, maxLen = <span class="number">0</span>, sum = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(right&lt;s.length())&#123;</span><br><span class="line">            <span class="comment">// 移动右至符合条件</span></span><br><span class="line">            <span class="keyword">while</span>(right&lt;s.length()&amp;&amp;sum&lt;=maxCost) sum+=Math.abs(s.charAt(right)-t.charAt(right++));</span><br><span class="line">            <span class="comment">// 判断，由于求最大长所以不用每次移动左都判断</span></span><br><span class="line">            <span class="comment">// 在右指针推出的情况下窗口内数组可能不满足条件，故判断并修正</span></span><br><span class="line">            <span class="keyword">if</span>(right&gt;=s.length()&amp;&amp;sum&lt;=maxCost) maxLen = Math.max(maxLen, right-left);</span><br><span class="line">            <span class="keyword">else</span> maxLen = Math.max(maxLen, right-left-<span class="number">1</span>);</span><br><span class="line">            <span class="comment">// 移动左至不符合条件</span></span><br><span class="line">            <span class="comment">// 注意sum==maxCost时不能移动，否则可能会错过最优解</span></span><br><span class="line">            <span class="keyword">while</span>(sum&gt;maxCost) sum-=Math.abs(s.charAt(left)-t.charAt(left++));</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> maxLen;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这种写法看似思路简单，每一轮循环都想一次将右指针移到合适的位置，其实需要考虑的边界条件较多，如右指针推出但窗口不符合条件。 另外，对于本题而言，有一共性问题需要注意，即左指针在<code>sum==maxCost</code>时不能继续移动，因为有可能出现窗口右侧<code>s[i]==t[i]</code>的情况。</p><h3 id="滑动窗口二（思想来自-1-）"><a href="#滑动窗口二（思想来自-1-）" class="headerlink" title="滑动窗口二（思想来自[1]）"></a>滑动窗口二（思想来自[1]）</h3><p>该写法的思想是每次右指针右移都附加判断和指针左移，这样一来就可以避免“右指针移动到初次不符合条件的位置”带来的问题。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: SilenceZheng66</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">equalSubstring</span><span class="params">(String s, String t, <span class="keyword">int</span> maxCost)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> left = <span class="number">0</span>, right = <span class="number">0</span>, maxLen = <span class="number">0</span>, sum = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(right&lt;s.length())&#123;</span><br><span class="line">            <span class="comment">// 移动右</span></span><br><span class="line">            sum+=Math.abs(s.charAt(right)-t.charAt(right++));</span><br><span class="line">            <span class="comment">// 移动左</span></span><br><span class="line">            <span class="keyword">while</span>(sum&gt;maxCost) sum-=Math.abs(s.charAt(left)-t.charAt(left++));</span><br><span class="line">            <span class="comment">// 判断</span></span><br><span class="line">            maxLen = Math.max(maxLen, right-left);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> maxLen;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看出，两种方法的窗口构造也不尽相同，笔者的方法是“真窗口存在于假窗口之中”。</p><h2 id="例三：滑动窗口最大值"><a href="#例三：滑动窗口最大值" class="headerlink" title="例三：滑动窗口最大值"></a>例三：<a href="https://leetcode.cn/problems/sliding-window-maximum/">滑动窗口最大值</a></h2><blockquote><p>给你一个整数数组 nums，有一个大小为 k 的滑动窗口从数组的最左侧移动到数组的最右侧。你只可以看到在滑动窗口内的 k 个数字。滑动窗口每次只向右移动一位</p><p>返回 滑动窗口中的最大值 。</p></blockquote><p>这是一道固定窗口大小的题目，给出两种解法，分别是稍加记忆的暴力法和滑动窗口法，后者的速度是前者的40倍。本题其实更能体现出滑动窗口与暴力法的区别，即算法对于单一元素指针划过的次数。</p><h3 id="稍加记忆的暴力法"><a href="#稍加记忆的暴力法" class="headerlink" title="稍加记忆的暴力法"></a>稍加记忆的暴力法</h3><p>选择使用左指针加偏移构造窗口，则暴力的求窗口内元素最值的方式为遍历窗口。窗口在移动的过程中，若最值仍位于窗口内，则只需比较新进入的元素和原始最值；若原始最值被排出，则需要重新选举最值。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: SilenceZheng66</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span>[] maxSlidingWindow(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> k) &#123;</span><br><span class="line">        <span class="keyword">int</span>[] ans = <span class="keyword">new</span> <span class="keyword">int</span>[nums.length-(k-<span class="number">1</span>)];</span><br><span class="line">        <span class="comment">// offset = k-1</span></span><br><span class="line">        <span class="keyword">int</span> left = <span class="number">0</span>, max = Integer.MIN_VALUE, maxIndex = -<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(left&lt;ans.length)&#123;</span><br><span class="line">            <span class="keyword">int</span> right = left + (k-<span class="number">1</span>);</span><br><span class="line">            <span class="keyword">if</span>(maxIndex&gt;=left&amp;&amp;maxIndex&lt;=right)&#123;</span><br><span class="line">                <span class="keyword">if</span>(nums[right]&gt;=max)&#123;</span><br><span class="line">                    max = nums[right];</span><br><span class="line">                    maxIndex = right;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">int</span> tmpMax = Integer.MIN_VALUE; </span><br><span class="line">                <span class="keyword">int</span> tmpMaxIndex = maxIndex;</span><br><span class="line">                <span class="keyword">for</span>(<span class="keyword">int</span> i = left; i &lt;= right; i++)&#123;</span><br><span class="line">                    <span class="comment">// 取等于有利于减少最大值索引被排出的情况</span></span><br><span class="line">                    <span class="keyword">if</span>(nums[i]&gt;=tmpMax)&#123;</span><br><span class="line">                        tmpMax = nums[i];</span><br><span class="line">                        tmpMaxIndex = i;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                max = tmpMax;</span><br><span class="line">                maxIndex = tmpMaxIndex;</span><br><span class="line">            &#125;</span><br><span class="line">            ans[left++] = max;</span><br><span class="line">        &#125; </span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>该方法在最优情况（原始最值永远不被排出，just fantasy）下可以取得不错的时间效率，但最坏情况下等同于暴力法，故属于效率较差的算法。</p><h3 id="滑动窗口（思想来自-1-）"><a href="#滑动窗口（思想来自-1-）" class="headerlink" title="滑动窗口（思想来自[1]）"></a>滑动窗口（思想来自[1]）</h3><p>上一个方法没有利用好窗口内的计算结果，下面介绍一个更好的思路，该方法采用右指针构造窗口，核心思路在于维持一个队首最大的双端队列（这里用LinkedList代替）以保存最大值，可以设想数组逐步进入该队列，当数组中第一个窗口进入队列后，就开始对队首元素进行检验，若已经滑出窗口则出队。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: SilenceZheng66</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span>[] maxSlidingWindow(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> k) &#123;</span><br><span class="line">        <span class="keyword">int</span> right = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span>[] res = <span class="keyword">new</span> <span class="keyword">int</span>[nums.length - (k-<span class="number">1</span>)];</span><br><span class="line">        <span class="keyword">int</span> index = <span class="number">0</span>;</span><br><span class="line">        LinkedList&lt;Integer&gt; list = <span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (right &lt; nums.length) &#123;</span><br><span class="line">            <span class="comment">// 维持list的首位为窗口中最大元素</span></span><br><span class="line">            <span class="keyword">while</span> (!list.isEmpty() &amp;&amp; nums[right] &gt; list.peekLast()) &#123;</span><br><span class="line">                list.removeLast();</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 不断添加</span></span><br><span class="line">            list.addLast(nums[right++]);</span><br><span class="line">            <span class="comment">// 构造窗口完成，这时候需要根据条件做一些操作</span></span><br><span class="line">            <span class="keyword">if</span> (right &gt;= k)&#123;</span><br><span class="line">                res[index++]=list.peekFirst();</span><br><span class="line">                <span class="comment">// 若list首位已位于窗口外</span></span><br><span class="line">                <span class="keyword">if</span>(list.peekFirst() == nums[right-k]) &#123;</span><br><span class="line">                    list.removeFirst();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由此可见，滑动窗口算法的核心思想在于如何表示窗口以及利用已有信息，有时窗口可能是抽象的。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]<a href="https://www.cnblogs.com/huansky/p/13488234.html">https://www.cnblogs.com/huansky/p/13488234.html</a><br>[2]<a href="https://programmercarl.com/0209.%E9%95%BF%E5%BA%A6%E6%9C%80%E5%B0%8F%E7%9A%84%E5%AD%90%E6%95%B0%E7%BB%84.html">https://programmercarl.com/0209.%E9%95%BF%E5%BA%A6%E6%9C%80%E5%B0%8F%E7%9A%84%E5%AD%90%E6%95%B0%E7%BB%84.html</a></p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于 <a href="https://silencezheng.top">silencezheng.top</a>，转载请注明出处。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;滑动窗口算法的基本思想和一些实践。&lt;/p&gt;</summary>
    
    
    
    
    <category term="数据结构与算法" scheme="http://silencezheng.top/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>实用主义的Git入门教程</title>
    <link href="http://silencezheng.top/2022/12/22/article87/"/>
    <id>http://silencezheng.top/2022/12/22/article87/</id>
    <published>2022-12-22T14:47:27.000Z</published>
    <updated>2022-12-23T02:23:20.797Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>实用主义的Git入门教程，快速了解常用的Git命令。</p><span id="more"></span><h2 id="1-git-clone"><a href="#1-git-clone" class="headerlink" title="1. git clone"></a>1. git clone</h2><p>从Git服务器拉取代码：<code>git clone https://github.com/SilenceZheng66/SilenceZheng66.github.io.git</code></p><p>代码会下载到当前工作目录中，名称为<code>SilenceZheng66.github.io</code>。</p><h2 id="2-git-init"><a href="#2-git-init" class="headerlink" title="2. git init"></a>2. git init</h2><p>当然，如果想要在本地新建一个git仓库，只需要在对应工作目录中输入<code>git init</code>命令。</p><p>首次创建仓库需要commit后才能创建主分支master。</p><h2 id="3-git-config"><a href="#3-git-config" class="headerlink" title="3. git config"></a>3. git config</h2><p>该命令用于获取并设置存储库或全局选项。这些变量可以控制Git的外观和操作的各个方面。</p><p>全局配置开发者用户名：<code>git config --global user.name SilenceZheng66</code></p><p>对某一项目配置开发者邮箱（需进入项目目录）：<code>git config user.email silencezheng66@126.com</code></p><p>查看当前用户全局配置：<code>git config --global --list</code></p><p>查看当前仓库配置：<code>git config --local --list</code></p><h2 id="4-git-branch"><a href="#4-git-branch" class="headerlink" title="4. git branch"></a>4. git branch</h2><p>创建、重命名、查看、删除项目分支，通过 Git 做项目开发时，一般都是在开发分支中进行，开发完成后合并分支到主干。</p><p><code>git branch daily/0.0.0</code>，创建一个名为<code>daily/0.0.0</code>的日常开发分支，分支名只要不包括特殊字符即可。</p><p><code>git branch -m daily/0.0.0 daily/0.0.1</code>，如果觉得之前的分支名不合适，可以为新建的分支重命名，如重命名为<code>daily/0.0.1</code>。</p><p>查看当前项目分支列表：<code>git branch</code>，按q退出。</p><p>删除分支：<code>git branch -d daily/0.0.1</code></p><h2 id="5-git-checkout"><a href="#5-git-checkout" class="headerlink" title="5. git checkout"></a>5. git checkout</h2><p>该命令用于切换分支。</p><p>如：<code>git checkout daily/0.0.1</code></p><p>切换到历史版本：<code>git checkout &lt;commit SHA&gt;</code></p><h2 id="6-git-status"><a href="#6-git-status" class="headerlink" title="6. git status"></a>6. git status</h2><p>该命令用于查看文件变动状态。</p><p>比如可以在项目中新建一个文件<code>hh.txt</code>后，键入该命令查看状态，得到：<code>Untracked files:...</code>。</p><h2 id="7-git-add"><a href="#7-git-add" class="headerlink" title="7. git add"></a>7. git add</h2><p>该命令用于添加文件变动到暂存区。</p><p><code>git add hh.txt</code>，查看状态得到：<code>Changes to be committed:...</code></p><p>将所有修改添加到暂存区：<code>git add .</code>。</p><h2 id="8-git-commit"><a href="#8-git-commit" class="headerlink" title="8. git commit"></a>8. git commit</h2><p>该命令用于提交文件变动到版本库。</p><p><code>git commit -m &#39;提交原因&#39;</code>，通过<code>-m</code>参数可直接在命令行里输入提交描述文本。</p><h2 id="9-git-push"><a href="#9-git-push" class="headerlink" title="9. git push"></a>9. git push</h2><p>该命令用于将本地的代码改动推送到服务器。</p><p><code>git push origin daily/0.0.1</code>，<code>origin</code>指代的是当前的git服务器地址。</p><h2 id="10-git-pull"><a href="#10-git-pull" class="headerlink" title="10. git pull"></a>10. git pull</h2><p>该命令用于将服务器上的最新代码拉取到本地。</p><p><code>git pull origin daily/0.0.1</code>，将服务器上<code>daily/0.0.1</code>分支的代码拉取到本地。</p><p>如果线上代码做了变动（其他项目成员进行了编辑），而你本地的代码也有变动（你做了改动），拉取的代码就有可能会跟你本地的改动<em>冲突</em>，一般情况下 Git 会自动处理这种冲突合并，但如果改动的是同一行，那就需要手动来合并代码，编辑文件，保存最新的改动，再通过<code>git add .</code>和<code>git commit -m &#39;xxx&#39;</code>来提交合并。</p><h2 id="11-git-log"><a href="#11-git-log" class="headerlink" title="11. git log"></a>11. git log</h2><p>该命令用于查看版本提交记录，我们可以查看整个项目的版本提交记录，它里面包含了散列码、提交人、日期、提交原因等信息。</p><p>提交记录可能会非常多，按 J 键往下翻，按 K 键往上翻，按 Q 键退出查看</p><p>查看分支树：<code>git log --oneline --graph --decorate --all</code></p><h2 id="12-git-tag"><a href="#12-git-tag" class="headerlink" title="12. git tag"></a>12. git tag</h2><p>该命令用于为项目标记里程碑。</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">git</span> tag publish/<span class="number">0</span>.<span class="number">0</span>.<span class="number">1</span></span><br><span class="line"><span class="attribute">git</span> push origin publish/<span class="number">0</span>.<span class="number">0</span>.<span class="number">1</span></span><br></pre></td></tr></table></figure><p>当我们完成某个功能需求准备发布上线时，应该将此次完整的项目代码做个标记，并将这个标记好的版本发布到线上，这里表示以<code>publish/0.0.1</code>为标记名并发布。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] gafish的Git教程，找不到原文链接了，可以在 GitHub 搜到作者。</p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于 <a href="https://silencezheng.top">silencezheng.top</a>，转载请注明出处。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;实用主义的Git入门教程，快速了解常用的Git命令。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Git" scheme="http://silencezheng.top/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>用Tkinter构建简易图形用户界面</title>
    <link href="http://silencezheng.top/2022/12/21/article86/"/>
    <id>http://silencezheng.top/2022/12/21/article86/</id>
    <published>2022-12-21T12:11:45.000Z</published>
    <updated>2022-12-21T12:12:52.362Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>介绍如何使用Tkinter构建一个简易的图形用户界面。</p><p>在使用Python方便我们的日常工作时，我们可能希望将脚本、程序打包起来，方便我们在多端使用或供他人使用。对于那些不了解如何使用命令行程序的用户而言，设计一个可用的图形用户界面就十分重要。</p><span id="more"></span><h2 id="Tkinter"><a href="#Tkinter" class="headerlink" title="Tkinter"></a>Tkinter</h2><p>如果你还不知道什么是Tkinter：</p><blockquote><p>Tkinter（即 tk interface） 是 Python 标准 GUI 库，简称 “Tk”；从本质上来说，它是对 TCL/TK 工具包的一种 Python 接口封装。Tkinter 是 Python 自带的标准库，因此无须另行安装，它支持跨平台运行，不仅可以在 Windows 平台上运行，还支持在 Linux 和 Mac 平台上运行。</p></blockquote><p>你可能会注意到 tkinter 下的 ttk 组件库，它们似乎拥有着与tkinter相似的组件：</p><blockquote><p>tkinter中的窗口小部件高度易于配置。您几乎可以完全控制它们的外观：边框宽度，字体，图像，颜色等。<br>ttk小部件使用样式定义外观，因此，如果要使用非标准按钮，则需要花费更多的工作。 ttk小部件的文档也很少。理解底层的主题和布局引擎(小部件本身中的布局，而不是pack，grid和place)是一个挑战。</p></blockquote><p>总的来说，如果想使GUI程序更加美观，更现代，ttk组件库能够帮到你，但本篇作为 silencezheng.top 的第一篇tkinter相关文章，将不会谈论这一组件库。</p><p>写本篇文章的目的在于讲述如何快速的为Python程序制作一个“单页GUI”，不会涉及过多的技术细节，仅要求读者对于GUI程序的基本组件和设计逻辑有一个基本的了解。</p><h2 id="设计界面布局"><a href="#设计界面布局" class="headerlink" title="设计界面布局"></a>设计界面布局</h2><p>有许多可视化布局工具可供使用，这里我推荐一个在线布局工具：<a href="https://www.pytk.net/tkinter-helper/">https://www.pytk.net/tkinter-helper/</a></p><p>当然，如果不想使用这个工具（或者以后它需要付费了），只要有一个合理的代码框架也可以很容易的对界面布局进行设计，下面我提供一个左右布局（包含两个Frame）的代码框架。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># author: SilenceZheng66</span></span><br><span class="line"><span class="keyword">from</span> tkinter <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WinGUI</span>(<span class="params">Tk</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.__win()</span><br><span class="line">        self.tk_frame_left = Frame_left(self)</span><br><span class="line">        self.tk_frame_right = Frame_right(self)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__win</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.title(<span class="string">&quot;Tkinter Demo&quot;</span>)</span><br><span class="line">        <span class="comment"># 设置窗口大小、居中</span></span><br><span class="line">        width = <span class="number">810</span></span><br><span class="line">        height = <span class="number">600</span></span><br><span class="line">        screenwidth = self.winfo_screenwidth()</span><br><span class="line">        screenheight = self.winfo_screenheight()</span><br><span class="line">        geometry = <span class="string">&#x27;%dx%d+%d+%d&#x27;</span> % (width, height, (screenwidth - width) / <span class="number">2</span>, (screenheight - height) / <span class="number">2</span>)</span><br><span class="line">        self.geometry(geometry)</span><br><span class="line">        self.resizable(width=<span class="literal">False</span>, height=<span class="literal">False</span>)</span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Frame_left</span>(<span class="params">Frame</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,parent</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(parent)</span><br><span class="line">        self.__frame()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__frame</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.place(x=<span class="number">0</span>, y=<span class="number">0</span>, width=<span class="number">400</span>, height=<span class="number">599</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Frame_right</span>(<span class="params">Frame</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,parent</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(parent)</span><br><span class="line">        self.__frame()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__frame</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.place(x=<span class="number">410</span>, y=<span class="number">0</span>, width=<span class="number">400</span>, height=<span class="number">599</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Win</span>(<span class="params">WinGUI</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.__event_bind()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__event_bind</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">        </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    win = Win()</span><br><span class="line">    win.mainloop()</span><br></pre></td></tr></table></figure><p>效果如下：</p><p><img src="/assets/post_img/article86/step1.png" alt="s1"></p><h2 id="设计一个简易的文件编辑器"><a href="#设计一个简易的文件编辑器" class="headerlink" title="设计一个简易的文件编辑器"></a>设计一个简易的文件编辑器</h2><p>做事情需要有目标，下面我们的目标就是以方才提供的布局为蓝本，设计一个<em>文件编辑器</em>。</p><p>左侧Frame用来提供输入和功能按钮，右侧Frame用来显示内容。</p><h3 id="输入和功能区域"><a href="#输入和功能区域" class="headerlink" title="输入和功能区域"></a>输入和功能区域</h3><p>我们希望可以提供两种打开文本文件的方式，输入路径或通过文件选择器进行选择。 这使用到四个组件：Label、Entry、Button和Filedailog，但我们先关注前三个。</p><p>另外，我们希望提供 <em>编辑</em> 和 <em>保存</em> 两个功能按钮，这方便我们对文本文件进行编辑操作。 </p><p>我习惯直接上代码，在代码中注释思路，这里我们仅关注左侧Frame部分。当然这一部分没什么可说的，相信读者看一眼就能明白。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># author: SilenceZheng66</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Frame_left</span>(<span class="params">Frame</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, parent</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(parent)</span><br><span class="line">        self.__frame()</span><br><span class="line">        self.tk_input_open_file = self.__tk_input_open_file()</span><br><span class="line">        self.tk_label_open_file = self.__tk_label_open_file()</span><br><span class="line">        self.tk_button_open_file = self.__tk_button_open_file()</span><br><span class="line">        self.tk_button_edit_file = self.__tk_button_edit_file()</span><br><span class="line">        self.tk_button_save_file = self.__tk_button_save_file()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__frame</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.place(x=<span class="number">0</span>, y=<span class="number">0</span>, width=<span class="number">400</span>, height=<span class="number">599</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__tk_input_open_file</span>(<span class="params">self</span>):</span></span><br><span class="line">        ipt = Entry(self)</span><br><span class="line">        ipt.place(x=<span class="number">100</span>, y=<span class="number">60</span>, width=<span class="number">140</span>, height=<span class="number">24</span>)</span><br><span class="line">        <span class="keyword">return</span> ipt</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__tk_label_open_file</span>(<span class="params">self</span>):</span></span><br><span class="line">        label = Label(self, text=<span class="string">&quot;打开文件&quot;</span>, anchor=<span class="string">&quot;center&quot;</span>)</span><br><span class="line">        label.place(x=<span class="number">10</span>, y=<span class="number">60</span>, width=<span class="number">69</span>, height=<span class="number">24</span>)</span><br><span class="line">        <span class="keyword">return</span> label</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__tk_button_open_file</span>(<span class="params">self</span>):</span></span><br><span class="line">        btn = Button(self, text=<span class="string">&quot;打开&quot;</span>)</span><br><span class="line">        btn.place(x=<span class="number">260</span>, y=<span class="number">60</span>, width=<span class="number">80</span>, height=<span class="number">24</span>)</span><br><span class="line">        <span class="keyword">return</span> btn</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__tk_button_edit_file</span>(<span class="params">self</span>):</span></span><br><span class="line">        btn = Button(self, text=<span class="string">&quot;编辑&quot;</span>)</span><br><span class="line">        btn.place(x=<span class="number">100</span>, y=<span class="number">100</span>, width=<span class="number">80</span>, height=<span class="number">24</span>)</span><br><span class="line">        <span class="keyword">return</span> btn</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__tk_button_save_file</span>(<span class="params">self</span>):</span></span><br><span class="line">        btn = Button(self, text=<span class="string">&quot;保存&quot;</span>)</span><br><span class="line">        btn.place(x=<span class="number">260</span>, y=<span class="number">100</span>, width=<span class="number">80</span>, height=<span class="number">24</span>)</span><br><span class="line">        <span class="keyword">return</span> btn</span><br></pre></td></tr></table></figure><p>效果如下：</p><p><img src="/assets/post_img/article86/step2.png" alt="s2"></p><h3 id="内容显示区域"><a href="#内容显示区域" class="headerlink" title="内容显示区域"></a>内容显示区域</h3><p>我们使用带滚动条的文本组件ScrolledText来实现这部分内容：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># author: SilenceZheng66</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Frame_right</span>(<span class="params">Frame</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, parent</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(parent)</span><br><span class="line">        self.__frame()</span><br><span class="line">        self.tk_text_workplace = self.__tk_text_workplace()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__frame</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.place(x=<span class="number">410</span>, y=<span class="number">0</span>, width=<span class="number">400</span>, height=<span class="number">599</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__tk_text_workplace</span>(<span class="params">self</span>):</span></span><br><span class="line">        sct = ScrolledText(self)</span><br><span class="line">        <span class="comment"># 注意这里的起始坐标是相对于Frame的，故应设置为（0，0）</span></span><br><span class="line">        sct.place(x=<span class="number">0</span>, y=<span class="number">0</span>, width=<span class="number">400</span>, height=<span class="number">599</span>)</span><br><span class="line">        <span class="keyword">return</span> sct</span><br></pre></td></tr></table></figure><p>效果如下：<br><img src="/assets/post_img/article86/step3.png" alt="s3"></p><h3 id="绑定事件"><a href="#绑定事件" class="headerlink" title="绑定事件"></a>绑定事件</h3><p>到这里，界面的主体框架就已经实现了，最后一步就是将按钮和事件进行绑定，并添加上一定的逻辑。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># author: SilenceZheng66</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Win</span>(<span class="params">WinGUI</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.__event_bind()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__event_bind</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 绑定按钮事件</span></span><br><span class="line">        self.tk_frame_left.tk_button_open_file.bind(<span class="string">&quot;&lt;Button-1&gt;&quot;</span>, self.__do_open_file)</span><br><span class="line">        self.tk_frame_left.tk_button_edit_file.bind(<span class="string">&quot;&lt;Button-1&gt;&quot;</span>, self.__do_edit_file)</span><br><span class="line">        self.tk_frame_left.tk_button_save_file.bind(<span class="string">&quot;&lt;Button-1&gt;&quot;</span>, self.__do_save_file)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__do_edit_file</span>(<span class="params">self, event</span>):</span></span><br><span class="line">        <span class="comment"># 切换文本编辑区的可用状态</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获取编辑区状态</span></span><br><span class="line">        state = self.tk_frame_right.tk_text_workplace.cget(<span class="string">&#x27;state&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> state == <span class="string">&#x27;disabled&#x27;</span>:</span><br><span class="line">            self.tk_frame_right.tk_text_workplace.config(state=<span class="string">&#x27;normal&#x27;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.tk_frame_right.tk_text_workplace.config(state=<span class="string">&#x27;disabled&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__do_save_file</span>(<span class="params">self, event</span>):</span></span><br><span class="line">        <span class="comment"># 保存文件</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__do_open_file</span>(<span class="params">self, event</span>):</span></span><br><span class="line">        <span class="comment"># 获取文件路径</span></span><br><span class="line">        <span class="keyword">if</span> self.tk_frame_left.tk_input_open_file.get() != <span class="string">&quot;&quot;</span>:</span><br><span class="line">            file_path = self.tk_frame_left.tk_input_open_file.get()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            file_path = tkinter.filedialog.askopenfilename()</span><br><span class="line">        <span class="comment"># 判断路径有效</span></span><br><span class="line">        <span class="comment"># 打开文件</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><h3 id="界面美化"><a href="#界面美化" class="headerlink" title="界面美化"></a>界面美化</h3><p>简易界面不代表完全没有美化，适当的颜色变化可以显著提高界面的可用性，方便用户操作。下面是部分代码示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># author: SilenceZheng66</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__tk_input_open_file</span>(<span class="params">self</span>):</span></span><br><span class="line">    ipt = Entry(self)</span><br><span class="line">    ipt.place(x=<span class="number">100</span>, y=<span class="number">60</span>, width=<span class="number">140</span>, height=<span class="number">24</span>)</span><br><span class="line">    <span class="comment"># 选中文本时的前景、背景色</span></span><br><span class="line">    ipt.config(selectforeground=<span class="string">&#x27;green&#x27;</span>, selectbackground=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> ipt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__tk_button_open_file</span>(<span class="params">self</span>):</span></span><br><span class="line">    btn = Button(self, text=<span class="string">&quot;打开&quot;</span>)</span><br><span class="line">    btn.place(x=<span class="number">260</span>, y=<span class="number">60</span>, width=<span class="number">80</span>, height=<span class="number">24</span>)</span><br><span class="line">    <span class="comment"># 鼠标点击按钮时，按钮的文本颜色</span></span><br><span class="line">    btn.config(activeforeground=<span class="string">&#x27;yellow&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> btn</span><br></pre></td></tr></table></figure><p>效果如下：</p><p><img src="/assets/post_img/article86/step4.gif" alt="s4"></p><p>这里没有过多调整组件背景颜色是由于MacOS下很多组件的背景调色会失效，比如Button。有一个库叫tkmacosx可以解决。</p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于 <a href="https://silencezheng.top">silencezheng.top</a>，转载请注明出处。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;介绍如何使用Tkinter构建一个简易的图形用户界面。&lt;/p&gt;
&lt;p&gt;在使用Python方便我们的日常工作时，我们可能希望将脚本、程序打包起来，方便我们在多端使用或供他人使用。对于那些不了解如何使用命令行程序的用户而言，设计一个可用的图形用户界面就十分重要。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Python" scheme="http://silencezheng.top/tags/Python/"/>
    
    <category term="Tkinter" scheme="http://silencezheng.top/tags/Tkinter/"/>
    
  </entry>
  
  <entry>
    <title>VMware Fusion13使用体验</title>
    <link href="http://silencezheng.top/2022/12/19/article85/"/>
    <id>http://silencezheng.top/2022/12/19/article85/</id>
    <published>2022-12-19T15:03:16.000Z</published>
    <updated>2022-12-19T15:09:10.865Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>2022年11月17日，VMware Fusion13.0发布，build号20802013，官方宣布Fusion 现在支持在 Apple Silicon Mac 上运行 Arm 虚拟机，刚刚得知消息的我决定来体验一下。</p><p>其实早在今年的4月份时，我便发布了一篇名为《M1芯片VMfusion安装win11教程》的博客，当时的VMF版本为12，整体用下来还是有不少痛点和BUG，希望VMF13能够解决！<br><span id="more"></span></p><h2 id="VMF12痛点"><a href="#VMF12痛点" class="headerlink" title="VMF12痛点"></a>VMF12痛点</h2><p>今年上旬，我使用的Fusion12专业版build为19431034，整体用下来主要有以下痛点：</p><ol><li>Win11安装复杂，需要绕TPM。</li><li>Win11使用定时闪退。</li><li>Linux系统无法安装，很奇怪，Ubantu无法安装含GUI版本，即便后期手动加装GUI也不行。</li></ol><h2 id="VMF13使用体验"><a href="#VMF13使用体验" class="headerlink" title="VMF13使用体验"></a>VMF13使用体验</h2><p>关于13版本的发行说明，请见：<a href="https://docs.vmware.com/cn/VMware-Fusion/13.0/rn/vmware-fusion-130-release-notes/index.html">https://docs.vmware.com/cn/VMware-Fusion/13.0/rn/vmware-fusion-130-release-notes/index.html</a></p><p>下载：<a href="https://www.vmware.com/cn/products/fusion/fusion-evaluation.html">https://www.vmware.com/cn/products/fusion/fusion-evaluation.html</a></p><h3 id="1-安装"><a href="#1-安装" class="headerlink" title="1. 安装"></a>1. 安装</h3><p>安装直接一路点即可，许可证部分可以自己申请，也可以从网上找，总之能安装上即可。</p><h3 id="2-打开先前的虚拟机"><a href="#2-打开先前的虚拟机" class="headerlink" title="2. 打开先前的虚拟机"></a>2. 打开先前的虚拟机</h3><p>在打开之前建立的Win11虚拟机时需要先升级虚拟机，升级后不再向下兼容。</p><p>正常打开后效果如下，VMware Tools安装选项亮起。</p><p><img src="/assets/post_img/article85/vmtools.png" alt="vmt"></p><h3 id="3-建立Ubuntu桌面版虚拟机"><a href="#3-建立Ubuntu桌面版虚拟机" class="headerlink" title="3. 建立Ubuntu桌面版虚拟机"></a>3. 建立Ubuntu桌面版虚拟机</h3><p>这里我选择的镜像是ARM版的focal20.04.4，截止目前（2022.12.19），官方说还不能安装Ubuntu22.04：</p><blockquote><p>使用 Fusion 在 Apple Silicon Mac 上安装 RHEL 9、Ubuntu 22.04 和 Fedora 36 客户机失败：<br>如果尝试在虚拟机上安装 Rhel 9、Ubuntu 22.04 或 Fedora 36，安装将显示黑屏，而非预期内容，并且进程不会继续。</p></blockquote><p>一路点然后直接安装即可，效果如下：</p><p><img src="/assets/post_img/article85/ubuntu.png" alt="ub"></p><p>但安装VMware Tools会报错，表示不支持该系统。</p><h3 id="4-建立Windows11桌面版虚拟机"><a href="#4-建立Windows11桌面版虚拟机" class="headerlink" title="4. 建立Windows11桌面版虚拟机"></a>4. 建立Windows11桌面版虚拟机</h3><p>为了验证究竟是否能够在Windows11虚拟机上安装VMware Tools，我决定换一个更新版本的镜像重新建立虚拟机进行测试。</p><p>下面记录安装流程：</p><ol><li>将Win11专业版镜像拖入VMF13中，连续点击默认设置。</li><li>遇到设置TPM密码，可以随机生成，说是会存放在APPLE keys中，我后来也没找到。</li><li>第一次运行时一定要快速按任意键，否则需要重启。</li><li>进入操作系统安装后一路点到联网，发现无法连接，先唤出命令行。</li><li>输入<code>OOBE\BYPASSSNRO</code>，跳过网络设置。</li><li>然后重新进入到网络设置步骤，就可以在不用网络的情况下进行受限设置了。</li><li>进入系统后，需要把网络配置回来，直接按步骤安装VMware Tools即可。</li></ol><p>全部配置好后如下：</p><p><img src="/assets/post_img/article85/win11.png" alt="win11"></p><p>配置好VMware Tools的机器可以联网、更改分辨率和缩放比例，但仍不能实现主客机间拖拽传送文件，这是一个缺憾（官方文档证实）。但是这一点可以通过其他手段解决，比如samba，配合finder使用效果也是不错的。</p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>关于VMF12的痛点，总的来说还是在很大程度上进行了解决，希望后续版本有更好的表现！</p><p>首发于 <a href="https://silencezheng.top">silencezheng.top</a>，转载请注明出处。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;2022年11月17日，VMware Fusion13.0发布，build号20802013，官方宣布Fusion 现在支持在 Apple Silicon Mac 上运行 Arm 虚拟机，刚刚得知消息的我决定来体验一下。&lt;/p&gt;
&lt;p&gt;其实早在今年的4月份时，我便发布了一篇名为《M1芯片VMfusion安装win11教程》的博客，当时的VMF版本为12，整体用下来还是有不少痛点和BUG，希望VMF13能够解决！&lt;br&gt;</summary>
    
    
    
    
    <category term="MacOS" scheme="http://silencezheng.top/tags/MacOS/"/>
    
  </entry>
  
  <entry>
    <title>数值溢出问题</title>
    <link href="http://silencezheng.top/2022/12/16/article84/"/>
    <id>http://silencezheng.top/2022/12/16/article84/</id>
    <published>2022-12-16T13:02:44.000Z</published>
    <updated>2022-12-16T13:03:36.222Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>数值溢出，不得不考虑的问题…</p><span id="more"></span><h2 id="什么是溢出"><a href="#什么是溢出" class="headerlink" title="什么是溢出"></a>什么是溢出</h2><p>以Java语言为例，整数类型的上下溢出如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: SilenceZheng66</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Overflow</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> posMax = <span class="number">0b1111111111111111111111111111111</span>;</span><br><span class="line">        <span class="keyword">int</span> negMin = -<span class="number">0b1111111111111111111111111111111</span>;</span><br><span class="line"></span><br><span class="line">        System.out.println(posMax); <span class="comment">// 2147483647</span></span><br><span class="line">        System.out.println(posMax+<span class="number">1</span>); <span class="comment">// -2147483648</span></span><br><span class="line">        System.out.println(posMax+<span class="number">2</span>); <span class="comment">// -2147483647</span></span><br><span class="line"></span><br><span class="line">        System.out.println(negMin); <span class="comment">// -2147483647</span></span><br><span class="line">        System.out.println(negMin-<span class="number">1</span>); <span class="comment">// -2147483648</span></span><br><span class="line">        System.out.println(negMin-<span class="number">2</span>); <span class="comment">// 2147483647</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>一旦出现溢出，程序的运行结果就变得不可控，逻辑上成立的算法实际上无法得到想要的结果，需要考虑数据类型的上下界限。</p><h2 id="如何防止数值溢出"><a href="#如何防止数值溢出" class="headerlink" title="如何防止数值溢出"></a>如何防止数值溢出</h2><p>以二分查找为例，查找在1到n内的某整数，若输入n为int型，则可能产生上溢出：<code>mid = (start + end)/2;</code>。</p><p>此处start的最小值为1，end的最大值为2147483647，在<code>start+end</code>值大于2147483647时就会发生上溢出。</p><p>一种聪明的解决办法是将求中位数的操作改变为：<code>mid = start + (end - start)/2;</code>，此时由于end不会超过int的上界，故不会发生溢出。</p><p>另一种解决办法是将求和操作放到long型中计算，然后转回int型，相较于上一种内存消耗增加：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: SilenceZheng66</span></span><br><span class="line"><span class="keyword">long</span> midL = ((<span class="keyword">long</span>)start + (<span class="keyword">long</span>)end)/<span class="number">2</span>;</span><br><span class="line">mid = (<span class="keyword">int</span>) midL;</span><br></pre></td></tr></table></figure><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于 <a href="https://silencezheng.top">silencezheng.top</a>，转载请注明出处。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;数值溢出，不得不考虑的问题…&lt;/p&gt;</summary>
    
    
    
    
    <category term="数据结构与算法" scheme="http://silencezheng.top/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>常见字符编码介绍</title>
    <link href="http://silencezheng.top/2022/11/29/article83/"/>
    <id>http://silencezheng.top/2022/11/29/article83/</id>
    <published>2022-11-29T06:36:09.000Z</published>
    <updated>2022-11-29T06:37:37.311Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>平时写代码、写博客总少不了与字符编码打交道，应该整理一下来做系统的了解。当然，我认为这种“目录”性质的博客不应写的过于深入，只要通过简短的文字介绍基本信息即可。<br><span id="more"></span></p><h2 id="字符编码（Character-encoding）"><a href="#字符编码（Character-encoding）" class="headerlink" title="字符编码（Character encoding）"></a>字符编码（Character encoding）</h2><p>字符编码是把<strong>字符集</strong>中的字符<strong>编码</strong>为指定集合中某一对象，以便文本在计算机中存储和通过通信网络的传递。</p><p>对于软件开发中的场景来说，字符编码就是构建一个字符到数字的一一对应的映射关系。然而，在实际的应用中我们还需要解决<strong>字符间分隔问题</strong>。</p><blockquote><p>在显示器上看见的文字、图片等信息在电脑里面其实并不是我们看见的样子，即使你知道所有信息都存储在硬盘里，把它拆开也看不见里面有任何东西，只有些盘片。假设，你用显微镜把盘片放大，会看见盘片表面凹凸不平，凸起的地方被磁化，凹的地方是没有被磁化；凸起的地方代表数字1，凹的地方代表数字0。硬盘只能用0和1来表示所有文字、图片等信息。</p></blockquote><p>如上面引文中提到的，计算机只能对二进制数字进行读写，当它遇到<code>00100001 00010001</code>，它该如何知道这是一个双字节编码字符，又或是两个单字节编码的字符呢？通常解决方案要么就是规定好每个字长度（例如所有文字都是2 bytes，不够的前面用0补齐），要么就是在用0和1表示的时候，不仅需要表示出数字编码，还要暗示给计算机接下来多少个连续byte构成一个字。</p><p>不同的字符编码（有时也称为字符集）的区别主要在于两点：<strong>可以表示的字符范围</strong> 和 <strong>编码方式</strong>。几种常见的中文编码之间兼容性如下图所示，兼容是指映射间的包含关系：</p><p><img src="/assets/post_img/article83/compatibility.webp" alt=""></p><h2 id="ASCII"><a href="#ASCII" class="headerlink" title="ASCII"></a>ASCII</h2><p><strong>ASCII 字符集</strong>和 <strong>ASCII 码</strong>（ ASCII 是 American Standard Code for Information Interchange 的缩写）可能是我们最先接触到的英文字符集及其编码，它同时也被国际标准化组织（ International Organization for Standardization, <strong>ISO</strong> ）批准为国际标准。</p><p><img src="/assets/post_img/article83/ascii-1-1.png" alt="ascii"></p><p>基本的 ASCII 字符集共有 128 个字符，其中有 96 个可打印字符（可打印和可显示仍有一个字符的区别），包括常用的字母、数字、标点符号等，另外还有 32 个控制字符。标准 ASCII 码使用 7 个二进位对字符进行编码，对应的 ISO 标准为 ISO646 标准。</p><p>由于标准 ASCII 字符集字符数目有限，在实际应用中往往无法满足要求。为此，国际标准化组织又制定了 ISO2022 标准，它规定了在保持与 ISO646 兼容的前提下将 ASCII 字符集扩充为 8 位代码的统一方法。 ISO 陆续制定了一批适用于不同地区的扩充 ASCII 字符集，每种扩充 ASCII 字符集分别可以扩充 128 个字符，这些扩充字符的编码均为高位为 1 的 8 位代码（即十进制数 128~255 ），称为扩展 ASCII 码。</p><p>字母和数字的 ASCII 码的记忆是非常简单的。我们只要记住了一个字母或数字的 ASCII 码（例如记住 A 为 65 ， 0 的 ASCII 码为 48 ），知道相应的大小写字母之间差 32 ，就可以推算出其余字母、数字的 ASCII 码。</p><p>ASCII编码几乎被世界上所有编码所兼容（UTF-16和UTF-32是个例外），因此如果一个文本文档里面的内容全都由ASCII里面的字母或符号构成，那么不管你如何展示该文档的内容，都不可能出现乱码的情况。</p><h2 id="ANSI"><a href="#ANSI" class="headerlink" title="ANSI"></a>ANSI</h2><p>准确说，并不存在哪种具体的编码方式叫做ANSI，它只是一个Windows操作系统上的别称而已。在中文简体Windows操作系统上，ANSI就是GBK；在泰语操作系统上，ANSI就是TIS-620（一种泰语编码）；在韩语操作系统上，ANSI就是EUC-KR（一种韩语编码）。</p><p>为了扩充ASCII编码，以用于显示本国的语言，不同的国家和地区制定了不同的标准，由此产生了 GB2312, BIG5, JIS 等各自的编码标准。这些使用 <strong>2 个字节</strong>来代表一个字符的各种汉字延伸编码方式，称为 <strong>ANSI 编码</strong>，又称为”MBCS（Muilti-Bytes Character Set，多字节字符集）”。 <strong>不同 ANSI 编码之间互不兼容</strong>，当信息在国际间交流时，无法将属于两种语言的文字，存储在同一段 ANSI 编码的文本中。一个很大的缺点是，同一个编码值，在不同的编码体系里代表着不同的字。这样就容易造成混乱。于是催生了Unicode。</p><p>其中每个语言下的ANSI编码，都有一套一对一的编码转换器，Unicode变成所有编码转换的中间介质。所有的编码都有一个转换器可以转换到Unicode，而Unicode也可以转换到其他所有的编码。</p><h2 id="GBK、GB2312"><a href="#GBK、GB2312" class="headerlink" title="GBK、GB2312"></a>GBK、GB2312</h2><p>GB即国标，为了满足国内在计算机中使用汉字的需要，中国国家标准总局发布了一系列的汉字字符集国家标准编码，统称为GB码，或国标码。其中最有影响的是于1980年发布的《信息交换用汉字编码字符集 基本集》，标准号为GB 2312-1980,因其使用非常普遍，也常被通称为国标码。GB2312编码通行于我国内地；新加坡等地也采用此编码。几乎所有的中文系统和国际化的软件都支持GB2312。</p><p>GBK和GB2312都是双字节编码。</p><p>GB2312是一个简体中文字符集，由6763个常用汉字和682个全角的非汉字字符组成。其中汉字根据使用的频率分为两级。一级汉字3755个，二级汉字3008个。</p><p>GB2312的出现，基本满足了汉字的计算机处理需要，但对于人名、古汉语等方面出现的罕用字，GB2312不能处理，这导致了后来GBK的出现。经过GBK编码后，可以表示的汉字达到了20902个，另有984个汉语标点符号、部首等。</p><p>当GBK仍然无法满足使用的需要时，就产生了<strong>GB18030</strong>，这时2bytes已经不能满足使用的需要（2bytes最多只有65536种组合，然而为了和ASCII兼容，最高位不能为0就已经直接淘汰了一半的组合，只剩下3万多种组合无法满足全部汉字要求），因此GB18030多出来的汉字使用4bytes编码。</p><h2 id="Unicode"><a href="#Unicode" class="headerlink" title="Unicode"></a>Unicode</h2><p>Unicode是为了解决传统的字符编码方案的局限而产生的，它为每种语言中的每个字符设定了统一并且唯一的二进制编码，以满足跨语言、跨平台进行文本转换、处理的要求。Universal Multiple-Octet Coded Character Set，简称为<strong>UCS</strong>。UCS-2，即2字节编码字符集，UCS-4则是4字节编码字符集。</p><p>但是，Unicode仅仅是一本很厚的字典，规定了符合对应的二进制代码，至于这个二进制代码如何存储则没有任何规定。也就是说，其中一个字符可能只对应7位二进制数，而另一个字符则对应27位二进制数。如果按照统一用4字节存储字符编码的话，无疑会造成极大的资源浪费（对磁盘、对网络都是）。为了解决这一问题，产生了许多Unicode的实现方式，如utf-8、utf-16等等。</p><h2 id="UTF-8（8-bit-Unicode-Transformation-Format）"><a href="#UTF-8（8-bit-Unicode-Transformation-Format）" class="headerlink" title="UTF-8（8-bit Unicode Transformation Format）"></a>UTF-8（8-bit Unicode Transformation Format）</h2><p>UTF-8解决字符间分隔的方式是数<strong>二进制中最高位连续1的个数</strong>来决定这个字是几字节编码。0开头的属于单字节，和ASCII码重合，做到了兼容。</p><p><img src="/assets/post_img/article83/utf-8.png" alt="uft-8"></p><p>从这种表示方式也可以很显然地看出来，UTF-8 和 GBK 没有任何关系，除了都兼容ASCII以外。这也是文件乱码的主要原因之一。</p><p>UTF-8中，中文占 3 个字节，其他数字、英文、符号占一个字节。但 emoji 符号占 4 个字节，一些较复杂的文字、繁体字也是 4 个字节。</p><p>我们可能会注意到MySQL中有两套UTF-8的实现，分别是<code>utf8</code>和<code>utf8mb4</code>，它们的区别如下：</p><ul><li><code>utf8</code>：只支持1至3个字节。</li><li><code>utf8mb4</code>：完整实现，最多支持4个字节表示字符。</li></ul><p>因此，如果需要存储emoji类型的数据或者一些比较复杂的文字、繁体字到MySQL的话，数据库的编码一定要指定为<code>utf8mb4</code>。</p><h2 id="关于更多Unicode编码方式的内容"><a href="#关于更多Unicode编码方式的内容" class="headerlink" title="关于更多Unicode编码方式的内容"></a>关于更多Unicode编码方式的内容</h2><p>其实我认为常用的字符编码除了utf-8，还有utf-16。汉字的Unicode范围在<code>[0x4E00, 0x9FA5]</code>，这里是码点的范围，也就是<code>U+4E00 到 U+9FA5</code>。这个范围是CJK Unified Ideographs。</p><p>更多内容可以参考[7]、[8]。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1]<a href="https://baike.baidu.com/item/%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81/8446880">https://baike.baidu.com/item/%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81/8446880</a><br>[2]<a href="https://www.cnblogs.com/zhanghengscnc/p/7664120.html">https://www.cnblogs.com/zhanghengscnc/p/7664120.html</a><br>[3]<a href="https://zhuanlan.zhihu.com/p/46216008">https://zhuanlan.zhihu.com/p/46216008</a><br>[4]<a href="https://baike.baidu.com/item/%E7%BB%9F%E4%B8%80%E7%A0%81/2985798">https://baike.baidu.com/item/%E7%BB%9F%E4%B8%80%E7%A0%81/2985798</a><br>[5]<a href="https://www.cnblogs.com/crazylqy/p/10184291.html">https://www.cnblogs.com/crazylqy/p/10184291.html</a><br>[6]<a href="https://javaguide.cn/database/character-set.html">https://javaguide.cn/database/character-set.html</a><br>[7]<a href="https://cloud.tencent.com/developer/article/1341908">https://cloud.tencent.com/developer/article/1341908</a><br>[8]<a href="https://www.cnblogs.com/benbenalin/p/6921553.html">https://www.cnblogs.com/benbenalin/p/6921553.html</a></p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于 <a href="https://silencezheng.top">silencezheng.top</a>，转载请注明出处。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;平时写代码、写博客总少不了与字符编码打交道，应该整理一下来做系统的了解。当然，我认为这种“目录”性质的博客不应写的过于深入，只要通过简短的文字介绍基本信息即可。&lt;br&gt;</summary>
    
    
    
    
    <category term="字符编码" scheme="http://silencezheng.top/tags/%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81/"/>
    
  </entry>
  
  <entry>
    <title>Linux常用命令--深度学习向</title>
    <link href="http://silencezheng.top/2022/11/28/article82/"/>
    <id>http://silencezheng.top/2022/11/28/article82/</id>
    <published>2022-11-28T11:44:22.000Z</published>
    <updated>2022-11-28T11:45:41.308Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>总结一下Linux服务器使用的常见操作，深度学习向。<br><span id="more"></span></p><h2 id="SSH密码登录"><a href="#SSH密码登录" class="headerlink" title="SSH密码登录"></a>SSH密码登录</h2><p><code>ssh account@ip</code><br><code>ssh -p port account@ip</code></p><h2 id="查看进程"><a href="#查看进程" class="headerlink" title="查看进程"></a>查看进程</h2><p><code>ps aux</code>，又称BSD方式，其中a表示显示所有用户的进程(show processes for all users)；u表示显示用户(display the process’s user/owner)；x表示显示无控制终端的进程(also show processes not attached to a terminal)。</p><p><code>ps -ef</code>，又称System V方式，e效果与a相同，f表示用ASCII字符显示树状结构，表达程序间的相互关系(ASCII art forest)。</p><p>查看用户abc运行的进程：<code>ps -u abc</code></p><p>显示System V格式下java进程：<code>ps -ef|grep java</code></p><p>BSD在grep java下获取title：<code>ps aux|head -1;ps aux|grep java</code></p><p>System V在grep java下获取title：<code>ps -ef|head -1;ps -ef|grep java</code></p><p>top工具：<code>top</code></p><h2 id="查看目前登入系统的用户信息"><a href="#查看目前登入系统的用户信息" class="headerlink" title="查看目前登入系统的用户信息"></a>查看目前登入系统的用户信息</h2><p><code>w</code></p><h2 id="查看网卡信息"><a href="#查看网卡信息" class="headerlink" title="查看网卡信息"></a>查看网卡信息</h2><p><code>ifconfig</code><br><code>ip addr show</code></p><h2 id="查看CPU"><a href="#查看CPU" class="headerlink" title="查看CPU"></a>查看CPU</h2><p>查看主频信息：<code>ls cpu</code></p><p>查看详细信息：<code>cat /proc/cpuinfo</code></p><h2 id="查看内存信息"><a href="#查看内存信息" class="headerlink" title="查看内存信息"></a>查看内存信息</h2><p>以MB为单位：<code>free -m</code></p><p>以GB为单位：<code>free -g</code></p><h2 id="查看系统版本"><a href="#查看系统版本" class="headerlink" title="查看系统版本"></a>查看系统版本</h2><p>查看系统版本：<code>cat /etc/issue</code></p><h2 id="查看硬盘空间大小"><a href="#查看硬盘空间大小" class="headerlink" title="查看硬盘空间大小"></a>查看硬盘空间大小</h2><p>查看文件系统磁盘使用情况统计：<code>df -h</code></p><p>查看指定目录所属磁盘情况：<code>df -h 目录名</code></p><h2 id="Nvidia显卡配置"><a href="#Nvidia显卡配置" class="headerlink" title="Nvidia显卡配置"></a>Nvidia显卡配置</h2><p>查看显卡驱动版本、CUDA版本和显卡信息：<code>nvidia-smi</code></p><p>每隔半秒刷新一次GPU信息：<code>watch -n 0.5 nvidia-smi</code></p><p>查看CUDA Runtime版本：<code>nvcc -V</code></p><p>TensorFlow查看显卡是否可用：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(tf.config.list_physical_devices(<span class="string">&#x27;GPU&#x27;</span>))</span><br></pre></td></tr></table></figure></p><p>Pytorch查看显卡是否可用：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.cuda.is_available())</span><br><span class="line"></span><br><span class="line">ngpu = <span class="number">1</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> (torch.cuda.is_available() <span class="keyword">and</span> ngpu &gt; <span class="number">0</span>) <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(device)</span><br><span class="line"><span class="built_in">print</span>(torch.cuda.get_device_name(<span class="number">0</span>))</span><br></pre></td></tr></table></figure></p><p>显卡驱动下载：<a href="https://www.nvidia.com/download/index.aspx?lang=en-us">https://www.nvidia.com/download/index.aspx?lang=en-us</a></p><h2 id="Conda"><a href="#Conda" class="headerlink" title="Conda"></a>Conda</h2><p>查看环境信息：<code>conda info -envs</code></p><p>创建环境：<code>conda create --name ENVNAME python=3.x pkg1 pkg2 ...</code></p><p>删除环境：<code>conda env remove -n ENVNAME</code></p><p>查看通道：<code>conda config --show</code></p><p>添加通道（中科大镜像）：<code>conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/free/</code></p><p>删除通道：<code>conda config --remove channels ...</code></p><h2 id="GNU-Screen"><a href="#GNU-Screen" class="headerlink" title="GNU Screen"></a>GNU Screen</h2><p>安装screen：<code>apt/yum install screen</code></p><p>版本查看：<code>screen -v</code></p><p>查看已创建的screen终端：<code>screenv -ls</code>，同名终端要用PID区分，输出格式为<code>PID.Name</code>；screen有两种状态<code>Attached</code>（活跃）和<code>Detached</code>（挂起）。</p><p>创建一个叫Hello的虚拟终端（可创建同名）：<code>screen -S Hello</code></p><p>创建（回到）一个叫Hello的虚拟终端（不可创建同名，如果存在则直接进入该终端）：<code>screen -R Hello</code></p><p>清除虚拟终端法一：<code>进入对应终端，exit</code></p><p>清除虚拟终端法二：<code>screen -R [PID/Name] -X quit</code></p><p>后台运行虚拟终端：<code>在终端中按下ctrl+a 再按下d</code></p><p>更多绑定键信息：<code>在终端中按下ctrl+a 再输入?</code></p><p>进入活跃的虚拟终端前需要先挂起：<code>screen -d [PID/Name]</code>，否则<code>-R</code>回到活跃终端反而会创建新终端。</p><p>总之，创建和回到终端前先<code>screenv -ls</code>确认一下是最好的。</p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于 <a href="https://silencezheng.top">silencezheng.top</a>，转载请注明出处。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;总结一下Linux服务器使用的常见操作，深度学习向。&lt;br&gt;</summary>
    
    
    
    
    <category term="Linux" scheme="http://silencezheng.top/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Java位运算</title>
    <link href="http://silencezheng.top/2022/11/24/article81/"/>
    <id>http://silencezheng.top/2022/11/24/article81/</id>
    <published>2022-11-24T02:41:37.000Z</published>
    <updated>2022-11-24T02:42:45.103Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>从原补反到Java位运算，学习记录。<br><span id="more"></span></p><h2 id="机器数"><a href="#机器数" class="headerlink" title="机器数"></a>机器数</h2><p>一个数在计算机的存储形式是二进制数，我们称这些二进制数为机器数，机器数是有符号，在计算机中用机器数的最高位存放符号位，0表示正数，1表示负数。</p><p>因为带有符号位，所以机器数的形式值不等于其真值，以机器数<code>1000 0111</code>为例，其真正表示的值为-7，而形式值为135。将带符号的机器数的真正表示的值称为机器数的真值。</p><p><strong>无符号数</strong>是指整个机器字长的全部二进制位均表示数值位，相当于数的绝对值。还是以<code>1000 0111</code>为例，无符号数就是指其形式值135。Java中不存在无符号整数类型。</p><h2 id="原码、反码、补码"><a href="#原码、反码、补码" class="headerlink" title="原码、反码、补码"></a>原码、反码、补码</h2><p>简单起见，假设使用8位二进制数（一个字节）存储整数，实际上在Java中为四个字节，即32位。</p><p><strong>原码</strong>的表示与机器数真值表示的一样，即用第一位表示符号，其余位表示数值。</p><p><strong>反码</strong>的表示中，正数的反码是其原码本身，负数的反码是在其原码的基础上，符号位不变，其余各位取反。</p><p><strong>补码</strong>的表示中，正数的补码是其原码本身，负数的补码是在其原码的基础上，符号位不变，其余各位取反后加1。</p><p>以十进制正数1和-1为例，其各码如下：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">1:</span></span><br><span class="line"><span class="string">原码=反码=补码=0000</span> <span class="number">0001</span></span><br><span class="line"></span><br><span class="line"><span class="number">-1</span><span class="string">:</span></span><br><span class="line"><span class="string">原码=1000</span> <span class="number">0001</span></span><br><span class="line"><span class="string">反码=1111</span> <span class="number">1110</span></span><br><span class="line"><span class="string">补码=1111</span> <span class="number">1111</span></span><br></pre></td></tr></table></figure></p><p>计算机实际只存储补码，因为<strong>只有补码的计算是准确的</strong>，这一点在此不作证明了。同时原码转换为补码的过程，也可以理解为数据存储到计算机内存中的过程，如下图：</p><p><img src="/assets/post_img/article81/yfb.png" alt="yfb"></p><p>八位原码和八位反码能够表示的区间只有<code>[-127, 127]</code>，而八位补码可以表示的范围为<code>[-128, 127]</code>，因为十进制-127和-1的相加运算用补码表示算得的结果为<code>1000 0000</code>，即用原本的“-0”来表示-128。所以计算机中一个字节的取值范围是<code>[-128,127]</code>。</p><p>需要注意的是，在计算机运算过后，对于负数结果需要转换为原码才能得到其真值。例如补码<code>0b1000 0001</code>，先转换为反码<code>0b1000 0000</code>，再按位取反（符号位不变）得到原码<code>0b1111 1111</code>，可知其真值为-127。</p><h2 id="Java中的位运算符"><a href="#Java中的位运算符" class="headerlink" title="Java中的位运算符"></a>Java中的位运算符</h2><p>Java 定义的位运算（bitwise operators）直接对整数类型的位进行操作，这些整数类型包括 long（64位），int（32位），short（16位），char（16位） 和 byte（8位）。</p><p>位运算符主要用来对操作数二进制的位进行运算。按位运算表示<strong>按每个二进制位（bit）进行计算</strong>，其操作数和运算结果都是整型值。</p><p>Java 语言中的位运算符分为<strong>位逻辑运算符</strong>和<strong>位移运算符</strong>两类，下面详细介绍每类包含的运算符。</p><h3 id="位逻辑运算符"><a href="#位逻辑运算符" class="headerlink" title="位逻辑运算符"></a>位逻辑运算符</h3><p>位逻辑运算符包含 4 个：<code>&amp;（AND）</code>、<code>|（OR）</code>、<code>~（NOT）</code>和 <code>^（XOR）</code>。除了 <code>~</code> 为单目运算符外，其余都为双目运算符。</p><p>位与运算符为<code>&amp;</code>，其运算规则是：参与运算的数字，低位对齐，高位不足的补零，如果对应的二进制位同时为 1，那么计算结果才为 1，否则为 0。因此，任何数与 0 进行按位与运算，其结果都为 0。</p><p>位或运算符为<code>|</code>，其运算规则是：参与运算的数字，低位对齐，高位不足的补零。如果对应的二进制位只要有一个为 1，那么结果就为 1；如果对应的二进制位都为 0，结果才为 0。</p><p>位异或运算符为<code>^</code>，其运算规则是：参与运算的数字，低位对齐，高位不足的补零，如果对应的二进制位相同（同时为 0 或同时为 1）时，结果为 0；如果对应的二进制位不相同，结果则为 1。即<strong>相同为0，不同为1</strong>。</p><p>位取反运算符为<code>~</code>，其运算规则是：只对一个操作数进行运算，将操作数二进制中的 1 改为 0，0 改为 1。</p><h3 id="位移运算符"><a href="#位移运算符" class="headerlink" title="位移运算符"></a>位移运算符</h3><p>位移运算符用来将操作数向某个方向（向左或者右）移动指定的二进制位数，它们都属于双目运算符。</p><p><strong>左移位运算符</strong>为<code>&lt;&lt;</code>，其运算规则是：按二进制形式把所有的数字向左移动对应的位数，高位移出（舍弃），低位的空位补零。</p><p><strong>有符号右位移运算符</strong>为<code>&gt;&gt;</code>，其运算规则是：按二进制形式把所有的数字向右移动对应的位数，低位移出（舍弃），若操作数为正数则高位补0，操作数为负数则高位补1。也称为<strong>算数右移</strong>。</p><p><strong>无符号右位移运算符</strong>为<code>&gt;&gt;&gt;</code>，其运算规则是：按二进制形式把所有的数字向右移动对应的位数，低位移出（舍弃），高位补0。也称为<strong>逻辑右移</strong>。</p><p>特别需要注意的是，在对char、byte、short类型的数进行移位操作前，<strong>编译器都会自动地将数值转化为int类型，然后才进行移位操作（这也导致位移表达式返回值为整型）</strong>。由于int型变量只占4字节，当右移位数超过32bit时，移位运算没有任何意义。所以，在Java语言中，为了保证移动位数地有效性，以使移动的位数不超过32bit，采取了取余的操作，即<code>a&gt;&gt;n</code>等价于<code>a&gt;&gt;(n%32)</code>。这一性质对于左右移位都有效。</p><p>另外，左移$n$位表示原来的值乘$2^n$，经常用来代替乘法操作，由于CPU直接支持位运算，因此位运算比乘法运算效率高。</p><h3 id="复合位赋值运算符"><a href="#复合位赋值运算符" class="headerlink" title="复合位赋值运算符"></a>复合位赋值运算符</h3><p>所有的二进制位运算符都有一种<strong>将赋值与位运算组合在一起</strong>的简写形式。复合位赋值运算符由赋值运算符与位逻辑运算符或位移运算符组合而成。</p><p>包括<code>&amp;=</code>、<code>｜=</code>、<code>^=</code>、<code>&lt;&lt;=</code>、<code>&gt;&gt;&gt;=</code>和<code>&gt;&gt;=</code>。</p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>关于位运算呢，虽然看起来容易掌握，但是实际使用中可能还是会遇到一些问题，有时是由于真值与补码混淆造成的，我写了一个小demo来解释一下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: SilenceZheng66</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">bitwiseOperation</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">byte</span> a = -<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">byte</span> b = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        System.out.println(~a); <span class="comment">// 0</span></span><br><span class="line">        System.out.println(~b); <span class="comment">// -2</span></span><br><span class="line">        System.out.println(a&amp;b); <span class="comment">// 1</span></span><br><span class="line">        System.out.println(a|b); <span class="comment">// -1</span></span><br><span class="line">        System.out.println(b^a); <span class="comment">// -2</span></span><br><span class="line"></span><br><span class="line">        b^=a;</span><br><span class="line">        System.out.println(b); <span class="comment">// -2</span></span><br><span class="line">        System.out.println(b&gt;&gt;<span class="number">1</span>); <span class="comment">// -1</span></span><br><span class="line">        System.out.println(b&lt;&lt;<span class="number">1</span>); <span class="comment">// -4</span></span><br><span class="line"></span><br><span class="line">        System.out.println(Integer.toBinaryString(a&gt;&gt;&gt;<span class="number">2</span>));<span class="comment">// 00111111111111111111111111111111</span></span><br><span class="line">        System.out.println(Integer.toBinaryString(a&gt;&gt;<span class="number">2</span>)); <span class="comment">// 11111111111111111111111111111111</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>首先明确“真值=原码“，所以可以得到$a$的原码为<code>1000 0001</code>，$b$的原码为<code>0000 0001</code>。但在内存中，$a$被存储为<code>1111 1111</code>，这是由<code>1000 0001 -&gt; 1111 1110 -&gt; 1111 1111</code>得到的，也就是原码转换为补码的过程。$b$由于是正数，仍然被存储为原码形式。</p><p>那么<code>~a</code>为何是$0$呢？可以列出计算过程：<code>1111 1111(a) -&gt; 0000 0000(~a)</code>。由于<code>0000 0000</code>的原码与补码相同，故其真值为$0$。下面再计算一个<code>b^a</code>吧，过程如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">先计算异或：0000</span> <span class="number">0001</span> <span class="string">^</span> <span class="number">1111 </span><span class="number">1111</span> <span class="string">=</span> <span class="number">1111 </span><span class="number">1110</span></span><br><span class="line"><span class="string">补码转化为真值：1111</span> <span class="number">1110</span> <span class="string">-&gt;</span> <span class="number">1111 </span><span class="number">1101</span> <span class="string">-&gt;</span> <span class="number">1000 </span><span class="number">0010</span></span><br><span class="line"><span class="string">得到结果为-2</span></span><br></pre></td></tr></table></figure><h2 id="例题一：Leetcode190-颠倒二进制位"><a href="#例题一：Leetcode190-颠倒二进制位" class="headerlink" title="例题一：Leetcode190. 颠倒二进制位"></a>例题一：Leetcode190. 颠倒二进制位</h2><p>问题描述：颠倒给定的 32 位无符号整数的二进制位。</p><p>思路一（我的愚蠢解法）：获取读入十进制数的二进制字符串，存入字符数组，反转后再转为int输出。</p><p>实现：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: SilenceZheng66</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="comment">// you need treat n as an unsigned value</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">reverseBits</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">        String input = Integer.toBinaryString(n);</span><br><span class="line">        <span class="keyword">char</span>[] input_arr = input.toCharArray();</span><br><span class="line">        <span class="keyword">char</span>[] ans = <span class="keyword">new</span> <span class="keyword">char</span>[<span class="number">32</span>];</span><br><span class="line">        <span class="keyword">int</span> j = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = input_arr.length-<span class="number">1</span>; i&gt;=<span class="number">0</span>; i--)&#123;</span><br><span class="line">            ans[j] = input_arr[i];</span><br><span class="line">            j++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// toBinaryString中高位的0不输出，需要补充。</span></span><br><span class="line">        <span class="keyword">while</span>(j&lt;<span class="number">32</span>)&#123;</span><br><span class="line">            ans[j] = <span class="string">&#x27;0&#x27;</span>;</span><br><span class="line">            j++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> Integer.parseUnsignedInt(String.valueOf(ans), <span class="number">2</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>思路二（官解一）：将 $n$ 视作一个长为 32 的二进制串，从低位往高位枚举 $n$ 的每一位，将其倒序添加到翻转结果中。代码实现中，每枚举一位就将 $n$ 右移一位，这样当前 $n$ 的最低位就是我们要枚举的比特位。当 $n$ 为 0 时即可结束循环。</p><p>实现：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">reverseBits</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> rev = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">32</span> &amp;&amp; n != <span class="number">0</span>; ++i) &#123;</span><br><span class="line">            <span class="comment">// n&amp;1 只保留低位</span></span><br><span class="line">            <span class="comment">// &lt;&lt; (31 - i) 移到反转后的位置</span></span><br><span class="line">            <span class="comment">// rev = rev ｜ (n &amp; 1) &lt;&lt; (31 - i) 存入翻转结果 rev</span></span><br><span class="line">            rev |= (n &amp; <span class="number">1</span>) &lt;&lt; (<span class="number">31</span> - i);</span><br><span class="line">            <span class="comment">// n = n &gt;&gt;&gt; 1 逻辑右移，高位补0</span></span><br><span class="line">            n &gt;&gt;&gt;= <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> rev;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>思路三（JDK用法）：Integer包装类提供了<code>reverse</code>方法，速度很快。</p><p>实现：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">reverseBits</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> Integer.reverse(n);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1]<a href="http://c.biancheng.net/view/784.html">http://c.biancheng.net/view/784.html</a><br>[2]<a href="https://blog.csdn.net/xwu_09/article/details/78285785">https://blog.csdn.net/xwu_09/article/details/78285785</a><br>[3]<a href="https://www.cnblogs.com/linjiaxin/p/14870850.html">https://www.cnblogs.com/linjiaxin/p/14870850.html</a><br>[4]<a href="https://zhuanlan.zhihu.com/p/371184302">https://zhuanlan.zhihu.com/p/371184302</a><br>[5]<a href="https://blog.csdn.net/MaybeForever/article/details/89109596">https://blog.csdn.net/MaybeForever/article/details/89109596</a></p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于 <a href="https://silencezheng.top">silencezheng.top</a>，转载请注明出处。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;从原补反到Java位运算，学习记录。&lt;br&gt;</summary>
    
    
    
    
    <category term="Java" scheme="http://silencezheng.top/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>parseInt() 和 parseUnsignedInt()</title>
    <link href="http://silencezheng.top/2022/11/23/article80/"/>
    <id>http://silencezheng.top/2022/11/23/article80/</id>
    <published>2022-11-23T15:38:54.000Z</published>
    <updated>2022-11-25T03:16:30.922Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>22.11.25：事后发现是我蠢了，Java的意思是表示一个数直接用正负号表示符号，用原码表示数值，哪有人用补码转int的…</p><p>在做题的过程中，发现了一个神奇的“bug”，Integer.valueOf()对于32位二进制数字符串转化成整型爆出了<code>java.lang.NumberFormatException.forInputString</code>错误，往下研究了一下，发现问题出在了parseInt()上。<br><span id="more"></span></p><h2 id="Integer-valueOf"><a href="#Integer-valueOf" class="headerlink" title="Integer.valueOf()"></a>Integer.valueOf()</h2><p>该函数返回的是整型包装类，其内部调用了<code>Integer.parseInt(String s, int radix)</code>。于是问题聚焦在这个函数上。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Integer <span class="title">valueOf</span><span class="params">(String s, <span class="keyword">int</span> radix)</span> <span class="keyword">throws</span> NumberFormatException </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> Integer.valueOf(parseInt(s,radix));</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h2 id="parseInt-和parseUnsignedInt"><a href="#parseInt-和parseUnsignedInt" class="headerlink" title="parseInt()和parseUnsignedInt()"></a>parseInt()和parseUnsignedInt()</h2><p>还原一下问题场景：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: SilenceZheng66</span></span><br><span class="line">String a = <span class="string">&quot;11111111111111111111111111111101&quot;</span>;</span><br><span class="line"><span class="comment">// 可行，返回-3</span></span><br><span class="line">System.out.println(Integer.parseUnsignedInt(b, <span class="number">2</span>)); </span><br><span class="line"><span class="comment">// 报错</span></span><br><span class="line">System.out.println(Integer.parseInt(b, <span class="number">2</span>));</span><br></pre></td></tr></table></figure><p>这就有点迷惑了，明明输入的二进制字符串是32位的（在Integer范围内），为什么会说我格式错误呢？另外，这个<code>parseUnsignedInt()</code>又是干什么用的呢？带着这样的问题，我们来阅读二者的源码。首先看<code>parseUnsignedInt()</code>：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Parses the string argument as an unsigned integer in the radix</span></span><br><span class="line"><span class="comment">     * specified by the second argument.  An unsigned integer maps the</span></span><br><span class="line"><span class="comment">     * values usually associated with negative numbers to positive</span></span><br><span class="line"><span class="comment">     * numbers larger than MAX_VALUE.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">parseUnsignedInt</span><span class="params">(String s, <span class="keyword">int</span> radix)</span></span></span><br><span class="line"><span class="function">                <span class="keyword">throws</span> NumberFormatException </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (s == <span class="keyword">null</span>)  &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> NumberFormatException(<span class="string">&quot;null&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> len = s.length();</span><br><span class="line">        <span class="keyword">if</span> (len &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">char</span> firstChar = s.charAt(<span class="number">0</span>);</span><br><span class="line">            <span class="keyword">if</span> (firstChar == <span class="string">&#x27;-&#x27;</span>) &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span></span><br><span class="line">                    NumberFormatException(String.format(<span class="string">&quot;Illegal leading minus sign &quot;</span> +</span><br><span class="line">                                                       <span class="string">&quot;on unsigned string %s.&quot;</span>, s));</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">if</span> (len &lt;= <span class="number">5</span> || <span class="comment">// Integer.MAX_VALUE in Character.MAX_RADIX is 6 digits</span></span><br><span class="line">                    (radix == <span class="number">10</span> &amp;&amp; len &lt;= <span class="number">9</span>) ) &#123; <span class="comment">// Integer.MAX_VALUE in base 10 is 10 digits</span></span><br><span class="line">                    <span class="keyword">return</span> parseInt(s, radix);</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="keyword">long</span> ell = Long.parseLong(s, radix);</span><br><span class="line">                    <span class="keyword">if</span> ((ell &amp; <span class="number">0xffff_ffff_0000_0000L</span>) == <span class="number">0</span>) &#123;</span><br><span class="line">                        <span class="keyword">return</span> (<span class="keyword">int</span>) ell;</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        <span class="keyword">throw</span> <span class="keyword">new</span></span><br><span class="line">                            NumberFormatException(String.format(<span class="string">&quot;String value %s exceeds &quot;</span> +</span><br><span class="line">                                                                <span class="string">&quot;range of unsigned int.&quot;</span>, s));</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">throw</span> NumberFormatException.forInputString(s);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>Java1.8中，<code>Character.MIN_RADIX = 2, Character.MAX_RADIX = 36</code>，也就是说最大支持到36进制。<code>parseUnsignedInt</code>方法首先要求<code>s</code>不能带有负号，对于上述问题场景，该方法首先将输入转化为长整型，然后再转回整型输出。由于Java中不存在无符号整型，在整型中，32位二进制数字的第一位必须为符号位，所以返回结果为带符号数-3。再阐述一下，我输入的无符号数 <code>11111111111111111111111111111101 = 4294967293</code> 超出了Java整型的大小限制（2147483647），但没有超过长度限制（32位），于是应该是可以正常表示的。</p><p>总之<code>parseUnsignedInt</code>方法是用于获取无符号数的，它尚且可以根据输入来获得整型，为什么<code>parseInt()</code>不行呢？我们再来看一下源码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">parseInt</span><span class="params">(String s, <span class="keyword">int</span> radix)</span></span></span><br><span class="line"><span class="function">                <span class="keyword">throws</span> NumberFormatException</span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * WARNING: This method may be invoked early during VM initialization</span></span><br><span class="line"><span class="comment">         * before IntegerCache is initialized. Care must be taken to not use</span></span><br><span class="line"><span class="comment">         * the valueOf method.</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (s == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> NumberFormatException(<span class="string">&quot;null&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (radix &lt; Character.MIN_RADIX) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> NumberFormatException(<span class="string">&quot;radix &quot;</span> + radix +</span><br><span class="line">                                            <span class="string">&quot; less than Character.MIN_RADIX&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (radix &gt; Character.MAX_RADIX) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> NumberFormatException(<span class="string">&quot;radix &quot;</span> + radix +</span><br><span class="line">                                            <span class="string">&quot; greater than Character.MAX_RADIX&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> result = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">boolean</span> negative = <span class="keyword">false</span>;</span><br><span class="line">        <span class="keyword">int</span> i = <span class="number">0</span>, len = s.length();</span><br><span class="line">        <span class="keyword">int</span> limit = -Integer.MAX_VALUE;</span><br><span class="line">        <span class="keyword">int</span> multmin;</span><br><span class="line">        <span class="keyword">int</span> digit;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (len &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">char</span> firstChar = s.charAt(<span class="number">0</span>);</span><br><span class="line">            <span class="keyword">if</span> (firstChar &lt; <span class="string">&#x27;0&#x27;</span>) &#123; <span class="comment">// Possible leading &quot;+&quot; or &quot;-&quot;</span></span><br><span class="line">                <span class="keyword">if</span> (firstChar == <span class="string">&#x27;-&#x27;</span>) &#123;</span><br><span class="line">                    negative = <span class="keyword">true</span>;</span><br><span class="line">                    limit = Integer.MIN_VALUE;</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (firstChar != <span class="string">&#x27;+&#x27;</span>)</span><br><span class="line">                    <span class="keyword">throw</span> NumberFormatException.forInputString(s);</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (len == <span class="number">1</span>) <span class="comment">// Cannot have lone &quot;+&quot; or &quot;-&quot;</span></span><br><span class="line">                    <span class="keyword">throw</span> NumberFormatException.forInputString(s);</span><br><span class="line">                i++;</span><br><span class="line">            &#125;</span><br><span class="line">            multmin = limit / radix;</span><br><span class="line">            <span class="keyword">while</span> (i &lt; len) &#123;</span><br><span class="line">                <span class="comment">// Accumulating negatively avoids surprises near MAX_VALUE</span></span><br><span class="line">                digit = Character.digit(s.charAt(i++),radix);</span><br><span class="line">                <span class="keyword">if</span> (digit &lt; <span class="number">0</span>) &#123;</span><br><span class="line">                    <span class="keyword">throw</span> NumberFormatException.forInputString(s);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">if</span> (result &lt; multmin) &#123;</span><br><span class="line">                    <span class="keyword">throw</span> NumberFormatException.forInputString(s);</span><br><span class="line">                &#125;</span><br><span class="line">                result *= radix;</span><br><span class="line">                <span class="keyword">if</span> (result &lt; limit + digit) &#123;</span><br><span class="line">                    <span class="keyword">throw</span> NumberFormatException.forInputString(s);</span><br><span class="line">                &#125;</span><br><span class="line">                result -= digit;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">throw</span> NumberFormatException.forInputString(s);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> negative ? result : -result;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>我们可以发现，<code>parseInt</code>方法首先判断第一个字符是不是正负号，是负号则说明值是负的，否则，值就是正的。这个逻辑在非二进制环境下没有问题，因为非二进制表示的int变量，都会前置负号来表示负数。然而，在二进制数中，并没有所谓的“正负号”概念，数值的正负由符号位表示。所以<code>parseInt()</code>在转换<code>&quot;11111111111111111111111111111101&quot;</code>时将符号位也当做实际的值计算进去了，导致了数值溢出报错。</p><p>一种解决办法是将首位数字改为正负号，可以运行成功，但丧失了原本的意义，为什么这么说呢？请看下例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: SilenceZheng66</span></span><br><span class="line">String a = <span class="string">&quot;11111111111111111111111111111101&quot;</span>;</span><br><span class="line">String b = <span class="string">&quot;-1111111111111111111111111111101&quot;</span>;</span><br><span class="line">System.out.println(Integer.parseUnsignedInt(a, <span class="number">2</span>)); <span class="comment">// -3</span></span><br><span class="line">System.out.println(Integer.valueOf(b, <span class="number">2</span>)); <span class="comment">// -2147483645</span></span><br></pre></td></tr></table></figure><p>b的输出竟然是-2147483645，并不是想要表达的真实含义（-3），这是因为<code>parseInt()</code>首先将<code>&quot;-1111111111111111111111111111101&quot;</code> 拆分为 <code>&quot;-&quot;和&quot;01111111111111111111111111111101&quot;</code>，其中后者表示的真值为2147483645，然后再把负号放进来组合成了输出，这显然与补码<code>&quot;11111111111111111111111111111101&quot;</code>的真值不一致。</p><p>综上，在处理32位补码时，需要采用<code>parseUnsignedInt()</code>。其实感觉这个<code>parseInt()</code>可以优化一下，对32位二进制数用首位数字判断符号就好了。</p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于 <a href="https://silencezheng.top">silencezheng.top</a>，转载请注明出处。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;22.11.25：事后发现是我蠢了，Java的意思是表示一个数直接用正负号表示符号，用原码表示数值，哪有人用补码转int的…&lt;/p&gt;
&lt;p&gt;在做题的过程中，发现了一个神奇的“bug”，Integer.valueOf()对于32位二进制数字符串转化成整型爆出了&lt;code&gt;java.lang.NumberFormatException.forInputString&lt;/code&gt;错误，往下研究了一下，发现问题出在了parseInt()上。&lt;br&gt;</summary>
    
    
    
    
    <category term="Java" scheme="http://silencezheng.top/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>Java中的==和equals</title>
    <link href="http://silencezheng.top/2022/11/20/article79/"/>
    <id>http://silencezheng.top/2022/11/20/article79/</id>
    <published>2022-11-20T06:10:47.000Z</published>
    <updated>2022-11-20T06:12:25.015Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>从Java数据类型出发，聊聊Java中的==和equals。<br><span id="more"></span></p><h2 id="Java数据类型"><a href="#Java数据类型" class="headerlink" title="Java数据类型"></a>Java数据类型</h2><p>Java 语言支持的数据类型分为两种：<strong>基本数据类型</strong>（Primitive Type）和<strong>引用数据类型</strong>（Reference Type）。</p><p><img src="/assets/post_img/article79/datatype.jpeg" alt="datatype"></p><p>引用数据类型建立在基本数据类型的基础上，包括数组、类和接口。引用数据类型是由用户自定义，用来限制其他数据的类型。<strong>空类型null也是一种引用类型，不能转换成基本类型，因此不要把一个 null 值赋给基本数据类型的变量。</strong></p><p>对于基本数据类型，Java 为每种基本数据类型分别设计了对应的类，称之为<strong>包装类</strong>（Wrapper Classes），除了Integer和Character以外，包装类名都和基本数据类型相同，只是首字母大写。关于数据类型的更多细节，就不在这里赘述了。</p><p><img src="/assets/post_img/article79/pd.jpeg" alt="pd"></p><h2 id="和-equals"><a href="#和-equals" class="headerlink" title="== 和 equals"></a>== 和 equals</h2><p>首先明确一点，<strong>equals方法不能作用于基本数据类型变量</strong>。关于这句话其实可以展开说明一下，首先基本类型是不能作为方法的主体的，这是一定的，但基本类型能不能作为equals中的参数呢？有时也是可以的，因为Java的“自动装箱”机制，基本类型在传入时可以被封装为包装类，事实上参与函数的是包装类。</p><p>在比较基本数据类型时，<code>==</code>比较的是值。</p><p>在比较引用类型时，<code>==</code>比较的是对象的内存地址。如果equals方法没有经过重写，则与<code>==</code>相同，比较地址；如果equals方法经过重写，对于Java提供的类来说，则是比较对象存储的内容是否相同，也就是比较“值”。</p><p>Java提供的绝大多数类（不是全部）都重写了equals方法，以上讲述的区别与联系主要是关于Java内置类和基本数据类型。</p><h2 id="equals-和-hashCode"><a href="#equals-和-hashCode" class="headerlink" title="equals() 和 hashCode()"></a>equals() 和 hashCode()</h2><p>对于我们自己写代码，新建一个类而言，要么就不重写equals方法，此时等同于<code>==</code>；要么就同时重写equals和hashCode方法，按照我们定义的规则来比较对象是否相等。</p><blockquote><p>Java中对equals()的规范</p><ol><li>对称性：如果x.equals(y)返回是”true”，那么y.equals(x)也应该返回是”true”。</li><li>自反性：x.equals(x)必须返回是”true”。</li><li>传递性：如果x.equals(y)返回是”true”，而且y.equals(z)返回是”true”，那么x.equals(z)也应该返回是”true”。</li><li>一致性：如果x.equals(y)返回是”true”，只要x和y内容一直不变，不管重复x.equals(y)多少次，返回都是”true”。</li><li>非空性，x.equals(null)，永远返回是”false”；假设z是和x不同类型的对象，则x.equals(z)永远返回是”false”。</li></ol></blockquote><p>hashCode()的作用是用来获取哈希码，用于确定对象在哈希表中的位置，与equals()一样，所有的类都有hashCode方法。hashCode()只有在创建某个类的哈希表时才有用，需要根据方法返回值确认对象在哈希表中的位置。如果一个对象一定不会在散列表中使用，那么是没有必要复写hashCode方法的。但一般情况下我们还是会复写hashCode方法，因为谁能保证这个对象不会出现在HashMap、HashSet、HashTable…中呢？</p><blockquote><p>Object.hashCode()的通用约定</p><ol><li>在应用程序中，只要对象的equals方法的比较操作所用的信息没有修改，那么对于同一个对象的多次调用hashCode()，必须始终返回同一个哈希值。</li><li>如果两个对象通过equals()比较相等，那么它们的哈希值相同。</li><li>如果两个对象通过equals()比较不等，他们的哈希值可能相同也可能不同，取决于hashCode的实现，由此哈希表的性能也会有区别。</li></ol></blockquote><p>考虑一个常见的场景，HashSet是一个不允许有重复元素的集合，该集合会维护一个已存入对象的哈希值表。当插入一个新的对象时，我们首先会想到使用equals()逐个比较来确定是否有重复元素，但这必然造成效率问题。另外一个合理的方式就是对该对象调用hashCode()得到哈希值，然后在哈希值表中进行比对，如果不存在则直接存入；如果存在，则再调用equals()进行比较，相同的话就不再存入，不同的话散列到其他地址。这样一来实际调用equals()的次数就大大降低了。</p><p>因此，如果不重写对象的hashCode()方法，就有可能造成相同对象产生不同哈希值的情况，这就破坏了HashSet的性质。当然，这只是不重写hashCode(),或者说不遵守hashCode()规范的其中一个坏处。</p><p>对于equals() 和 hashCode()的部分，可以用两个问题来加深印象：<br>1、两个对象，如果a.equals(b)==true，那么a和b是否相等？<br>答：相等，但对象地址不一定相等。</p><p>2、两个对象，如果哈希值一样，那么两个对象是否相等？<br>答：不一定相等，判断两个对象是否相等，需要使用equals()。</p><p>最后，记录一个实现高质量equals()的诀窍：</p><blockquote><ol><li>使用<code>==</code>操作符检查“参数是否为这个对象的引用”；</li><li>使用<code>instanceof</code>操作符检查“参数是否为正确的类型”；</li><li>对于类中的关键属性，检查参数传入对象的属性是否与之相匹配；</li><li>编写完equals方法后，问自己它是否满足对称性、传递性、一致性；</li><li>重写equals方法时总是要重写hashCode方法；</li><li>不要将equals方法参数中的Object对象替换为其他的类型，在重写时不要忘掉<code>@Override</code>注解。</li></ol></blockquote><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1]<a href="http://c.biancheng.net/view/5672.html">http://c.biancheng.net/view/5672.html</a><br>[2]<a href="https://blog.csdn.net/qq_44543508/article/details/95449363">https://blog.csdn.net/qq_44543508/article/details/95449363</a><br>[3]<a href="https://www.jianshu.com/p/da7491e5be53">https://www.jianshu.com/p/da7491e5be53</a><br>[4]<a href="https://blog.csdn.net/u013063153/article/details/78808923">https://blog.csdn.net/u013063153/article/details/78808923</a><br>[5]<a href="https://cloud.tencent.com/developer/article/1018529">https://cloud.tencent.com/developer/article/1018529</a></p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于 <a href="https://silencezheng.top">silencezheng.top</a>，转载请注明出处。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;从Java数据类型出发，聊聊Java中的==和equals。&lt;br&gt;</summary>
    
    
    
    
    <category term="Java" scheme="http://silencezheng.top/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>记忆化递归</title>
    <link href="http://silencezheng.top/2022/11/19/article78/"/>
    <id>http://silencezheng.top/2022/11/19/article78/</id>
    <published>2022-11-19T05:59:19.000Z</published>
    <updated>2022-11-19T05:59:52.552Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>一旦写完一个可行的算法，便不想推倒重来，不知道有多少人和我一样有这样的想法。然而写完一个trash的递归算法，计算时间太慢固然也是不行的，这时记忆化递归可能会帮到你。</p><span id="more"></span><h2 id="记忆化递归"><a href="#记忆化递归" class="headerlink" title="记忆化递归"></a>记忆化递归</h2><p><strong>记忆化递归</strong>的核心思想就是将已经算好的值给存起来，等再次需要用到的时候，就直接取而不用计算，这样就大大节省了计算时间。笔者认为，虽然看起来是用空间换时间，但是相比于每次都进行庞大的递归计算来说，用合理的空间存放之前求得的值是明智的。</p><p>思想很简单，实现起来其实也并不复杂，下面举例说明之。</p><h2 id="例一：Leetcode119-杨辉三角-II"><a href="#例一：Leetcode119-杨辉三角-II" class="headerlink" title="例一：Leetcode119. 杨辉三角 II"></a>例一：Leetcode119. 杨辉三角 II</h2><p>简单描述：给定一个非负索引 rowIndex，返回「杨辉三角」的第 rowIndex 行。</p><p>普通递归（超时）：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: SilenceZheng66</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">recusion</span><span class="params">(<span class="keyword">int</span> pos, <span class="keyword">int</span> rowIndex)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(pos&gt;rowIndex) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">if</span>(pos==rowIndex||pos==<span class="number">0</span>) <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">return</span> (recusion(pos-<span class="number">1</span>, rowIndex-<span class="number">1</span>) + recusion(pos, rowIndex-<span class="number">1</span>));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;Integer&gt; <span class="title">getRow</span><span class="params">(<span class="keyword">int</span> rowIndex)</span> </span>&#123;</span><br><span class="line">        ArrayList&lt;Integer&gt; ans = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        ans.add(<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">if</span>(rowIndex==<span class="number">0</span>) <span class="keyword">return</span> ans;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=rowIndex;i++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(i==rowIndex) ans.add(<span class="number">1</span>);</span><br><span class="line">            <span class="keyword">else</span>&#123;</span><br><span class="line">                ans.add(recusion(i, rowIndex));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>利用对称（超时）：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: SilenceZheng66</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">recusion</span><span class="params">(<span class="keyword">int</span> pos, <span class="keyword">int</span> rowIndex)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(pos&gt;rowIndex) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">if</span>(pos==rowIndex||pos==<span class="number">0</span>) <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">return</span> (recusion(pos-<span class="number">1</span>, rowIndex-<span class="number">1</span>) + recusion(pos, rowIndex-<span class="number">1</span>));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;Integer&gt; <span class="title">getRow</span><span class="params">(<span class="keyword">int</span> rowIndex)</span> </span>&#123;</span><br><span class="line">        ArrayList&lt;Integer&gt; ans = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        ans.add(<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">if</span>(rowIndex==<span class="number">0</span>) <span class="keyword">return</span> ans;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> step = rowIndex/<span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=step;i++)&#123;</span><br><span class="line">            ans.add(recusion(i, rowIndex));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(rowIndex%<span class="number">2</span>==<span class="number">0</span>)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i=step-<span class="number">1</span>;i&gt;=<span class="number">0</span>;i--)&#123;</span><br><span class="line">                ans.add(ans.get(i));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i=step;i&gt;=<span class="number">0</span>;i--)&#123;</span><br><span class="line">                ans.add(ans.get(i));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>记忆化递归+利用对称：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: SilenceZheng66</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span>[][] rem = <span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">34</span>][<span class="number">34</span>];</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">recusion</span><span class="params">(<span class="keyword">int</span> pos, <span class="keyword">int</span> rowIndex)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(pos&gt;rowIndex) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">if</span>(pos==rowIndex||pos==<span class="number">0</span>) <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">if</span>(rem[rowIndex][pos]!=<span class="number">0</span>) <span class="keyword">return</span> rem[rowIndex][pos];</span><br><span class="line"></span><br><span class="line">        rem[rowIndex][pos] = (recusion(pos-<span class="number">1</span>, rowIndex-<span class="number">1</span>) + recusion(pos, rowIndex-<span class="number">1</span>));</span><br><span class="line">        <span class="keyword">return</span> rem[rowIndex][pos];</span><br><span class="line">    &#125;</span><br><span class="line">     </span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;Integer&gt; <span class="title">getRow</span><span class="params">(<span class="keyword">int</span> rowIndex)</span> </span>&#123;</span><br><span class="line">        ArrayList&lt;Integer&gt; ans = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        ans.add(<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">if</span>(rowIndex==<span class="number">0</span>) <span class="keyword">return</span> ans;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> step = rowIndex/<span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=step;i++)&#123;</span><br><span class="line">            ans.add(recusion(i, rowIndex));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(rowIndex%<span class="number">2</span>==<span class="number">0</span>)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i=step-<span class="number">1</span>;i&gt;=<span class="number">0</span>;i--)&#123;</span><br><span class="line">                ans.add(ans.get(i));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i=step;i&gt;=<span class="number">0</span>;i--)&#123;</span><br><span class="line">                ans.add(ans.get(i));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于 <a href="https://silencezheng.top">silencezheng.top</a>，转载请注明出处。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;一旦写完一个可行的算法，便不想推倒重来，不知道有多少人和我一样有这样的想法。然而写完一个trash的递归算法，计算时间太慢固然也是不行的，这时记忆化递归可能会帮到你。&lt;/p&gt;</summary>
    
    
    
    
    <category term="数据结构与算法" scheme="http://silencezheng.top/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>MySQL中的Join查询</title>
    <link href="http://silencezheng.top/2022/11/18/article77/"/>
    <id>http://silencezheng.top/2022/11/18/article77/</id>
    <published>2022-11-18T04:50:46.000Z</published>
    <updated>2022-11-18T04:52:50.466Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>业精于勤，荒于嬉。</p><span id="more"></span><h2 id="闲谈"><a href="#闲谈" class="headerlink" title="闲谈"></a>闲谈</h2><p>在整理Join相关的内容时，我提出了几个问题，整理到一个部分记录一下，想看“干货”的读者可以跳过了。</p><p>问题一：<strong>Join和Key有啥关系？</strong><br>Key无非主、外、候选、公共之类的内容，一个row的identifier罢了，无非是对内对外，同时它也是一个field。那么我们在规划表结构、塞数据的时候就有了一个方便的方法，把想表达的一个row的数据用一个key概括，需要获取所有数据时通过多表查询即可。总之，笔者认为，Join和Key可以说没关系，Key在任何时候都发挥着identifier的作用。</p><p>问题二：<strong>MySQL中不用Key也能Join，为什么？</strong><br>其实这个问题本身有点奇怪（我突然想出来的），首先关系代数中连接（Join）也没有要求一定要用键做连接，其次上面也说了这俩没多大关系。但我一搜吧，还真有个<a href="https://blog.csdn.net/lamanchas/article/details/121366276">回答</a>，主要是说外键约束有成本，对高并发情况不合适之类的，一时不知道是我有问题还是理解不到位，知道的大神可以告诉我，感谢。</p><p>问题三：<strong>为什么不能用Where替代Join?</strong><br>关于这个问题，我简单思考了一下，首先就拿纯<code>WHERE</code>、<code>JOIN</code>和<code>LEFT JOIN</code>来说，我写了以下三个查询：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">select</span> *</span><br><span class="line"><span class="attribute">from</span> node_graph<span class="number">1</span> left join edge_graph<span class="number">1</span> <span class="literal">on</span> node_graph<span class="number">1</span>.node_list = edge_graph<span class="number">1</span>.node<span class="number">1</span>_name;</span><br><span class="line"></span><br><span class="line"><span class="attribute">select</span> *</span><br><span class="line"><span class="attribute">from</span> node_graph<span class="number">1</span>, edge_graph<span class="number">1</span> where node_graph<span class="number">1</span>.node_list = edge_graph<span class="number">1</span>.node<span class="number">1</span>_name;</span><br><span class="line"></span><br><span class="line"><span class="attribute">select</span> *</span><br><span class="line"><span class="attribute">from</span> node_graph<span class="number">1</span> join edge_graph<span class="number">1</span> <span class="literal">on</span> node_graph<span class="number">1</span>.node_list = edge_graph<span class="number">1</span>.node<span class="number">1</span>_name;</span><br></pre></td></tr></table></figure><p>其中后两个的效果是一致的，而<code>LEFT JOIN</code>会返回node_graph1的所有结果，即使没有match到，这可能是<code>WHERE</code>做不到的一个地方。更多还是要在实践中发掘。</p><h2 id="Join查询"><a href="#Join查询" class="headerlink" title="Join查询"></a>Join查询</h2><p>SQL中Join用于根据两个或多个表中的列之间的关系，从这些表中查询数据。日常使用中对多表查询有广泛的需求，Join查询自然是必不可少。</p><p>用Join联合表时需要在每个表中选择一个<strong>字段</strong>，并对这些字段的值进行比较，值相同的两条记录将合并为一条。联合表的本质就是将不同表的记录合并起来，形成一张新表。当然，这张新表只是临时的，它仅存在于本次查询期间。</p><p>那么SQL中的Join都有哪些呢？先上一张总览：</p><p><img src="/assets/post_img/article77/SQL-Join.png" alt="overview"></p><p>下面开始逐个说一下。</p><h3 id="1-内连接（Inner-Join"><a href="#1-内连接（Inner-Join" class="headerlink" title="1. 内连接（Inner Join)"></a>1. 内连接（Inner Join)</h3><p>INNER JOIN 是 SQL 中最重要、最常用的表连接形式，只有当连接的两个或者多个表中都存在满足条件的记录时，才返回行。任何一条只存在于某一张表中的数据，都不会返回。</p><h3 id="2-左外连接（Left-Outer-Join"><a href="#2-左外连接（Left-Outer-Join" class="headerlink" title="2. 左外连接（Left Outer Join)"></a>2. 左外连接（Left Outer Join)</h3><p>LEFT OUTER JOIN 以左表为主，即左表中的<strong>所有记录</strong>都会被返回，具体分为以下三种情况：</p><ul><li>如果 TableA 中的某条记录在 TableB 中刚好只有一条记录可以匹配，那么在返回的结果中会生成一个新的行。</li><li>如果 TableA 中的某条记录在 TableB 中有 N 条记录可以匹配，那么在返回结果中也会生成 N 个新的行，这些行所包含的 TableA 的字段值是重复的。</li><li>如果 TableA 中的某条记录在 TableB 中没有匹配的记录，那么在返回结果中仍然会生成一个新的行，只是该行所包含的 TableB 的字段值都是 NULL。<h3 id="3-右外连接（Right-Outer-Join"><a href="#3-右外连接（Right-Outer-Join" class="headerlink" title="3. 右外连接（Right Outer Join)"></a>3. 右外连接（Right Outer Join)</h3>RIGHT OUTER JOIN 以右表为主，即右表中的<strong>所有记录</strong>都会被返回，具体分为以下三种情况：</li><li>如果 TableB 中的某条记录在 TableA 中刚好只有一条记录可以匹配，那么在返回的结果中会生成一个新的行。</li><li>如果 TableB 中的某条记录在 TableA 中有 N 条记录可以匹配，那么在返回的结果中也会生成 N 个新的行，这些行所包含的 TableB 的字段值是重复的。</li><li>如果 TableB 中的某条记录在 TableA 中没有匹配记录，那么在返回结果中仍然会生成一个新的行，只是该行所包含的 TableA 的字段值都是 NULL。<h3 id="4-左外连接-with-exclusion（Left-Outer-Join-with-exclusion"><a href="#4-左外连接-with-exclusion（Left-Outer-Join-with-exclusion" class="headerlink" title="4. 左外连接 with exclusion（Left Outer Join with exclusion)"></a>4. 左外连接 with exclusion（Left Outer Join with exclusion)</h3>在左外连接的基础上，去除TableB可匹配到的部分，只返回B.Key为NULL的记录。<h3 id="5-右外连接-with-exclusion（Right-Outer-Join-with-exclusion"><a href="#5-右外连接-with-exclusion（Right-Outer-Join-with-exclusion" class="headerlink" title="5. 右外连接 with exclusion（Right Outer Join with exclusion)"></a>5. 右外连接 with exclusion（Right Outer Join with exclusion)</h3>在右外连接的基础上，去除TableA可匹配到的部分，只返回A.Key为NULL的记录。<h3 id="6-全外连接（Full-Outer-Join）"><a href="#6-全外连接（Full-Outer-Join）" class="headerlink" title="6. 全外连接（Full Outer Join）"></a>6. 全外连接（Full Outer Join）</h3>FULL OUTER JOIN 先执行 LEFT OUTER JOIN 遍历左表，再执行 RIGHT OUTER JOIN 遍历右表，最后将 RIGHT OUTER JOIN 的结果直接追加到 LEFT OUTER JOIN 后面。注意，FULL OUTER JOIN 会返回重复的行，它们会被保留，不会被删除。<h3 id="7-全外连接-with-exclusion（Full-Outer-Join-with-exclusion）"><a href="#7-全外连接-with-exclusion（Full-Outer-Join-with-exclusion）" class="headerlink" title="7. 全外连接 with exclusion（Full Outer Join with exclusion）"></a>7. 全外连接 with exclusion（Full Outer Join with exclusion）</h3>两表的FULL OUTER JOIN去除重合部分，也就是返回 左外连接 with exclusion 和 右外连接 with exclusion 的 FULL OUTER JOIN 记录。</li></ul><h2 id="MySQL支持的Join方式"><a href="#MySQL支持的Join方式" class="headerlink" title="MySQL支持的Join方式"></a>MySQL支持的Join方式</h2><p>在聊这个之前，先简单了解一下<strong>驱动表和被驱动表</strong>的概念。在LEFT OUTER JOIN时，左表为驱动表，右表为被驱动表；在RIGHT OUTER JOIN时，右表为驱动表，左表为被驱动表。关于驱动表和被驱动表的作用，实际上是与MySQL表关联算法和SQL优化有关的，通常来说，用小表<strong>驱动</strong>大表能够获得更高的效率，这里不详细展开了。</p><p>以MySQL8.0.11为例，MySQL提供的JOIN关键字有：<code>JOIN</code>、<code>INNER JOIN</code>、<code>LEFT JOIN</code>、<code>LEFT OUTER JOIN</code>、<code>RIGHT JOIN</code>、<code>RIGHT OUTER JOIN</code>、<code>CROSS JOIN</code>和<code>STRAIGHT_JOIN</code>。</p><p>其中，<code>JOIN</code>和<code>INNER JOIN</code>为内连接，<code>LEFT JOIN</code>与<code>LEFT OUTER JOIN</code>是等价的，都对应着左外连接（右也是一样的道理）。也就是说，上面提到的七种JOIN方式，MySQL关键字只支持前三种，对于4、5可以结合WHERE来实现，但不提供全外连接关键字。</p><p>这三种（或六个）关键字的通用Join查询结构如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">&lt;</span>row_list<span class="operator">&gt;</span> </span><br><span class="line">  <span class="keyword">FROM</span> <span class="operator">&lt;</span>left_table<span class="operator">&gt;</span> </span><br><span class="line">    <span class="operator">&lt;</span><span class="keyword">inner</span><span class="operator">|</span><span class="keyword">left</span><span class="operator">|</span><span class="keyword">right</span><span class="operator">&gt;</span> <span class="keyword">JOIN</span> <span class="operator">&lt;</span>right_table<span class="operator">&gt;</span> </span><br><span class="line">      <span class="keyword">ON</span> <span class="operator">&lt;</span>join_condition<span class="operator">&gt;</span> </span><br><span class="line">        <span class="keyword">WHERE</span> <span class="operator">&lt;</span>where_condition<span class="operator">&gt;</span></span><br></pre></td></tr></table></figure><p>可以看到这里有两种条件，分别是<strong>join_condition</strong>和<strong>where_condition</strong>，两者执行存在先后顺序。数据库通过JOIN关键字返回记录时会先生成一张临时表，通过临时表返回记录，<strong>join_condition</strong>是在生成临时表时使用的条件，而<strong>where_condition</strong>是在临时表生成后再对其进行过滤的条件。以<code>LEFT JOIN</code>为例，在生成临时表时无论<strong>join_condition</strong>是否为真都会将左表记录加入到临时表中，所以“左表中的<strong>所有记录</strong>都会被返回”。</p><p>那么<code>CROSS JOIN</code>和<code>STRAIGHT_JOIN</code>又是什么呢？</p><p><code>CROSS JOIN</code>子句从连接的表返回行的笛卡儿积，它的通用查询结构如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">&lt;</span>row_list<span class="operator">&gt;</span> </span><br><span class="line">  <span class="keyword">FROM</span> <span class="operator">&lt;</span>left_table<span class="operator">&gt;</span> </span><br><span class="line">    <span class="keyword">CROSS</span> <span class="keyword">JOIN</span> <span class="operator">&lt;</span>right_table<span class="operator">&gt;</span> </span><br><span class="line">        <span class="keyword">ON</span> <span class="operator">&lt;</span>join_condition<span class="operator">&gt;</span></span><br><span class="line">            <span class="keyword">WHERE</span> <span class="operator">&lt;</span>where_condition<span class="operator">&gt;</span></span><br></pre></td></tr></table></figure><p>注意，仅当不添加<strong>join_condition</strong>和<strong>where_condition</strong>的时候，<code>CROSS JOIN</code>才能返回笛卡尔积，如果添加了这些条件，那么工作方式将和<code>JOIN</code>相同。</p><p>至于<code>STRAIGHT_JOIN</code>，其实是提供给用户一种自主决定驱动表与被驱动表关系的方式，它的用法与<code>JOIN</code>相同，只是<code>STRAIGHT_JOIN</code>前面的表一定是驱动表，后面的表一定是被驱动表。而在MySQL中，<code>JOIN</code>会自动选择小表作为驱动表，大表作为被驱动表。用户可以使用<code>STRAIGHT_JOIN</code>来解决MySQL优化器不能解决的部分。</p><p>关于全外连接以及其他各种连接方式在MySQL中的实现，我找到了一张图，是由Steve Stedman制作的，供读者参考。</p><p><img src="/assets/post_img/article77/MySQL-Join.png" alt="MySQLJoinType"></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1]<a href="https://blog.csdn.net/lamanchas/article/details/121366276">https://blog.csdn.net/lamanchas/article/details/121366276</a><br>[2]<a href="https://blog.csdn.net/asd051377305/article/details/115320564">https://blog.csdn.net/asd051377305/article/details/115320564</a><br>[3]<a href="http://c.biancheng.net/sql/join.html">http://c.biancheng.net/sql/join.html</a><br>[4]<a href="https://cloud.tencent.com/developer/article/1167929">https://cloud.tencent.com/developer/article/1167929</a><br>[5]<a href="https://www.jianshu.com/p/76c90b03b7bd">https://www.jianshu.com/p/76c90b03b7bd</a><br>[6]<a href="https://blog.csdn.net/javaanddonet/article/details/109693672">https://blog.csdn.net/javaanddonet/article/details/109693672</a><br>[7]<a href="https://blog.csdn.net/weixin_37692493/article/details/106970429">https://blog.csdn.net/weixin_37692493/article/details/106970429</a></p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于 <a href="https://silencezheng.top">silencezheng.top</a>，转载请注明出处。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;业精于勤，荒于嬉。&lt;/p&gt;</summary>
    
    
    
    
    <category term="MySQL" scheme="http://silencezheng.top/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>二叉树非递归遍历</title>
    <link href="http://silencezheng.top/2022/11/09/article76/"/>
    <id>http://silencezheng.top/2022/11/09/article76/</id>
    <published>2022-11-09T13:41:12.000Z</published>
    <updated>2022-11-11T02:21:08.306Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>不积跬步，无以至千里。</p><span id="more"></span><p>用Java写一下二叉树的非递归遍历，用print表示操作了，主要关注算法。</p><p>树定义如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TreeNode</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> val;</span><br><span class="line">        TreeNode left;</span><br><span class="line">        TreeNode right;</span><br><span class="line">        TreeNode() &#123;&#125;</span><br><span class="line">        TreeNode(<span class="keyword">int</span> val) &#123; <span class="keyword">this</span>.val = val; &#125;</span><br><span class="line">        TreeNode(<span class="keyword">int</span> val, TreeNode left, TreeNode right) &#123;</span><br><span class="line">          <span class="keyword">this</span>.val = val;</span><br><span class="line">          <span class="keyword">this</span>.left = left;</span><br><span class="line">          <span class="keyword">this</span>.right = right;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h2 id="先序遍历（直觉版）"><a href="#先序遍历（直觉版）" class="headerlink" title="先序遍历（直觉版）"></a>先序遍历（直觉版）</h2><p>思路：根左右，从根节点开始先走到最左下节点，然后依次出栈，如果出栈的节点有右子节点则对右子节点再走到最左下，直至栈空。</p><p>步骤：</p><ol><li>对当前节点走到最左下，每次对当前节点操作+入栈。</li><li>元素出栈，若有右子节点则重复1，若无则重复2。</li><li>栈空结束。</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">preorderTraversal</span><span class="params">(TreeNode root)</span></span>&#123;</span><br><span class="line">        Stack&lt;TreeNode&gt; nodeStack = <span class="keyword">new</span> Stack&lt;&gt;();</span><br><span class="line">        <span class="keyword">if</span>(root == <span class="keyword">null</span>)&#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;Null root error!&quot;</span>);</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            TreeNode temp = root;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> (temp!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                <span class="comment">// 操作</span></span><br><span class="line">                System.out.println(temp.val);</span><br><span class="line">                nodeStack.add(temp);</span><br><span class="line">                temp = temp.left;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> (!nodeStack.isEmpty())&#123;</span><br><span class="line">                temp = nodeStack.pop();</span><br><span class="line">                <span class="keyword">if</span> (temp.right!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                    temp = temp.right;</span><br><span class="line">                    <span class="keyword">while</span> (temp!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                        <span class="comment">// 操作</span></span><br><span class="line">                        System.out.println(temp.val);</span><br><span class="line">                        nodeStack.add(temp);</span><br><span class="line">                        temp = temp.left;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>这个版本的基础上可以进行各种优化。</p><h2 id="中序遍历（直觉版）"><a href="#中序遍历（直觉版）" class="headerlink" title="中序遍历（直觉版）"></a>中序遍历（直觉版）</h2><p>思路：左根右，从根节点走到最左下节点，然后依次出栈并操作，如果出栈的节点有右节点则对右节点再走到最左下，直至栈空。</p><p>步骤：</p><ol><li>对当前节点走到最左下，每次对当前节点入栈。</li><li>元素出栈，操作当前节点，若有右子节点则重复1，若无则重复2。</li><li>栈空结束。</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">inorderTraversal</span><span class="params">(TreeNode root)</span></span>&#123;</span><br><span class="line">        Stack&lt;TreeNode&gt; nodeStack = <span class="keyword">new</span> Stack&lt;&gt;();</span><br><span class="line">        <span class="keyword">if</span>(root == <span class="keyword">null</span>)&#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;Null root error!&quot;</span>);</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            TreeNode temp = root;</span><br><span class="line">            <span class="keyword">while</span> (temp!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                nodeStack.add(temp);</span><br><span class="line">                temp = temp.left;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            temp = nodeStack.pop();</span><br><span class="line">            <span class="keyword">while</span> (temp!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                <span class="comment">// 操作</span></span><br><span class="line">                System.out.println(temp.val);</span><br><span class="line">                <span class="keyword">if</span>(temp.right!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                    temp = temp.right;</span><br><span class="line">                    <span class="keyword">while</span> (temp!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                        nodeStack.add(temp);</span><br><span class="line">                        temp = temp.left;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (nodeStack.isEmpty())&#123;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                temp = nodeStack.pop();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>这个版本的基础上可以进行各种优化。</p><h2 id="后序遍历（直觉版）"><a href="#后序遍历（直觉版）" class="headerlink" title="后序遍历（直觉版）"></a>后序遍历（直觉版）</h2><p>思路：左右根，后序不能采用先序和中序的同款算法的主要原因是判断到当前节点存在右子树时，则不能对当前节点进行操作，而需要先对右子树做后序遍历，而即便是保留当前节点，并把右子树遍历完毕后，再对当前节点进行操作，仍然需要对当前节点的右子树是否已被遍历的状态进行判断，判断的依据是上一次操作的节点是否是右子节点。</p><p>步骤：</p><ol><li>对当前节点走到最左下，每次对当前节点入栈。</li><li>元素出栈，判断当前节点，若无右子树则操作并标记当前节点，若有右子树则判断右子树是否被访问过，若未被访问则保留当前节点状态，并依次入栈右子树左支，标记右子节点；若已访问过则操作当前节点，并标记当前节点。重复2。</li><li>栈空结束。</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">postorderTraversal</span><span class="params">(TreeNode root)</span></span>&#123;</span><br><span class="line">        Stack&lt;TreeNode&gt; nodeStack = <span class="keyword">new</span> Stack&lt;&gt;();</span><br><span class="line">        <span class="keyword">if</span>(root == <span class="keyword">null</span>)&#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;Null root error!&quot;</span>);</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            TreeNode temp = root;</span><br><span class="line">            TreeNode mark = <span class="keyword">null</span>;</span><br><span class="line">            <span class="keyword">while</span> (temp!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                nodeStack.add(temp);</span><br><span class="line">                temp = temp.left;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            temp = nodeStack.pop();</span><br><span class="line">            <span class="keyword">while</span> (temp!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                <span class="keyword">if</span>(temp.right==<span class="keyword">null</span>)&#123;</span><br><span class="line">                    <span class="comment">// 操作</span></span><br><span class="line">                    System.out.println(temp.val);</span><br><span class="line">                    <span class="comment">// 标记当前节点</span></span><br><span class="line">                    mark = temp;</span><br><span class="line">                &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                    <span class="comment">// 当前节点含右子树的情况，判断前次处理节点是否是右子节点。</span></span><br><span class="line">                    <span class="keyword">if</span>(temp.right==mark)&#123;</span><br><span class="line">                        <span class="comment">// 操作</span></span><br><span class="line">                        System.out.println(temp.val);</span><br><span class="line">                        <span class="comment">// 标记当前节点</span></span><br><span class="line">                        mark = temp;</span><br><span class="line">                    &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">                        <span class="comment">// 保留状态</span></span><br><span class="line">                        nodeStack.add(temp);</span><br><span class="line">                        <span class="comment">// 入栈右子树的左支，并标记右子节点。</span></span><br><span class="line">                        temp = temp.right;</span><br><span class="line">                        mark = temp;</span><br><span class="line">                        <span class="keyword">while</span> (temp!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                            nodeStack.add(temp);</span><br><span class="line">                            temp = temp.left;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (nodeStack.isEmpty())&#123;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                temp = nodeStack.pop();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>这个版本的基础上可以进行各种优化。</p><h2 id="层序遍历"><a href="#层序遍历" class="headerlink" title="层序遍历"></a>层序遍历</h2><p>思路：按从上到下，从左到右的顺序遍历。用队列实现，先入当前节点，出队再入左、右两子节点，然后每出一个就入队该节点的左、右子节点，直到队空。</p><p>步骤：</p><ol><li>入队当前节点。</li><li>出队一个节点，入队该节点的左、右子节点，重复2。</li><li>队空结束。</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">layerSequenceTraversal</span><span class="params">(TreeNode root)</span></span>&#123;</span><br><span class="line">        LinkedBlockingQueue&lt;TreeNode&gt; que =  <span class="keyword">new</span> LinkedBlockingQueue&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(root == <span class="keyword">null</span>)&#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;Null root error!&quot;</span>);</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            que.offer(root);</span><br><span class="line">            TreeNode temp = <span class="keyword">null</span>;</span><br><span class="line">            <span class="keyword">while</span> (!que.isEmpty())&#123;</span><br><span class="line">                temp = que.poll();</span><br><span class="line">                System.out.println(temp.val);</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (temp.left!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                    que.offer(temp.left);</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (temp.right!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                    que.offer(temp.right);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>先把我个人认为符合直觉的遍历方法写一下，看起来比较复杂但是容易理解，后面再进行优化补充。</p><p>首发于 <a href="https://silencezheng.top">silencezheng.top</a>。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;不积跬步，无以至千里。&lt;/p&gt;</summary>
    
    
    
    
    <category term="数据结构与算法" scheme="http://silencezheng.top/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>深度学习常用术语解释</title>
    <link href="http://silencezheng.top/2022/11/08/article75/"/>
    <id>http://silencezheng.top/2022/11/08/article75/</id>
    <published>2022-11-08T14:00:16.000Z</published>
    <updated>2022-11-08T14:00:52.846Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>深度学习常用术语解释，持续更新～<br><span id="more"></span></p><h2 id="Backbone"><a href="#Backbone" class="headerlink" title="Backbone"></a>Backbone</h2><p>主干网络，或称骨干网络，通常是网络的一部分，大多时候指的是提取特征的网络，其作用就是提取图片中的信息，供后面的网络使用。这些网络经常使用的是ResNet、VGG等，而不是我们自己设计的网络，因为这些网络已经证明了在分类等问题上的特征提取能力是很强的。在用这些网络作为Backbone的时候，都是直接加载官方已经训练好的模型参数，后面接着我们自己的网络。让网络的这两个部分同时进行训练，因为加载的Backbone模型已经具有提取特征的能力了，在我们的训练过程中，会对它进行微调，使得其更适合于我们自己的任务。</p><h2 id="Head"><a href="#Head" class="headerlink" title="Head"></a>Head</h2><p>Head即整个网络的头部，是获取网络输出内容的网络，利用之前（Backbone）提取的特征，做出预测。</p><h2 id="Neck"><a href="#Neck" class="headerlink" title="Neck"></a>Neck</h2><p>是指放在Backbone和Head之间的层，是为了更好的利用Backbone提取的特征。</p><h2 id="Pretext-task"><a href="#Pretext-task" class="headerlink" title="Pretext task"></a>Pretext task</h2><p>用于预训练的任务，可以翻译为前置任务或代理任务。</p><h2 id="Downstream-task"><a href="#Downstream-task" class="headerlink" title="Downstream task"></a>Downstream task</h2><p>下游任务，用于微调的任务。</p><h2 id="Warm-up"><a href="#Warm-up" class="headerlink" title="Warm up"></a>Warm up</h2><p>用一个小的学习率先训练几个epoch，这是因为网络的参数是随机初始化的，一开始就采用较大的学习率容易数值不稳定。</p><h2 id="End-to-End"><a href="#End-to-End" class="headerlink" title="End to End"></a>End to End</h2><p>端到端，给一个输入，获得一个输出，中间的处理过程处于黑箱中，相当于打包成应用了。</p><h2 id="Standardization"><a href="#Standardization" class="headerlink" title="Standardization"></a>Standardization</h2><p>标准化，指将数据按比例缩放，使其落入一个小区间中，缩放后均值为$0$，方差为$1$。在某些比较和评价的指标处理中经常会用到，去除数据的单位限制，将其转化为无量纲的纯数值，便于不同单位或量级的指标能够进行比较和加权。</p><p>例如在数据预处理时为了将所有特征放在一个共同的尺度上，会通过<strong>将特征重新缩放到零均值和单位方差</strong>来标准化数据。这既能方便优化，又能避免惩罚分配给某一特征的系数超过其他特征（一视同仁）。</p><p>在训练过程中，对输入进行规范化可以加速深度网络权重参数的收敛速度。</p><h2 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h2><p>这个词真的需要好好理解一下，其实首先应该想到翻译为“规范化”，它包括归一化、标准化甚至正则化，作为一个统称。比如Batch Normalization其实做的是Standardization的事，所以翻译成批量规范化或者批量标准化。</p><p>其次这个词又可以指归一化，即把数值放缩到$0$到$1$的小区间中。归一化可以消除量纲对最终结果的影响，使不同变量具有可比性。</p><p>关于这个词的解读是要具体问题具体分析了，甚至有时Standardization也被作为统称。</p><h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><p>正则化，一般形式是在整个平均损失函数的最后增加一个正则项（比如L2范数正则化，也有其他形式的正则化，作用不同）。正则项越大表明惩罚力度越大，等于0表示不做惩罚。</p><h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p>[1]<a href="https://zhuanlan.zhihu.com/p/348800083">https://zhuanlan.zhihu.com/p/348800083</a><br>[2]<a href="https://zhuanlan.zhihu.com/p/343692147">https://zhuanlan.zhihu.com/p/343692147</a><br>[3]<a href="https://blog.csdn.net/u014381464/article/details/81101551">https://blog.csdn.net/u014381464/article/details/81101551</a></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;深度学习常用术语解释，持续更新～&lt;br&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://silencezheng.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="机器学习" scheme="http://silencezheng.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>注意力机制--《动手学深度学习》笔记0x0B</title>
    <link href="http://silencezheng.top/2022/11/07/article74/"/>
    <id>http://silencezheng.top/2022/11/07/article74/</id>
    <published>2022-11-07T07:25:27.000Z</published>
    <updated>2022-11-07T07:27:27.834Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><p>灵长类动物的视觉系统接受了大量的感官输入，这些感官输入远远超过了大脑能够完全处理的程度。然而，并非所有刺激的影响都是相等的。意识的聚集和专注使灵长类动物能够在复杂的视觉环境中将注意力引向感兴趣的物体，例如猎物和天敌。只关注一小部分信息的能力对进化更加有意义，使人类得以生存和成功。</p><p>自19世纪以来，科学家们一直致力于研究认知神经科学领域的注意力。本章的很多章节将涉及到一些研究。<br><span id="more"></span><br>本章首先回顾一个经典注意力框架，解释如何在视觉场景中展开注意力。受此框架中的<em>注意力提示</em>（attention cues）的启发，我们将设计能够利用这些注意力提示的模型。1964年的Nadaraya-Waston核回归（kernel regression）正是具有<em>注意力机制</em>（attention mechanism）的机器学习的简单演示。</p><p>然后继续介绍注意力函数，它们在深度学习的注意力模型设计中被广泛使用。具体来说将展示如何使用这些函数来设计<em>Bahdanau注意力</em>。Bahdanau注意力是深度学习中的具有突破性价值的注意力模型，它双向对齐并且可以微分。</p><p>最后将描述仅仅基于注意力机制的<em>Transformer</em>架构，该架构中使用了<em>多头注意力</em>（multi-head attention）和<em>自注意力</em>（self-attention）。自2017年横空出世，Transformer一直都普遍存在于现代的深度学习应用中，例如语言、视觉、语音和强化学习领域。</p><p>这一章目前只做了解，关于NLP的内容没有实验，也没有详细调查。</p><h3 id="0-1-小结"><a href="#0-1-小结" class="headerlink" title="0.1. 小结"></a>0.1. 小结</h3><ul><li>人类的注意力是有限的、有价值和稀缺的资源。</li><li>受试者使用非自主性和自主性提示有选择性地引导注意力。前者基于突出性，后者则依赖于主体的意识。</li><li>注意力机制与全连接层或者池化层的区别源于增加的自主提示。</li><li>由于包含了自主性提示，注意力机制与全连接的层或池化层不同。</li><li>注意力机制通过注意力池化使选择偏向于值（感官输入），其中包含查询（自主性提示）和键（非自主性提示）。键和值是成对的。</li><li>我们可以可视化查询和键之间的注意力权重。</li><li>Nadaraya-Watson核回归是具有注意力机制的机器学习范例。</li><li>Nadaraya-Watson核回归的注意力池化是对训练数据中输出的加权平均。从注意力的角度来看，分配给每个值的注意力权重取决于将值所对应的键和查询作为输入的函数。</li><li>注意力池化可以分为非参数型和带参数型</li><li>注意力池化的输出可以计算为值的加权平均，选择不同的注意力评分函数会带来不同的注意力池化操作。</li><li>当查询和键是不同长度的矢量时，可以使用<em>加性注意力评分函数</em>。当它们的长度相同时，使用<em>缩放的“点－积”注意力评分函数</em>的计算效率更高。</li><li>在预测词元时，如果不是所有输入词元都是相关的，那么具有Bahdanau注意力的循环神经网络编码器-解码器会有选择地统计输入序列的不同部分。这是通过将上下文变量视为加性注意力池化的输出来实现的。</li><li>在循环神经网络编码器-解码器中，Bahdanau注意力将上一时间步的解码器隐状态视为查询，在所有时间步的编码器隐状态同时视为键和值。</li><li>多头注意力融合了来自于多个注意力池化的不同知识，这些知识的不同来源于相同的查询、键和值的不同的子空间表示。</li><li>基于适当的张量操作，可以实现多头注意力的并行计算。</li><li>在自注意力中，查询、键和值都来自同一组输入。</li><li>卷积神经网络和自注意力都拥有并行计算的优势，而且自注意力的最大路径长度最短。但是因为其计算复杂度是关于序列长度的二次方，所以在很长的序列中计算会非常慢。</li><li>为了使用序列的顺序信息，我们可以通过在输入表示中添加位置编码，来注入绝对的或相对的位置信息。</li><li>transformer是编码器－解码器架构的一个实践，尽管在实际情况中编码器或解码器可以单独使用。</li><li>在transformer中，多头自注意力用于表示输入序列和输出序列，不过解码器必须通过掩蔽机制来保留自回归属性。</li><li>transformer中的残差连接和层规范化是训练非常深度模型的重要工具。</li><li>transformer模型中基于位置的前馈网络使用同一个多层感知机，作用是对所有序列位置的表示进行转换。</li></ul><h2 id="1-注意力提示"><a href="#1-注意力提示" class="headerlink" title="1. 注意力提示"></a>1. 注意力提示</h2><p>感谢读者对本书的关注，因为读者的注意力是一种稀缺的资源：此刻读者正在阅读本书（而忽略了其他的书），因此读者的注意力是用机会成本（与金钱类似）来支付的。为了确保读者现在投入的注意力是值得的，作者们尽全力（全部的注意力）创作一本好书。</p><p>自经济学研究稀缺资源分配以来，人们正处在“注意力经济”时代，即人类的注意力被视为可以交换的、有限的、有价值的且稀缺的商品。许多商业模式也被开发出来去利用这一点：在音乐或视频流媒体服务上，人们要么消耗注意力在广告上，要么付钱来隐藏广告；为了在网络游戏世界的成长，人们要么消耗注意力在游戏战斗中，从而帮助吸引新的玩家，要么付钱立即变得强大。总之，注意力不是免费的。</p><p>注意力是稀缺的，而环境中的干扰注意力的信息却并不少。比如人类的视觉神经系统大约每秒收到$10^8$位的信息，这远远超过了大脑能够完全处理的水平。幸运的是，人类的祖先已经从经验（也称为数据）中认识到“并非感官的所有输入都是一样的”。在整个人类历史中，这种只将注意力引向感兴趣的一小部分信息的能力，使人类的大脑能够更明智地分配资源来生存、成长和社交，例如发现天敌、找寻食物和伴侣。</p><h3 id="1-1-生物学中的注意力提示"><a href="#1-1-生物学中的注意力提示" class="headerlink" title="1.1. 生物学中的注意力提示"></a>1.1. 生物学中的注意力提示</h3><p>注意力是如何应用于视觉世界中的呢？这要从当今十分普及的<em>双组件</em>（two-component）的框架开始讲起：这个框架的出现可以追溯到19世纪90年代的威廉·詹姆斯，他被认为是“美国心理学之父” [<code>James.2007</code>]。在这个框架中，受试者基于<em>非自主性提示</em>和<em>自主性提示</em>有选择地引导注意力的焦点。</p><p>非自主性提示是基于环境中物体的突出性和易见性。想象一下，假如我们面前有五个物品：一份报纸、一篇研究论文、一杯咖啡、一本笔记本和一本书，就像下图。所有纸制品都是黑白印刷的，但咖啡杯是红色的。换句话说，这个咖啡杯在这种视觉环境中是突出和显眼的，不由自主地引起人们的注意。所以我们会把视力最敏锐的地方放到咖啡上，如下图所示。</p><p><img src="/assets/post_img/article74/eye-coffee.svg" alt="由于突出性的非自主性提示（红杯子），注意力不自主地指向了咖啡杯"></p><p>喝咖啡后，我们会变得兴奋并想读书，所以转过头，重新聚焦眼睛，然后看看书，就像下图中描述那样。与上图中由于突出性导致的选择不同，此时选择书是受到了认知和意识的控制，因此注意力在基于自主性提示去辅助选择时将更为谨慎。受试者的主观意愿推动，选择的力量也就更强大。</p><p><img src="/assets/post_img/article74/eye-book.svg" alt="依赖于任务的意志提示（想读一本书），注意力被自主引导到书上"></p><h3 id="1-2-查询、键和值"><a href="#1-2-查询、键和值" class="headerlink" title="1.2. 查询、键和值"></a>1.2. 查询、键和值</h3><p>自主性的与非自主性的注意力提示解释了人类注意力的方式，下面来看看如何通过这两种注意力提示，用神经网络来设计注意力机制的框架。</p><p>首先，对于只使用非自主性提示的情况。要想将选择偏向于感官输入，则可以简单地使用参数化的全连接层，甚至是非参数化的最大池化层或平均池化层。个人理解，这就是说，这种情况下不需要在以往的神经网络上做出修改，因为非自主性提示来自客体的差异。</p><p>因此，“是否包含自主性提示”将<strong>注意力机制</strong>与全连接层或池化层区别开来。在注意力机制的背景下，自主性提示被称为<em>查询</em>（query）。给定任何查询，注意力机制通过<em>注意力池化</em>（attention pooling）将选择引导至<em>感官输入</em>（sensory inputs，例如中间特征表示）。在注意力机制中，这些感官输入被称为<em>值</em>（value）。更通俗的解释是，每个值都与一个感官输入的非自主提示配对，这些对应的非自主性提示称为<em>键</em>（key）。如下图所示，可以通过设计注意力池化的方式，使给定的查询（自主性提示）与键（非自主性提示）进行匹配，这将引导得出最匹配的值（感官输入）。</p><p><img src="/assets/post_img/article74/qkv.svg" alt="注意力机制通过注意力池化将*查询*（自主性提示）和*键*（非自主性提示）结合在一起，实现对*值*（感官输入）的选择倾向"></p><p>鉴于上面所提框架在上图中的主导地位，因此这个框架下的模型将成为本章的中心。然而，注意力机制的设计有许多替代方案。例如可以设计一个不可微的注意力模型，该模型可以使用强化学习方法 [<code>Mnih.Heess.Graves.ea.2014</code>]进行训练。</p><h3 id="1-3-注意力的可视化"><a href="#1-3-注意力的可视化" class="headerlink" title="1.3. 注意力的可视化"></a>1.3. 注意力的可视化</h3><p>平均池化层可以被视为输入的加权平均值，其中各输入的权重是一样的。实际上，注意力池化得到的是加权平均的总和值，其中权重是在给定的查询和不同的键之间计算得出的。</p><p>为了可视化注意力权重，需要定义一个<code>show_heatmaps</code>函数。其输入<code>matrices</code>的形状是（要显示的行数，要显示的列数，查询的数目，键的数目）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_heatmaps</span>(<span class="params">matrices, xlabel, ylabel, titles=<span class="literal">None</span>, figsize=(<span class="params"><span class="number">2.5</span>, <span class="number">2.5</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="function">                  cmap=<span class="string">&#x27;Reds&#x27;</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;显示矩阵热图&quot;&quot;&quot;</span></span><br><span class="line">    d2l.use_svg_display()</span><br><span class="line">    num_rows, num_cols = matrices.shape[<span class="number">0</span>], matrices.shape[<span class="number">1</span>]</span><br><span class="line">    fig, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize,</span><br><span class="line">                                 sharex=<span class="literal">True</span>, sharey=<span class="literal">True</span>, squeeze=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">for</span> i, (row_axes, row_matrices) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(axes, matrices)):</span><br><span class="line">        <span class="keyword">for</span> j, (ax, matrix) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(row_axes, row_matrices)):</span><br><span class="line">            pcm = ax.imshow(matrix.detach().numpy(), cmap=cmap)</span><br><span class="line">            <span class="keyword">if</span> i == num_rows - <span class="number">1</span>:</span><br><span class="line">                ax.set_xlabel(xlabel)</span><br><span class="line">            <span class="keyword">if</span> j == <span class="number">0</span>:</span><br><span class="line">                ax.set_ylabel(ylabel)</span><br><span class="line">            <span class="keyword">if</span> titles:</span><br><span class="line">                ax.set_title(titles[j])</span><br><span class="line">    fig.colorbar(pcm, ax=axes, shrink=<span class="number">0.6</span>);</span><br></pre></td></tr></table></figure><p>下面使用一个简单的例子进行演示，本例中，仅当查询和键相同时（即客体特征符合主体意识时），注意力权重为1，否则为0。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">attention_weights = torch.eye(<span class="number">10</span>).reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">show_heatmaps(attention_weights, xlabel=<span class="string">&#x27;Keys&#x27;</span>, ylabel=<span class="string">&#x27;Queries&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_attention-cues_1.svg" alt="输出"></p><p>后面的章节将经常调用<code>show_heatmaps</code>函数来显示注意力权重。</p><h2 id="2-注意力池化：Nadaraya-Watson-核回归"><a href="#2-注意力池化：Nadaraya-Watson-核回归" class="headerlink" title="2. 注意力池化：Nadaraya-Watson 核回归"></a>2. 注意力池化：Nadaraya-Watson 核回归</h2><p>上节介绍了框架下的注意力机制的主要成分：查询（自主提示）和键（非自主提示）之间的交互形成了注意力池化；注意力池化有选择地聚合了值（感官输入）以生成最终的输出。本节将介绍注意力池化的更多细节，以便从宏观上了解注意力机制在实践中的运作方式。1964年提出的Nadaraya-Watson核回归模型是一个简单但完整的例子，可以用于演示具有注意力机制的机器学习。</p><h3 id="2-1-生成数据集"><a href="#2-1-生成数据集" class="headerlink" title="2.1. 生成数据集"></a>2.1. 生成数据集</h3><p>简单起见，考虑这个回归问题：给定的成对的“输入－输出”数据集${(x_1, y_1), \ldots, (x_n, y_n)}$，如何学习$f$来预测任意新输入$x$的输出$\hat{y} = f(x)$？</p><p>根据下面的非线性函数生成一个人工数据集，其中加入的噪声项为$\epsilon$：</p><script type="math/tex; mode=display">y_i = 2\sin(x_i) + x_i^{0.8} + \epsilon,</script><p>其中$\epsilon$服从均值为$0$和标准差为$0.5$的正态分布。在这里生成了$50$个训练样本和$50$个测试样本。为了更好地可视化之后的注意力模式，需要将训练样本进行排序。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">n_train = <span class="number">50</span>  <span class="comment"># 训练样本数</span></span><br><span class="line"><span class="comment"># torch.sort返回排序后的张量和原张量在排序后张量中的对应索引</span></span><br><span class="line">x_train, _ = torch.sort(torch.rand(n_train) * <span class="number">5</span>)   <span class="comment"># 排序后的训练样本</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * torch.sin(x) + x**<span class="number">0.8</span></span><br><span class="line"></span><br><span class="line">y_train = f(x_train) + torch.normal(<span class="number">0.0</span>, <span class="number">0.5</span>, (n_train,))  <span class="comment"># 训练样本的输出</span></span><br><span class="line">x_test = torch.arange(<span class="number">0</span>, <span class="number">5</span>, <span class="number">0.1</span>)  <span class="comment"># 测试样本</span></span><br><span class="line">y_truth = f(x_test)  <span class="comment"># 测试样本的真实输出</span></span><br><span class="line">n_test = <span class="built_in">len</span>(x_test)  <span class="comment"># 测试样本数</span></span><br></pre></td></tr></table></figure><p>下面的函数将绘制所有的训练样本（样本由圆圈表示），不带噪声项的真实数据生成函数$f$（标记为“Truth”），以及学习得到的预测函数（标记为“Pred”）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_kernel_reg</span>(<span class="params">y_hat</span>):</span></span><br><span class="line">    d2l.plot(x_test, [y_truth, y_hat], <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;y&#x27;</span>, legend=[<span class="string">&#x27;Truth&#x27;</span>, <span class="string">&#x27;Pred&#x27;</span>],</span><br><span class="line">             xlim=[<span class="number">0</span>, <span class="number">5</span>], ylim=[-<span class="number">1</span>, <span class="number">5</span>])</span><br><span class="line">    d2l.plt.plot(x_train, y_train, <span class="string">&#x27;o&#x27;</span>, alpha=<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure><h3 id="2-2-平均池化"><a href="#2-2-平均池化" class="headerlink" title="2.2. 平均池化"></a>2.2. 平均池化</h3><p>先使用最简单的估计器来解决回归问题。基于平均池化来计算所有训练样本输出值的平均值：</p><script type="math/tex; mode=display">f(x) = \frac{1}{n}\sum_{i=1}^n y_i,</script><p>如下图所示，这个估计器确实不够聪明。真实函数$f$（“Truth”）和预测函数（“Pred”）相差很大。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里先求了一下训练集标签的平均值，得到一个单值张量（无维度）。</span></span><br><span class="line"><span class="comment"># torch.repeat_interleave(): 将输入张量按照指定维度进行扩展，若未指定维度则会将输入拉张开为1维向量再进行扩展。</span></span><br><span class="line"><span class="comment"># 这里没有指定dim，故先将单值张量转为1维张量，然后在该维度上复制成n_test个元素。</span></span><br><span class="line">y_hat = torch.repeat_interleave(y_train.mean(), n_test)</span><br><span class="line">plot_kernel_reg(y_hat)</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_nadaraya-waston_1.svg" alt="输出"></p><h3 id="2-3-非参数注意力池化"><a href="#2-3-非参数注意力池化" class="headerlink" title="2.3. 非参数注意力池化"></a>2.3. 非参数注意力池化</h3><p>显然，平均池化忽略了输入$x_i$。于是Nadaraya[<code>Nadaraya.1964</code>]和Watson[<code>Watson.1964</code>]提出了一个更好的想法，根据输入的位置对输出$y_i$进行加权：</p><script type="math/tex; mode=display">f(x) = \sum_{i=1}^n \frac{K(x - x_i)}{\sum_{j=1}^n K(x - x_j)} y_i,</script><p>其中$K$是<em>核</em>（kernel）。上式所描述的估计器被称为<em>Nadaraya-Watson核回归</em>（Nadaraya-Watson kernel regression）。这里不会深入讨论核函数的细节，但受此启发，我们可以从<a href="#12-查询键和值">第一节图中</a>的注意力机制框架的角度重写上式，成为一个更加通用的<em>注意力池化</em>（attention pooling）公式：</p><script type="math/tex; mode=display">f(x) = \sum_{i=1}^n \alpha(x, x_i) y_i,</script><p>其中$x$是查询，$(x_i, y_i)$是键值对。比较两个公式，注意力池化是$y_i$的加权平均。将查询$x$和键$x_i$之间的关系建模为<em>注意力权重</em>（attention weight）$\alpha(x, x_i)$，如上式所示，这个权重将被分配给每一个对应值$y_i$。对于任何查询，模型在所有键值对注意力权重都是一个有效的概率分布：它们是非负的，并且总和为1。</p><p>为了更好地理解注意力池化，考虑一个<em>高斯核</em>（Gaussian kernel），其定义为：</p><script type="math/tex; mode=display">K(u) = \frac{1}{\sqrt{2\pi}} \exp(-\frac{u^2}{2}).</script><p>将高斯核代入<em>Nadaraya-Watson核回归公式</em> 和 <em>注意力池化公式</em> 可以得到：</p><script type="math/tex; mode=display">\begin{aligned} f(x) &=\sum_{i=1}^n \alpha(x, x_i) y_i\\ &= \sum_{i=1}^n \frac{\exp\left(-\frac{1}{2}(x - x_i)^2\right)}{\sum_{j=1}^n \exp\left(-\frac{1}{2}(x - x_j)^2\right)} y_i \\&= \sum_{i=1}^n \mathrm{softmax}\left(-\frac{1}{2}(x - x_i)^2\right) y_i. \end{aligned}</script><p>在上式中，如果一个键$x_i$越是接近给定的查询$x$，那么分配给这个键对应值$y_i$的注意力权重就会越大，也就“获得了更多的注意力”。</p><p>这里穿插解释一下参数模型和非参数模型。</p><p>参数模型<br>: 在统计学中，参数模型通常假设总体服从某个分布，这个分布可以由一些参数确定，如正态分布由均值和标准差确定，在此基础上构建的模型称为参数模型。</p><p>非参数模型<br>: 非参数模型对于总体的分布不做任何假设或者说是数据分布假设自由，只知道其分布是存在的，所以就无法得到其分布的相关参数，只能通过非参数统计的方法进行推断。</p><p>总之，参数模型和非参数模型中的“参数”并不是模型中的参数，而是数据分布的参数。</p><p>Nadaraya-Watson核回归是一个非参数模型。因此，上式是<em>非参数的注意力池化</em>（nonparametric attention pooling）模型。接下来将基于这个非参数的注意力池化模型来绘制预测结果。从绘制的结果会发现新的模型预测线是平滑的，并且比平均池化的预测更接近真实。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># X_repeat的形状:(n_test,n_train),</span></span><br><span class="line"><span class="comment"># 每一行都包含着相同的测试输入（例如：同样的查询）</span></span><br><span class="line">X_repeat = x_test.repeat_interleave(n_train).reshape((-<span class="number">1</span>, n_train))</span><br><span class="line"><span class="comment"># x_train包含着键。attention_weights的形状：(n_test,n_train),</span></span><br><span class="line"><span class="comment"># 每一行都包含着要在给定的每个查询的值（y_train）之间分配的注意力权重</span></span><br><span class="line">attention_weights = nn.functional.softmax(-(X_repeat - x_train)**<span class="number">2</span> / <span class="number">2</span>, dim=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># y_hat的每个元素都是值的加权平均值，其中的权重是注意力权重</span></span><br><span class="line">y_hat = torch.matmul(attention_weights, y_train)</span><br><span class="line">plot_kernel_reg(y_hat)</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_nadaraya-waston_2.svg" alt="输出"></p><p>现在来观察注意力的权重。 这里测试数据的输入相当于查询，而训练数据的输入相当于键。 因为两个输入都是经过排序的，因此由观察可知“查询-键”对越接近，注意力池化的注意力权重就越高。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(attention_weights.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>),</span><br><span class="line">                  xlabel=<span class="string">&#x27;Sorted training inputs&#x27;</span>,</span><br><span class="line">                  ylabel=<span class="string">&#x27;Sorted testing inputs&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_nadaraya-waston_3.svg" alt="输出"></p><h3 id="2-4-带参数注意力池化"><a href="#2-4-带参数注意力池化" class="headerlink" title="2.4. 带参数注意力池化"></a>2.4. 带参数注意力池化</h3><p>非参数的Nadaraya-Watson核回归具有<em>一致性</em>（consistency）的优点：如果有足够的数据，此模型会收敛到最优结果。尽管如此，我们还是可以轻松地将可学习的参数集成到注意力池化中。</p><p>例如，与上一节略有不同，在下面的查询$x$和键$x_i$之间的距离乘以可学习参数$w$：</p><script type="math/tex; mode=display">\begin{aligned}f(x) &= \sum_{i=1}^n \alpha(x, x_i) y_i \\&= \sum_{i=1}^n \frac{\exp\left(-\frac{1}{2}((x - x_i)w)^2\right)}{\sum_{j=1}^n \exp\left(-\frac{1}{2}((x - x_j)w)^2\right)} y_i \\&= \sum_{i=1}^n \mathrm{softmax}\left(-\frac{1}{2}((x - x_i)w)^2\right) y_i \end{aligned}</script><p>本节的余下部分将通过训练这个模型来学习注意力池化的参数。</p><h4 id="2-4-1-批量矩阵乘法"><a href="#2-4-1-批量矩阵乘法" class="headerlink" title="2.4.1. 批量矩阵乘法"></a>2.4.1. 批量矩阵乘法</h4><p>为了更有效地计算小批量数据的注意力，可以利用深度学习开发框架中提供的批量矩阵乘法。</p><p>假设第一个小批量数据包含$n$个矩阵$\mathbf{X}_1,\ldots, \mathbf{X}_n$，第二个小批量包含$n$个矩阵$\mathbf{Y}_1, \ldots, \mathbf{Y}_n$，形状为$a\times b$，形状为$b\times c$。它们的批量矩阵乘法得到$n$个矩阵$\mathbf{X}_1\mathbf{Y}_1, \ldots, \mathbf{X}_n\mathbf{Y}_n$，形状为$a\times c$。因此，假定两个张量的形状分别是$(n,a,b)$和$(n,b,c)$，它们的批量矩阵乘法输出的形状为$(n,a,c)$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X = torch.ones((<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>))</span><br><span class="line">Y = torch.ones((<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>))</span><br><span class="line">torch.bmm(X, Y).shape</span><br></pre></td></tr></table></figure><p>在注意力机制的背景中，我们可以使用小批量矩阵乘法来计算小批量数据中的加权平均值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">weights = torch.ones((<span class="number">2</span>, <span class="number">10</span>)) * <span class="number">0.1</span></span><br><span class="line">values = torch.arange(<span class="number">20.0</span>).reshape((<span class="number">2</span>, <span class="number">10</span>))</span><br><span class="line"><span class="comment"># 权重在第一维升维，值在第二维升维，这里unsqueeze(-1) = unsqueeze(2)</span></span><br><span class="line">torch.bmm(weights.unsqueeze(<span class="number">1</span>), values.unsqueeze(-<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[[ <span class="number">4.5000</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">14.5000</span>]]])</span><br></pre></td></tr></table></figure><h4 id="2-4-2-定义模型"><a href="#2-4-2-定义模型" class="headerlink" title="2.4.2. 定义模型"></a>2.4.2. 定义模型</h4><p>基于上述的带参数的注意力池化，使用小批量矩阵乘法，定义Nadaraya-Watson核回归的带参数版本为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NWKernelRegression</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line">        self.w = nn.Parameter(torch.rand((<span class="number">1</span>,), requires_grad=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, queries, keys, values</span>):</span></span><br><span class="line">        <span class="comment"># queries和attention_weights的形状为(查询个数，“键－值”对个数)</span></span><br><span class="line">        queries = queries.repeat_interleave(keys.shape[<span class="number">1</span>]).reshape((-<span class="number">1</span>, keys.shape[<span class="number">1</span>]))</span><br><span class="line">        self.attention_weights = nn.functional.softmax(</span><br><span class="line">            -((queries - keys) * self.w)**<span class="number">2</span> / <span class="number">2</span>, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># values的形状为(查询个数，“键－值”对个数)</span></span><br><span class="line">        <span class="keyword">return</span> torch.bmm(self.attention_weights.unsqueeze(<span class="number">1</span>),</span><br><span class="line">                         values.unsqueeze(-<span class="number">1</span>)).reshape(-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h4 id="2-4-3-训练"><a href="#2-4-3-训练" class="headerlink" title="2.4.3. 训练"></a>2.4.3. 训练</h4><p>接下来，将训练数据集变换为键和值用于训练注意力模型。 在带参数的注意力池化模型中， 任何一个训练样本的输入都会和除自己以外的所有训练样本的“键－值”对进行计算， 从而得到其对应的预测输出。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># X_tile的形状:(n_train，n_train)，每一行都包含着相同的训练输入</span></span><br><span class="line">X_tile = x_train.repeat((n_train, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># Y_tile的形状:(n_train，n_train)，每一行都包含着相同的训练输出</span></span><br><span class="line">Y_tile = y_train.repeat((n_train, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># eye函数为了生成对角线全1，其余部分全0的二维数组。然后转化为对角线False的矩阵，从X_tile中去除了对角线元素后reshape为新的二维数组。</span></span><br><span class="line"><span class="comment"># keys的形状:(&#x27;n_train&#x27;，&#x27;n_train&#x27;-1)</span></span><br><span class="line">keys = X_tile[(<span class="number">1</span> - torch.eye(n_train)).<span class="built_in">type</span>(torch.<span class="built_in">bool</span>)].reshape((n_train, -<span class="number">1</span>))</span><br><span class="line"><span class="comment"># values的形状:(&#x27;n_train&#x27;，&#x27;n_train&#x27;-1)</span></span><br><span class="line">values = Y_tile[(<span class="number">1</span> - torch.eye(n_train)).<span class="built_in">type</span>(torch.<span class="built_in">bool</span>)].reshape((n_train, -<span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>训练带参数的注意力池化模型时，使用平方损失函数和随机梯度下降。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">net = NWKernelRegression()</span><br><span class="line">loss = nn.MSELoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.5</span>)</span><br><span class="line">animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>, xlim=[<span class="number">1</span>, <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    trainer.zero_grad()</span><br><span class="line">    l = loss(net(x_train, keys, values), y_train)</span><br><span class="line">    l.<span class="built_in">sum</span>().backward()</span><br><span class="line">    trainer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;<span class="built_in">float</span>(l.<span class="built_in">sum</span>()):<span class="number">.6</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    animator.add(epoch + <span class="number">1</span>, <span class="built_in">float</span>(l.<span class="built_in">sum</span>()))</span><br></pre></td></tr></table></figure><p>如下所示，训练完带参数的注意力池化模型后，我们发现： 在尝试拟合带噪声的训练数据时， 预测结果绘制的线不如之前非参数模型的平滑。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># keys的形状:(n_test，n_train)，每一行包含着相同的训练输入（例如，相同的键）</span></span><br><span class="line">keys = x_train.repeat((n_test, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># value的形状:(n_test，n_train)</span></span><br><span class="line">values = y_train.repeat((n_test, <span class="number">1</span>))</span><br><span class="line">y_hat = net(x_test, keys, values).unsqueeze(<span class="number">1</span>).detach()</span><br><span class="line">plot_kernel_reg(y_hat)</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_nadaraya-waston_4.svg" alt="输出"></p><p>为什么新的模型更不平滑了呢？ 来看一下输出结果的绘制图： 与非参数的注意力池化模型相比，带参数的模型加入可学习的参数后， 曲线在注意力权重较大的区域变得更不平滑。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(net.attention_weights.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>),</span><br><span class="line">                  xlabel=<span class="string">&#x27;Sorted training inputs&#x27;</span>,</span><br><span class="line">                  ylabel=<span class="string">&#x27;Sorted testing inputs&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_nadaraya-waston_5.svg" alt="5"></p><h2 id="3-注意力评分函数"><a href="#3-注意力评分函数" class="headerlink" title="3. 注意力评分函数"></a>3. 注意力评分函数</h2><p>上一节中，我们使用高斯核来对查询和键之间的关系建模。可以将其中的高斯核指数部分视为<strong>注意力评分函数</strong>（attention scoring function）， 简称<em>评分函数</em>（scoring function），然后把这个函数的输出结果输入到softmax函数中进行运算。 通过上述步骤，我们将得到与键对应的值的概率分布（即注意力权重）。 最后，注意力池化的输出就是基于这些注意力权重的值的加权和。</p><p>从宏观来看，我们可以使用上述算法来实现<a href="#12-查询键和值">1.2</a>中的注意力机制框架。下图说明了如何将注意力池化的输出计算成为值的加权和，其中 $a$ 表示注意力评分函数。 由于注意力权重是概率分布， 因此加权和其本质上是加权平均值。</p><p><img src="/assets/post_img/article74/attention-output.svg" alt="attention output"></p><p>用数学语言描述，假设有一个查询 $\mathbf{q} \in \mathbb{R}^q$ 和 $m$ 个“键－值”对 $(\mathbf{k}_1, \mathbf{v}_1), \ldots, (\mathbf{k}_m, \mathbf{v}_m)$，其中$\mathbf{k}_i \in \mathbb{R}^k$，$\mathbf{v}_i \in \mathbb{R}^v$。注意力池化函数 $f$ 就被表示成值的加权和：</p><script type="math/tex; mode=display">f(\mathbf{q}, (\mathbf{k}_1, \mathbf{v}_1), \ldots, (\mathbf{k}_m, \mathbf{v}_m)) = \sum_{i=1}^m \alpha(\mathbf{q}, \mathbf{k}_i) \mathbf{v}_i \in \mathbb{R}^v</script><p>其中查询$\mathbf{q}$和键$\mathbf{k}_i$的注意力权重（标量）是通过注意力评分函数$a$将两个向量映射成标量，再经过softmax运算得到的：</p><script type="math/tex; mode=display">\alpha(\mathbf{q}, \mathbf{k}_i) = \mathrm{softmax}(a(\mathbf{q}, \mathbf{k}_i)) = \frac{\exp(a(\mathbf{q}, \mathbf{k}_i))}{\sum_{j=1}^m \exp(a(\mathbf{q}, \mathbf{k}_j))} \in \mathbb{R}</script><p>正如上图所示，选择不同的注意力评分函数 $a$ 会导致不同的注意力池化操作。本节将介绍两个流行的评分函数，稍后将用他们来实现更复杂的注意力机制。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><h3 id="3-1-掩蔽softmax操作"><a href="#3-1-掩蔽softmax操作" class="headerlink" title="3.1. 掩蔽softmax操作"></a>3.1. 掩蔽softmax操作</h3><p>正如上面提到的，softmax操作用于输出一个概率分布作为注意力权重。在某些情况下，并非所有的值都应该被纳入到注意力池化中。例如，为了在“机器翻译与数据集”一节中高效处理小批量数据集，某些文本序列被填充了没有意义的特殊词元。为了仅将有意义的词元作为值来获取注意力池化，可以指定一个有效序列长度（即词元的个数），以便在计算softmax时过滤掉超出指定范围的位置。下面的<code>masked_softmax</code>函数实现了这样的<em>掩蔽softmax操作</em>（masked softmax operation），其中任何超出有效长度的位置都被掩蔽并置为0。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">masked_softmax</span>(<span class="params">X, valid_lens</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;通过在最后一个轴上掩蔽元素来执行softmax操作&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># X:3D张量，valid_lens:1D或2D张量</span></span><br><span class="line">    <span class="keyword">if</span> valid_lens <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> nn.functional.softmax(X, dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        shape = X.shape</span><br><span class="line">        <span class="keyword">if</span> valid_lens.dim() == <span class="number">1</span>:</span><br><span class="line">            valid_lens = torch.repeat_interleave(valid_lens, shape[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            valid_lens = valid_lens.reshape(-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 最后一轴上被掩蔽的元素使用一个非常大的负值替换，从而其softmax输出为0</span></span><br><span class="line">        X = d2l.sequence_mask(X.reshape(-<span class="number">1</span>, shape[-<span class="number">1</span>]), valid_lens,</span><br><span class="line">                              value=-<span class="number">1e6</span>)</span><br><span class="line">        <span class="keyword">return</span> nn.functional.softmax(X.reshape(shape), dim=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>为了演示此函数是如何工作的，考虑由两个$2 \times 4$矩阵表示的样本，这两个样本的有效长度分别为$2$和$3$。经过掩蔽softmax操作，超出有效长度的值都被掩蔽为0。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">masked_softmax(torch.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>), torch.tensor([<span class="number">2</span>, <span class="number">3</span>]))</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[[<span class="number">0.5423</span>, <span class="number">0.4577</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>],</span><br><span class="line">         [<span class="number">0.6133</span>, <span class="number">0.3867</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">0.3324</span>, <span class="number">0.2348</span>, <span class="number">0.4329</span>, <span class="number">0.0000</span>],</span><br><span class="line">         [<span class="number">0.2444</span>, <span class="number">0.3943</span>, <span class="number">0.3613</span>, <span class="number">0.0000</span>]]])</span><br></pre></td></tr></table></figure><p>也可以使用二维张量，为矩阵样本中的每一行指定有效长度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">masked_softmax(torch.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>), torch.tensor([[<span class="number">1</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">4</span>]]))</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[[<span class="number">1.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>],</span><br><span class="line">         [<span class="number">0.4142</span>, <span class="number">0.3582</span>, <span class="number">0.2275</span>, <span class="number">0.0000</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">0.5565</span>, <span class="number">0.4435</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>],</span><br><span class="line">         [<span class="number">0.3305</span>, <span class="number">0.2070</span>, <span class="number">0.2827</span>, <span class="number">0.1798</span>]]])</span><br></pre></td></tr></table></figure><h3 id="3-2-加性注意力"><a href="#3-2-加性注意力" class="headerlink" title="3.2. 加性注意力"></a>3.2. 加性注意力</h3><p>一般来说，当查询和键是<strong>不同长度</strong>的矢量时，可以使用加性注意力作为评分函数。给定查询$\mathbf{q} \in \mathbb{R}^q$和键$\mathbf{k} \in \mathbb{R}^k$，<em>加性注意力</em>（additive attention）的评分函数为</p><script type="math/tex; mode=display">a(\mathbf q, \mathbf k) = \mathbf w_v^\top \text{tanh}(\mathbf W_q\mathbf q + \mathbf W_k \mathbf k) \in \mathbb{R}</script><p>其中可学习的参数是$\mathbf W_q\in\mathbb R^{h\times q}$、$\mathbf W_k\in\mathbb R^{h\times k}$和$\mathbf w_v\in\mathbb R^{h}$。如上式所示，将查询和键连结起来后输入到一个多层感知机（MLP）中，感知机包含一个隐藏层，其隐藏单元数是一个超参数$h$。通过使用$\tanh$作为激活函数，并且禁用偏置项。</p><p>下面来实现加性注意力。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AdditiveAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;加性注意力&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, key_size, query_size, num_hiddens, dropout, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(AdditiveAttention, self).__init__(**kwargs)</span><br><span class="line">        self.W_k = nn.Linear(key_size, num_hiddens, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_q = nn.Linear(query_size, num_hiddens, bias=<span class="literal">False</span>)</span><br><span class="line">        self.w_v = nn.Linear(num_hiddens, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, queries, keys, values, valid_lens</span>):</span></span><br><span class="line">        queries, keys = self.W_q(queries), self.W_k(keys)</span><br><span class="line">        <span class="comment"># 在维度扩展后，</span></span><br><span class="line">        <span class="comment"># queries的形状：(batch_size，查询的个数，1，num_hidden)</span></span><br><span class="line">        <span class="comment"># key的形状：(batch_size，1，“键－值”对的个数，num_hiddens)</span></span><br><span class="line">        <span class="comment"># 使用广播方式进行求和</span></span><br><span class="line">        features = queries.unsqueeze(<span class="number">2</span>) + keys.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        features = torch.tanh(features)</span><br><span class="line">        <span class="comment"># self.w_v仅有一个输出，因此从形状中移除最后那个维度。</span></span><br><span class="line">        <span class="comment"># scores的形状：(batch_size，查询的个数，“键-值”对的个数)</span></span><br><span class="line">        scores = self.w_v(features).squeeze(-<span class="number">1</span>)</span><br><span class="line">        self.attention_weights = masked_softmax(scores, valid_lens)</span><br><span class="line">        <span class="comment"># values的形状：(batch_size，“键－值”对的个数，值的维度)</span></span><br><span class="line">        <span class="keyword">return</span> torch.bmm(self.dropout(self.attention_weights), values)</span><br></pre></td></tr></table></figure><p>用一个小例子来演示上面的<code>AdditiveAttention</code>类，其中查询、键和值的形状为（批量大小，步数或词元序列长度，特征大小），实际输出为$(2,1,20)$、$(2,10,2)$和$(2,10,4)$。注意力池化输出的形状为（批量大小，查询的步数，值的维度）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">queries, keys = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">1</span>, <span class="number">20</span>)), torch.ones((<span class="number">2</span>, <span class="number">10</span>, <span class="number">2</span>))</span><br><span class="line"><span class="comment"># values的小批量，两个值矩阵是相同的</span></span><br><span class="line">values = torch.arange(<span class="number">40</span>, dtype=torch.float32).reshape(<span class="number">1</span>, <span class="number">10</span>, <span class="number">4</span>).repeat(</span><br><span class="line">    <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">valid_lens = torch.tensor([<span class="number">2</span>, <span class="number">6</span>])</span><br><span class="line"></span><br><span class="line">attention = AdditiveAttention(key_size=<span class="number">2</span>, query_size=<span class="number">20</span>, num_hiddens=<span class="number">8</span>,</span><br><span class="line">                              dropout=<span class="number">0.1</span>)</span><br><span class="line">attention.<span class="built_in">eval</span>()</span><br><span class="line">attention(queries, keys, values, valid_lens)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[[ <span class="number">2.0000</span>,  <span class="number">3.0000</span>,  <span class="number">4.0000</span>,  <span class="number">5.0000</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">10.0000</span>, <span class="number">11.0000</span>, <span class="number">12.0000</span>, <span class="number">13.0000</span>]]], grad_fn=&lt;BmmBackward0&gt;)</span><br></pre></td></tr></table></figure><p>尽管加性注意力包含了可学习的参数，但由于本例子中每个键都是相同的， 所以注意力权重是均匀的，由指定的有效长度决定。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(attention.attention_weights.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">10</span>)),</span><br><span class="line">                  xlabel=<span class="string">&#x27;Keys&#x27;</span>, ylabel=<span class="string">&#x27;Queries&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_attention-scoring-functions.svg" alt="oasf"></p><h3 id="3-3-缩放点积注意力"><a href="#3-3-缩放点积注意力" class="headerlink" title="3.3. 缩放点积注意力"></a>3.3. 缩放点积注意力</h3><p>使用点积可以得到计算效率更高的评分函数，但是点积操作要求查询和键具有相同的长度$d$。假设查询和键的所有元素都是独立的随机变量，并且都满足零均值和单位方差，那么两个向量的点积的均值为$0$，方差为$d$。为确保无论向量长度如何，点积的方差在不考虑向量长度的情况下仍然是$1$，我们再将点积除以$\sqrt{d}$，则<em>缩放点积注意力</em>（scaled dot-product attention）评分函数为：</p><script type="math/tex; mode=display">a(\mathbf q, \mathbf k) = \mathbf{q}^\top \mathbf{k}  /\sqrt{d}</script><p>在实践中，我们通常从小批量的角度来考虑提高效率，例如基于$n$个查询和$m$个键－值对计算注意力，其中查询和键的长度为$d$，值的长度为$v$。查询$\mathbf Q\in\mathbb R^{n\times d}$、键$\mathbf K\in\mathbb R^{m\times d}$和值$\mathbf V\in\mathbb R^{m\times v}$的缩放点积注意力是：</p><script type="math/tex; mode=display">\mathrm{softmax}\left(\frac{\mathbf Q \mathbf K^\top }{\sqrt{d}}\right) \mathbf V \in \mathbb{R}^{n\times v}</script><p>下面的缩放点积注意力的实现使用了暂退法进行模型正则化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DotProductAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;缩放点积注意力&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dropout, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(DotProductAttention, self).__init__(**kwargs)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># queries的形状：(batch_size，查询的个数，d)</span></span><br><span class="line">    <span class="comment"># keys的形状：(batch_size，“键－值”对的个数，d)</span></span><br><span class="line">    <span class="comment"># values的形状：(batch_size，“键－值”对的个数，值的维度)</span></span><br><span class="line">    <span class="comment"># valid_lens的形状:(batch_size，)或者(batch_size，查询的个数)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, queries, keys, values, valid_lens=<span class="literal">None</span></span>):</span></span><br><span class="line">        d = queries.shape[-<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 设置transpose_b=True为了交换keys的最后两个维度</span></span><br><span class="line">        scores = torch.bmm(queries, keys.transpose(<span class="number">1</span>,<span class="number">2</span>)) / math.sqrt(d)</span><br><span class="line">        self.attention_weights = masked_softmax(scores, valid_lens)</span><br><span class="line">        <span class="keyword">return</span> torch.bmm(self.dropout(self.attention_weights), values)</span><br></pre></td></tr></table></figure><p>为了演示上述的DotProductAttention类， 我们使用与先前加性注意力例子中相同的键、值和有效长度。 对于点积操作，我们令查询的特征维度与键的特征维度大小相同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">queries = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">attention = DotProductAttention(dropout=<span class="number">0.5</span>)</span><br><span class="line">attention.<span class="built_in">eval</span>()</span><br><span class="line">attention(queries, keys, values, valid_lens)</span><br></pre></td></tr></table></figure><p>与加性注意力演示相同，由于键包含的是相同的元素， 而这些元素无法通过任何查询进行区分，因此获得了均匀的注意力权重。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(attention.attention_weights.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">10</span>)),</span><br><span class="line">                  xlabel=<span class="string">&#x27;Keys&#x27;</span>, ylabel=<span class="string">&#x27;Queries&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_attention-scoring-functions.svg" alt="oasf"></p><h2 id="4-Bahdanau-注意力"><a href="#4-Bahdanau-注意力" class="headerlink" title="4. Bahdanau 注意力"></a>4. Bahdanau 注意力</h2><p>之前章节中探讨了机器翻译问题：通过设计一个基于两个循环神经网络的编码器-解码器架构，用于序列到序列学习（seq2seq）。具体来说，循环神经网络编码器将长度可变的序列转换为固定形状的上下文变量，然后循环神经网络解码器根据生成的词元和上下文变量按词元生成输出（目标）序列词元。然而，即使并非所有输入（源）词元都对解码某个词元都有用，在每个解码步骤中仍使用编码<em>相同</em>的上下文变量。有什么方法能改变上下文变量呢？</p><p>试着从<code>Graves.2013</code>中找到灵感：在为给定文本序列生成手写的挑战中，Graves设计了一种可微注意力模型，将文本字符与更长的笔迹对齐，其中对齐方式仅向一个方向移动。受学习对齐想法的启发，Bahdanau等人提出了一个没有严格单向对齐限制的可微注意力模型 <code>Bahdanau.Cho.Bengio.2014</code>。在预测词元时，如果不是所有输入词元都相关，模型将仅对齐（或参与）输入序列中与当前预测相关的部分。这是通过将上下文变量视为注意力集中的输出来实现的。</p><p>由于这段是在NLP方向上加注意力机制，就粗略浏览一下，没做什么笔记。 —SZ</p><h3 id="4-1-模型"><a href="#4-1-模型" class="headerlink" title="4.1. 模型"></a>4.1. 模型</h3><p>下面描述的Bahdanau注意力模型将遵循之前seq2seq中的相同符号表达。这个新的基于注意力的模型与seq2seq中的模型相同，只不过上下文变量$\mathbf{c}$在任何解码时间步$t’$都会被$\mathbf{c}_{t’}$替换。假设输入序列中有$T$个词元，解码时间步$t’$的上下文变量是注意力集中的输出：</p><script type="math/tex; mode=display">\mathbf{c}_{t'} = \sum_{t=1}^T \alpha(\mathbf{s}_{t' - 1}, \mathbf{h}_t) \mathbf{h}_t</script><p>其中，时间步$t’ - 1$时的解码器隐状态$\mathbf{s}_{t’ - 1}$是查询，编码器隐状态$\mathbf{h}_t$既是键，也是值，注意力权重$\alpha$是使用<em>加性注意力打分函数</em>计算的。</p><p>与之前描述的循环神经网络编码器-解码器架构略有不同，下图描述了Bahdanau注意力的架构。</p><p><img src="/assets/post_img/article74/seq2seq-attention-details.svg" alt="一个带有Bahdanau注意力的循环神经网络编码器-解码器模型"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><h3 id="4-2-定义注意力解码器"><a href="#4-2-定义注意力解码器" class="headerlink" title="4.2. 定义注意力解码器"></a>4.2. 定义注意力解码器</h3><p>下面我们看看如何定义Bahdanau注意力，实现循环神经网络编码器-解码器。 其实，我们只需重新定义解码器即可。 为了更方便地显示学习的注意力权重， 以下AttentionDecoder类定义了带有注意力机制解码器的基本接口。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AttentionDecoder</span>(<span class="params">d2l.Decoder</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;带有注意力机制解码器的基本接口&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(AttentionDecoder, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">attention_weights</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure><p>接下来，让我们在接下来的<code>Seq2SeqAttentionDecoder</code>类中实现带有Bahdanau注意力的循环神经网络解码器。首先，初始化解码器的状态，需要下面的输入：</p><ol><li>编码器在所有时间步的最终层隐状态，将作为注意力的键和值；</li><li>上一时间步的编码器全层隐状态，将作为初始化解码器的隐状态；</li><li>编码器有效长度（排除在注意力池中填充词元）。</li></ol><p>在每个解码时间步骤中，解码器上一个时间步的最终层隐状态将用作查询。<br>因此，注意力输出和输入嵌入都连结为循环神经网络解码器的输入。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Seq2SeqAttentionDecoder</span>(<span class="params">AttentionDecoder</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embed_size, num_hiddens, num_layers,</span></span></span><br><span class="line"><span class="params"><span class="function">                 dropout=<span class="number">0</span>, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Seq2SeqAttentionDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.attention = d2l.AdditiveAttention(</span><br><span class="line">            num_hiddens, num_hiddens, num_hiddens, dropout)</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.rnn = nn.GRU(</span><br><span class="line">            embed_size + num_hiddens, num_hiddens, num_layers,</span><br><span class="line">            dropout=dropout)</span><br><span class="line">        self.dense = nn.Linear(num_hiddens, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_state</span>(<span class="params">self, enc_outputs, enc_valid_lens, *args</span>):</span></span><br><span class="line">        <span class="comment"># outputs的形状为(batch_size，num_steps，num_hiddens).</span></span><br><span class="line">        <span class="comment"># hidden_state的形状为(num_layers，batch_size，num_hiddens)</span></span><br><span class="line">        outputs, hidden_state = enc_outputs</span><br><span class="line">        <span class="keyword">return</span> (outputs.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>), hidden_state, enc_valid_lens)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, state</span>):</span></span><br><span class="line">        <span class="comment"># enc_outputs的形状为(batch_size,num_steps,num_hiddens).</span></span><br><span class="line">        <span class="comment"># hidden_state的形状为(num_layers,batch_size,</span></span><br><span class="line">        <span class="comment"># num_hiddens)</span></span><br><span class="line">        enc_outputs, hidden_state, enc_valid_lens = state</span><br><span class="line">        <span class="comment"># 输出X的形状为(num_steps,batch_size,embed_size)</span></span><br><span class="line">        X = self.embedding(X).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        outputs, self._attention_weights = [], []</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> X:</span><br><span class="line">            <span class="comment"># query的形状为(batch_size,1,num_hiddens)</span></span><br><span class="line">            query = torch.unsqueeze(hidden_state[-<span class="number">1</span>], dim=<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># context的形状为(batch_size,1,num_hiddens)</span></span><br><span class="line">            context = self.attention(</span><br><span class="line">                query, enc_outputs, enc_outputs, enc_valid_lens)</span><br><span class="line">            <span class="comment"># 在特征维度上连结</span></span><br><span class="line">            x = torch.cat((context, torch.unsqueeze(x, dim=<span class="number">1</span>)), dim=-<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 将x变形为(1,batch_size,embed_size+num_hiddens)</span></span><br><span class="line">            out, hidden_state = self.rnn(x.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>), hidden_state)</span><br><span class="line">            outputs.append(out)</span><br><span class="line">            self._attention_weights.append(self.attention.attention_weights)</span><br><span class="line">        <span class="comment"># 全连接层变换后，outputs的形状为</span></span><br><span class="line">        <span class="comment"># (num_steps,batch_size,vocab_size)</span></span><br><span class="line">        outputs = self.dense(torch.cat(outputs, dim=<span class="number">0</span>))</span><br><span class="line">        <span class="keyword">return</span> outputs.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>), [enc_outputs, hidden_state,</span><br><span class="line">                                          enc_valid_lens]</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">attention_weights</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self._attention_weights</span><br></pre></td></tr></table></figure><p>接下来，我们使用包含7个时间步的4个序列输入的小批量测试Bahdanau注意力解码器。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">encoder = d2l.Seq2SeqEncoder(vocab_size=<span class="number">10</span>, embed_size=<span class="number">8</span>, num_hiddens=<span class="number">16</span>,</span><br><span class="line">                             num_layers=<span class="number">2</span>)</span><br><span class="line">encoder.<span class="built_in">eval</span>()</span><br><span class="line">decoder = Seq2SeqAttentionDecoder(vocab_size=<span class="number">10</span>, embed_size=<span class="number">8</span>, num_hiddens=<span class="number">16</span>,</span><br><span class="line">                                  num_layers=<span class="number">2</span>)</span><br><span class="line">decoder.<span class="built_in">eval</span>()</span><br><span class="line">X = torch.zeros((<span class="number">4</span>, <span class="number">7</span>), dtype=torch.long)  <span class="comment"># (batch_size,num_steps)</span></span><br><span class="line">state = decoder.init_state(encoder(X), <span class="literal">None</span>)</span><br><span class="line">output, state = decoder(X, state)</span><br><span class="line">output.shape, <span class="built_in">len</span>(state), state[<span class="number">0</span>].shape, <span class="built_in">len</span>(state[<span class="number">1</span>]), state[<span class="number">1</span>][<span class="number">0</span>].shape</span><br></pre></td></tr></table></figure><h3 id="4-3-训练"><a href="#4-3-训练" class="headerlink" title="4.3. 训练"></a>4.3. 训练</h3><p>在这里指定超参数，实例化一个带有Bahdanau注意力的编码器和解码器， 并对这个模型进行机器翻译训练。 由于新增的注意力机制，训练要比没有注意力机制的慢得多。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">embed_size, num_hiddens, num_layers, dropout = <span class="number">32</span>, <span class="number">32</span>, <span class="number">2</span>, <span class="number">0.1</span></span><br><span class="line">batch_size, num_steps = <span class="number">64</span>, <span class="number">10</span></span><br><span class="line">lr, num_epochs, device = <span class="number">0.005</span>, <span class="number">250</span>, d2l.try_gpu()</span><br><span class="line"></span><br><span class="line">train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)</span><br><span class="line">encoder = d2l.Seq2SeqEncoder(</span><br><span class="line">    <span class="built_in">len</span>(src_vocab), embed_size, num_hiddens, num_layers, dropout)</span><br><span class="line">decoder = Seq2SeqAttentionDecoder(</span><br><span class="line">    <span class="built_in">len</span>(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)</span><br><span class="line">net = d2l.EncoderDecoder(encoder, decoder)</span><br><span class="line">d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)</span><br></pre></td></tr></table></figure><p>模型训练后，我们用它将几个英语句子翻译成法语并计算它们的BLEU分数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">engs = [<span class="string">&#x27;go .&#x27;</span>, <span class="string">&quot;i lost .&quot;</span>, <span class="string">&#x27;he\&#x27;s calm .&#x27;</span>, <span class="string">&#x27;i\&#x27;m home .&#x27;</span>]</span><br><span class="line">fras = [<span class="string">&#x27;va !&#x27;</span>, <span class="string">&#x27;j\&#x27;ai perdu .&#x27;</span>, <span class="string">&#x27;il est calme .&#x27;</span>, <span class="string">&#x27;je suis chez moi .&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> eng, fra <span class="keyword">in</span> <span class="built_in">zip</span>(engs, fras):</span><br><span class="line">    translation, dec_attention_weight_seq = d2l.predict_seq2seq(</span><br><span class="line">        net, eng, src_vocab, tgt_vocab, num_steps, device, <span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;eng&#125;</span> =&gt; <span class="subst">&#123;translation&#125;</span>, &#x27;</span>,</span><br><span class="line">          <span class="string">f&#x27;bleu <span class="subst">&#123;d2l.bleu(translation, fra, k=<span class="number">2</span>):<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">go . =&gt; va !,  bleu <span class="number">1.000</span></span><br><span class="line">i lost . =&gt; j<span class="string">&#x27;ai perdu .,  bleu 1.000</span></span><br><span class="line"><span class="string">he&#x27;</span>s calm . =&gt; je suis ici .,  bleu <span class="number">0.000</span></span><br><span class="line">i<span class="string">&#x27;m home . =&gt; je suis chez moi .,  bleu 1.000</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">attention_weights = torch.cat([step[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>] <span class="keyword">for</span> step <span class="keyword">in</span> dec_attention_weight_seq], <span class="number">0</span>).reshape((</span><br><span class="line">    <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, num_steps))</span><br></pre></td></tr></table></figure><p>训练结束后，通过可视化注意力权重你会发现，每个查询都会在键值对上分配不同的权重，这说明 在每个解码步中，输入序列的不同部分被选择性地聚集在注意力池中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加上一个包含序列结束词元</span></span><br><span class="line">d2l.show_heatmaps(</span><br><span class="line">    attention_weights[:, :, :, :<span class="built_in">len</span>(engs[-<span class="number">1</span>].split()) + <span class="number">1</span>].cpu(),</span><br><span class="line">    xlabel=<span class="string">&#x27;Key positions&#x27;</span>, ylabel=<span class="string">&#x27;Query positions&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_bahdanau-attention_seq.svg" alt="seq"></p><h2 id="5-多头注意力"><a href="#5-多头注意力" class="headerlink" title="5. 多头注意力"></a>5. 多头注意力</h2><p>在实践中，当给定相同的查询、键和值的集合时，我们希望模型可以基于相同的注意力机制学习到不同的行为，然后将不同的行为作为知识组合起来，捕获序列内各种范围的依赖关系（例如，短距离依赖和长距离依赖关系）。因此，允许注意力机制组合使用查询、键和值的不同<em>子空间表示</em>（representation subspaces）可能是有益的。</p><p>为此，与其只使用单独一个注意力池化，我们可以用独立学习得到的$h$组不同的<em>线性投影</em>（linear projections）来变换查询、键和值。然后，这$h$组变换后的查询、键和值将并行地送到注意力池化中。最后，将这$h$个注意力池化的输出拼接在一起，并且通过另一个可以学习的线性投影进行变换，以产生最终输出。这种设计被称为<em>多头注意力</em>（multihead attention）<code>Vaswani.Shazeer.Parmar.ea.2017</code>。对于$h$个注意力池化输出，每一个注意力池化都被称作一个<em>头</em>（head）。 下图展示了使用全连接层来实现可学习的线性变换的多头注意力。</p><p><img src="/assets/post_img/article74/multi-head-attention.svg" alt="多头注意力：多个头连结然后线性变换"></p><h3 id="5-1-模型"><a href="#5-1-模型" class="headerlink" title="5.1. 模型"></a>5.1. 模型</h3><p>在实现多头注意力之前，让我们用数学语言将这个模型形式化地描述出来。给定查询$\mathbf{q} \in \mathbb{R}^{d_q}$、键$\mathbf{k} \in \mathbb{R}^{d_k}$和值$\mathbf{v} \in \mathbb{R}^{d_v}$，每个注意力头$\mathbf{h}_i$（$i = 1, \ldots, h$）的计算方法为：</p><script type="math/tex; mode=display">\mathbf{h}_i = f(\mathbf W_i^{(q)}\mathbf q, \mathbf W_i^{(k)}\mathbf k,\mathbf W_i^{(v)}\mathbf v) \in \mathbb R^{p_v}</script><p>其中，可学习的参数包括$\mathbf W_i^{(q)}\in\mathbb R^{p_q\times d_q}$、$\mathbf W_i^{(k)}\in\mathbb R^{p_k\times d_k}$和$\mathbf W_i^{(v)}\in\mathbb R^{p_v\times d_v}$，以及代表注意力池化的函数$f$。$f$可以是<a href="#3-注意力评分函数">注意力评分函数</a>中的加性注意力和缩放点积注意力。多头注意力的输出需要经过另一个线性转换，它对应着$h$个头连结后的结果，因此其可学习参数是$\mathbf W_o\in\mathbb R^{p_o\times h p_v}$：</p><script type="math/tex; mode=display">\mathbf W_o \begin{bmatrix}\mathbf h_1\\\vdots\\\mathbf h_h\end{bmatrix} \in \mathbb{R}^{p_o}</script><p>基于这种设计，每个头都可能会关注输入的不同部分，可以表示比简单加权平均值更复杂的函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><h3 id="5-2-实现"><a href="#5-2-实现" class="headerlink" title="5.2. 实现"></a>5.2. 实现</h3><p>在实现过程中通常选择缩放点积注意力作为每一个注意力头。为了避免计算代价和参数代价的大幅增长，我们设定$p_q = p_k = p_v = p_o / h$。值得注意的是，如果将查询、键和值的线性变换的输出数量设置为$p_q h = p_k h = p_v h = p_o$，则可以并行计算$h$个头。在下面的实现中，$p_o$是通过参数<code>num_hiddens</code>指定的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;多头注意力&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, key_size, query_size, value_size, num_hiddens,</span></span></span><br><span class="line"><span class="params"><span class="function">                 num_heads, dropout, bias=<span class="literal">False</span>, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttention, self).__init__(**kwargs)</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        self.attention = d2l.DotProductAttention(dropout)</span><br><span class="line">        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)</span><br><span class="line">        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)</span><br><span class="line">        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)</span><br><span class="line">        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, queries, keys, values, valid_lens</span>):</span></span><br><span class="line">        <span class="comment"># queries，keys，values的形状:</span></span><br><span class="line">        <span class="comment"># (batch_size，查询或者“键－值”对的个数，num_hiddens)</span></span><br><span class="line">        <span class="comment"># valid_lens　的形状:</span></span><br><span class="line">        <span class="comment"># (batch_size，)或(batch_size，查询的个数)</span></span><br><span class="line">        <span class="comment"># 经过变换后，输出的queries，keys，values　的形状:</span></span><br><span class="line">        <span class="comment"># (batch_size*num_heads，查询或者“键－值”对的个数，</span></span><br><span class="line">        <span class="comment"># num_hiddens/num_heads)</span></span><br><span class="line">        queries = transpose_qkv(self.W_q(queries), self.num_heads)</span><br><span class="line">        keys = transpose_qkv(self.W_k(keys), self.num_heads)</span><br><span class="line">        values = transpose_qkv(self.W_v(values), self.num_heads)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> valid_lens <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 在轴0，将第一项（标量或者矢量）复制num_heads次，</span></span><br><span class="line">            <span class="comment"># 然后如此复制第二项，然后诸如此类。</span></span><br><span class="line">            valid_lens = torch.repeat_interleave(</span><br><span class="line">                valid_lens, repeats=self.num_heads, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># output的形状:(batch_size*num_heads，查询的个数，</span></span><br><span class="line">        <span class="comment"># num_hiddens/num_heads)</span></span><br><span class="line">        output = self.attention(queries, keys, values, valid_lens)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># output_concat的形状:(batch_size，查询的个数，num_hiddens)</span></span><br><span class="line">        output_concat = transpose_output(output, self.num_heads)</span><br><span class="line">        <span class="keyword">return</span> self.W_o(output_concat)</span><br></pre></td></tr></table></figure><p>为了能够使多个头并行计算，上面的<code>MultiHeadAttention</code>类将使用下面定义的两个转置函数。具体来说，<code>transpose_output</code>函数反转了<code>transpose_qkv</code>函数的操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transpose_qkv</span>(<span class="params">X, num_heads</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;为了多注意力头的并行计算而变换形状&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 输入X的形状:(batch_size，查询或者“键－值”对的个数，num_hiddens)</span></span><br><span class="line">    <span class="comment"># 输出X的形状:(batch_size，查询或者“键－值”对的个数，num_heads，</span></span><br><span class="line">    <span class="comment"># num_hiddens/num_heads)</span></span><br><span class="line">    X = X.reshape(X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>], num_heads, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出X的形状:(batch_size，num_heads，查询或者“键－值”对的个数,</span></span><br><span class="line">    <span class="comment"># num_hiddens/num_heads)</span></span><br><span class="line">    X = X.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 最终输出的形状:(batch_size*num_heads,查询或者“键－值”对的个数,</span></span><br><span class="line">    <span class="comment"># num_hiddens/num_heads)</span></span><br><span class="line">    <span class="keyword">return</span> X.reshape(-<span class="number">1</span>, X.shape[<span class="number">2</span>], X.shape[<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transpose_output</span>(<span class="params">X, num_heads</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;逆转transpose_qkv函数的操作&quot;&quot;&quot;</span></span><br><span class="line">    X = X.reshape(-<span class="number">1</span>, num_heads, X.shape[<span class="number">1</span>], X.shape[<span class="number">2</span>])</span><br><span class="line">    X = X.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">    <span class="keyword">return</span> X.reshape(X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>], -<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>下面我们使用键和值相同的小例子来测试我们编写的MultiHeadAttention类。 多头注意力输出的形状是（batch_size，num_queries，num_hiddens）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens, num_heads = <span class="number">100</span>, <span class="number">5</span></span><br><span class="line">attention = MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens,</span><br><span class="line">                               num_hiddens, num_heads, <span class="number">0.5</span>)</span><br><span class="line">attention.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">MultiHeadAttention(</span><br><span class="line">  (attention): DotProductAttention(</span><br><span class="line">    (dropout): Dropout(p=<span class="number">0.5</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">  )</span><br><span class="line">  (W_q): Linear(in_features=<span class="number">100</span>, out_features=<span class="number">100</span>, bias=<span class="literal">False</span>)</span><br><span class="line">  (W_k): Linear(in_features=<span class="number">100</span>, out_features=<span class="number">100</span>, bias=<span class="literal">False</span>)</span><br><span class="line">  (W_v): Linear(in_features=<span class="number">100</span>, out_features=<span class="number">100</span>, bias=<span class="literal">False</span>)</span><br><span class="line">  (W_o): Linear(in_features=<span class="number">100</span>, out_features=<span class="number">100</span>, bias=<span class="literal">False</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">batch_size, num_queries = <span class="number">2</span>, <span class="number">4</span></span><br><span class="line">num_kvpairs, valid_lens =  <span class="number">6</span>, torch.tensor([<span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">X = torch.ones((batch_size, num_queries, num_hiddens))</span><br><span class="line">Y = torch.ones((batch_size, num_kvpairs, num_hiddens))</span><br><span class="line">attention(X, Y, Y, valid_lens).shape</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">4</span>, <span class="number">100</span>])</span><br></pre></td></tr></table></figure><h2 id="6-自注意力和位置编码"><a href="#6-自注意力和位置编码" class="headerlink" title="6. 自注意力和位置编码"></a>6. 自注意力和位置编码</h2><p>在深度学习中，经常使用卷积神经网络（CNN）或循环神经网络（RNN）对序列进行编码。想象一下，有了注意力机制之后，我们将词元序列输入注意力池化中，以便同一组词元同时充当查询、键和值。具体来说，每个查询都会关注所有的键－值对并生成一个注意力输出。由于查询、键和值来自同一组输入，因此被称为<em>自注意力</em>（self-attention）<code>Lin.Feng.Santos.ea.2017,Vaswani.Shazeer.Parmar.ea.2017</code>，也被称为<em>内部注意力</em>（intra-attention）<code>Cheng.Dong.Lapata.2016,Parikh.Tackstrom.Das.ea.2016,Paulus.Xiong.Socher.2017</code>。本节将使用自注意力进行序列编码，以及如何使用序列的顺序作为补充信息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><h3 id="6-1-自注意力"><a href="#6-1-自注意力" class="headerlink" title="6.1. 自注意力"></a>6.1. 自注意力</h3><p>给定一个由词元组成的输入序列$\mathbf{x}_1, \ldots, \mathbf{x}_n$，其中任意$\mathbf{x}_i \in \mathbb{R}^d$（$1 \leq i \leq n$）。该序列的自注意力输出为一个长度相同的序列<br>$\mathbf{y}_1, \ldots, \mathbf{y}_n$，其中：</p><script type="math/tex; mode=display">\mathbf{y}_i = f(\mathbf{x}_i, (\mathbf{x}_1, \mathbf{x}_1), \ldots, (\mathbf{x}_n, \mathbf{x}_n)) \in \mathbb{R}^d</script><p>根据<a href="#24-带参数注意力池化">2.4</a>中定义的注意力池化函数$f$。下面的代码片段是基于多头注意力对一个张量完成自注意力的计算，张量的形状为（批量大小，时间步的数目或词元序列的长度，$d$）。输出与输入的张量形状相同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens, num_heads = <span class="number">100</span>, <span class="number">5</span></span><br><span class="line">attention = d2l.MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens,</span><br><span class="line">                                   num_hiddens, num_heads, <span class="number">0.5</span>)</span><br><span class="line">attention.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">MultiHeadAttention(</span><br><span class="line">  (attention): DotProductAttention(</span><br><span class="line">    (dropout): Dropout(p=<span class="number">0.5</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">  )</span><br><span class="line">  (W_q): Linear(in_features=<span class="number">100</span>, out_features=<span class="number">100</span>, bias=<span class="literal">False</span>)</span><br><span class="line">  (W_k): Linear(in_features=<span class="number">100</span>, out_features=<span class="number">100</span>, bias=<span class="literal">False</span>)</span><br><span class="line">  (W_v): Linear(in_features=<span class="number">100</span>, out_features=<span class="number">100</span>, bias=<span class="literal">False</span>)</span><br><span class="line">  (W_o): Linear(in_features=<span class="number">100</span>, out_features=<span class="number">100</span>, bias=<span class="literal">False</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">batch_size, num_queries, valid_lens = <span class="number">2</span>, <span class="number">4</span>, torch.tensor([<span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">X = torch.ones((batch_size, num_queries, num_hiddens))</span><br><span class="line">attention(X, X, X, valid_lens).shape</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">4</span>, <span class="number">100</span>])</span><br></pre></td></tr></table></figure><h3 id="6-2-比较卷积神经网络、循环神经网络和自注意力"><a href="#6-2-比较卷积神经网络、循环神经网络和自注意力" class="headerlink" title="6.2. 比较卷积神经网络、循环神经网络和自注意力"></a>6.2. 比较卷积神经网络、循环神经网络和自注意力</h3><p>接下来比较下面几个架构，目标都是将由$n$个词元组成的序列映射到另一个长度相等的序列，其中的每个输入词元或输出词元都由$d$维向量表示。具体来说，将比较的是卷积神经网络、循环神经网络和自注意力这几个架构的计算复杂性、顺序操作和最大路径长度。请注意，顺序操作会妨碍并行计算，而任意的序列位置组合之间的路径越短，则能更轻松地学习序列中的远距离依赖关系 <code>Hochreiter.Bengio.Frasconi.ea.2001</code>。</p><p><img src="/assets/post_img/article74/cnn-rnn-self-attention.svg" alt="比较卷积神经网络（填充词元被忽略）、循环神经网络和自注意力三种架构"></p><p>考虑一个卷积核大小为$k$的卷积层。在后面的章节将提供关于使用卷积神经网络处理序列的更多详细信息。目前只需要知道的是，由于序列长度是$n$，输入和输出的通道数量都是$d$，所以卷积层的计算复杂度为$\mathcal{O}(knd^2)$。如上图所示，卷积神经网络是分层的，因此为有$\mathcal{O}(1)$个顺序操作，最大路径长度为$\mathcal{O}(n/k)$。例如，$\mathbf{x}_1$和$\mathbf{x}_5$处于上图中卷积核大小为3的双层卷积神经网络的感受野内。</p><p>当更新循环神经网络的隐状态时，$d \times d$权重矩阵和$d$维隐状态的乘法计算复杂度为$\mathcal{O}(d^2)$。由于序列长度为$n$，因此循环神经网络层的计算复杂度为$\mathcal{O}(nd^2)$。根据上图，有$\mathcal{O}(n)$个顺序操作无法并行化，最大路径长度也是$\mathcal{O}(n)$。</p><p>在自注意力中，查询、键和值都是$n \times d$矩阵。考虑<a href="#33-缩放点积注意力">3.3</a>中缩放的”点－积“注意力，其中$n \times d$矩阵乘以$d \times n$矩阵。之后输出的$n \times n$矩阵乘以$n \times d$矩阵。因此，自注意力具有$\mathcal{O}(n^2d)$计算复杂性。正如在上图中所讲，每个词元都通过自注意力直接连接到任何其他词元。因此，有$\mathcal{O}(1)$个顺序操作可以并行计算，最大路径长度也是$\mathcal{O}(1)$。</p><p>总而言之，卷积神经网络和自注意力都拥有并行计算的优势，而且自注意力的最大路径长度最短。但是因为其计算复杂度是关于序列长度的二次方，所以在很长的序列中计算会非常慢。</p><h3 id="6-3-位置编码"><a href="#6-3-位置编码" class="headerlink" title="6.3. 位置编码"></a>6.3. 位置编码</h3><p>在处理词元序列时，循环神经网络是逐个的重复地处理词元的，而自注意力则因为并行计算而放弃了顺序操作。为了使用序列的顺序信息，通过在输入表示中添加<em>位置编码</em>（positional encoding）来注入绝对的或相对的位置信息。位置编码可以通过学习得到也可以直接固定得到。接下来描述的是基于正弦函数和余弦函数的固定位置编码<code>Vaswani.Shazeer.Parmar.ea.2017</code>。</p><p>假设输入表示 $\mathbf{X} \in \mathbb{R}^{n \times d}$ 包含一个序列中$n$个词元的$d$维嵌入表示。位置编码使用相同形状的位置嵌入矩阵 $\mathbf{P} \in \mathbb{R}^{n \times d}$输出$\mathbf{X} + \mathbf{P}$，矩阵第$i$行、第$2j$列和$2j+1$列上的元素为：</p><script type="math/tex; mode=display">\begin{aligned} p_{i, 2j} &= \sin\left(\frac{i}{10000^{2j/d}}\right),\\p_{i, 2j+1} &= \cos\left(\frac{i}{10000^{2j/d}}\right).\end{aligned}</script><p>乍一看，这种基于三角函数的设计看起来很奇怪。在解释这个设计之前，让我们先在下面的<code>PositionalEncoding</code>类中实现它。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;位置编码&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_hiddens, dropout, max_len=<span class="number">1000</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="comment"># 创建一个足够长的P</span></span><br><span class="line">        self.P = torch.zeros((<span class="number">1</span>, max_len, num_hiddens))</span><br><span class="line">        X = torch.arange(max_len, dtype=torch.float32).reshape(</span><br><span class="line">            -<span class="number">1</span>, <span class="number">1</span>) / torch.<span class="built_in">pow</span>(<span class="number">10000</span>, torch.arange(</span><br><span class="line">            <span class="number">0</span>, num_hiddens, <span class="number">2</span>, dtype=torch.float32) / num_hiddens)</span><br><span class="line">        self.P[:, :, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(X)</span><br><span class="line">        self.P[:, :, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(X)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        X = X + self.P[:, :X.shape[<span class="number">1</span>], :].to(X.device)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(X)</span><br></pre></td></tr></table></figure><p>在位置嵌入矩阵$\mathbf{P}$中，行代表词元在序列中的位置，列代表位置编码的不同维度。从下面的例子中可以看到位置嵌入矩阵的第$6$列和第$7$列的频率高于第$8$列和第$9$列。第$6$列和第$7$列之间的偏移量（第$8$列和第$9$列相同）是由于正弦函数和余弦函数的交替。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">encoding_dim, num_steps = <span class="number">32</span>, <span class="number">60</span></span><br><span class="line">pos_encoding = PositionalEncoding(encoding_dim, <span class="number">0</span>)</span><br><span class="line">pos_encoding.<span class="built_in">eval</span>()</span><br><span class="line">X = pos_encoding(torch.zeros((<span class="number">1</span>, num_steps, encoding_dim)))</span><br><span class="line">P = pos_encoding.P[:, :X.shape[<span class="number">1</span>], :]</span><br><span class="line">d2l.plot(torch.arange(num_steps), P[<span class="number">0</span>, :, <span class="number">6</span>:<span class="number">10</span>].T, xlabel=<span class="string">&#x27;Row (position)&#x27;</span>,</span><br><span class="line">         figsize=(<span class="number">6</span>, <span class="number">2.5</span>), legend=[<span class="string">&quot;Col %d&quot;</span> % d <span class="keyword">for</span> d <span class="keyword">in</span> torch.arange(<span class="number">6</span>, <span class="number">10</span>)])</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_self-attention-and-positional-encoding.svg" alt="row（pos）"></p><h4 id="6-3-1-绝对位置信息"><a href="#6-3-1-绝对位置信息" class="headerlink" title="6.3.1. 绝对位置信息"></a>6.3.1. 绝对位置信息</h4><p>为了明白沿着编码维度单调降低的频率与绝对位置信息的关系，让我们打印出$0, 1, \ldots, 7$的[<strong>二进制表示</strong>]形式。正如所看到的，每个数字、每两个数字和每四个数字上的比特值在第一个最低位、第二个最低位和第三个最低位上分别交替。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;i&#125;</span>的二进制是：<span class="subst">&#123;i:&gt;03b&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line"><span class="number">0</span>的二进制是：<span class="number">000</span></span><br><span class="line"><span class="number">1</span>的二进制是：001</span><br><span class="line"><span class="number">2</span>的二进制是：010</span><br><span class="line"><span class="number">3</span>的二进制是：011</span><br><span class="line"><span class="number">4</span>的二进制是：<span class="number">100</span></span><br><span class="line"><span class="number">5</span>的二进制是：<span class="number">101</span></span><br><span class="line"><span class="number">6</span>的二进制是：<span class="number">110</span></span><br><span class="line"><span class="number">7</span>的二进制是：<span class="number">111</span></span><br></pre></td></tr></table></figure><p>在二进制表示中，较高比特位的交替频率低于较低比特位， 与下面的热图所示相似，只是位置编码通过使用三角函数在编码维度上降低频率。 由于输出是浮点数，因此此类连续表示比二进制表示法更节省空间。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">P = P[<span class="number">0</span>, :, :].unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>)</span><br><span class="line">d2l.show_heatmaps(P, xlabel=<span class="string">&#x27;Column (encoding dimension)&#x27;</span>,</span><br><span class="line">                  ylabel=<span class="string">&#x27;Row (position)&#x27;</span>, figsize=(<span class="number">3.5</span>, <span class="number">4</span>), cmap=<span class="string">&#x27;Blues&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_self-attention-and-positional-encoding_2.svg" alt=""></p><h4 id="6-3-2-相对位置信息"><a href="#6-3-2-相对位置信息" class="headerlink" title="6.3.2. 相对位置信息"></a>6.3.2. 相对位置信息</h4><p>除了捕获绝对位置信息之外，上述的位置编码还允许模型学习得到输入序列中相对位置信息。这是因为对于任何确定的位置偏移$\delta$，位置$i + \delta$处的位置编码可以线性投影位置$i$处的位置编码来表示。</p><p>这种投影的数学解释是，令$\omega_j = 1/10000^{2j/d}$，对于任何确定的位置偏移$\delta$，<a href="#63-位置编码">位置编码</a>中的任何一对$(p_{i, 2j}, p_{i, 2j+1})$都可以线性投影到$(p_{i+\delta, 2j}, p_{i+\delta, 2j+1})$：</p><script type="math/tex; mode=display">\begin{aligned}&\begin{bmatrix} \cos(\delta \omega_j) & \sin(\delta \omega_j) \\  -\sin(\delta \omega_j) & \cos(\delta \omega_j) \\ \end{bmatrix}\begin{bmatrix} p_{i, 2j} \\  p_{i, 2j+1} \\ \end{bmatrix}\\=&\begin{bmatrix} \cos(\delta \omega_j) \sin(i \omega_j) + \sin(\delta \omega_j) \cos(i \omega_j) \\  -\sin(\delta \omega_j) \sin(i \omega_j) + \cos(\delta \omega_j) \cos(i \omega_j) \\ \end{bmatrix}\\=&\begin{bmatrix} \sin\left((i+\delta) \omega_j\right) \\  \cos\left((i+\delta) \omega_j\right) \\ \end{bmatrix}\\=& \begin{bmatrix} p_{i+\delta, 2j} \\  p_{i+\delta, 2j+1} \\ \end{bmatrix},\end{aligned}</script><p>$2\times 2$投影矩阵不依赖于任何位置的索引$i$。</p><h2 id="7-Transformer"><a href="#7-Transformer" class="headerlink" title="7. Transformer"></a>7. Transformer</h2><p>在<a href="#62-比较卷积神经网络循环神经网络和自注意力">6.2</a>中比较了卷积神经网络（CNN）、循环神经网络（RNN）和自注意力（self-attention）。值得注意的是，自注意力同时具有并行计算和最短的最大路径长度这两个优势。因此，使用自注意力来设计深度架构是很有吸引力的。对比之前仍然依赖循环神经网络实现输入表示的自注意力模型 <code>Cheng.Dong.Lapata.2016,Lin.Feng.Santos.ea.2017,Paulus.Xiong.Socher.2017</code>，Transformer模型完全基于注意力机制，没有任何卷积层或循环神经网络层 <code>Vaswani.Shazeer.Parmar.ea.2017</code>。尽管Transformer最初是应用于在文本数据上的序列到序列学习，但现在已经推广到各种现代的深度学习中，例如语言、视觉、语音和强化学习领域。</p><h3 id="7-1-模型"><a href="#7-1-模型" class="headerlink" title="7.1. 模型"></a>7.1. 模型</h3><p>Transformer作为编码器－解码器架构的一个实例，其整体架构图如下所示。正如所见到的，Transformer是由编码器和解码器组成的。与<a href="#41-模型">带有Bahdanau注意力的循环神经网络编码器-解码器模型</a>中基于Bahdanau注意力实现的序列到序列的学习相比，Transformer的编码器和解码器是基于自注意力的模块叠加而成的，源（输入）序列和目标（输出）序列的<em>嵌入</em>（embedding）表示将加上<em>位置编码</em>（positional encoding），再分别输入到编码器和解码器中。</p><p><img src="/assets/post_img/article74/transformer.svg" alt="transformer架构"></p><p>上图概述了Transformer的架构。从宏观角度来看，Transformer的编码器是由多个相同的层叠加而成的，每个层都有两个子层（子层表示为$\mathrm{sublayer}$）。第一个子层是<em>多头自注意力</em>（multi-head self-attention）池化；第二个子层是<em>基于位置的前馈网络</em>（positionwise feed-forward network）。具体来说，在计算编码器的自注意力时，查询、键和值都来自前一个编码器层的输出。受ResNet中残差网络的启发，每个子层都采用了<em>残差连接</em>（residual connection）。在Transformer中，对于序列中任何位置的任何输入$\mathbf{x} \in \mathbb{R}^d$，都要求满足$\mathrm{sublayer}(\mathbf{x}) \in \mathbb{R}^d$，以便残差连接满足$\mathbf{x} + \mathrm{sublayer}(\mathbf{x}) \in \mathbb{R}^d$。在残差连接的加法计算之后，紧接着应用<em>层规范化</em>（layer normalization）<code>Ba.Kiros.Hinton.2016</code>。因此，输入序列对应的每个位置，Transformer编码器都将输出一个$d$维表示向量。</p><p>Transformer解码器也是由多个相同的层叠加而成的，并且层中使用了残差连接和层规范化。除了编码器中描述的两个子层之外，解码器还在这两个子层之间插入了第三个子层，称为<em>编码器－解码器注意力</em>（encoder-decoder attention）层。在编码器－解码器注意力中，查询来自前一个解码器层的输出，而键和值来自整个编码器的输出。在解码器自注意力中，查询、键和值都来自上一个解码器层的输出。但是，解码器中的每个位置只能考虑该位置之前的所有位置。这种<em>掩蔽</em>（masked）注意力保留了<em>自回归</em>（auto-regressive）属性，确保预测仅依赖于已生成的输出词元。</p><p>在此之前已经描述并实现了基于缩放点积多头注意力和位置编码。接下来将实现Transformer模型的剩余部分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><h3 id="7-2-基于位置的前馈网络"><a href="#7-2-基于位置的前馈网络" class="headerlink" title="7.2. 基于位置的前馈网络"></a>7.2. 基于位置的前馈网络</h3><p>基于位置的前馈网络对序列中的所有位置的表示进行变换时使用的是同一个多层感知机（MLP），这就是称前馈网络是<em>基于位置的</em>（positionwise）的原因。在下面的实现中，输入<code>X</code>的形状（批量大小，时间步数或序列长度，隐单元数或特征维度）将被一个两层的感知机转换成形状为（批量大小，时间步数，<code>ffn_num_outputs</code>）的输出张量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionWiseFFN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;基于位置的前馈网络&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs,</span></span></span><br><span class="line"><span class="params"><span class="function">                 **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PositionWiseFFN, self).__init__(**kwargs)</span><br><span class="line">        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.dense2(self.relu(self.dense1(X)))</span><br></pre></td></tr></table></figure><p>下面的例子显示，改变张量的最里层维度的尺寸，会改变成基于位置的前馈网络的输出尺寸。因为用同一个多层感知机对所有位置上的输入进行变换，所以当所有这些位置的输入相同时，它们的输出也是相同的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ffn = PositionWiseFFN(<span class="number">4</span>, <span class="number">4</span>, <span class="number">8</span>)</span><br><span class="line">ffn.<span class="built_in">eval</span>()</span><br><span class="line">ffn(torch.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)))[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">tensor([[-<span class="number">1.2386</span>, -<span class="number">0.9917</span>,  <span class="number">0.4708</span>, -<span class="number">0.4516</span>, -<span class="number">0.3069</span>,  <span class="number">0.0447</span>,  <span class="number">0.9740</span>,  <span class="number">0.1313</span>],</span><br><span class="line">        [-<span class="number">1.2386</span>, -<span class="number">0.9917</span>,  <span class="number">0.4708</span>, -<span class="number">0.4516</span>, -<span class="number">0.3069</span>,  <span class="number">0.0447</span>,  <span class="number">0.9740</span>,  <span class="number">0.1313</span>],</span><br><span class="line">        [-<span class="number">1.2386</span>, -<span class="number">0.9917</span>,  <span class="number">0.4708</span>, -<span class="number">0.4516</span>, -<span class="number">0.3069</span>,  <span class="number">0.0447</span>,  <span class="number">0.9740</span>,  <span class="number">0.1313</span>]],</span><br><span class="line">       grad_fn=&lt;SelectBackward0&gt;)</span><br></pre></td></tr></table></figure><h3 id="7-3-残差连接和层规范化"><a href="#7-3-残差连接和层规范化" class="headerlink" title="7.3. 残差连接和层规范化"></a>7.3. 残差连接和层规范化</h3><p>现在让我们关注<em>加法和规范化</em>（add&amp;norm）组件。正如在本节开头所述，这是由残差连接和紧随其后的层规范化组成的。两者都是构建有效的深度架构的关键。</p><p>“批量规范化”章节中解释了在一个小批量的样本内基于批量规范化对数据进行重新中心化和重新缩放的调整。层规范化和批量规范化的目标相同，但层规范化是基于特征维度进行规范化。尽管批量规范化在计算机视觉中被广泛应用，但在自然语言处理任务中（输入通常是变长序列）批量规范化通常不如层规范化的效果好。</p><p>以下代码对比不同维度的层规范化和批量规范化的效果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">ln = nn.LayerNorm(<span class="number">2</span>)</span><br><span class="line">bn = nn.BatchNorm1d(<span class="number">2</span>)</span><br><span class="line">X = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">3</span>]], dtype=torch.float32)</span><br><span class="line"><span class="comment"># 在训练模式下计算X的均值和方差</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;layer norm:&#x27;</span>, ln(X), <span class="string">&#x27;\nbatch norm:&#x27;</span>, bn(X))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># layer norm: tensor([[-1.0000,  1.0000],</span></span><br><span class="line"><span class="comment">#         [-1.0000,  1.0000]], grad_fn=&lt;NativeLayerNormBackward0&gt;)</span></span><br><span class="line"><span class="comment"># batch norm: tensor([[-1.0000, -1.0000],</span></span><br><span class="line"><span class="comment">#         [ 1.0000,  1.0000]], grad_fn=&lt;NativeBatchNormBackward0&gt;)</span></span><br></pre></td></tr></table></figure><p>现在我们可以使用残差连接和层规范化来实现AddNorm类。暂退法也被作为正则化方法使用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AddNorm</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;残差连接后进行层规范化&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, normalized_shape, dropout, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(AddNorm, self).__init__(**kwargs)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.ln = nn.LayerNorm(normalized_shape)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, Y</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.ln(self.dropout(Y) + X)</span><br></pre></td></tr></table></figure><p>残差连接要求两个输入的形状相同，以便加法操作后输出张量的形状相同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">add_norm = AddNorm([<span class="number">3</span>, <span class="number">4</span>], <span class="number">0.5</span>)</span><br><span class="line">add_norm.<span class="built_in">eval</span>()</span><br><span class="line">add_norm(torch.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)), torch.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))).shape</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure><h3 id="7-4-编码器"><a href="#7-4-编码器" class="headerlink" title="7.4. 编码器"></a>7.4. 编码器</h3><p>有了组成transformer编码器的基础组件，现在可以先实现编码器中的一个层。下面的EncoderBlock类包含两个子层：多头自注意力和基于位置的前馈网络，这两个子层都使用了残差连接和紧随的层规范化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;transformer编码器块&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, key_size, query_size, value_size, num_hiddens,</span></span></span><br><span class="line"><span class="params"><span class="function">                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span></span></span><br><span class="line"><span class="params"><span class="function">                 dropout, use_bias=<span class="literal">False</span>, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(EncoderBlock, self).__init__(**kwargs)</span><br><span class="line">        self.attention = d2l.MultiHeadAttention(</span><br><span class="line">            key_size, query_size, value_size, num_hiddens, num_heads, dropout,</span><br><span class="line">            use_bias)</span><br><span class="line">        self.addnorm1 = AddNorm(norm_shape, dropout)</span><br><span class="line">        self.ffn = PositionWiseFFN(</span><br><span class="line">            ffn_num_input, ffn_num_hiddens, num_hiddens)</span><br><span class="line">        self.addnorm2 = AddNorm(norm_shape, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, valid_lens</span>):</span></span><br><span class="line">        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))</span><br><span class="line">        <span class="keyword">return</span> self.addnorm2(Y, self.ffn(Y))</span><br></pre></td></tr></table></figure><p>正如我们所看到的，transformer编码器中的任何层都不会改变其输入的形状。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">X = torch.ones((<span class="number">2</span>, <span class="number">100</span>, <span class="number">24</span>))</span><br><span class="line">valid_lens = torch.tensor([<span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">encoder_blk = EncoderBlock(<span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, [<span class="number">100</span>, <span class="number">24</span>], <span class="number">24</span>, <span class="number">48</span>, <span class="number">8</span>, <span class="number">0.5</span>)</span><br><span class="line">encoder_blk.<span class="built_in">eval</span>()</span><br><span class="line">encoder_blk(X, valid_lens).shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># torch.Size([2, 100, 24])</span></span><br></pre></td></tr></table></figure><p>下面实现的Transformer编码器的代码中，堆叠了<code>num_layers</code>个<code>EncoderBlock</code>类的实例。由于这里使用的是值范围在$-1$和$1$之间的固定位置编码，因此通过学习得到的输入的嵌入表示的值需要先乘以嵌入维度的平方根进行重新缩放，然后再与位置编码相加。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerEncoder</span>(<span class="params">d2l.Encoder</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;transformer编码器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, key_size, query_size, value_size,</span></span></span><br><span class="line"><span class="params"><span class="function">                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,</span></span></span><br><span class="line"><span class="params"><span class="function">                 num_heads, num_layers, dropout, use_bias=<span class="literal">False</span>, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(TransformerEncoder, self).__init__(**kwargs)</span><br><span class="line">        self.num_hiddens = num_hiddens</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, num_hiddens)</span><br><span class="line">        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)</span><br><span class="line">        self.blks = nn.Sequential()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            self.blks.add_module(<span class="string">&quot;block&quot;</span>+<span class="built_in">str</span>(i),</span><br><span class="line">                EncoderBlock(key_size, query_size, value_size, num_hiddens,</span><br><span class="line">                             norm_shape, ffn_num_input, ffn_num_hiddens,</span><br><span class="line">                             num_heads, dropout, use_bias))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, valid_lens, *args</span>):</span></span><br><span class="line">        <span class="comment"># 因为位置编码值在-1和1之间，</span></span><br><span class="line">        <span class="comment"># 因此嵌入值乘以嵌入维度的平方根进行缩放，</span></span><br><span class="line">        <span class="comment"># 然后再与位置编码相加。</span></span><br><span class="line">        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))</span><br><span class="line">        self.attention_weights = [<span class="literal">None</span>] * <span class="built_in">len</span>(self.blks)</span><br><span class="line">        <span class="keyword">for</span> i, blk <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.blks):</span><br><span class="line">            X = blk(X, valid_lens)</span><br><span class="line">            self.attention_weights[</span><br><span class="line">                i] = blk.attention.attention.attention_weights</span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><p>下面我们指定了超参数来创建一个两层的transformer编码器。 Transformer编码器输出的形状是（批量大小，时间步数目，num_hiddens）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">encoder = TransformerEncoder(</span><br><span class="line">    <span class="number">200</span>, <span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, [<span class="number">100</span>, <span class="number">24</span>], <span class="number">24</span>, <span class="number">48</span>, <span class="number">8</span>, <span class="number">2</span>, <span class="number">0.5</span>)</span><br><span class="line">encoder.<span class="built_in">eval</span>()</span><br><span class="line">encoder(torch.ones((<span class="number">2</span>, <span class="number">100</span>), dtype=torch.long), valid_lens).shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># torch.Size([2, 100, 24])</span></span><br></pre></td></tr></table></figure><h3 id="7-5-解码器"><a href="#7-5-解码器" class="headerlink" title="7.5. 解码器"></a>7.5. 解码器</h3><p>如模型图所示，Transformer解码器也是由多个相同的层组成。在<code>DecoderBlock</code>类中实现的每个层包含了三个子层：解码器自注意力、“编码器-解码器”注意力和基于位置的前馈网络。这些子层也都被残差连接和紧随的层规范化围绕。</p><p>正如在本节前面所述，在掩蔽多头解码器自注意力层（第一个子层）中，查询、键和值都来自上一个解码器层的输出。关于<em>序列到序列模型</em>（sequence-to-sequence model），在训练阶段，其输出序列的所有位置（时间步）的词元都是已知的；然而，在预测阶段，其输出序列的词元是逐个生成的。因此，在任何解码器时间步中，只有生成的词元才能用于解码器的自注意力计算中。为了在解码器中保留自回归的属性，其掩蔽自注意力设定了参数<code>dec_valid_lens</code>，以便任何查询都只会与解码器中所有已经生成词元的位置（即直到该查询位置为止）进行注意力计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;解码器中第i个块&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, key_size, query_size, value_size, num_hiddens,</span></span></span><br><span class="line"><span class="params"><span class="function">                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span></span></span><br><span class="line"><span class="params"><span class="function">                 dropout, i, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(DecoderBlock, self).__init__(**kwargs)</span><br><span class="line">        self.i = i</span><br><span class="line">        self.attention1 = d2l.MultiHeadAttention(</span><br><span class="line">            key_size, query_size, value_size, num_hiddens, num_heads, dropout)</span><br><span class="line">        self.addnorm1 = AddNorm(norm_shape, dropout)</span><br><span class="line">        self.attention2 = d2l.MultiHeadAttention(</span><br><span class="line">            key_size, query_size, value_size, num_hiddens, num_heads, dropout)</span><br><span class="line">        self.addnorm2 = AddNorm(norm_shape, dropout)</span><br><span class="line">        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens,</span><br><span class="line">                                   num_hiddens)</span><br><span class="line">        self.addnorm3 = AddNorm(norm_shape, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, state</span>):</span></span><br><span class="line">        enc_outputs, enc_valid_lens = state[<span class="number">0</span>], state[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 训练阶段，输出序列的所有词元都在同一时间处理，</span></span><br><span class="line">        <span class="comment"># 因此state[2][self.i]初始化为None。</span></span><br><span class="line">        <span class="comment"># 预测阶段，输出序列是通过词元一个接着一个解码的，</span></span><br><span class="line">        <span class="comment"># 因此state[2][self.i]包含着直到当前时间步第i个块解码的输出表示</span></span><br><span class="line">        <span class="keyword">if</span> state[<span class="number">2</span>][self.i] <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            key_values = X</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            key_values = torch.cat((state[<span class="number">2</span>][self.i], X), axis=<span class="number">1</span>)</span><br><span class="line">        state[<span class="number">2</span>][self.i] = key_values</span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            batch_size, num_steps, _ = X.shape</span><br><span class="line">            <span class="comment"># dec_valid_lens的开头:(batch_size,num_steps),</span></span><br><span class="line">            <span class="comment"># 其中每一行是[1,2,...,num_steps]</span></span><br><span class="line">            dec_valid_lens = torch.arange(</span><br><span class="line">                <span class="number">1</span>, num_steps + <span class="number">1</span>, device=X.device).repeat(batch_size, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            dec_valid_lens = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 自注意力</span></span><br><span class="line">        X2 = self.attention1(X, key_values, key_values, dec_valid_lens)</span><br><span class="line">        Y = self.addnorm1(X, X2)</span><br><span class="line">        <span class="comment"># 编码器－解码器注意力。</span></span><br><span class="line">        <span class="comment"># enc_outputs的开头:(batch_size,num_steps,num_hiddens)</span></span><br><span class="line">        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)</span><br><span class="line">        Z = self.addnorm2(Y, Y2)</span><br><span class="line">        <span class="keyword">return</span> self.addnorm3(Z, self.ffn(Z)), state</span><br></pre></td></tr></table></figure><p>为了便于在“编码器－解码器”注意力中进行缩放点积计算和残差连接中进行加法计算，编码器和解码器的特征维度都是num_hiddens。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">decoder_blk = DecoderBlock(<span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, [<span class="number">100</span>, <span class="number">24</span>], <span class="number">24</span>, <span class="number">48</span>, <span class="number">8</span>, <span class="number">0.5</span>, <span class="number">0</span>)</span><br><span class="line">decoder_blk.<span class="built_in">eval</span>()</span><br><span class="line">X = torch.ones((<span class="number">2</span>, <span class="number">100</span>, <span class="number">24</span>))</span><br><span class="line">state = [encoder_blk(X, valid_lens), valid_lens, [<span class="literal">None</span>]]</span><br><span class="line">decoder_blk(X, state)[<span class="number">0</span>].shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># torch.Size([2, 100, 24])</span></span><br></pre></td></tr></table></figure><p>现在我们构建了由num_layers个DecoderBlock实例组成的完整的transformer解码器。最后，通过一个全连接层计算所有vocab_size个可能的输出词元的预测值。解码器的自注意力权重和编码器解码器注意力权重都被存储下来，方便日后可视化的需要。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerDecoder</span>(<span class="params">d2l.AttentionDecoder</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, key_size, query_size, value_size,</span></span></span><br><span class="line"><span class="params"><span class="function">                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,</span></span></span><br><span class="line"><span class="params"><span class="function">                 num_heads, num_layers, dropout, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(TransformerDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.num_hiddens = num_hiddens</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, num_hiddens)</span><br><span class="line">        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)</span><br><span class="line">        self.blks = nn.Sequential()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            self.blks.add_module(<span class="string">&quot;block&quot;</span>+<span class="built_in">str</span>(i),</span><br><span class="line">                DecoderBlock(key_size, query_size, value_size, num_hiddens,</span><br><span class="line">                             norm_shape, ffn_num_input, ffn_num_hiddens,</span><br><span class="line">                             num_heads, dropout, i))</span><br><span class="line">        self.dense = nn.Linear(num_hiddens, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_state</span>(<span class="params">self, enc_outputs, enc_valid_lens, *args</span>):</span></span><br><span class="line">        <span class="keyword">return</span> [enc_outputs, enc_valid_lens, [<span class="literal">None</span>] * self.num_layers]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, state</span>):</span></span><br><span class="line">        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))</span><br><span class="line">        self._attention_weights = [[<span class="literal">None</span>] * <span class="built_in">len</span>(self.blks) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span> (<span class="number">2</span>)]</span><br><span class="line">        <span class="keyword">for</span> i, blk <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.blks):</span><br><span class="line">            X, state = blk(X, state)</span><br><span class="line">            <span class="comment"># 解码器自注意力权重</span></span><br><span class="line">            self._attention_weights[<span class="number">0</span>][</span><br><span class="line">                i] = blk.attention1.attention.attention_weights</span><br><span class="line">            <span class="comment"># “编码器－解码器”自注意力权重</span></span><br><span class="line">            self._attention_weights[<span class="number">1</span>][</span><br><span class="line">                i] = blk.attention2.attention.attention_weights</span><br><span class="line">        <span class="keyword">return</span> self.dense(X), state</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">attention_weights</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self._attention_weights</span><br></pre></td></tr></table></figure><h3 id="7-6-训练"><a href="#7-6-训练" class="headerlink" title="7.6. 训练"></a>7.6. 训练</h3><p>依照Transformer架构来实例化编码器－解码器模型。在这里，指定Transformer的编码器和解码器都是2层，都使用4头注意力。为了进行序列到序列的学习，下面在“英语－法语”机器翻译数据集上训练Transformer模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens, num_layers, dropout, batch_size, num_steps = <span class="number">32</span>, <span class="number">2</span>, <span class="number">0.1</span>, <span class="number">64</span>, <span class="number">10</span></span><br><span class="line">lr, num_epochs, device = <span class="number">0.005</span>, <span class="number">200</span>, d2l.try_gpu()</span><br><span class="line">ffn_num_input, ffn_num_hiddens, num_heads = <span class="number">32</span>, <span class="number">64</span>, <span class="number">4</span></span><br><span class="line">key_size, query_size, value_size = <span class="number">32</span>, <span class="number">32</span>, <span class="number">32</span></span><br><span class="line">norm_shape = [<span class="number">32</span>]</span><br><span class="line"></span><br><span class="line">train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)</span><br><span class="line"></span><br><span class="line">encoder = TransformerEncoder(</span><br><span class="line">    <span class="built_in">len</span>(src_vocab), key_size, query_size, value_size, num_hiddens,</span><br><span class="line">    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span><br><span class="line">    num_layers, dropout)</span><br><span class="line">decoder = TransformerDecoder(</span><br><span class="line">    <span class="built_in">len</span>(tgt_vocab), key_size, query_size, value_size, num_hiddens,</span><br><span class="line">    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span><br><span class="line">    num_layers, dropout)</span><br><span class="line">net = d2l.EncoderDecoder(encoder, decoder)</span><br><span class="line">d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)</span><br></pre></td></tr></table></figure><p>训练结束后，使用transformer模型将一些英语句子翻译成法语，并且计算它们的BLEU分数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">engs = [<span class="string">&#x27;go .&#x27;</span>, <span class="string">&quot;i lost .&quot;</span>, <span class="string">&#x27;he\&#x27;s calm .&#x27;</span>, <span class="string">&#x27;i\&#x27;m home .&#x27;</span>]</span><br><span class="line">fras = [<span class="string">&#x27;va !&#x27;</span>, <span class="string">&#x27;j\&#x27;ai perdu .&#x27;</span>, <span class="string">&#x27;il est calme .&#x27;</span>, <span class="string">&#x27;je suis chez moi .&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> eng, fra <span class="keyword">in</span> <span class="built_in">zip</span>(engs, fras):</span><br><span class="line">    translation, dec_attention_weight_seq = d2l.predict_seq2seq(</span><br><span class="line">        net, eng, src_vocab, tgt_vocab, num_steps, device, <span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;eng&#125;</span> =&gt; <span class="subst">&#123;translation&#125;</span>, &#x27;</span>,</span><br><span class="line">          <span class="string">f&#x27;bleu <span class="subst">&#123;d2l.bleu(translation, fra, k=<span class="number">2</span>):<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># go . =&gt; va !,  bleu 1.000</span></span><br><span class="line"><span class="comment"># i lost . =&gt; je suis avons été battues .,  bleu 0.000</span></span><br><span class="line"><span class="comment"># he&#x27;s calm . =&gt; il est malade .,  bleu 0.658</span></span><br><span class="line"><span class="comment"># i&#x27;m home . =&gt; je suis chez moi .,  bleu 1.000</span></span><br></pre></td></tr></table></figure><p>当进行最后一个英语到法语的句子翻译工作时，让我们可视化transformer的注意力权重。编码器自注意力权重的形状为（编码器层数，注意力头数，num_steps或查询的数目，num_steps或“键－值”对的数目）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">enc_attention_weights = torch.cat(net.encoder.attention_weights, <span class="number">0</span>).reshape((num_layers, num_heads,</span><br><span class="line">    -<span class="number">1</span>, num_steps))</span><br><span class="line">enc_attention_weights.shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># torch.Size([2, 4, 10, 10])</span></span><br></pre></td></tr></table></figure><p>在编码器的自注意力中，查询和键都来自相同的输入序列。因为填充词元是不携带信息的，因此通过指定输入序列的有效长度可以避免查询与使用填充词元的位置计算注意力。接下来，将逐行呈现两层多头注意力的权重。每个注意力头都根据查询、键和值的不同的表示子空间来表示不同的注意力。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(</span><br><span class="line">    enc_attention_weights.cpu(), xlabel=<span class="string">&#x27;Key positions&#x27;</span>,</span><br><span class="line">    ylabel=<span class="string">&#x27;Query positions&#x27;</span>, titles=[<span class="string">&#x27;Head %d&#x27;</span> % i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">5</span>)],</span><br><span class="line">    figsize=(<span class="number">7</span>, <span class="number">3.5</span>))</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_transformer_1.svg" alt="o1"></p><p>为了可视化解码器的自注意力权重和“编码器－解码器”的注意力权重，我们需要完成更多的数据操作工作。例如用零填充被掩蔽住的注意力权重。值得注意的是，解码器的自注意力权重和“编码器－解码器”的注意力权重都有相同的查询：即以<em>序列开始词元</em>（beginning-of-sequence,BOS）打头，再与后续输出的词元共同组成序列。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">dec_attention_weights_2d = [head[<span class="number">0</span>].tolist()</span><br><span class="line">                            <span class="keyword">for</span> step <span class="keyword">in</span> dec_attention_weight_seq</span><br><span class="line">                            <span class="keyword">for</span> attn <span class="keyword">in</span> step <span class="keyword">for</span> blk <span class="keyword">in</span> attn <span class="keyword">for</span> head <span class="keyword">in</span> blk]</span><br><span class="line">dec_attention_weights_filled = torch.tensor(</span><br><span class="line">    pd.DataFrame(dec_attention_weights_2d).fillna(<span class="number">0.0</span>).values)</span><br><span class="line">dec_attention_weights = dec_attention_weights_filled.reshape((-<span class="number">1</span>, <span class="number">2</span>, num_layers, num_heads, num_steps))</span><br><span class="line">dec_self_attention_weights, dec_inter_attention_weights = \</span><br><span class="line">    dec_attention_weights.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">4</span>)</span><br><span class="line">dec_self_attention_weights.shape, dec_inter_attention_weights.shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># (torch.Size([2, 4, 6, 10]), torch.Size([2, 4, 6, 10]))</span></span><br></pre></td></tr></table></figure><p>由于解码器自注意力的自回归属性，查询不会对当前位置之后的“键－值”对进行注意力计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plusonetoincludethebeginning-of-sequencetoken</span></span><br><span class="line">d2l.show_heatmaps(</span><br><span class="line">    dec_self_attention_weights[:, :, :, :<span class="built_in">len</span>(translation.split()) + <span class="number">1</span>],</span><br><span class="line">    xlabel=<span class="string">&#x27;Key positions&#x27;</span>, ylabel=<span class="string">&#x27;Query positions&#x27;</span>,</span><br><span class="line">    titles=[<span class="string">&#x27;Head %d&#x27;</span> % i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">5</span>)], figsize=(<span class="number">7</span>, <span class="number">3.5</span>))</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_transformer_2.svg" alt="o2"></p><p>与编码器的自注意力的情况类似，通过指定输入序列的有效长度，输出序列的查询不会与输入序列中填充位置的词元进行注意力计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(</span><br><span class="line">    dec_inter_attention_weights, xlabel=<span class="string">&#x27;Key positions&#x27;</span>,</span><br><span class="line">    ylabel=<span class="string">&#x27;Query positions&#x27;</span>, titles=[<span class="string">&#x27;Head %d&#x27;</span> % i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">5</span>)],</span><br><span class="line">    figsize=(<span class="number">7</span>, <span class="number">3.5</span>))</span><br></pre></td></tr></table></figure><p><img src="/assets/post_img/article74/output_transformer_3.svg" alt="o3"></p><p>尽管transformer架构是为了“序列到序列”的学习而提出的，但正如我们将在本书后面提及的那样，transformer编码器或transformer解码器通常被单独用于不同的深度学习任务中。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;p&gt;灵长类动物的视觉系统接受了大量的感官输入，这些感官输入远远超过了大脑能够完全处理的程度。然而，并非所有刺激的影响都是相等的。意识的聚集和专注使灵长类动物能够在复杂的视觉环境中将注意力引向感兴趣的物体，例如猎物和天敌。只关注一小部分信息的能力对进化更加有意义，使人类得以生存和成功。&lt;/p&gt;
&lt;p&gt;自19世纪以来，科学家们一直致力于研究认知神经科学领域的注意力。本章的很多章节将涉及到一些研究。&lt;br&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://silencezheng.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>导向滤波</title>
    <link href="http://silencezheng.top/2022/11/04/article73/"/>
    <id>http://silencezheng.top/2022/11/04/article73/</id>
    <published>2022-11-04T13:27:06.000Z</published>
    <updated>2022-11-04T13:28:52.059Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>导向滤波学习，也可以说是论文笔记了。<br><span id="more"></span></p><h2 id="各向同性（isotropy）滤波与各向异性（anisotropy）滤波"><a href="#各向同性（isotropy）滤波与各向异性（anisotropy）滤波" class="headerlink" title="各向同性（isotropy）滤波与各向异性（anisotropy）滤波"></a>各向同性（isotropy）滤波与各向异性（anisotropy）滤波</h2><p>对图像来说，各向同性滤波就是说滤波器在各方向上的梯度变化是相同的，对各个方向一视同仁的进行滤波。各向异性滤波是指滤波器会对各方向进行区别对待，有选择的进行滤波。</p><p>高斯滤波属于<strong>各向同性滤波</strong>，根据二维高斯的图像就可以看出。</p><p>双边滤波属于<strong>各向异性滤波</strong>，因为它会根据图像梯度变化情况改变滤波器形状。</p><h2 id="导向滤波（Guided-Filter）"><a href="#导向滤波（Guided-Filter）" class="headerlink" title="导向滤波（Guided Filter）"></a>导向滤波（Guided Filter）</h2><p>导向滤波也属于各向异性滤波，也可以作为一种保边（Edge-preserving）滤波算法。</p><blockquote><p>Derived from a local linear model, the guided filter generates the filtering output by considering the content of a guidance image, which can be the input image itself or another different image.</p><p>We demonstrate that the guided filter is both effective and efficient in a great variety of computer vision and computer graphics applications including noise reduction, detail smoothing/enhancement, HDR compression, image matting/feathering, haze removal, and joint upsampling.</p></blockquote><p>导向滤波通过考虑<strong>引导图像</strong>的内容来生成滤波输出，引导图像可以是输入图像本身或另一个不同的图像。[1]中提到了双边滤波相比于导向滤波的两点缺陷，“have unwanted gradient reversal artifacts near edges”和难以进行维持精度的快速计算，这也是导向滤波的优势。</p><p>文中还提到了<strong>联合双边滤波器</strong>（joint bilateral filter），它也是利用了引导图来改善双边滤波<strong>权值不稳定</strong>的问题（双边滤波边缘出现<strong>梯度翻转</strong>现象的原因）。</p><p>文中先定义了一个<strong>通用的平移不变的线性滤波过程</strong>，包括一个引导图像 $\textit{I}$、一个输入图像 $\textit{p}$ 和一个输出图像 $\textit{q}$。</p><script type="math/tex; mode=display">\begin{equation}q_i=\sum_j W_{i j}(I) p_j \tag{1}\end{equation}</script><p>这里 $i$ 和 $j$ 表示的是两个像素，而不是像素的横纵坐标！像素 $i$ 的位置坐标表示为 $\mathbb{x}_i$，$q_i$ 表示输出图的像素 $i$ 处的值，滤波核 $W_{i j}(I)$ 是关于导向图和输入图的函数，当然也与 $i$ 有关。</p><p>联合双边滤波器符合上述的线性滤波过程，其滤波核 $W^{\mathrm{bf}}$ 如下：</p><script type="math/tex; mode=display">\begin{equation}W_{i j}^{\mathrm{bf}}(I)=\frac{1}{K_i} \exp \left(-\frac{\left|\mathbf{x}_i-\mathbf{x}_j\right|^2}{\sigma_{\mathrm{s}}^2}\right) \exp \left(-\frac{\left|I_i-I_j\right|^2}{\sigma_{\mathrm{r}}^2}\right) \tag{2}\end{equation}</script><p>其中，$K_i$是归一化参数，用于保证$\sum_j W_{i j}^{\mathrm{bf}} = 1$，$\sigma_{\mathrm{s}}$ 和 $\sigma_{\mathrm{r}}$ 分别代表空域和值域（原文说值可以是 intensity 或 color）的对应参数，分别用于调整空域和值域的滤波程度（原文为similarity，我理解就和高斯中的标准差类似）。原文说当导向图和输入图相同时，联合双边滤波就退化为双边滤波，我理解里的双边滤波应该是要加两个2倍在分母的，但是效果应该一样。</p><p>下面简单讨论下导向滤波的推导，主要还是学习如何计算。 文中首先假设导向滤波是一个导向图 $I$ 和 输出图 $q$ 间的<strong>局部线性模型</strong>。令 $w_k$ 是 $I$ 中的一个正方形窗口，其中心为像素 $k$，输出 $q$ 为 $I$ 在 $w_k$ 上的一个线性变换：</p><script type="math/tex; mode=display">\begin{equation}q_i=a_k I_i+b_k, \forall i \in \omega_k \tag{3}\end{equation}</script><p>这个稍微想一下就可以理解，假设 $w_k$ 是一个在 $I$ 上移动的九宫格，由于 $q$、$I$ 和 $p$都是尺寸一致的，那么每次移动产生的九个线性变化值就是 $q$ 对应 $I$ 位置上的值。当然这会引出一个问题，即同一个像素可能会被不同的窗口计算出多个值，如何确定最终输出的 $q_i$ 呢？文中提出了一种简单的处理办法，即取所有这些输出的平均。</p><p>$a_k$ 和 $b_k$ 是窗口 $k$ 中的常量线性系数，顺便一提 $w_k$ 的半径定义为 $r$。 关于为何使用局部线性模型，是因为它确保了输出和导向图的边缘一致，具体可以看原文。</p><p>为确定上述的两个线性系数，原文将输出建模为输入减去不想要的内容（噪声、纹理等）:</p><script type="math/tex; mode=display">q_i = p_i - n_i</script><p>然后通过最小化输入和输出的差异来实现，具体来说，最小化如下函数(cost function in $w_k$)：</p><script type="math/tex; mode=display">\begin{equation}E\left(a_k, b_k\right)=\sum_{i \in \omega_k}\left(\left(a_k I_i+b_k-p_i\right)^2+\epsilon a_k^2\right) \tag{4}\end{equation}</script><p>其中 $\epsilon$ 是正则化参数，用于防止 $a_k$ 变得太大。通过线性回归求解$(4)$可得如下：</p><script type="math/tex; mode=display">\begin{equation}a_k=\frac{\frac{1}{|\omega|} \sum_{i \in \omega_k} I_i p_i-\mu_k \bar{p}_k}{\sigma_k^2+\epsilon} \tag{5}\end{equation}</script><script type="math/tex; mode=display">\begin{equation}b_k=\bar{p}_k-a_k \mu_k \tag{6}\end{equation}</script><p>其中 $\mu_k$ 和 $\sigma_k^2$ 分别是导向图在窗口 $k$ 中部分的均值和方差（像素值），$|\omega|$ 是窗口 $k$ 中的像素数，$\bar{p}_k$ 是 输入图 $p$ 在窗口 $k$ 中部分的均值，表示为 $\bar{p}_k=\frac{1}{|\omega|} \sum_{i \in \omega_k} p_i$。</p><p>解释完各个符号的含义，让我们思考将上述的局部线性模型<strong>应用在整幅图像上</strong>，将图像中每一个能放置 $w_k$ 的区域进行运算（考虑将 $I$ 和 $p$ 叠放），并采用之前提到的取均值的方式，可得到滤波器输出如下：</p><script type="math/tex; mode=display">\begin{equation}q_i =\frac{1}{|\omega|} \sum_{k: i \in \omega_k}\left(a_k I_i+b_k\right) \tag{7}\end{equation}</script><script type="math/tex; mode=display">\begin{equation}=\bar{a}_i I_i+\bar{b}_i \tag{8}\end{equation}</script><p>其中$\bar{a}_i=\frac{1}{|\omega|} \sum_{k \in \omega_i} a_k$，$\bar{b}_i=\frac{1}{|\omega|} \sum_{k \in \omega_i} b_k$。</p><p>下面原文对上述输出的保边性进行了一些论证，并指出$(5), (6), (8)$中的关系是在图像滤波中真实存在的，并且这三个关系（公式）可以分别重写为输入的加权和，如下：</p><script type="math/tex; mode=display">a_k=\sum_j A_{k j}(I) p_j \\b_k=\sum_j B_{k j}(I) p_j \\q_i=\sum_j W_{i j}(I) p_j</script><p>其中 $A_{i j}, B_{i j}, W_{i j}$ 为三个仅依赖于导向图 $I$ 的权重，可以看到最终推出了一个输出和导向图与输入的<strong>平移不变线性滤波</strong>：$q_i=\sum_j W_{i j}(I) p_j$。那么我们只需要关心权重如何计算即可：</p><script type="math/tex; mode=display">\begin{equation}W_{i j}(I)=\frac{1}{|\omega|^2} \sum_{k:(i, j) \in \omega_k}\left(1+\frac{\left(I_i-\mu_k\right)\left(I_j-\mu_k\right)}{\sigma_k^2+\epsilon}\right) \tag{9}\end{equation}</script><p>解读一下这个权重，对于给定的输出像素 $i$，我们先寻找重叠图像（$I$ 和 $p$）中包含该像素的所有窗口 $k$，对于每个窗口 $k$，在 $I$ 上计算 $\mu_k$ 和 $\sigma_k^2$ 后代入计算权重，然后求得加权和。由于可以证明 $\sum_j W_{i j}(I)=1$，所以不需要对权重再进行归一化处理。</p><p>总之，导向滤波符合一个平移不变的线性滤波模型，其权重通过导向图计算，最终结合输入图产生输出。</p><p><img src="/assets/post_img/article73/idea.jpeg" alt="idea"></p><h2 id="计算方式"><a href="#计算方式" class="headerlink" title="计算方式"></a>计算方式</h2><p>$(5), (6), (8)$ 的三个方程是导向滤波器的一种定义，令 $f_{mean}$ 表示半径为 $r$ 的均值滤波器，correlation（corr，<strong>相关系数</strong>），variance（var，<strong>方差</strong>）和covariance（cov，<strong>协方差</strong>）分别表示其本身含义。则可给出算法的伪代码如下：</p><p><img src="/assets/post_img/article73/pseudocode-normal.png" alt="normal"></p><p>算法中出现的<code>./ .*</code>等分别代表逐像素除、逐像素乘等操作。</p><h2 id="个人的一点理解"><a href="#个人的一点理解" class="headerlink" title="个人的一点理解"></a>个人的一点理解</h2><p>文中总共提出了两种对导向滤波的定义方式，先是得到全图范围上的线性模型 $q_i=\bar{a}_i I_i+\bar{b}_i$，后又重新整理成线性滤波的形式 $q_i=\sum_j W_{i j}(I) p_j$，并求出了仅依赖导向图的滤波核权重。然后从线性模型和滤波核的角度分别分析了导向滤波Edge-Preserving的性质，考虑 $I \equiv p$ 的情况。</p><h2 id="快速导向滤波（Fast-Guided-Filter）"><a href="#快速导向滤波（Fast-Guided-Filter）" class="headerlink" title="快速导向滤波（Fast Guided Filter）"></a>快速导向滤波（Fast Guided Filter）</h2><p>还没看[3]，先贴一个伪代码，日后需要再回来整理。</p><p><img src="/assets/post_img/article73/pseudocode-fast.png" alt="fast"></p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]Kaiming He, Jian Sun, Xiaoou Tang, Guided Image Filtering. IEEE Transactions on Pattern Analysis and Machine Intelligence, Volume 35, Issue 6, pp. 1397-1409, June 2013<br>[2]<a href="https://zhuanlan.zhihu.com/p/161666126">https://zhuanlan.zhihu.com/p/161666126</a><br>[3]<a href="https://arxiv.org/abs/1505.00996v1">https://arxiv.org/abs/1505.00996v1</a><br>[4]<a href="http://kaiminghe.com/eccv10/">http://kaiminghe.com/eccv10/</a><br>[5]Guided Image Filtering, by Kaiming He, Jian Sun, and Xiaoou Tang, in ECCV 2010 (Oral).</p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于 <a href="https://silencezheng.top">silencezheng.top</a>。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;导向滤波学习，也可以说是论文笔记了。&lt;br&gt;</summary>
    
    
    
    
    <category term="计算机视觉" scheme="http://silencezheng.top/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>目标检测常用评价指标</title>
    <link href="http://silencezheng.top/2022/10/24/article72/"/>
    <id>http://silencezheng.top/2022/10/24/article72/</id>
    <published>2022-10-23T16:50:28.000Z</published>
    <updated>2022-10-23T16:52:21.268Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>关于目标检测领域的常用评价指标进行一些个人解读，欢迎批评指正。</p><p>评价指标系列第二篇～上一次说了语义分割的内容，这次来搞目标检测，看了十余篇内容后终于搞明白些了，记录一下。本文说的内容比较浅，一方面是笔者水平实在菜鸡，另一方面写文的主要目的是让不了解目标检测的人读完能上手去做，而不用查三十个资料后再开始，就像博客的Slogan一样，“帮你在学习技术的路上节约半小时”。<br><span id="more"></span></p><h2 id="前置理解"><a href="#前置理解" class="headerlink" title="前置理解"></a>前置理解</h2><p>上一篇中，介绍了混淆矩阵和一些语义分割常用评价指标，例如P、R、IoU，这些在目标检测中都会用到，但是相对于语义分割来说，更加抽象一些。</p><p>目标检测对我个人来说是一个相对陌生的领域，所以本文准备先从目标检测的一般过程说起。首先，目标检测中的基本单元是框，我把Ground Truth称为<strong>真实框</strong>（标签），把Prediction称为<strong>预测框</strong>。框中的内容可能是<strong>目标</strong>（需要），也可能是<strong>背景</strong>（不需要）。</p><p>对于目标检测来说，模型需要完成的任务有两个，一是产生目标的预测框，二是对框内目标的类别进行预测，这又称为<strong>回归分支</strong>（连续）和<strong>分类分支</strong>（离散）。模型的预测输出通常如下所示（假设三分类，<strong>实际上不同模型的输出格式也不尽相同的</strong>。）：</p><script type="math/tex; mode=display">\mathrm{y}=\left[\begin{array}{l}\mathrm{p}_{\mathrm{c}} \\\mathrm{b}_{\mathrm{x}} \\\mathrm{b}_{\mathrm{y}} \\\mathrm{b}_{\mathrm{w}} \\\mathrm{b}_{\mathrm{h}} \\\mathrm{C}_1 \\\mathrm{C}_2 \\\mathrm{C}_3\end{array}\right], \mathrm{y}_{\text {true }}=\left[\begin{array}{c}1 \\40 \\45 \\80 \\60 \\0 \\1 \\0\end{array}\right], \mathrm{y}_{\text {pred }}=\left[\begin{array}{c}0.88 \\41 \\46 \\82 \\59 \\0.01 \\0.95 \\0.04\end{array}\right]</script><p>其中, $\mathrm{p}_{\mathrm{c}}$ 为预测结果的置信度（Confidence），表达预测框内包含目标的概率。$\mathrm{b}_{\mathrm{x}}, \mathrm{b}_{\mathrm{y}}, \mathrm{b}_{\mathrm{w}}, \mathrm{b}_{\mathrm{h}}$ 分别为预测框左上点$x$坐标和$y$坐标以及预测框的宽度、长度，也可以是预测框的左上、右下两点的$x$坐标和$y$坐标，表达的意思是相同的。 $\mathrm{C}_1, \mathrm{C}_2, \mathrm{C}_3$ 为目标属于某个类别的概率。</p><p>真实框和预测框的数量可能是不对等的，这是目标检测与语义分割的一大区别。现在我准备把这些内容再简化，归类为三个信息，<strong>目标置信度</strong>、<strong>定位信息</strong>和<strong>分类置信度</strong>，这里面有些事情需要讲清楚。</p><p>第一，<strong>分类置信度</strong>可能有互斥和不互斥两种，取决于是否使用了softmax计算，本文中，默认分类置信度是互斥的，也就是说当模型输出了一个预测框，框内“目标”的类别就固定了（不论是否真的存在目标）。</p><p>第二，预测框与真实框的贴合程度可以由<strong>定位信息</strong>计算IoU表示，上次提到了。</p><p>以上，前置理解完毕，可以开始进行模型评估了。</p><h2 id="重返混淆矩阵"><a href="#重返混淆矩阵" class="headerlink" title="重返混淆矩阵"></a>重返混淆矩阵</h2><p>从我们现有的信息来看，我们只有两个数值：<strong>目标置信度</strong>和<strong>定位信息</strong>。定位信息可以求IoU，但它依然是数值。基于mAP是目标检测中热门的评估指标的现状，我们希望能够继续用混淆矩阵来对预测框进行分类。众所周知，对数值分类的最简单方式就是在集合中画一条界限，将数值集分为两块，这条界限我们称为<strong>阈值</strong>。</p><p>我们有两个数值，自然产生了两个阈值，即<strong>置信度阈值</strong>和<strong>IoU阈值</strong>。显然，前者能把预测分成里面大概率是目标的框和里面大概率是背景的框，后者则把预测分成了很像某个目标的框和与某个目标不沾边的框。好了，现在我们来说混淆矩阵的事。</p><p>由于预测框类别已经固定，目标置信度和定位信息就是用来确定这框里面到底有没有目标，两个阈值用来判断一件事，这不就是二分类吗？没错，目标检测中我们还是用二分类，和之前多分类按照每个类别来看依然是二分类问题一样。</p><p>那现在事情就变得简单了，只需要搞懂如何区别正负就可以了，这是由目标置信度和定位信息共同决定的。下面给出方法，首先我们假设测试集有$N$张图片，我们从中取出一张图片$Img_n$，取该图片中属于类别$C_1$的目标的真实框和预测为类别$C_1$的预测框作为待分类集合。</p><ul><li>TP：与某一真实框的IoU值大于IoU阈值，且目标置信度大于置信度阈值的预测框数量。每个真实框仅能匹配一个预测框，这意味这TP的最大值为真实框的数量。</li><li>FN：漏检测的真实框数量。</li><li>FP：目标置信度大于置信度阈值但不满足TP条件的预测框数量。</li><li>TN：不考虑，因为Negative的样本想画可以画无数个，没有价值。</li></ul><p>初次看到这个判断方法多少还是有点懵，笔者解释一下，首先我们希望预测结果对每个目标至多只有一个预测框，这导致了即便有多个符合双阈值要求的预测框，对同一目标也只能<strong>选择一个置信度最高的预测框作为TP</strong>，其余重复预测框（虽然满足置信度要求）都进入FP。这样以后，能进入TP的目标也被确定了，剩下的真实框自然就成为了FN。</p><h2 id="准确率-召回率曲线（Precision-Recall-Curve）"><a href="#准确率-召回率曲线（Precision-Recall-Curve）" class="headerlink" title="准确率-召回率曲线（Precision-Recall Curve）"></a>准确率-召回率曲线（Precision-Recall Curve）</h2><p>先回顾一下Precision和Recall的计算方式，这里就只考虑Positive了：</p><script type="math/tex; mode=display">Precision = \frac{TP}{TP+FP} \\Recall = \frac{TP}{TP+FN}</script><p>由上面的分析可以看出，双阈值会影响P和R，通过调整阈值，就可以获得多个$(R,P)$对，通常我们通过指定IoU阈值（不小于0.5），调整置信度阈值的方式获得P-R曲线。事实上，精确率和召回率是一对由置信度阈值控制的冲突的变量，如果想要精准率提高，召回率则会下降，如果要召回率提高，精准率则会下降。当置信度阈值下降时，Recall单调上升，Precision总体呈下降趋势，这也很好理解，查的更全但是查的不准。</p><p><img src="/assets/post_img/article72/p-r.webp" alt="pr"></p><p>需要注意的是，在计算P-R曲线的过程中，置信度阈值的调整方式通常是对所有预测框按置信度降序排序，依次将置信度阈值设为某一预测框的置信度。并且<strong>P-R曲线的计算是以全测试集为域，按类别划分的</strong>，也就是说，计算类别$C_1$的P-R曲线时，预测框的基数为$C_1$在所有图片中的预测框数量之和，$100$个预测框可以计算$100$组$(R,P)$。另外，由于FP也需要满足置信度大于阈值的条件，<strong>参与整个P-R计算的预测框数量应该是持续增加的，最终达到$C_1$在所有图片中的预测框数量之和</strong>。（这里主要的意思就是粗体的地方，前面的原因可以有很多，主要就是需要按照前面定义的规则计算。）</p><p>更重要的是，<strong>TP、FP和FN却需要在单张图片的范围内按类别计算，再以图片为单位求和构成总TP、FP和FN值，进而求得类别在数据集上的一个$(R,P)$</strong>。</p><h2 id="AP（Average-Precision）"><a href="#AP（Average-Precision）" class="headerlink" title="AP（Average Precision）"></a>AP（Average Precision）</h2><p>经历了前面曲折艰难的分析，终于能够接近AP了。首先从字面上看，平均精确度，很容易联想到一种计算方式（笔者就这么干过）：对数据集中的每张图片单独计算Precision，然后求平均值，这不就是AP吗？实际上这是不对的，错误出在了对于Precision的理解上，Precision是一个<strong>Rate</strong>，我们现在的目的是评估模型<strong>对类别</strong>的检测效果，一个类别的查准率是建立在全数据集的样本之上的，显然，上述计算无法准确的表达这一内涵。</p><p>如果有读者想不明白这一点，我可以再举一个例子来验证，例如有三堆球摆在我们面前，其中两堆相同，都由$3$个白球和$1$个红球构成，另外一堆由$1$个红球和$1$个白球构成，我希望得到这些球中的红球率。如果按照上述理解，我们可以分别计算三堆中的红球率，得到$\frac{1}{4},\frac{1}{4},\frac{1}{2}$，再求它们的算数平均值得到$\frac{1}{3}$。而实际上呢，红球率是$\frac{3}{10}$，因为红球率是指以球为基本单位的红球比率（也不知道我解释清楚没）。</p><p>总之，Precision就是这样一个指标，要计算类别的AP，就要先获得P-R曲线。得到P-R曲线后，AP的计算方式就有很多了，根据不同的流行数据集，有几种常见的方式，罗列如下：</p><ul><li><p>按照VOC2007的方法，是先平滑曲线，对于每个点取其右边最大的P值，连成直线，然后等间距取11个点的最大P值，AP就是这11个Precision的平均值。</p></li><li><p>VOC2012，还是先按07的方法平滑曲线，然后计算PR曲线下面积作为AP值，因为本身P和R就是Rate，构成的正方形区域面积就是1，求曲线积分就完事了。</p></li><li><p>COCO数据集，设定多个IoU阈值（0.5至0.95，0.05为步长），在每一个IoU阈值下都有某一类别的AP值，然后求所有IOU阈值下的平均AP，以该平均AP值作为最终的某类别的AP值。</p></li></ul><p>总的来说，就是一个精度越来越高的过程。</p><h2 id="mAP（mean-Average-Precision）"><a href="#mAP（mean-Average-Precision）" class="headerlink" title="mAP（mean Average Precision）"></a>mAP（mean Average Precision）</h2><p>mAP和速度是最常用的目标检测模型评价指标，mAP顾名思义就是对所有分类的AP再求平均值。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]<a href="https://blog.csdn.net/yegeli/article/details/109861867">https://blog.csdn.net/yegeli/article/details/109861867</a><br>[2]<a href="https://zhuanlan.zhihu.com/p/70306015">https://zhuanlan.zhihu.com/p/70306015</a><br>[3]<a href="https://www.jianshu.com/p/86b8208f634f">https://www.jianshu.com/p/86b8208f634f</a><br>[4]<a href="https://zhuanlan.zhihu.com/p/94597205">https://zhuanlan.zhihu.com/p/94597205</a><br>[5]<a href="https://zhuanlan.zhihu.com/p/101566089">https://zhuanlan.zhihu.com/p/101566089</a><br>[6]<a href="https://www.jianshu.com/p/fd9b1e89f983">https://www.jianshu.com/p/fd9b1e89f983</a></p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于<a href="https://silencezheng.top/">silencezheng.top</a>。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;关于目标检测领域的常用评价指标进行一些个人解读，欢迎批评指正。&lt;/p&gt;
&lt;p&gt;评价指标系列第二篇～上一次说了语义分割的内容，这次来搞目标检测，看了十余篇内容后终于搞明白些了，记录一下。本文说的内容比较浅，一方面是笔者水平实在菜鸡，另一方面写文的主要目的是让不了解目标检测的人读完能上手去做，而不用查三十个资料后再开始，就像博客的Slogan一样，“帮你在学习技术的路上节约半小时”。&lt;br&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://silencezheng.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="计算机视觉" scheme="http://silencezheng.top/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>语义分割常用评价指标</title>
    <link href="http://silencezheng.top/2022/10/23/article71/"/>
    <id>http://silencezheng.top/2022/10/23/article71/</id>
    <published>2022-10-22T16:26:05.000Z</published>
    <updated>2022-10-22T16:33:20.039Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>关于语义分割领域的常用评价指标进行一些个人解读，欢迎批评指正。</p><p>本来是想连目标检测的一起说了的，但是关于AP的一些东西始终有疑问，留着后面搞懂了再写一篇吧。<br><span id="more"></span></p><h2 id="前置理解"><a href="#前置理解" class="headerlink" title="前置理解"></a>前置理解</h2><p>在解读这些评价指标之前，需要对深度学习方法有一个基础认识。</p><p>首先，评价指标，是指测试阶段将预测结果与真实值进行比对得到的量化结论。<strong>评价指标（Metric）和损失函数（Loss）有联系也有区别</strong>，目前在我个人理解来说，评价指标是以实用的角度评价模型，只关心模型的结果，而损失函数是从数学的角度收敛模型，但损失往往应该与一个你最关心的评价指标相对应，通过收敛损失能够达到向指标增大的方向靠拢。并且，损失应该是容易优化的，很多时候它们对模型参数可微，甚至是凸的。下面引[10]中的例子做说明。</p><blockquote><p>假设某同学备战高考，他给自己定下了一个奋斗的方向，即每周要把自己的各科总成绩提高5分；经过多年的准备，终于在高考中取得了好成绩（710分，总分750），被北大录取。<br>分析该例子，该同学“每周要把自己的各科总成绩提高5分”这个指导原则相当于目标函数，在这个指导原则的指引下，想必该同学的总分会越来越高，即模型被训练的越来越好。<br>最终，该同学高考成绩优异，相当于模型的测试效果良好，至于用从哪个角度评价这名同学，可以用其高考总分与750分的差距来衡量，也可以用其被录取的大学的水平来衡量，这就如同模型的评估指标是多种多样的，比如分类问题中的准确率、召回率等。<br>当然，模型的评估指标多样，模型的损失函数也是多样的；该例中，该同学可以将“每周要把自己的各科总成绩提高5分”作为指导原则，也可将“每周比之前多学2个知识点”作为指导原则。<br>另外，如果该同学将“每周模拟高考总分与750分的差距”同时作为指导原则与评价角度，则类似于线性回归模型将“MSE均方误差”同时作为损失函数与评估指标。<br>该例中，备考的“指导原则”相当于“损失函数”，“评价角度”相当于“评估指标”，该同学相当于一个机器学习模型。</p></blockquote><p>其次，在多分类任务中，通常包含$n$个类别，而对于某一样本的最终预测只能是$n$个类别中的一个。但是，算法对一个样本的类别预测通常以置信度的形式表示，最终选择置信度最高的类别作为预测输出。</p><p>最后，多分类任务对于每个类来看，可以看作是一个二分类问题，以样本对于该类别预测是否正确作为区分。</p><h2 id="混淆矩阵（Confusion-Matrix）"><a href="#混淆矩阵（Confusion-Matrix）" class="headerlink" title="混淆矩阵（Confusion Matrix）"></a>混淆矩阵（Confusion Matrix）</h2><p>混淆矩阵用于直观的显示模型预测结果的情形。 混淆矩阵中的横纵轴都是类别，对于$p_{ij}$（横坐标为$i$,纵坐标为$j$处的值），其含义为属于类别$i$并被预测为类别$j$的<strong>样本数量</strong>（在语义分割中通常样本等同于像素）。也就是说，每个位置的<strong>横坐标表示模型的预测，纵坐标表示真实标签</strong>。</p><p>对于二分类问题，混淆矩阵可以表示如下：</p><p><img src="/assets/post_img/article71/confusion-matrix-2classes.jpeg" alt="cm2"></p><p>若令其中$1$表示正类，$0$表示负类，则可以定义如下四个量：</p><ul><li>TP(True Positive)：将正样本预测为正类的数量，即图中的$a$。</li><li>FN(False Negative)：将正样本预测为负类的数量，即图中的$b$。</li><li>FP(False Positive)：将负样本预测为正类的数量，即图中的$c$。</li><li>TN(True Negative)：将负样本预测为负类的数量，即图中的$d$。</li></ul><p>对于多分类问题，只是把该矩阵由$2 \times 2$变化为$n \times n$，其中$n$表示类别数量。</p><p>从混淆矩阵中我们可以获得一些基础信息，如：</p><ul><li>$i$行的和$\sum^n_{j=1}p_{ij}$表示数据集中属于类别$i$的样本个数</li><li>$j$列的和$\sum^n_{i=1}p_{ij}$表示模型预测中属于类别$j$的样本个数</li><li>矩阵中所有元素的和$\sum^n_{i=1}\sum^n_{j=1}p_{ij}$表示图像中的总样本个数</li><li>…</li></ul><h2 id="精确率（Precision）和召回率（Recall）"><a href="#精确率（Precision）和召回率（Recall）" class="headerlink" title="精确率（Precision）和召回率（Recall）"></a>精确率（Precision）和召回率（Recall）</h2><p>这两个指标都是<strong>针对某一类别</strong>而言的，是分类任务的常用评价指标。</p><p>精确率又称查准率，含义是对于模型预测中属于类别$j$的样本，预测结果正确的比例。例如对于二分类问题，正类的精确率$Precision_{positive} = \frac{TP}{TP+FP} = \frac{a}{a+c}$。</p><p>召回率又称查全率，如果说精准率是站在预测的角度看问题，那么召回率就是站在现实的角度看问题，其含义是对于数据集中属于类别$i$的样本，被正确预测的比例。例如对于二分类问题，负类的召回率$Recall_{negative} = \frac{TN}{FP+TN} = \frac{d}{c+d}$。</p><h2 id="准确率（Accuracy）"><a href="#准确率（Accuracy）" class="headerlink" title="准确率（Accuracy）"></a>准确率（Accuracy）</h2><p>准确率需要和精确率区别开，准确率是站在预测的整体角度看问题，其含义是预测正确的样本占所有样本的比例。例如对于二分类问题，预测的准确率$Accuracy_{predict} = \frac{TP+TN}{TP+FN+FP+TN} = \frac{a+d}{a+b+c+d}$。可以看出，准确率其实就是混淆矩阵对角线元素和与所有元素和的比值。</p><h2 id="F1指标（F1-Score）和F-Beta指标（F-Beta-Score）"><a href="#F1指标（F1-Score）和F-Beta指标（F-Beta-Score）" class="headerlink" title="F1指标（F1 Score）和F-Beta指标（F-Beta Score）"></a>F1指标（F1 Score）和F-Beta指标（F-Beta Score）</h2><p>单独用精确率或召回率有时不能很好的评估模型，例如在二分类问题中，模型选择对所有样本预测为正类，此时所有正类样本都被“准确”的预测了，正类召回率为$1$，但模型实际上很差。</p><p>F1指标就是用来平衡精确率和召回率的重要程度的度量指标，它被定义为两者的<strong>调和平均值</strong>，表示二者重要程度一致。F1指标的计算公式如下：</p><script type="math/tex; mode=display">F_1 = 2 \times \frac{precision \cdot recall}{precision + recall}</script><p>调和平均值的一个重要特性就是如果两者极度不平衡，调和平均值会很小，只有当两者都较高时，调和平均才会比较高。</p><p>而F-Beta指标则是更一般的形式，他的计算方式如下：</p><script type="math/tex; mode=display">F_{\beta} = (1+\beta^2) \cdot \frac{precision \cdot recall}{(\beta^2 \cdot precision) + recall}</script><p>其中参数$\beta$决定了精确率和召回率的重要程度比值，当$\beta&gt;1$时召回率比重更大，当$\beta&lt;1$时精确率比重更大。</p><h2 id="特异性（Specificity）和敏感性（Sensitivity）"><a href="#特异性（Specificity）和敏感性（Sensitivity）" class="headerlink" title="特异性（Specificity）和敏感性（Sensitivity）"></a>特异性（Specificity）和敏感性（Sensitivity）</h2><p>关于这两个指标，似乎是仅针对于二分类问题而言的，这里只谈一些个人理解。</p><p>还是参照二分类的混淆矩阵，特异性实际上指的就是负类的召回率$Recall_{negative}$，而敏感性则指的是正类的召回率$Recall_{positive}$，看了网上许多解释，都是聚焦在医疗领域，把患病作为正类，健康作为负类，说什么敏感性越高，漏诊概率越低；特异性越高，确诊概率越高。</p><p>个人理解，实际上就是召回率在特定情况下的应用吧。</p><h2 id="交并比（Intersection-over-Union，IoU）和平均交并比（mIoU）"><a href="#交并比（Intersection-over-Union，IoU）和平均交并比（mIoU）" class="headerlink" title="交并比（Intersection over Union，IoU）和平均交并比（mIoU）"></a>交并比（Intersection over Union，IoU）和平均交并比（mIoU）</h2><p>给定两个区域$A$和$B$，IoU就是两区域的交集与两区域并集的比值：</p><script type="math/tex; mode=display">IoU = \frac{A \cap B}{A \cup B}</script><p><img src="/assets/post_img/article71/IoU.png" alt="iou"></p><p>在分类任务中，可以对某一类别的预测结果和真实标签求IoU，例如对于二分类求正类的IoU如下：</p><script type="math/tex; mode=display">IoU_{positive} = \frac{TP}{TP+FP+FN} = \frac{a}{a+b+c}</script><p>也就是说，混淆矩阵中$i$行和$i$列的交集比上它们的并集。</p><p><strong>平均交并比</strong>（mean IoU）就是对每一个类别求IoU，再求和求平均得到的值。</p><p>对于目标检测，IoU还有一个重要的应用，就是判断预测框与真实框的贴合程度，两部分重合面积越大，则IoU值越大。IoU是一个比较严格的评价指标，当两区域稍微有偏差时，IoU值也可能变得相当小，于是通常认为IoU大于$0.5$时就获得了一个比较不错的预测框。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]<a href="https://zhuanlan.zhihu.com/p/111234566">https://zhuanlan.zhihu.com/p/111234566</a><br>[2]<a href="https://zhuanlan.zhihu.com/p/101566089">https://zhuanlan.zhihu.com/p/101566089</a><br>[3]<a href="https://blog.csdn.net/h1yupyp/article/details/80842172">https://blog.csdn.net/h1yupyp/article/details/80842172</a><br>[4]<a href="https://blog.csdn.net/lhxez6868/article/details/108150777">https://blog.csdn.net/lhxez6868/article/details/108150777</a><br>[5]<a href="https://zhuanlan.zhihu.com/p/371819054">https://zhuanlan.zhihu.com/p/371819054</a><br>[6]<a href="https://www.jianshu.com/p/22d947ffb71e">https://www.jianshu.com/p/22d947ffb71e</a><br>[7]<a href="https://zhuanlan.zhihu.com/p/372402161">https://zhuanlan.zhihu.com/p/372402161</a><br>[8]<a href="https://zhuanlan.zhihu.com/p/373658488">https://zhuanlan.zhihu.com/p/373658488</a><br>[9]<a href="https://zhuanlan.zhihu.com/p/373032887">https://zhuanlan.zhihu.com/p/373032887</a><br>[10]<a href="https://www.cnblogs.com/pythonfl/p/13705143.html">https://www.cnblogs.com/pythonfl/p/13705143.html</a></p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于<a href="https://silencezheng.top/">silencezheng.top</a>。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;关于语义分割领域的常用评价指标进行一些个人解读，欢迎批评指正。&lt;/p&gt;
&lt;p&gt;本来是想连目标检测的一起说了的，但是关于AP的一些东西始终有疑问，留着后面搞懂了再写一篇吧。&lt;br&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="http://silencezheng.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="计算机视觉" scheme="http://silencezheng.top/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>M1 Mac weasyprint安装使用</title>
    <link href="http://silencezheng.top/2022/10/19/article70/"/>
    <id>http://silencezheng.top/2022/10/19/article70/</id>
    <published>2022-10-19T15:54:48.000Z</published>
    <updated>2022-10-19T15:57:18.582Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>为了把HTML转成PDF（带很多css的），折腾了一晚上。试了一下pdfkit是基本没戏（<a href="https://pypi.org/project/pdfkit/">pdfkit css bug</a>），转头想用weasyprint，没想到适配更差，记录一下。<br><span id="more"></span></p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p><code>brew install cairo pango gdk-pixbuf libffi</code></p><p><code>pip install weasyprint</code></p><h2 id="在conda环境下使用weasyprint"><a href="#在conda环境下使用weasyprint" class="headerlink" title="在conda环境下使用weasyprint"></a>在conda环境下使用weasyprint</h2><p>本来如果在brew的python3下使用应该没什么问题，但是如果要用conda环境的解释器就会报错：<code>OSError: cannot load library &#39;gobject-2.0-0&#39;</code></p><p>解决方案如下：<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo ln -s <span class="regexp">/opt/</span>homebrew<span class="regexp">/opt/g</span>lib<span class="regexp">/lib/</span>libgobject-<span class="number">2.0</span>.<span class="number">0</span>.dylib <span class="regexp">/usr/</span>local<span class="regexp">/lib/g</span>object-<span class="number">2.0</span></span><br><span class="line">sudo ln -s <span class="regexp">/opt/</span>homebrew<span class="regexp">/opt/</span>pango<span class="regexp">/lib/</span>libpango-<span class="number">1.0</span>.dylib <span class="regexp">/usr/</span>local<span class="regexp">/lib/</span>pango-<span class="number">1.0</span></span><br><span class="line">sudo ln -s <span class="regexp">/opt/</span>homebrew<span class="regexp">/opt/</span>harfbuzz<span class="regexp">/lib/</span>libharfbuzz.dylib <span class="regexp">/usr/</span>local<span class="regexp">/lib/</span>harfbuzz</span><br><span class="line">sudo ln -s <span class="regexp">/opt/</span>homebrew<span class="regexp">/opt/</span>fontconfig<span class="regexp">/lib/</span>libfontconfig.<span class="number">1</span>.dylib <span class="regexp">/usr/</span>local<span class="regexp">/lib/</span>fontconfig-<span class="number">1</span></span><br><span class="line">sudo ln -s <span class="regexp">/opt/</span>homebrew<span class="regexp">/opt/</span>pango<span class="regexp">/lib/</span>libpangoft2-<span class="number">1.0</span>.dylib <span class="regexp">/usr/</span>local<span class="regexp">/lib/</span>pangoft2-<span class="number">1.0</span></span><br></pre></td></tr></table></figure></p><p>创建对应位置的软链接，<a href="https://github.com/Kozea/WeasyPrint/issues/1448">issue在这</a>。</p><p>这样以后在终端用是没什么问题了，<code>weasyprint url xx.pdf</code>，中文支持不佳，css部分支持不好。</p><h2 id="仍然报错"><a href="#仍然报错" class="headerlink" title="仍然报错"></a>仍然报错</h2><p>在正常Python调用中仍然会报错，<code>RuntimeError: cannot use unpack() on &lt;cdata &#39;char *&#39; NULL&gt;</code>，定位到<code>cffi/api.py</code>，一个空指针，暂时不知道怎么解决。</p><p>CFFI(C Foreign Function Interface) 是Python的C语言外部函数接口。通过CFFI，Python可以与几乎任何C语言代码进行交互。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;为了把HTML转成PDF（带很多css的），折腾了一晚上。试了一下pdfkit是基本没戏（&lt;a href=&quot;https://pypi.org/project/pdfkit/&quot;&gt;pdfkit css bug&lt;/a&gt;），转头想用weasyprint，没想到适配更差，记录一下。&lt;br&gt;</summary>
    
    
    
    
    <category term="MacOS" scheme="http://silencezheng.top/tags/MacOS/"/>
    
  </entry>
  
  <entry>
    <title>动态规划</title>
    <link href="http://silencezheng.top/2022/10/11/article69/"/>
    <id>http://silencezheng.top/2022/10/11/article69/</id>
    <published>2022-10-11T10:15:29.000Z</published>
    <updated>2023-01-03T14:09:35.029Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Dynamic Programming学习，由于动态规划问题很多，且没有固定套路，本文会随时进行增补、修改。</p><p>另外，笔者对于DP的理解很可能是错误的，希望大家多多指教。<br><span id="more"></span></p><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>动态规划是研究多步决策过程<strong>最优化</strong>问题的一种数学方法。在动态规划中，为了寻找一个问题的最优解（即最优决策过程），将整个问题划分成若干个相应的阶段，并在每个阶段都根据先前所作出的决策作出当前阶段最优决策，进而得出整个问题的最优解。即<em>记住已知问题的答案，在已知的答案的基础上解决未知的问题。</em></p><h2 id="能解决的问题"><a href="#能解决的问题" class="headerlink" title="能解决的问题"></a>能解决的问题</h2><p>1、计数问题<br>例如“有多少种方式使得…”，或者“有多少种方法选出…”。</p><p>2、最值问题<br>例如经典的“最少用多少枚硬币能组合出目标面值”，或者“求最长上升子序列”。</p><p>3、存在性问题<br>例如“能不能选出k个数使得…”，或者“先手方是否必胜”。</p><p>注意，通常求所有解法的问题不能用DP方法来做，因为与最优解无关的解在动态规划中不会被全部计算。</p><h2 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h2><p>在上述问题中，可能会有许多可行解。每一个解都对应于一个值，我们希望找到具有最优值的解。动态规划算法的基本思想是将待求解问题分解成若干个子问题，先求解子问题，然后从这些子问题的解得到原问题的解。与分治法不同的是，适合于用动态规划求解的问题，经分解得到<strong>子问题往往不是互相独立的</strong>。如果能够保存已解决的子问题的答案，而在需要时再找出已求得的答案，这样就可以避免大量的重复计算，节省时间。</p><p>这种记住子问题的做法很容易联想到记忆化，但 <strong>动态规划</strong> 和 <strong>记忆化搜索</strong> 的一个重要区别是动态规划通常不使用递归实现。 另外在计算顺序方面，多数动态规划问题是自底向上的（这与状态转移有关），而记忆化搜索是自顶向下的。</p><p>通常可以用一个表来记录所有已解的子问题的答案。不管该子问题以后是否被用到，只要它被计算过，就将其最优解填入表中，方便在求解之后的子问题时可以方便调用，进而求出整个问题的最优解。这就是动态规划法的基本思路。具体的动态规划算法多种多样，但它们具有相同的填表格式。</p><p>也可以说，动态规划最核心的思想，就在于<strong>拆分子问题，记住过往，减少重复计算</strong>。</p><h2 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h2><p>通常通过动态规划求解问题由四个主要部分组成：<br>1、拆分子问题<br>2、确定状态转移方程<br>3、确定初状态和边界条件<br>4、确定计算顺序</p><p>这四个部分看起来简单易懂，但是在实践过程中却会遇到重重困难。下面分别详细的解释一下。</p><p>首先需要明确动态规划解决的是一个多步决策问题，该问题由初状态、若干中间决策和末状态组成。</p><h3 id="1、拆分子问题"><a href="#1、拆分子问题" class="headerlink" title="1、拆分子问题"></a>1、拆分子问题</h3><p>拆分子问题，即将原问题拆解成为<strong>范围更小但性质不变的子问题</strong>。</p><p>一般首先要做的是找出“最后一步”，也就是若干中间决策的最后一步，经历该决策后，问题转变到末状态。</p><p>找到“最后一步”后，我们研究“最后一步”前的状态，它应该能够构成一个子问题，i.e.，如果说“最后一步”使问题达到末状态，且这一系列决策构成问题的最优解，那么去掉“最后一步”的决策链应该是最优解的一个子集，同时“最后一步”前的状态构成一个子问题，该子集为该子问题的最优解。</p><p>这样一来，我们就从原问题中拆出了一个子问题。<strong>拆分子问题的目的是找出状态</strong>，状态是对问题各阶段的客观表述，同时需要满足<em>无后效性</em>，即当前状态之后的决策对该状态无影响。</p><h3 id="2、确定状态转移方程"><a href="#2、确定状态转移方程" class="headerlink" title="2、确定状态转移方程"></a>2、确定状态转移方程</h3><p>状态转移方程是动态规划的重中之重，一旦确定了状态转移方程，问题通常也就迎刃而解了。</p><p><strong>状态转移就是根据上一阶段的状态和之后的一次决策来导出本阶段的状态</strong>。所以如果确定了决策，状态转移方程也就可写出。 但事实上常常是反过来做，根据相邻两个阶段的状态之间的关系来确定决策方法和状态转移方程。 状态转移方程通常是一个递推公式。</p><p>同时，状态转移方程也影响着最终算法的计算顺序，通常靠后的状态会需求前方状态的信息，原问题为求到最后状态的最优解，因此动态规划问题通常都是自底向上计算的。</p><p>我们用一个数组或哈希表来记录状态的最优解，本文暂时称其为<strong>状态表</strong>。</p><h3 id="3、确定初状态和边界情况"><a href="#3、确定初状态和边界情况" class="headerlink" title="3、确定初状态和边界情况"></a>3、确定初状态和边界情况</h3><p>初状态就是问题的初始状态。 确定初状态主要是确定状态表如何初始化。</p><p>边界情况也称边界条件，即状态转移方程的边界，有时在递推公式的某一部分会出现不合理的或不属于问题范围内的项，需要通过边界条件来限制它。</p><h3 id="4、确定计算顺序"><a href="#4、确定计算顺序" class="headerlink" title="4、确定计算顺序"></a>4、确定计算顺序</h3><p>如上面所说的，根据状态转移公式来确定。</p><h2 id="案例一：摩天大楼"><a href="#案例一：摩天大楼" class="headerlink" title="案例一：摩天大楼"></a>案例一：摩天大楼</h2><p>题目如下：</p><p><img src="/assets/post_img/article69/question.jpeg" alt="skyscraper"></p><p>拿到题目我们首先考虑一下暴力法，从暴力法的实现中可以看出有没有优化的余地，下面是我手写的一个过程，以输入$[2, 5, 1, 4, 8]$为例。</p><p><img src="/assets/post_img/article69/brutal-force.png" alt="bf"></p><p>可以发现有很多重复计算，可以通过记忆化进行优化。 暴力法代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># author：SilenceZheng66</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">brutal_force</span>(<span class="params">L: <span class="built_in">list</span>[<span class="built_in">int</span>]</span>):</span></span><br><span class="line">    n = <span class="built_in">len</span>(L)</span><br><span class="line"></span><br><span class="line">    results = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> n &lt;= <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">to_top</span>(<span class="params">pos, times</span>):</span></span><br><span class="line">        <span class="keyword">if</span> pos == n - <span class="number">1</span>:</span><br><span class="line">            results.append(times)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(pos + <span class="number">1</span>, pos + L[pos] + <span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> j &lt; n:</span><br><span class="line">                    to_top(j, times + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> L[<span class="number">0</span>] &gt;= n - <span class="number">1</span>:</span><br><span class="line">        results.append(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, L[<span class="number">0</span>] + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> i &lt; n:</span><br><span class="line">                to_top(i, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">min</span>(results)</span><br></pre></td></tr></table></figure><p>下面我们开始用动态规划的解题步骤尝试去优化我们的算法。</p><h3 id="1、拆分子问题-1"><a href="#1、拆分子问题-1" class="headerlink" title="1、拆分子问题"></a>1、拆分子问题</h3><p>原问题是：找出从底层到楼顶的最少乘电梯数。我们以数组的下标$pos$来进行说明，底层的$pos = 0$，楼顶的$pos = n-1$。</p><p>假设“最后一步”前的状态为 $pos = m$ ，那么“最后一步”就是 $n-1$ 位于 $m + 1$ 和 $m + L[m]$ 之内，也可以表示为 $m + L[m] \geq n-1$。 此时原问题就被拆分成了子问题：找出从底层到$m+1$层的最少乘电梯数。 原问题的答案为该子问题答案$+1$。 </p><p>那么此时我们就可以描述状态了，$f(x) = 到达x层的最少乘电梯数$。</p><h3 id="2、确定状态转移方程-1"><a href="#2、确定状态转移方程-1" class="headerlink" title="2、确定状态转移方程"></a>2、确定状态转移方程</h3><p>第一步中我们拆分了子问题，确定了状态$f(x)$，则原问题可以描述为状态表中的最后一个状态，即$f(n-1)$。 根据“最后一步”中的推断，最理想的状态是能直接找到$f(n-1)$的前一个最优状态$f(m)$，然后再找到$f(m)$的前一最优个状态…直到正好找到$f(0)$。但是这是不可能实现的。</p><p>回到现实，还是考虑输入$[2, 5, 1, 4, 8]$，很自然的能够得出$f(4) = min(f(3), f(1)) + 1$，那么问题来了，为什么不是$min(f(3),f(2),f(1),f(0))$？很明显，我们需要考虑电梯的上升能力，即$5$层前有哪些层是能直接到达$5$层的？这些能直接到达的层里，哪些层的$f(x)$最小？找出他们，再加上$1$，就得到了$f(4)$。</p><p>基于以上分析，我们可以得出状态转移方程：</p><script type="math/tex; mode=display">f(x) = min(g(f(x-1), ... ,f(0))) + 1</script><p>其中$g(f(x-1), … ,f(0))$表示从$pos=0$到$pos=x-1$中选出那些满足$L[pos] + pos \geq x $的$f(pos)$。</p><p>得出了状态转移方程，这道题也就拿下来一多半了。</p><h3 id="3、确定初状态和边界情况-1"><a href="#3、确定初状态和边界情况-1" class="headerlink" title="3、确定初状态和边界情况"></a>3、确定初状态和边界情况</h3><p>初状态是好确定的，因为要做求最小运算，所以状态表初始化时所有元素应为系统能取到的最大正整数值。而$f(0)$应该被置为$0$，因为到达ground floor的最少乘电梯次数是$0$次。</p><p>关于边界情况，可以看到状态转移方程中有涉及到$L[pos] + pos$的运算，这可能会超出数组上界，故需要考虑进去。 同时如果大楼的层数$\leq 1$，则可以直接得出$f(n-1) = f(0) = 0$，也可以考虑进去。</p><h3 id="4、确定计算顺序-1"><a href="#4、确定计算顺序-1" class="headerlink" title="4、确定计算顺序"></a>4、确定计算顺序</h3><p>从状态转移公式可以看出，靠后的状态依赖前方状态信息，故应为自底向上。</p><p>最后就是写代码了，这里提供一个我写的示范。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># author：SilenceZheng66</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dp</span>(<span class="params">L: <span class="built_in">list</span>[<span class="built_in">int</span>]</span>):</span></span><br><span class="line">    n = <span class="built_in">len</span>(L)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 状态数组</span></span><br><span class="line">    conditions = [sys.maxsize <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始状态</span></span><br><span class="line">    conditions[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 边界情况</span></span><br><span class="line">    <span class="keyword">if</span> n &lt;= <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 自底向上</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n - <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 边界情况判断</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i + <span class="number">1</span>, i + L[i] + <span class="number">1</span>) <span class="keyword">if</span> i + L[i] + <span class="number">1</span> &lt;= n <span class="keyword">else</span> <span class="built_in">range</span>(i + <span class="number">1</span>, n):</span><br><span class="line">            conditions[j] = <span class="built_in">min</span>(conditions[i] + <span class="number">1</span>, conditions[j])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> conditions[-<span class="number">1</span>]</span><br></pre></td></tr></table></figure></p><h2 id="案例二：比特位计数"><a href="#案例二：比特位计数" class="headerlink" title="案例二：比特位计数"></a>案例二：比特位计数</h2><p>Leetcode的一道简单题，但有三种DP官解，很适合练习思路。</p><p>题目：给你一个整数 n ，对于 0 &lt;= i &lt;= n 中的每个 i ，计算其二进制表示中 1 的个数 ，返回一个长度为 n + 1 的数组 ans 作为答案。</p><p>先考虑暴力法，外层循环肯定是必要的，对1计数的话，我的想法是从最低位一直与一做与操作，然后逻辑右移，这样内层就是固定32次，外层n次：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: SilenceZheng66</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span>[] countBits(<span class="keyword">int</span> n) &#123;</span><br><span class="line">        <span class="keyword">int</span>[] ans = <span class="keyword">new</span> <span class="keyword">int</span>[n+<span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;=n;i++)&#123;</span><br><span class="line">            <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">int</span> tmp = i;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;<span class="number">32</span>;j++)&#123;</span><br><span class="line">                <span class="keyword">if</span>((tmp&amp;<span class="number">1</span>)==<span class="number">1</span>) count++;</span><br><span class="line">                tmp&gt;&gt;&gt;=<span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            ans[i] = count;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>那么这种方式有没有重复计算的地方呢？不妨用数字10举例，对于数字10（<code>1010</code>），在计算一次最低位后，剩余部分是之前已经计算过的数字5（<code>0101</code>）。那么可以采用记忆化方法，这样一来能提高不少效率：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: SilenceZheng66</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span>[] countBits(<span class="keyword">int</span> n) &#123;</span><br><span class="line">        <span class="keyword">int</span>[] ans = <span class="keyword">new</span> <span class="keyword">int</span>[n+<span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;=n;i++)&#123;</span><br><span class="line">            <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">int</span> tmp = i;</span><br><span class="line">            ans[i] = -<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;<span class="number">32</span>;j++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(ans[tmp]!=-<span class="number">1</span>)&#123;</span><br><span class="line">                    count+=ans[tmp];</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">if</span>((tmp&amp;<span class="number">1</span>)==<span class="number">1</span>) count++;</span><br><span class="line">                tmp&gt;&gt;&gt;=<span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            ans[i] = count;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>好了，以上是我对这道题的思路，但是对于以上解法，我没有想出如何使用DP来优化，下面是官解思路。</p><p>官解首先提出了另一个计算1bit数量的算法，即对于任意整数 x，令 <code>x = x&amp;(x-1)</code>，该运算将 x 的二进制表示的最后一个 1 变成 0。因此，对 x 重复该操作，直到 x 变成 0，则操作次数即为 x 的「一比特数」。 该方法称为Brian Kernighan 算法。</p><p>利用这个计数法写出暴力法不是什么难事：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: LeetCode-Solution</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span>[] countBits(<span class="keyword">int</span> n) &#123;</span><br><span class="line">        <span class="keyword">int</span>[] bits = <span class="keyword">new</span> <span class="keyword">int</span>[n + <span class="number">1</span>];</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt;= n; i++) &#123;</span><br><span class="line">            bits[i] = countOnes(i);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> bits;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">countOnes</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> ones = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (x &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            x &amp;= (x - <span class="number">1</span>);</span><br><span class="line">            ones++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ones;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>下面开始用动态规划的解题步骤尝试去优化此算法。</p><h3 id="1、拆分子问题-2"><a href="#1、拆分子问题-2" class="headerlink" title="1、拆分子问题"></a>1、拆分子问题</h3><p>首先需要明确该案例与案例一的区别，案例一为“最值问题”，显然可使用动态规划求解。而该案例并不明显的属于前文所提到的三类问题中的某一类，为什么可以由采用DP求解呢？我个人的理解是，题目要求返回的序列中后方的信息可以依赖前方的信息求出，这暗中符合了状态转移的性质，如若独立对每一项进行计算，则会出现计算冗余，此时采用DP是合适的。</p><p>在我的解法中，已经体现了一部分借助前方信息的思想，但并没有整理出一个状态转移方程，仅仅是简单的记忆化。现在，基于Brian Kernighan算法，我们来寻找一种使用动态规划解决问题的方法。</p><p>对于该问题，可以很明显的找到“最后一步”，即求n的二进制表示中的1bit数，我们将其记为$bits[n]$那么去除掉最后一步后的子问题就变为了“求$0$到$n-1$的$1$比特位计数序列”。原问题的答案为该子问题的答案$bits[0…n-1]$+$bits[n]$。</p><p>那么此时我们可以描述状态为$f(x) = 在求出0到x-1序列的基础上，求x的二进制表示中的1比特数量$。</p><h3 id="2、确定状态转移方程-2"><a href="#2、确定状态转移方程-2" class="headerlink" title="2、确定状态转移方程"></a>2、确定状态转移方程</h3><p>状态转移方程是DP的关键，这道题有很多状态转移的思路，这里介绍从最高有效位入手的思路。</p><p>对于正整数$x$，若可以知道最大正整数$y$，使得$y\leq x$且$y$是$2$的整数次幂，则$y$的二进制表示中只有最高位是$1$，其余为$0$。此时将$y$称为$x$的<strong>最高有效位</strong>。则显然存在$z = x - y, 0 \leq z \le x$，有$bits[x] = bits[z] + 1$。根据上述推导，我们可以发现$y, z$都落在$bits[0…x]$中，我们只需要找到对$x$求$y$的方法即可，但这个方法一定要高效，在$O(1)$里计算。</p><p>可以利用<code>y&amp;(y-1)==0</code>来判断，因为$y$中仅含有一个最高位$1$，也就是说正整数$y$是$2$的整数次幂，当且仅当<code>y&amp;(y-1)==0</code>。由于我们自底向上进行求解，数字$x$的最高有效位逐步递增，我们可以在遍历的过程中更新$y$。</p><p>至此，我们可以写出状态转移方程：</p><script type="math/tex; mode=display">f(x) = f(x - g(0, ..., x)) + 1, 其中函数g表示在0到x中寻找x的最高有效位。</script><h3 id="3、确定初状态和边界情况-2"><a href="#3、确定初状态和边界情况-2" class="headerlink" title="3、确定初状态和边界情况"></a>3、确定初状态和边界情况</h3><p>初状态自然就是$f(0)$了，此时$bits[0]=0$。</p><p>边界的话，不存在越上界的情况，下界保证初状态不越界即可。</p><h3 id="4、确定计算顺序-2"><a href="#4、确定计算顺序-2" class="headerlink" title="4、确定计算顺序"></a>4、确定计算顺序</h3><p>计算顺序自底向上，最终代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: LeetCode-Solution</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span>[] countBits(<span class="keyword">int</span> n) &#123;</span><br><span class="line">        <span class="keyword">int</span>[] bits = <span class="keyword">new</span> <span class="keyword">int</span>[n + <span class="number">1</span>];</span><br><span class="line">        <span class="keyword">int</span> highBit = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> ((i &amp; (i - <span class="number">1</span>)) == <span class="number">0</span>) &#123;</span><br><span class="line">                highBit = i;</span><br><span class="line">            &#125;</span><br><span class="line">            bits[i] = bits[i - highBit] + <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> bits;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="5、其他思路"><a href="#5、其他思路" class="headerlink" title="5、其他思路"></a>5、其他思路</h3><p>除了从最高有效位出发以外，从我原本的思路出发也可以转化到DP解法，即最低有效位方式。</p><p>其核心思想是$bits[x]$的值等于$\textit{bits}\big[\lfloor \frac{x}{2} \rfloor\big]$的值加上$x$除以$2$的余数。用代码表示就是<code>bits[x] = bits[x&gt;&gt;1] + (x&amp;1)</code>。代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// author: LeetCode-Solution</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span>[] countBits(<span class="keyword">int</span> n) &#123;</span><br><span class="line">        <span class="keyword">int</span>[] bits = <span class="keyword">new</span> <span class="keyword">int</span>[n + <span class="number">1</span>];</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++) &#123;</span><br><span class="line">            bits[i] = bits[i &gt;&gt; <span class="number">1</span>] + (i &amp; <span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> bits;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>首发于 <a href="https://silencezheng.top">silencezheng.top</a>，转载请注明出处。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;Dynamic Programming学习，由于动态规划问题很多，且没有固定套路，本文会随时进行增补、修改。&lt;/p&gt;
&lt;p&gt;另外，笔者对于DP的理解很可能是错误的，希望大家多多指教。&lt;br&gt;</summary>
    
    
    
    
    <category term="数据结构与算法" scheme="http://silencezheng.top/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
</feed>
